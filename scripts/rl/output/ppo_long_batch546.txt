CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
2024-06-08 23:48:55.428924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-08 23:48:55.534775: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-06-08 23:48:56.260920: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-08 23:48:56.260977: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-08 23:48:56.260985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
CUDA available: True
net architecture - {'net_arch': {'pi': [128, 128, 128, 128], 'vf': [128, 128, 128, 128]}}
params: 
NUM_TIMESTEPS -50000000
EVAL_FREQ=40961
EVAL_EPISODES=400
BEST_THRESHOLD=0.2
LOGDIR=scripts/rl/output/v3/
model params: 
 {'learning_rate': 0.0001, 'n_steps': 40960, 'n_epochs': 10, 'clip_range': 0.15, 'batch_size': 128, 'ent_coef': 0.01, 'gamma': 0.99, 'verbose': 1}
Using cuda device
Wrapping the env in a DummyVecEnv.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 412      |
|    iterations      | 1        |
|    time_elapsed    | 99       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=40961, episode_reward=0.10 +/- 0.97
Episode length: 30.02 +/- 0.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 40961        |
| train/                  |              |
|    approx_kl            | 0.0072453646 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.15         |
|    entropy_loss         | -1.99        |
|    explained_variance   | -0.605       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00925      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0189      |
|    value_loss           | 0.171        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 350      |
|    iterations      | 2        |
|    time_elapsed    | 233      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=81922, episode_reward=0.15 +/- 0.97
Episode length: 29.94 +/- 1.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.155       |
| time/                   |             |
|    total_timesteps      | 81922       |
| train/                  |             |
|    approx_kl            | 0.006649743 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.182       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0311      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.181       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 334      |
|    iterations      | 3        |
|    time_elapsed    | 367      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=122883, episode_reward=0.22 +/- 0.96
Episode length: 30.03 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.217       |
| time/                   |             |
|    total_timesteps      | 122883      |
| train/                  |             |
|    approx_kl            | 0.006862423 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.205       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0235      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.185       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2175
SELFPLAY: new best model, bumping up generation to 1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 327      |
|    iterations      | 4        |
|    time_elapsed    | 500      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=163844, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.0575    |
| time/                   |           |
|    total_timesteps      | 163844    |
| train/                  |           |
|    approx_kl            | 0.0070408 |
|    clip_fraction        | 0.137     |
|    clip_range           | 0.15      |
|    entropy_loss         | -1.96     |
|    explained_variance   | 0.216     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0227    |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0256   |
|    value_loss           | 0.193     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 323      |
|    iterations      | 5        |
|    time_elapsed    | 633      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=204805, episode_reward=0.06 +/- 0.98
Episode length: 29.94 +/- 0.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.06         |
| time/                   |              |
|    total_timesteps      | 204805       |
| train/                  |              |
|    approx_kl            | 0.0074252835 |
|    clip_fraction        | 0.153        |
|    clip_range           | 0.15         |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.238        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0243       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0275      |
|    value_loss           | 0.183        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 320      |
|    iterations      | 6        |
|    time_elapsed    | 766      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=245766, episode_reward=0.20 +/- 0.96
Episode length: 30.00 +/- 0.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 245766      |
| train/                  |             |
|    approx_kl            | 0.007439021 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0219      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0272     |
|    value_loss           | 0.186       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 318      |
|    iterations      | 7        |
|    time_elapsed    | 899      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=286727, episode_reward=0.32 +/- 0.93
Episode length: 30.06 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.32        |
| time/                   |             |
|    total_timesteps      | 286727      |
| train/                  |             |
|    approx_kl            | 0.007929598 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.251       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0172      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.184       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.32
SELFPLAY: new best model, bumping up generation to 2
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 317      |
|    iterations      | 8        |
|    time_elapsed    | 1032     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=327688, episode_reward=0.18 +/- 0.97
Episode length: 30.03 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.182       |
| time/                   |             |
|    total_timesteps      | 327688      |
| train/                  |             |
|    approx_kl            | 0.007923645 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0118      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 0.186       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 316      |
|    iterations      | 9        |
|    time_elapsed    | 1164     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=368649, episode_reward=0.10 +/- 0.97
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.105       |
| time/                   |             |
|    total_timesteps      | 368649      |
| train/                  |             |
|    approx_kl            | 0.008164528 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.225       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00885     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 0.189       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 315      |
|    iterations      | 10       |
|    time_elapsed    | 1296     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=409610, episode_reward=0.13 +/- 0.97
Episode length: 30.02 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 409610      |
| train/                  |             |
|    approx_kl            | 0.008213168 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0113      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 0.184       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 315      |
|    iterations      | 11       |
|    time_elapsed    | 1429     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=450571, episode_reward=0.30 +/- 0.93
Episode length: 30.01 +/- 1.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.302       |
| time/                   |             |
|    total_timesteps      | 450571      |
| train/                  |             |
|    approx_kl            | 0.008359577 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0105      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.177       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3025
SELFPLAY: new best model, bumping up generation to 3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 314      |
|    iterations      | 12       |
|    time_elapsed    | 1561     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=491532, episode_reward=0.04 +/- 0.97
Episode length: 29.89 +/- 1.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.035       |
| time/                   |             |
|    total_timesteps      | 491532      |
| train/                  |             |
|    approx_kl            | 0.008470252 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.249       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0224      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 0.185       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 314      |
|    iterations      | 13       |
|    time_elapsed    | 1693     |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=532493, episode_reward=0.14 +/- 0.97
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.135       |
| time/                   |             |
|    total_timesteps      | 532493      |
| train/                  |             |
|    approx_kl            | 0.008627998 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.23        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0202      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0308     |
|    value_loss           | 0.196       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 314      |
|    iterations      | 14       |
|    time_elapsed    | 1826     |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=573454, episode_reward=0.16 +/- 0.97
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.163       |
| time/                   |             |
|    total_timesteps      | 573454      |
| train/                  |             |
|    approx_kl            | 0.008899123 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0168      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.187       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 313      |
|    iterations      | 15       |
|    time_elapsed    | 1958     |
|    total_timesteps | 614400   |
---------------------------------
Eval num_timesteps=614415, episode_reward=0.28 +/- 0.95
Episode length: 29.94 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.278       |
| time/                   |             |
|    total_timesteps      | 614415      |
| train/                  |             |
|    approx_kl            | 0.008883702 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.24        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00686     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.186       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2775
SELFPLAY: new best model, bumping up generation to 4
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 313      |
|    iterations      | 16       |
|    time_elapsed    | 2091     |
|    total_timesteps | 655360   |
---------------------------------
Eval num_timesteps=655376, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.065       |
| time/                   |             |
|    total_timesteps      | 655376      |
| train/                  |             |
|    approx_kl            | 0.009032255 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0171      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.186       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 313      |
|    iterations      | 17       |
|    time_elapsed    | 2223     |
|    total_timesteps | 696320   |
---------------------------------
Eval num_timesteps=696337, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0175      |
| time/                   |             |
|    total_timesteps      | 696337      |
| train/                  |             |
|    approx_kl            | 0.008972476 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00373     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0323     |
|    value_loss           | 0.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 312      |
|    iterations      | 18       |
|    time_elapsed    | 2356     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=737298, episode_reward=0.21 +/- 0.96
Episode length: 29.97 +/- 1.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.212        |
| time/                   |              |
|    total_timesteps      | 737298       |
| train/                  |              |
|    approx_kl            | 0.0092716925 |
|    clip_fraction        | 0.193        |
|    clip_range           | 0.15         |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0.243        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0125       |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.0326      |
|    value_loss           | 0.182        |
------------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2125
SELFPLAY: new best model, bumping up generation to 5
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 312      |
|    iterations      | 19       |
|    time_elapsed    | 2488     |
|    total_timesteps | 778240   |
---------------------------------
Eval num_timesteps=778259, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0225      |
| time/                   |             |
|    total_timesteps      | 778259      |
| train/                  |             |
|    approx_kl            | 0.009733752 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.227       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00302    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.195       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 312      |
|    iterations      | 20       |
|    time_elapsed    | 2621     |
|    total_timesteps | 819200   |
---------------------------------
Eval num_timesteps=819220, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0425      |
| time/                   |             |
|    total_timesteps      | 819220      |
| train/                  |             |
|    approx_kl            | 0.009515883 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0163      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.191       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 312      |
|    iterations      | 21       |
|    time_elapsed    | 2753     |
|    total_timesteps | 860160   |
---------------------------------
Eval num_timesteps=860181, episode_reward=0.14 +/- 0.97
Episode length: 29.91 +/- 1.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 860181      |
| train/                  |             |
|    approx_kl            | 0.009901291 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0209      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 312      |
|    iterations      | 22       |
|    time_elapsed    | 2886     |
|    total_timesteps | 901120   |
---------------------------------
Eval num_timesteps=901142, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.125       |
| time/                   |             |
|    total_timesteps      | 901142      |
| train/                  |             |
|    approx_kl            | 0.009614331 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0194      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.187       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 312      |
|    iterations      | 23       |
|    time_elapsed    | 3018     |
|    total_timesteps | 942080   |
---------------------------------
Eval num_timesteps=942103, episode_reward=0.27 +/- 0.95
Episode length: 30.01 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.265       |
| time/                   |             |
|    total_timesteps      | 942103      |
| train/                  |             |
|    approx_kl            | 0.009860863 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0147      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.188       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 311      |
|    iterations      | 24       |
|    time_elapsed    | 3151     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=983064, episode_reward=0.14 +/- 0.98
Episode length: 29.96 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 983064      |
| train/                  |             |
|    approx_kl            | 0.010013526 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0028      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.192       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 311      |
|    iterations      | 25       |
|    time_elapsed    | 3283     |
|    total_timesteps | 1024000  |
---------------------------------
Eval num_timesteps=1024025, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.115       |
| time/                   |             |
|    total_timesteps      | 1024025     |
| train/                  |             |
|    approx_kl            | 0.009997795 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00427     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.193       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 311      |
|    iterations      | 26       |
|    time_elapsed    | 3416     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1064986, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0975      |
| time/                   |             |
|    total_timesteps      | 1064986     |
| train/                  |             |
|    approx_kl            | 0.010391005 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0238      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.186       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 311      |
|    iterations      | 27       |
|    time_elapsed    | 3549     |
|    total_timesteps | 1105920  |
---------------------------------
Eval num_timesteps=1105947, episode_reward=0.20 +/- 0.96
Episode length: 30.04 +/- 0.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.198        |
| time/                   |              |
|    total_timesteps      | 1105947      |
| train/                  |              |
|    approx_kl            | 0.0104602855 |
|    clip_fraction        | 0.203        |
|    clip_range           | 0.15         |
|    entropy_loss         | -1.55        |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.016        |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.0333      |
|    value_loss           | 0.186        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 311      |
|    iterations      | 28       |
|    time_elapsed    | 3681     |
|    total_timesteps | 1146880  |
---------------------------------
Eval num_timesteps=1146908, episode_reward=0.25 +/- 0.95
Episode length: 30.05 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.25        |
| time/                   |             |
|    total_timesteps      | 1146908     |
| train/                  |             |
|    approx_kl            | 0.010374637 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.301       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0184      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.185       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.25
SELFPLAY: new best model, bumping up generation to 7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 311      |
|    iterations      | 29       |
|    time_elapsed    | 3813     |
|    total_timesteps | 1187840  |
---------------------------------
Eval num_timesteps=1187869, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 1187869     |
| train/                  |             |
|    approx_kl            | 0.010463668 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0173      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.197       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 311      |
|    iterations      | 30       |
|    time_elapsed    | 3946     |
|    total_timesteps | 1228800  |
---------------------------------
Eval num_timesteps=1228830, episode_reward=0.11 +/- 0.98
Episode length: 30.06 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.113       |
| time/                   |             |
|    total_timesteps      | 1228830     |
| train/                  |             |
|    approx_kl            | 0.010240075 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0068      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.188       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 311      |
|    iterations      | 31       |
|    time_elapsed    | 4079     |
|    total_timesteps | 1269760  |
---------------------------------
Eval num_timesteps=1269791, episode_reward=0.07 +/- 0.97
Episode length: 29.98 +/- 1.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.075        |
| time/                   |              |
|    total_timesteps      | 1269791      |
| train/                  |              |
|    approx_kl            | 0.0106050195 |
|    clip_fraction        | 0.201        |
|    clip_range           | 0.15         |
|    entropy_loss         | -1.46        |
|    explained_variance   | 0.301        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0118       |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.0331      |
|    value_loss           | 0.199        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 311      |
|    iterations      | 32       |
|    time_elapsed    | 4211     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1310752, episode_reward=0.21 +/- 0.96
Episode length: 29.98 +/- 1.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.212       |
| time/                   |             |
|    total_timesteps      | 1310752     |
| train/                  |             |
|    approx_kl            | 0.011105426 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.301       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00265     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.195       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2125
SELFPLAY: new best model, bumping up generation to 8
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 311      |
|    iterations      | 33       |
|    time_elapsed    | 4344     |
|    total_timesteps | 1351680  |
---------------------------------
Eval num_timesteps=1351713, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0175      |
| time/                   |             |
|    total_timesteps      | 1351713     |
| train/                  |             |
|    approx_kl            | 0.010783642 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0421      |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0331     |
|    value_loss           | 0.196       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 311      |
|    iterations      | 34       |
|    time_elapsed    | 4477     |
|    total_timesteps | 1392640  |
---------------------------------
Eval num_timesteps=1392674, episode_reward=0.14 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.135       |
| time/                   |             |
|    total_timesteps      | 1392674     |
| train/                  |             |
|    approx_kl            | 0.010826226 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00902     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.199       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 311      |
|    iterations      | 35       |
|    time_elapsed    | 4609     |
|    total_timesteps | 1433600  |
---------------------------------
Eval num_timesteps=1433635, episode_reward=0.21 +/- 0.95
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.215       |
| time/                   |             |
|    total_timesteps      | 1433635     |
| train/                  |             |
|    approx_kl            | 0.010910703 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0329      |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 0.203       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.215
SELFPLAY: new best model, bumping up generation to 9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 36       |
|    time_elapsed    | 4742     |
|    total_timesteps | 1474560  |
---------------------------------
Eval num_timesteps=1474596, episode_reward=0.06 +/- 0.98
Episode length: 29.93 +/- 1.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.0575      |
| time/                   |             |
|    total_timesteps      | 1474596     |
| train/                  |             |
|    approx_kl            | 0.010933695 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0464      |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.195       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 37       |
|    time_elapsed    | 4874     |
|    total_timesteps | 1515520  |
---------------------------------
Eval num_timesteps=1515557, episode_reward=0.04 +/- 0.99
Episode length: 30.02 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.0425       |
| time/                   |              |
|    total_timesteps      | 1515557      |
| train/                  |              |
|    approx_kl            | 0.0110442275 |
|    clip_fraction        | 0.206        |
|    clip_range           | 0.15         |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.32         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0303       |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.0324      |
|    value_loss           | 0.2          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 38       |
|    time_elapsed    | 5007     |
|    total_timesteps | 1556480  |
---------------------------------
Eval num_timesteps=1556518, episode_reward=0.13 +/- 0.97
Episode length: 29.96 +/- 1.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 1556518     |
| train/                  |             |
|    approx_kl            | 0.011509981 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0277      |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.205       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 310      |
|    iterations      | 39       |
|    time_elapsed    | 5139     |
|    total_timesteps | 1597440  |
---------------------------------
Eval num_timesteps=1597479, episode_reward=0.22 +/- 0.96
Episode length: 30.06 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.217       |
| time/                   |             |
|    total_timesteps      | 1597479     |
| train/                  |             |
|    approx_kl            | 0.011205644 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00803     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 0.198       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2175
SELFPLAY: new best model, bumping up generation to 10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 40       |
|    time_elapsed    | 5272     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1638440, episode_reward=0.00 +/- 0.99
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 1638440     |
| train/                  |             |
|    approx_kl            | 0.011059523 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0574      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.204       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 310      |
|    iterations      | 41       |
|    time_elapsed    | 5405     |
|    total_timesteps | 1679360  |
---------------------------------
Eval num_timesteps=1679401, episode_reward=0.05 +/- 0.98
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0525      |
| time/                   |             |
|    total_timesteps      | 1679401     |
| train/                  |             |
|    approx_kl            | 0.011264456 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0428      |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.2         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 42       |
|    time_elapsed    | 5537     |
|    total_timesteps | 1720320  |
---------------------------------
Eval num_timesteps=1720362, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0875      |
| time/                   |             |
|    total_timesteps      | 1720362     |
| train/                  |             |
|    approx_kl            | 0.011228275 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0246      |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.205       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 43       |
|    time_elapsed    | 5669     |
|    total_timesteps | 1761280  |
---------------------------------
Eval num_timesteps=1761323, episode_reward=0.14 +/- 0.97
Episode length: 30.00 +/- 1.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 1761323     |
| train/                  |             |
|    approx_kl            | 0.011295911 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00892     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.209       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 44       |
|    time_elapsed    | 5802     |
|    total_timesteps | 1802240  |
---------------------------------
Eval num_timesteps=1802284, episode_reward=0.15 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.152       |
| time/                   |             |
|    total_timesteps      | 1802284     |
| train/                  |             |
|    approx_kl            | 0.011575644 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0388      |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.205       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 45       |
|    time_elapsed    | 5935     |
|    total_timesteps | 1843200  |
---------------------------------
Eval num_timesteps=1843245, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.172       |
| time/                   |             |
|    total_timesteps      | 1843245     |
| train/                  |             |
|    approx_kl            | 0.011130038 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.297       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00636     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 0.193       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 46       |
|    time_elapsed    | 6067     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1884206, episode_reward=0.17 +/- 0.97
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.168       |
| time/                   |             |
|    total_timesteps      | 1884206     |
| train/                  |             |
|    approx_kl            | 0.011790959 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0145      |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0323     |
|    value_loss           | 0.195       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 47       |
|    time_elapsed    | 6200     |
|    total_timesteps | 1925120  |
---------------------------------
Eval num_timesteps=1925167, episode_reward=0.34 +/- 0.92
Episode length: 30.07 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.338        |
| time/                   |              |
|    total_timesteps      | 1925167      |
| train/                  |              |
|    approx_kl            | 0.0115326485 |
|    clip_fraction        | 0.202        |
|    clip_range           | 0.15         |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.298        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0273       |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.0319      |
|    value_loss           | 0.195        |
------------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3375
SELFPLAY: new best model, bumping up generation to 11
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 48       |
|    time_elapsed    | 6333     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1966128, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0325      |
| time/                   |             |
|    total_timesteps      | 1966128     |
| train/                  |             |
|    approx_kl            | 0.011508886 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0211      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.212       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 49       |
|    time_elapsed    | 6465     |
|    total_timesteps | 2007040  |
---------------------------------
Eval num_timesteps=2007089, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.0075     |
| time/                   |             |
|    total_timesteps      | 2007089     |
| train/                  |             |
|    approx_kl            | 0.011870636 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0287      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.205       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 310      |
|    iterations      | 50       |
|    time_elapsed    | 6598     |
|    total_timesteps | 2048000  |
---------------------------------
Eval num_timesteps=2048050, episode_reward=0.03 +/- 0.98
Episode length: 29.93 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.0325      |
| time/                   |             |
|    total_timesteps      | 2048050     |
| train/                  |             |
|    approx_kl            | 0.011655919 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0234      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.204       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 310      |
|    iterations      | 51       |
|    time_elapsed    | 6731     |
|    total_timesteps | 2088960  |
---------------------------------
Eval num_timesteps=2089011, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 2089011     |
| train/                  |             |
|    approx_kl            | 0.012156497 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0216      |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.209       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 52       |
|    time_elapsed    | 6863     |
|    total_timesteps | 2129920  |
---------------------------------
Eval num_timesteps=2129972, episode_reward=0.08 +/- 0.98
Episode length: 29.92 +/- 1.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.0825      |
| time/                   |             |
|    total_timesteps      | 2129972     |
| train/                  |             |
|    approx_kl            | 0.012342327 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0238      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0323     |
|    value_loss           | 0.207       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 53       |
|    time_elapsed    | 6995     |
|    total_timesteps | 2170880  |
---------------------------------
Eval num_timesteps=2170933, episode_reward=0.15 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.147       |
| time/                   |             |
|    total_timesteps      | 2170933     |
| train/                  |             |
|    approx_kl            | 0.011801049 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0149      |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.202       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 54       |
|    time_elapsed    | 7128     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2211894, episode_reward=0.21 +/- 0.97
Episode length: 30.02 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.215       |
| time/                   |             |
|    total_timesteps      | 2211894     |
| train/                  |             |
|    approx_kl            | 0.011765659 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0202      |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.203       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.215
SELFPLAY: new best model, bumping up generation to 12
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 55       |
|    time_elapsed    | 7260     |
|    total_timesteps | 2252800  |
---------------------------------
Eval num_timesteps=2252855, episode_reward=-0.07 +/- 0.99
Episode length: 29.95 +/- 1.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.07       |
| time/                   |             |
|    total_timesteps      | 2252855     |
| train/                  |             |
|    approx_kl            | 0.012133591 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0233      |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.211       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 56       |
|    time_elapsed    | 7393     |
|    total_timesteps | 2293760  |
---------------------------------
Eval num_timesteps=2293816, episode_reward=0.06 +/- 0.99
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 2293816     |
| train/                  |             |
|    approx_kl            | 0.011763155 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0475      |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.209       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 57       |
|    time_elapsed    | 7525     |
|    total_timesteps | 2334720  |
---------------------------------
Eval num_timesteps=2334777, episode_reward=0.02 +/- 0.99
Episode length: 29.95 +/- 1.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0175      |
| time/                   |             |
|    total_timesteps      | 2334777     |
| train/                  |             |
|    approx_kl            | 0.012067616 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.303       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0249      |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 58       |
|    time_elapsed    | 7658     |
|    total_timesteps | 2375680  |
---------------------------------
Eval num_timesteps=2375738, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 2375738     |
| train/                  |             |
|    approx_kl            | 0.012081496 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0168      |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.211       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 59       |
|    time_elapsed    | 7790     |
|    total_timesteps | 2416640  |
---------------------------------
Eval num_timesteps=2416699, episode_reward=0.06 +/- 0.99
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 2416699     |
| train/                  |             |
|    approx_kl            | 0.012158571 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0283      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 0.212       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 310      |
|    iterations      | 60       |
|    time_elapsed    | 7922     |
|    total_timesteps | 2457600  |
---------------------------------
Eval num_timesteps=2457660, episode_reward=0.14 +/- 0.98
Episode length: 29.98 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.135       |
| time/                   |             |
|    total_timesteps      | 2457660     |
| train/                  |             |
|    approx_kl            | 0.012247384 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0326      |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 61       |
|    time_elapsed    | 8055     |
|    total_timesteps | 2498560  |
---------------------------------
Eval num_timesteps=2498621, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 2498621     |
| train/                  |             |
|    approx_kl            | 0.012337613 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0164      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.209       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 62       |
|    time_elapsed    | 8187     |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2539582, episode_reward=0.10 +/- 0.97
Episode length: 29.97 +/- 0.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.105       |
| time/                   |             |
|    total_timesteps      | 2539582     |
| train/                  |             |
|    approx_kl            | 0.012395674 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0383      |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.211       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 63       |
|    time_elapsed    | 8320     |
|    total_timesteps | 2580480  |
---------------------------------
Eval num_timesteps=2580543, episode_reward=0.14 +/- 0.97
Episode length: 30.01 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 2580543     |
| train/                  |             |
|    approx_kl            | 0.012566256 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.022       |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 64       |
|    time_elapsed    | 8452     |
|    total_timesteps | 2621440  |
---------------------------------
Eval num_timesteps=2621504, episode_reward=0.16 +/- 0.97
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.158       |
| time/                   |             |
|    total_timesteps      | 2621504     |
| train/                  |             |
|    approx_kl            | 0.012371963 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0275      |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.211       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 65       |
|    time_elapsed    | 8585     |
|    total_timesteps | 2662400  |
---------------------------------
Eval num_timesteps=2662465, episode_reward=0.12 +/- 0.98
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 2662465     |
| train/                  |             |
|    approx_kl            | 0.012586465 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0429      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.032      |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 66       |
|    time_elapsed    | 8718     |
|    total_timesteps | 2703360  |
---------------------------------
Eval num_timesteps=2703426, episode_reward=0.27 +/- 0.95
Episode length: 30.05 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.265       |
| time/                   |             |
|    total_timesteps      | 2703426     |
| train/                  |             |
|    approx_kl            | 0.012347609 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.281       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0307      |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.21        |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 13
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 310      |
|    iterations      | 67       |
|    time_elapsed    | 8850     |
|    total_timesteps | 2744320  |
---------------------------------
Eval num_timesteps=2744387, episode_reward=-0.04 +/- 0.98
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.0375     |
| time/                   |             |
|    total_timesteps      | 2744387     |
| train/                  |             |
|    approx_kl            | 0.012652459 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.297       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0126      |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 68       |
|    time_elapsed    | 8982     |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2785348, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 2785348     |
| train/                  |             |
|    approx_kl            | 0.012892753 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0324      |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0324     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 69       |
|    time_elapsed    | 9115     |
|    total_timesteps | 2826240  |
---------------------------------
Eval num_timesteps=2826309, episode_reward=0.04 +/- 0.99
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.045       |
| time/                   |             |
|    total_timesteps      | 2826309     |
| train/                  |             |
|    approx_kl            | 0.013120597 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0184      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 310      |
|    iterations      | 70       |
|    time_elapsed    | 9247     |
|    total_timesteps | 2867200  |
---------------------------------
Eval num_timesteps=2867270, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 2867270     |
| train/                  |             |
|    approx_kl            | 0.012928429 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0384      |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 71       |
|    time_elapsed    | 9379     |
|    total_timesteps | 2908160  |
---------------------------------
Eval num_timesteps=2908231, episode_reward=0.12 +/- 0.98
Episode length: 29.92 +/- 1.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 2908231     |
| train/                  |             |
|    approx_kl            | 0.012770802 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00928     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.217       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 310      |
|    iterations      | 72       |
|    time_elapsed    | 9512     |
|    total_timesteps | 2949120  |
---------------------------------
Eval num_timesteps=2949192, episode_reward=0.11 +/- 0.98
Episode length: 30.01 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.107       |
| time/                   |             |
|    total_timesteps      | 2949192     |
| train/                  |             |
|    approx_kl            | 0.013127387 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00048    |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 310      |
|    iterations      | 73       |
|    time_elapsed    | 9645     |
|    total_timesteps | 2990080  |
---------------------------------
Eval num_timesteps=2990153, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.113       |
| time/                   |             |
|    total_timesteps      | 2990153     |
| train/                  |             |
|    approx_kl            | 0.013242605 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0254      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 74       |
|    time_elapsed    | 9777     |
|    total_timesteps | 3031040  |
---------------------------------
Eval num_timesteps=3031114, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 1.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.0225       |
| time/                   |              |
|    total_timesteps      | 3031114      |
| train/                  |              |
|    approx_kl            | 0.0130095305 |
|    clip_fraction        | 0.204        |
|    clip_range           | 0.15         |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.282        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.063        |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.0317      |
|    value_loss           | 0.216        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 75       |
|    time_elapsed    | 9910     |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3072075, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0325      |
| time/                   |             |
|    total_timesteps      | 3072075     |
| train/                  |             |
|    approx_kl            | 0.012929535 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0417      |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 76       |
|    time_elapsed    | 10042    |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3113036, episode_reward=0.15 +/- 0.97
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.152       |
| time/                   |             |
|    total_timesteps      | 3113036     |
| train/                  |             |
|    approx_kl            | 0.013357723 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0461      |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 77       |
|    time_elapsed    | 10175    |
|    total_timesteps | 3153920  |
---------------------------------
Eval num_timesteps=3153997, episode_reward=0.09 +/- 0.99
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.085      |
| time/                   |            |
|    total_timesteps      | 3153997    |
| train/                  |            |
|    approx_kl            | 0.01314826 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.15       |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.254      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0259     |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.0315    |
|    value_loss           | 0.216      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 78       |
|    time_elapsed    | 10307    |
|    total_timesteps | 3194880  |
---------------------------------
Eval num_timesteps=3194958, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.115       |
| time/                   |             |
|    total_timesteps      | 3194958     |
| train/                  |             |
|    approx_kl            | 0.013419457 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.282       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0218      |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 79       |
|    time_elapsed    | 10439    |
|    total_timesteps | 3235840  |
---------------------------------
Eval num_timesteps=3235919, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.145       |
| time/                   |             |
|    total_timesteps      | 3235919     |
| train/                  |             |
|    approx_kl            | 0.013493809 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0327      |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.217       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 80       |
|    time_elapsed    | 10572    |
|    total_timesteps | 3276800  |
---------------------------------
Eval num_timesteps=3276880, episode_reward=0.22 +/- 0.96
Episode length: 29.98 +/- 1.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.223       |
| time/                   |             |
|    total_timesteps      | 3276880     |
| train/                  |             |
|    approx_kl            | 0.013712941 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0353      |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.208       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2225
SELFPLAY: new best model, bumping up generation to 14
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 81       |
|    time_elapsed    | 10704    |
|    total_timesteps | 3317760  |
---------------------------------
Eval num_timesteps=3317841, episode_reward=0.04 +/- 0.99
Episode length: 29.91 +/- 1.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.0375      |
| time/                   |             |
|    total_timesteps      | 3317841     |
| train/                  |             |
|    approx_kl            | 0.013279918 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.049       |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.217       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 82       |
|    time_elapsed    | 10837    |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3358802, episode_reward=0.02 +/- 0.99
Episode length: 29.93 +/- 1.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.0175      |
| time/                   |             |
|    total_timesteps      | 3358802     |
| train/                  |             |
|    approx_kl            | 0.013428653 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0773      |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 83       |
|    time_elapsed    | 10969    |
|    total_timesteps | 3399680  |
---------------------------------
Eval num_timesteps=3399763, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 3399763     |
| train/                  |             |
|    approx_kl            | 0.013519773 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0528      |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 84       |
|    time_elapsed    | 11102    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3440724, episode_reward=0.06 +/- 0.97
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 3440724     |
| train/                  |             |
|    approx_kl            | 0.013591242 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0151      |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 85       |
|    time_elapsed    | 11234    |
|    total_timesteps | 3481600  |
---------------------------------
Eval num_timesteps=3481685, episode_reward=0.08 +/- 0.98
Episode length: 29.99 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.0775     |
| time/                   |            |
|    total_timesteps      | 3481685    |
| train/                  |            |
|    approx_kl            | 0.01374803 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.994     |
|    explained_variance   | 0.278      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0319     |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.032     |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 86       |
|    time_elapsed    | 11366    |
|    total_timesteps | 3522560  |
---------------------------------
Eval num_timesteps=3522646, episode_reward=-0.01 +/- 0.98
Episode length: 29.95 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.015      |
| time/                   |             |
|    total_timesteps      | 3522646     |
| train/                  |             |
|    approx_kl            | 0.013821453 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.983      |
|    explained_variance   | 0.268       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0569      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 87       |
|    time_elapsed    | 11499    |
|    total_timesteps | 3563520  |
---------------------------------
Eval num_timesteps=3563607, episode_reward=0.10 +/- 0.98
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0975      |
| time/                   |             |
|    total_timesteps      | 3563607     |
| train/                  |             |
|    approx_kl            | 0.013980609 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0278      |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 88       |
|    time_elapsed    | 11632    |
|    total_timesteps | 3604480  |
---------------------------------
Eval num_timesteps=3604568, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 3604568     |
| train/                  |             |
|    approx_kl            | 0.013975734 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0251      |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 89       |
|    time_elapsed    | 11764    |
|    total_timesteps | 3645440  |
---------------------------------
Eval num_timesteps=3645529, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 1.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 3645529     |
| train/                  |             |
|    approx_kl            | 0.014551565 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0.257       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0391      |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 90       |
|    time_elapsed    | 11896    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3686490, episode_reward=0.12 +/- 0.98
Episode length: 30.03 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.117       |
| time/                   |             |
|    total_timesteps      | 3686490     |
| train/                  |             |
|    approx_kl            | 0.013730762 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0428      |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 91       |
|    time_elapsed    | 12029    |
|    total_timesteps | 3727360  |
---------------------------------
Eval num_timesteps=3727451, episode_reward=0.20 +/- 0.97
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.198       |
| time/                   |             |
|    total_timesteps      | 3727451     |
| train/                  |             |
|    approx_kl            | 0.014464838 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0273      |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.032      |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 92       |
|    time_elapsed    | 12161    |
|    total_timesteps | 3768320  |
---------------------------------
Eval num_timesteps=3768412, episode_reward=0.11 +/- 0.98
Episode length: 29.95 +/- 1.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.113       |
| time/                   |             |
|    total_timesteps      | 3768412     |
| train/                  |             |
|    approx_kl            | 0.014406765 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0144      |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.217       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 93       |
|    time_elapsed    | 12294    |
|    total_timesteps | 3809280  |
---------------------------------
Eval num_timesteps=3809373, episode_reward=0.09 +/- 0.99
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0875      |
| time/                   |             |
|    total_timesteps      | 3809373     |
| train/                  |             |
|    approx_kl            | 0.014614065 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.997      |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0278      |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.032      |
|    value_loss           | 0.214       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 94       |
|    time_elapsed    | 12426    |
|    total_timesteps | 3850240  |
---------------------------------
Eval num_timesteps=3850334, episode_reward=0.07 +/- 0.99
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0675      |
| time/                   |             |
|    total_timesteps      | 3850334     |
| train/                  |             |
|    approx_kl            | 0.014099145 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.994      |
|    explained_variance   | 0.249       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0292      |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 95       |
|    time_elapsed    | 12559    |
|    total_timesteps | 3891200  |
---------------------------------
Eval num_timesteps=3891295, episode_reward=0.15 +/- 0.98
Episode length: 30.03 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.152       |
| time/                   |             |
|    total_timesteps      | 3891295     |
| train/                  |             |
|    approx_kl            | 0.014589325 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.983      |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0318      |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 96       |
|    time_elapsed    | 12691    |
|    total_timesteps | 3932160  |
---------------------------------
Eval num_timesteps=3932256, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.142      |
| time/                   |            |
|    total_timesteps      | 3932256    |
| train/                  |            |
|    approx_kl            | 0.01414676 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.983     |
|    explained_variance   | 0.263      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0361     |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0314    |
|    value_loss           | 0.219      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 97       |
|    time_elapsed    | 12823    |
|    total_timesteps | 3973120  |
---------------------------------
Eval num_timesteps=3973217, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.125       |
| time/                   |             |
|    total_timesteps      | 3973217     |
| train/                  |             |
|    approx_kl            | 0.014955279 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.989      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0402      |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.211       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 98       |
|    time_elapsed    | 12956    |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4014178, episode_reward=0.19 +/- 0.97
Episode length: 29.98 +/- 1.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.188        |
| time/                   |              |
|    total_timesteps      | 4014178      |
| train/                  |              |
|    approx_kl            | 0.0147323115 |
|    clip_fraction        | 0.214        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.988       |
|    explained_variance   | 0.252        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0448       |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.032       |
|    value_loss           | 0.213        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 99       |
|    time_elapsed    | 13088    |
|    total_timesteps | 4055040  |
---------------------------------
Eval num_timesteps=4055139, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.107       |
| time/                   |             |
|    total_timesteps      | 4055139     |
| train/                  |             |
|    approx_kl            | 0.014846826 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.986      |
|    explained_variance   | 0.281       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0288      |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 0.214       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 100      |
|    time_elapsed    | 13220    |
|    total_timesteps | 4096000  |
---------------------------------
Eval num_timesteps=4096100, episode_reward=0.13 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.133       |
| time/                   |             |
|    total_timesteps      | 4096100     |
| train/                  |             |
|    approx_kl            | 0.014716169 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.978      |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0619      |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 101      |
|    time_elapsed    | 13353    |
|    total_timesteps | 4136960  |
---------------------------------
Eval num_timesteps=4137061, episode_reward=0.11 +/- 0.98
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.113       |
| time/                   |             |
|    total_timesteps      | 4137061     |
| train/                  |             |
|    approx_kl            | 0.015099347 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.979      |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0235      |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 102      |
|    time_elapsed    | 13485    |
|    total_timesteps | 4177920  |
---------------------------------
Eval num_timesteps=4178022, episode_reward=0.12 +/- 0.97
Episode length: 30.06 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.117      |
| time/                   |            |
|    total_timesteps      | 4178022    |
| train/                  |            |
|    approx_kl            | 0.01457656 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.977     |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0356     |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0324    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 103      |
|    time_elapsed    | 13618    |
|    total_timesteps | 4218880  |
---------------------------------
Eval num_timesteps=4218983, episode_reward=0.12 +/- 0.97
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 4218983    |
| train/                  |            |
|    approx_kl            | 0.01489695 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.973     |
|    explained_variance   | 0.27       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0446     |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.032     |
|    value_loss           | 0.215      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 104      |
|    time_elapsed    | 13750    |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4259944, episode_reward=0.14 +/- 0.98
Episode length: 29.94 +/- 1.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.14        |
| time/                   |             |
|    total_timesteps      | 4259944     |
| train/                  |             |
|    approx_kl            | 0.014454385 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.972      |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0343      |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.214       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 105      |
|    time_elapsed    | 13883    |
|    total_timesteps | 4300800  |
---------------------------------
Eval num_timesteps=4300905, episode_reward=0.17 +/- 0.98
Episode length: 29.93 +/- 1.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.17        |
| time/                   |             |
|    total_timesteps      | 4300905     |
| train/                  |             |
|    approx_kl            | 0.015019184 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.974      |
|    explained_variance   | 0.282       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0299      |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.213       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 106      |
|    time_elapsed    | 14015    |
|    total_timesteps | 4341760  |
---------------------------------
Eval num_timesteps=4341866, episode_reward=0.15 +/- 0.97
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.155       |
| time/                   |             |
|    total_timesteps      | 4341866     |
| train/                  |             |
|    approx_kl            | 0.015579661 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.982      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0572      |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 107      |
|    time_elapsed    | 14148    |
|    total_timesteps | 4382720  |
---------------------------------
Eval num_timesteps=4382827, episode_reward=0.10 +/- 0.98
Episode length: 29.95 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.105       |
| time/                   |             |
|    total_timesteps      | 4382827     |
| train/                  |             |
|    approx_kl            | 0.015235171 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.971      |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0538      |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0324     |
|    value_loss           | 0.208       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 108      |
|    time_elapsed    | 14280    |
|    total_timesteps | 4423680  |
---------------------------------
Eval num_timesteps=4423788, episode_reward=0.14 +/- 0.98
Episode length: 29.93 +/- 1.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 4423788     |
| train/                  |             |
|    approx_kl            | 0.015137526 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.977      |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0444      |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 109      |
|    time_elapsed    | 14412    |
|    total_timesteps | 4464640  |
---------------------------------
Eval num_timesteps=4464749, episode_reward=0.15 +/- 0.97
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 4464749    |
| train/                  |            |
|    approx_kl            | 0.01579303 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.974     |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0425     |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0325    |
|    value_loss           | 0.217      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 110      |
|    time_elapsed    | 14544    |
|    total_timesteps | 4505600  |
---------------------------------
Eval num_timesteps=4505710, episode_reward=0.23 +/- 0.96
Episode length: 30.05 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.235       |
| time/                   |             |
|    total_timesteps      | 4505710     |
| train/                  |             |
|    approx_kl            | 0.015343169 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.965      |
|    explained_variance   | 0.281       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0208      |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0325     |
|    value_loss           | 0.214       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 15
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 111      |
|    time_elapsed    | 14677    |
|    total_timesteps | 4546560  |
---------------------------------
Eval num_timesteps=4546671, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.025       |
| time/                   |             |
|    total_timesteps      | 4546671     |
| train/                  |             |
|    approx_kl            | 0.015578523 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.977      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0265      |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 112      |
|    time_elapsed    | 14809    |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4587632, episode_reward=-0.04 +/- 0.98
Episode length: 29.96 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.0375     |
| time/                   |             |
|    total_timesteps      | 4587632     |
| train/                  |             |
|    approx_kl            | 0.015634881 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.973      |
|    explained_variance   | 0.254       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0267      |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 113      |
|    time_elapsed    | 14942    |
|    total_timesteps | 4628480  |
---------------------------------
Eval num_timesteps=4628593, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0125      |
| time/                   |             |
|    total_timesteps      | 4628593     |
| train/                  |             |
|    approx_kl            | 0.015608455 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.97       |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0301      |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.0331     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 114      |
|    time_elapsed    | 15074    |
|    total_timesteps | 4669440  |
---------------------------------
Eval num_timesteps=4669554, episode_reward=-0.02 +/- 0.98
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.0225     |
| time/                   |             |
|    total_timesteps      | 4669554     |
| train/                  |             |
|    approx_kl            | 0.015606642 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.974      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0693      |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 115      |
|    time_elapsed    | 15206    |
|    total_timesteps | 4710400  |
---------------------------------
Eval num_timesteps=4710515, episode_reward=-0.00 +/- 0.99
Episode length: 29.93 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.0025     |
| time/                   |             |
|    total_timesteps      | 4710515     |
| train/                  |             |
|    approx_kl            | 0.016223583 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.967      |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0503      |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 116      |
|    time_elapsed    | 15339    |
|    total_timesteps | 4751360  |
---------------------------------
Eval num_timesteps=4751476, episode_reward=0.07 +/- 0.98
Episode length: 29.93 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 4751476     |
| train/                  |             |
|    approx_kl            | 0.015537498 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.968      |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0353      |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 117      |
|    time_elapsed    | 15471    |
|    total_timesteps | 4792320  |
---------------------------------
Eval num_timesteps=4792437, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.168       |
| time/                   |             |
|    total_timesteps      | 4792437     |
| train/                  |             |
|    approx_kl            | 0.015825083 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.968      |
|    explained_variance   | 0.268       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0253      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 118      |
|    time_elapsed    | 15603    |
|    total_timesteps | 4833280  |
---------------------------------
Eval num_timesteps=4833398, episode_reward=0.02 +/- 0.99
Episode length: 29.94 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.0225     |
| time/                   |            |
|    total_timesteps      | 4833398    |
| train/                  |            |
|    approx_kl            | 0.01588301 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.966     |
|    explained_variance   | 0.262      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0314     |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.033     |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 119      |
|    time_elapsed    | 15736    |
|    total_timesteps | 4874240  |
---------------------------------
Eval num_timesteps=4874359, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.075       |
| time/                   |             |
|    total_timesteps      | 4874359     |
| train/                  |             |
|    approx_kl            | 0.015953572 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.959      |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0515      |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 120      |
|    time_elapsed    | 15868    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4915320, episode_reward=0.06 +/- 0.98
Episode length: 29.95 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0625      |
| time/                   |             |
|    total_timesteps      | 4915320     |
| train/                  |             |
|    approx_kl            | 0.015450755 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.957      |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0486      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 121      |
|    time_elapsed    | 16001    |
|    total_timesteps | 4956160  |
---------------------------------
Eval num_timesteps=4956281, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.055       |
| time/                   |             |
|    total_timesteps      | 4956281     |
| train/                  |             |
|    approx_kl            | 0.016532488 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.96       |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0317      |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 122      |
|    time_elapsed    | 16133    |
|    total_timesteps | 4997120  |
---------------------------------
Eval num_timesteps=4997242, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.095       |
| time/                   |             |
|    total_timesteps      | 4997242     |
| train/                  |             |
|    approx_kl            | 0.016060773 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.961      |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0397      |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 123      |
|    time_elapsed    | 16266    |
|    total_timesteps | 5038080  |
---------------------------------
Eval num_timesteps=5038203, episode_reward=0.02 +/- 0.99
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 5038203     |
| train/                  |             |
|    approx_kl            | 0.016402638 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.965      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0243      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0332     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 124      |
|    time_elapsed    | 16398    |
|    total_timesteps | 5079040  |
---------------------------------
Eval num_timesteps=5079164, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0625      |
| time/                   |             |
|    total_timesteps      | 5079164     |
| train/                  |             |
|    approx_kl            | 0.016298518 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.957      |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0526      |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 125      |
|    time_elapsed    | 16530    |
|    total_timesteps | 5120000  |
---------------------------------
Eval num_timesteps=5120125, episode_reward=-0.02 +/- 0.98
Episode length: 29.98 +/- 0.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | -0.0175   |
| time/                   |           |
|    total_timesteps      | 5120125   |
| train/                  |           |
|    approx_kl            | 0.0163494 |
|    clip_fraction        | 0.224     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.957    |
|    explained_variance   | 0.267     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.00012   |
|    n_updates            | 1250      |
|    policy_gradient_loss | -0.0333   |
|    value_loss           | 0.219     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 126      |
|    time_elapsed    | 16663    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5161086, episode_reward=0.01 +/- 0.98
Episode length: 29.93 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.005       |
| time/                   |             |
|    total_timesteps      | 5161086     |
| train/                  |             |
|    approx_kl            | 0.016124118 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.953      |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0478      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 127      |
|    time_elapsed    | 16795    |
|    total_timesteps | 5201920  |
---------------------------------
Eval num_timesteps=5202047, episode_reward=0.13 +/- 0.97
Episode length: 30.02 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.133       |
| time/                   |             |
|    total_timesteps      | 5202047     |
| train/                  |             |
|    approx_kl            | 0.016954582 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.958      |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.03        |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 128      |
|    time_elapsed    | 16927    |
|    total_timesteps | 5242880  |
---------------------------------
Eval num_timesteps=5243008, episode_reward=0.07 +/- 0.98
Episode length: 29.95 +/- 1.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0675      |
| time/                   |             |
|    total_timesteps      | 5243008     |
| train/                  |             |
|    approx_kl            | 0.016506637 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.95       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0451      |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.0332     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 129      |
|    time_elapsed    | 17060    |
|    total_timesteps | 5283840  |
---------------------------------
Eval num_timesteps=5283969, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0725      |
| time/                   |             |
|    total_timesteps      | 5283969     |
| train/                  |             |
|    approx_kl            | 0.016878555 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.947      |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0303      |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 130      |
|    time_elapsed    | 17192    |
|    total_timesteps | 5324800  |
---------------------------------
Eval num_timesteps=5324930, episode_reward=-0.04 +/- 0.98
Episode length: 29.99 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.035      |
| time/                   |             |
|    total_timesteps      | 5324930     |
| train/                  |             |
|    approx_kl            | 0.017019216 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0263      |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 131      |
|    time_elapsed    | 17325    |
|    total_timesteps | 5365760  |
---------------------------------
Eval num_timesteps=5365891, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 5365891     |
| train/                  |             |
|    approx_kl            | 0.016251585 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.95       |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0388      |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0323     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 132      |
|    time_elapsed    | 17457    |
|    total_timesteps | 5406720  |
---------------------------------
Eval num_timesteps=5406852, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.045       |
| time/                   |             |
|    total_timesteps      | 5406852     |
| train/                  |             |
|    approx_kl            | 0.016613588 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0505      |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.217       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 133      |
|    time_elapsed    | 17590    |
|    total_timesteps | 5447680  |
---------------------------------
Eval num_timesteps=5447813, episode_reward=0.15 +/- 0.97
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.152       |
| time/                   |             |
|    total_timesteps      | 5447813     |
| train/                  |             |
|    approx_kl            | 0.016947096 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.943      |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0426      |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.217       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 134      |
|    time_elapsed    | 17722    |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5488774, episode_reward=-0.05 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.0475     |
| time/                   |             |
|    total_timesteps      | 5488774     |
| train/                  |             |
|    approx_kl            | 0.016990433 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.944      |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00113     |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.217       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 135      |
|    time_elapsed    | 17855    |
|    total_timesteps | 5529600  |
---------------------------------
Eval num_timesteps=5529735, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.115       |
| time/                   |             |
|    total_timesteps      | 5529735     |
| train/                  |             |
|    approx_kl            | 0.017270831 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.943      |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0718      |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 136      |
|    time_elapsed    | 17987    |
|    total_timesteps | 5570560  |
---------------------------------
Eval num_timesteps=5570696, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.055       |
| time/                   |             |
|    total_timesteps      | 5570696     |
| train/                  |             |
|    approx_kl            | 0.017035514 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.942      |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0271      |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 137      |
|    time_elapsed    | 18119    |
|    total_timesteps | 5611520  |
---------------------------------
Eval num_timesteps=5611657, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 5611657     |
| train/                  |             |
|    approx_kl            | 0.017108463 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.944      |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0504      |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0332     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 138      |
|    time_elapsed    | 18251    |
|    total_timesteps | 5652480  |
---------------------------------
Eval num_timesteps=5652618, episode_reward=0.23 +/- 0.96
Episode length: 30.03 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.228      |
| time/                   |            |
|    total_timesteps      | 5652618    |
| train/                  |            |
|    approx_kl            | 0.01707586 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.933     |
|    explained_variance   | 0.26       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0294     |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 0.214      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2275
SELFPLAY: new best model, bumping up generation to 16
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 139      |
|    time_elapsed    | 18384    |
|    total_timesteps | 5693440  |
---------------------------------
Eval num_timesteps=5693579, episode_reward=-0.04 +/- 0.99
Episode length: 29.96 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.0425     |
| time/                   |             |
|    total_timesteps      | 5693579     |
| train/                  |             |
|    approx_kl            | 0.017394647 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.953      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0469      |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 140      |
|    time_elapsed    | 18517    |
|    total_timesteps | 5734400  |
---------------------------------
Eval num_timesteps=5734540, episode_reward=0.10 +/- 0.99
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.095       |
| time/                   |             |
|    total_timesteps      | 5734540     |
| train/                  |             |
|    approx_kl            | 0.017050084 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.953      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0185      |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.25    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 141      |
|    time_elapsed    | 18649    |
|    total_timesteps | 5775360  |
---------------------------------
Eval num_timesteps=5775501, episode_reward=-0.07 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.075      |
| time/                   |             |
|    total_timesteps      | 5775501     |
| train/                  |             |
|    approx_kl            | 0.017210687 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.949      |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.036       |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 142      |
|    time_elapsed    | 18782    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5816462, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.045      |
| time/                   |            |
|    total_timesteps      | 5816462    |
| train/                  |            |
|    approx_kl            | 0.01726711 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.951     |
|    explained_variance   | 0.263      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0584     |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0338    |
|    value_loss           | 0.226      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 143      |
|    time_elapsed    | 18915    |
|    total_timesteps | 5857280  |
---------------------------------
Eval num_timesteps=5857423, episode_reward=0.04 +/- 0.98
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0375      |
| time/                   |             |
|    total_timesteps      | 5857423     |
| train/                  |             |
|    approx_kl            | 0.017706161 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0565      |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 144      |
|    time_elapsed    | 19047    |
|    total_timesteps | 5898240  |
---------------------------------
Eval num_timesteps=5898384, episode_reward=-0.04 +/- 0.98
Episode length: 29.93 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.04       |
| time/                   |             |
|    total_timesteps      | 5898384     |
| train/                  |             |
|    approx_kl            | 0.017266689 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.935      |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0396      |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 145      |
|    time_elapsed    | 19179    |
|    total_timesteps | 5939200  |
---------------------------------
Eval num_timesteps=5939345, episode_reward=-0.02 +/- 0.98
Episode length: 29.95 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 5939345    |
| train/                  |            |
|    approx_kl            | 0.01759546 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.942     |
|    explained_variance   | 0.262      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0415     |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.0334    |
|    value_loss           | 0.225      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 146      |
|    time_elapsed    | 19312    |
|    total_timesteps | 5980160  |
---------------------------------
Eval num_timesteps=5980306, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 1.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.107       |
| time/                   |             |
|    total_timesteps      | 5980306     |
| train/                  |             |
|    approx_kl            | 0.017343298 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.938      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0155      |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 147      |
|    time_elapsed    | 19444    |
|    total_timesteps | 6021120  |
---------------------------------
Eval num_timesteps=6021267, episode_reward=0.06 +/- 0.98
Episode length: 29.92 +/- 1.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.0575     |
| time/                   |            |
|    total_timesteps      | 6021267    |
| train/                  |            |
|    approx_kl            | 0.01778104 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.945     |
|    explained_variance   | 0.24       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.037      |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0333    |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 148      |
|    time_elapsed    | 19577    |
|    total_timesteps | 6062080  |
---------------------------------
Eval num_timesteps=6062228, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 1.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.0075      |
| time/                   |             |
|    total_timesteps      | 6062228     |
| train/                  |             |
|    approx_kl            | 0.017511286 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0368      |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 149      |
|    time_elapsed    | 19709    |
|    total_timesteps | 6103040  |
---------------------------------
Eval num_timesteps=6103189, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.095      |
| time/                   |            |
|    total_timesteps      | 6103189    |
| train/                  |            |
|    approx_kl            | 0.01760837 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.939     |
|    explained_variance   | 0.279      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0147     |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0337    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 150      |
|    time_elapsed    | 19842    |
|    total_timesteps | 6144000  |
---------------------------------
Eval num_timesteps=6144150, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.015      |
| time/                   |            |
|    total_timesteps      | 6144150    |
| train/                  |            |
|    approx_kl            | 0.01746963 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.94      |
|    explained_variance   | 0.261      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0213     |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0333    |
|    value_loss           | 0.227      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 151      |
|    time_elapsed    | 19974    |
|    total_timesteps | 6184960  |
---------------------------------
Eval num_timesteps=6185111, episode_reward=0.09 +/- 0.99
Episode length: 29.92 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.0925      |
| time/                   |             |
|    total_timesteps      | 6185111     |
| train/                  |             |
|    approx_kl            | 0.017570246 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.935      |
|    explained_variance   | 0.251       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0309      |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 152      |
|    time_elapsed    | 20107    |
|    total_timesteps | 6225920  |
---------------------------------
Eval num_timesteps=6226072, episode_reward=0.15 +/- 0.97
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.152       |
| time/                   |             |
|    total_timesteps      | 6226072     |
| train/                  |             |
|    approx_kl            | 0.017628439 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.934      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0332      |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 153      |
|    time_elapsed    | 20239    |
|    total_timesteps | 6266880  |
---------------------------------
Eval num_timesteps=6267033, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.075       |
| time/                   |             |
|    total_timesteps      | 6267033     |
| train/                  |             |
|    approx_kl            | 0.017700648 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.927      |
|    explained_variance   | 0.22        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0398      |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 154      |
|    time_elapsed    | 20371    |
|    total_timesteps | 6307840  |
---------------------------------
Eval num_timesteps=6307994, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0675      |
| time/                   |             |
|    total_timesteps      | 6307994     |
| train/                  |             |
|    approx_kl            | 0.017692104 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.931      |
|    explained_variance   | 0.257       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0157      |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 155      |
|    time_elapsed    | 20504    |
|    total_timesteps | 6348800  |
---------------------------------
Eval num_timesteps=6348955, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 6348955    |
| train/                  |            |
|    approx_kl            | 0.01750205 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.932     |
|    explained_variance   | 0.241      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0301     |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0337    |
|    value_loss           | 0.225      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 156      |
|    time_elapsed    | 20636    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6389916, episode_reward=0.09 +/- 0.97
Episode length: 30.06 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.085       |
| time/                   |             |
|    total_timesteps      | 6389916     |
| train/                  |             |
|    approx_kl            | 0.018054152 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.924      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0299      |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 157      |
|    time_elapsed    | 20769    |
|    total_timesteps | 6430720  |
---------------------------------
Eval num_timesteps=6430877, episode_reward=0.01 +/- 0.99
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.005       |
| time/                   |             |
|    total_timesteps      | 6430877     |
| train/                  |             |
|    approx_kl            | 0.018224983 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.924      |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0341      |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 158      |
|    time_elapsed    | 20901    |
|    total_timesteps | 6471680  |
---------------------------------
Eval num_timesteps=6471838, episode_reward=0.14 +/- 0.97
Episode length: 29.98 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 6471838     |
| train/                  |             |
|    approx_kl            | 0.018141884 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.924      |
|    explained_variance   | 0.257       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0297      |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 159      |
|    time_elapsed    | 21034    |
|    total_timesteps | 6512640  |
---------------------------------
Eval num_timesteps=6512799, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0925      |
| time/                   |             |
|    total_timesteps      | 6512799     |
| train/                  |             |
|    approx_kl            | 0.017434606 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.917      |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00815     |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 160      |
|    time_elapsed    | 21166    |
|    total_timesteps | 6553600  |
---------------------------------
Eval num_timesteps=6553760, episode_reward=0.17 +/- 0.97
Episode length: 29.96 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.165       |
| time/                   |             |
|    total_timesteps      | 6553760     |
| train/                  |             |
|    approx_kl            | 0.017695408 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.917      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0324      |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 161      |
|    time_elapsed    | 21299    |
|    total_timesteps | 6594560  |
---------------------------------
Eval num_timesteps=6594721, episode_reward=0.13 +/- 0.98
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 6594721     |
| train/                  |             |
|    approx_kl            | 0.018124128 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.917      |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0463      |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 162      |
|    time_elapsed    | 21431    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6635682, episode_reward=0.09 +/- 0.99
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0925      |
| time/                   |             |
|    total_timesteps      | 6635682     |
| train/                  |             |
|    approx_kl            | 0.017952561 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.909      |
|    explained_variance   | 0.248       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0431      |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 163      |
|    time_elapsed    | 21563    |
|    total_timesteps | 6676480  |
---------------------------------
Eval num_timesteps=6676643, episode_reward=0.04 +/- 0.99
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 6676643     |
| train/                  |             |
|    approx_kl            | 0.017749228 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.911      |
|    explained_variance   | 0.268       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0268      |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 164      |
|    time_elapsed    | 21695    |
|    total_timesteps | 6717440  |
---------------------------------
Eval num_timesteps=6717604, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 1.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.065       |
| time/                   |             |
|    total_timesteps      | 6717604     |
| train/                  |             |
|    approx_kl            | 0.018255297 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.905      |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0373      |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 165      |
|    time_elapsed    | 21828    |
|    total_timesteps | 6758400  |
---------------------------------
Eval num_timesteps=6758565, episode_reward=0.08 +/- 0.98
Episode length: 29.95 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0825      |
| time/                   |             |
|    total_timesteps      | 6758565     |
| train/                  |             |
|    approx_kl            | 0.018539747 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.91       |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0235      |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 166      |
|    time_elapsed    | 21960    |
|    total_timesteps | 6799360  |
---------------------------------
Eval num_timesteps=6799526, episode_reward=0.07 +/- 0.98
Episode length: 29.95 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.075       |
| time/                   |             |
|    total_timesteps      | 6799526     |
| train/                  |             |
|    approx_kl            | 0.018403426 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.914      |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0273      |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 167      |
|    time_elapsed    | 22093    |
|    total_timesteps | 6840320  |
---------------------------------
Eval num_timesteps=6840487, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.035      |
| time/                   |            |
|    total_timesteps      | 6840487    |
| train/                  |            |
|    approx_kl            | 0.01779532 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.906     |
|    explained_variance   | 0.25       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0246     |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0338    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 168      |
|    time_elapsed    | 22225    |
|    total_timesteps | 6881280  |
---------------------------------
Eval num_timesteps=6881448, episode_reward=0.07 +/- 0.99
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.065       |
| time/                   |             |
|    total_timesteps      | 6881448     |
| train/                  |             |
|    approx_kl            | 0.018882554 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.906      |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0254      |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 169      |
|    time_elapsed    | 22358    |
|    total_timesteps | 6922240  |
---------------------------------
Eval num_timesteps=6922409, episode_reward=0.07 +/- 0.98
Episode length: 29.92 +/- 0.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.075       |
| time/                   |             |
|    total_timesteps      | 6922409     |
| train/                  |             |
|    approx_kl            | 0.018424813 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.901      |
|    explained_variance   | 0.243       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0284      |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 170      |
|    time_elapsed    | 22490    |
|    total_timesteps | 6963200  |
---------------------------------
Eval num_timesteps=6963370, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0875      |
| time/                   |             |
|    total_timesteps      | 6963370     |
| train/                  |             |
|    approx_kl            | 0.018147418 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.206       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0492      |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 171      |
|    time_elapsed    | 22622    |
|    total_timesteps | 7004160  |
---------------------------------
Eval num_timesteps=7004331, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.085       |
| time/                   |             |
|    total_timesteps      | 7004331     |
| train/                  |             |
|    approx_kl            | 0.018723909 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.897      |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.04        |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 172      |
|    time_elapsed    | 22755    |
|    total_timesteps | 7045120  |
---------------------------------
Eval num_timesteps=7045292, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 1.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0675      |
| time/                   |             |
|    total_timesteps      | 7045292     |
| train/                  |             |
|    approx_kl            | 0.018552586 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.901      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0405      |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 173      |
|    time_elapsed    | 22887    |
|    total_timesteps | 7086080  |
---------------------------------
Eval num_timesteps=7086253, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 7086253     |
| train/                  |             |
|    approx_kl            | 0.018774673 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.25        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0238      |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 174      |
|    time_elapsed    | 23019    |
|    total_timesteps | 7127040  |
---------------------------------
Eval num_timesteps=7127214, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0725      |
| time/                   |             |
|    total_timesteps      | 7127214     |
| train/                  |             |
|    approx_kl            | 0.018838534 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.907      |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0176      |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 175      |
|    time_elapsed    | 23152    |
|    total_timesteps | 7168000  |
---------------------------------
Eval num_timesteps=7168175, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.133       |
| time/                   |             |
|    total_timesteps      | 7168175     |
| train/                  |             |
|    approx_kl            | 0.019619191 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.906      |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0333      |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.217       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 176      |
|    time_elapsed    | 23284    |
|    total_timesteps | 7208960  |
---------------------------------
Eval num_timesteps=7209136, episode_reward=0.14 +/- 0.98
Episode length: 30.01 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 7209136     |
| train/                  |             |
|    approx_kl            | 0.018727083 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.907      |
|    explained_variance   | 0.255       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0579      |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 177      |
|    time_elapsed    | 23417    |
|    total_timesteps | 7249920  |
---------------------------------
Eval num_timesteps=7250097, episode_reward=0.12 +/- 0.98
Episode length: 30.08 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 7250097    |
| train/                  |            |
|    approx_kl            | 0.01970048 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.913     |
|    explained_variance   | 0.238      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0401     |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0347    |
|    value_loss           | 0.222      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 178      |
|    time_elapsed    | 23549    |
|    total_timesteps | 7290880  |
---------------------------------
Eval num_timesteps=7291058, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.113       |
| time/                   |             |
|    total_timesteps      | 7291058     |
| train/                  |             |
|    approx_kl            | 0.019015497 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.906      |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0154      |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 179      |
|    time_elapsed    | 23682    |
|    total_timesteps | 7331840  |
---------------------------------
Eval num_timesteps=7332019, episode_reward=0.07 +/- 0.98
Episode length: 30.09 +/- 0.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.075       |
| time/                   |             |
|    total_timesteps      | 7332019     |
| train/                  |             |
|    approx_kl            | 0.018801037 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.906      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0206      |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 180      |
|    time_elapsed    | 23814    |
|    total_timesteps | 7372800  |
---------------------------------
Eval num_timesteps=7372980, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0925      |
| time/                   |             |
|    total_timesteps      | 7372980     |
| train/                  |             |
|    approx_kl            | 0.019103413 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0328      |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 181      |
|    time_elapsed    | 23946    |
|    total_timesteps | 7413760  |
---------------------------------
Eval num_timesteps=7413941, episode_reward=0.13 +/- 0.97
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 7413941     |
| train/                  |             |
|    approx_kl            | 0.019156037 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0216      |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 182      |
|    time_elapsed    | 24079    |
|    total_timesteps | 7454720  |
---------------------------------
Eval num_timesteps=7454902, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.13        |
| time/                   |             |
|    total_timesteps      | 7454902     |
| train/                  |             |
|    approx_kl            | 0.019228885 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.899      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0435      |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 183      |
|    time_elapsed    | 24211    |
|    total_timesteps | 7495680  |
---------------------------------
Eval num_timesteps=7495863, episode_reward=0.14 +/- 0.97
Episode length: 29.96 +/- 1.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 7495863     |
| train/                  |             |
|    approx_kl            | 0.018980794 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.899      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0294      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 184      |
|    time_elapsed    | 24344    |
|    total_timesteps | 7536640  |
---------------------------------
Eval num_timesteps=7536824, episode_reward=0.14 +/- 0.97
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.145       |
| time/                   |             |
|    total_timesteps      | 7536824     |
| train/                  |             |
|    approx_kl            | 0.019798625 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.899      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0329      |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 185      |
|    time_elapsed    | 24476    |
|    total_timesteps | 7577600  |
---------------------------------
Eval num_timesteps=7577785, episode_reward=0.14 +/- 0.97
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.14        |
| time/                   |             |
|    total_timesteps      | 7577785     |
| train/                  |             |
|    approx_kl            | 0.018590385 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.895      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0381      |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 186      |
|    time_elapsed    | 24609    |
|    total_timesteps | 7618560  |
---------------------------------
Eval num_timesteps=7618746, episode_reward=0.12 +/- 0.98
Episode length: 29.94 +/- 1.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.12        |
| time/                   |             |
|    total_timesteps      | 7618746     |
| train/                  |             |
|    approx_kl            | 0.018730775 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.895      |
|    explained_variance   | 0.249       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0351      |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 187      |
|    time_elapsed    | 24742    |
|    total_timesteps | 7659520  |
---------------------------------
Eval num_timesteps=7659707, episode_reward=0.12 +/- 0.99
Episode length: 30.02 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 7659707     |
| train/                  |             |
|    approx_kl            | 0.019296382 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.894      |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0597      |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 188      |
|    time_elapsed    | 24874    |
|    total_timesteps | 7700480  |
---------------------------------
Eval num_timesteps=7700668, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.0675     |
| time/                   |            |
|    total_timesteps      | 7700668    |
| train/                  |            |
|    approx_kl            | 0.01943235 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.894     |
|    explained_variance   | 0.25       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0106     |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 189      |
|    time_elapsed    | 25006    |
|    total_timesteps | 7741440  |
---------------------------------
Eval num_timesteps=7741629, episode_reward=0.09 +/- 0.99
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 7741629    |
| train/                  |            |
|    approx_kl            | 0.01946033 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.895     |
|    explained_variance   | 0.234      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0278     |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0339    |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 190      |
|    time_elapsed    | 25139    |
|    total_timesteps | 7782400  |
---------------------------------
Eval num_timesteps=7782590, episode_reward=0.14 +/- 0.97
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 7782590     |
| train/                  |             |
|    approx_kl            | 0.019926988 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.888      |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0293      |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 191      |
|    time_elapsed    | 25271    |
|    total_timesteps | 7823360  |
---------------------------------
Eval num_timesteps=7823551, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 7823551     |
| train/                  |             |
|    approx_kl            | 0.019750927 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0319      |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 192      |
|    time_elapsed    | 25403    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7864512, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 7864512     |
| train/                  |             |
|    approx_kl            | 0.019262584 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0379      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.33     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 193      |
|    time_elapsed    | 25536    |
|    total_timesteps | 7905280  |
---------------------------------
Eval num_timesteps=7905473, episode_reward=0.04 +/- 0.99
Episode length: 29.99 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 7905473     |
| train/                  |             |
|    approx_kl            | 0.019522183 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0.251       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0573      |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 194      |
|    time_elapsed    | 25668    |
|    total_timesteps | 7946240  |
---------------------------------
Eval num_timesteps=7946434, episode_reward=0.17 +/- 0.97
Episode length: 30.08 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.172       |
| time/                   |             |
|    total_timesteps      | 7946434     |
| train/                  |             |
|    approx_kl            | 0.020330595 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.891      |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.041       |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 195      |
|    time_elapsed    | 25800    |
|    total_timesteps | 7987200  |
---------------------------------
Eval num_timesteps=7987395, episode_reward=0.21 +/- 0.95
Episode length: 30.06 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.207       |
| time/                   |             |
|    total_timesteps      | 7987395     |
| train/                  |             |
|    approx_kl            | 0.019608257 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.039       |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.22        |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2075
SELFPLAY: new best model, bumping up generation to 17
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 196      |
|    time_elapsed    | 25933    |
|    total_timesteps | 8028160  |
---------------------------------
Eval num_timesteps=8028356, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.005      |
| time/                   |             |
|    total_timesteps      | 8028356     |
| train/                  |             |
|    approx_kl            | 0.019909553 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.88       |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0121      |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 197      |
|    time_elapsed    | 26065    |
|    total_timesteps | 8069120  |
---------------------------------
Eval num_timesteps=8069317, episode_reward=0.02 +/- 0.98
Episode length: 29.93 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 8069317    |
| train/                  |            |
|    approx_kl            | 0.02006216 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.885     |
|    explained_variance   | 0.267      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0398     |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0343    |
|    value_loss           | 0.226      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 198      |
|    time_elapsed    | 26198    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8110278, episode_reward=0.02 +/- 0.99
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0175      |
| time/                   |             |
|    total_timesteps      | 8110278     |
| train/                  |             |
|    approx_kl            | 0.020576373 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.249       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0548      |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 199      |
|    time_elapsed    | 26331    |
|    total_timesteps | 8151040  |
---------------------------------
Eval num_timesteps=8151239, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.0225     |
| time/                   |            |
|    total_timesteps      | 8151239    |
| train/                  |            |
|    approx_kl            | 0.02004596 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.884     |
|    explained_variance   | 0.255      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0472     |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.0344    |
|    value_loss           | 0.225      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 200      |
|    time_elapsed    | 26463    |
|    total_timesteps | 8192000  |
---------------------------------
Eval num_timesteps=8192200, episode_reward=-0.03 +/- 0.98
Episode length: 29.98 +/- 1.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.025     |
| time/                   |            |
|    total_timesteps      | 8192200    |
| train/                  |            |
|    approx_kl            | 0.01970172 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.881     |
|    explained_variance   | 0.266      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0325     |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 201      |
|    time_elapsed    | 26596    |
|    total_timesteps | 8232960  |
---------------------------------
Eval num_timesteps=8233161, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 8233161     |
| train/                  |             |
|    approx_kl            | 0.020273317 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.878      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0213      |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 202      |
|    time_elapsed    | 26728    |
|    total_timesteps | 8273920  |
---------------------------------
Eval num_timesteps=8274122, episode_reward=-0.01 +/- 0.98
Episode length: 29.99 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 8274122     |
| train/                  |             |
|    approx_kl            | 0.020914111 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.881      |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.064       |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 203      |
|    time_elapsed    | 26860    |
|    total_timesteps | 8314880  |
---------------------------------
Eval num_timesteps=8315083, episode_reward=-0.09 +/- 0.98
Episode length: 29.93 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.0925     |
| time/                   |             |
|    total_timesteps      | 8315083     |
| train/                  |             |
|    approx_kl            | 0.019904563 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.888      |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0265      |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 204      |
|    time_elapsed    | 26993    |
|    total_timesteps | 8355840  |
---------------------------------
Eval num_timesteps=8356044, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 8356044     |
| train/                  |             |
|    approx_kl            | 0.019935817 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.881      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.03        |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 205      |
|    time_elapsed    | 27126    |
|    total_timesteps | 8396800  |
---------------------------------
Eval num_timesteps=8397005, episode_reward=-0.10 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.105      |
| time/                   |             |
|    total_timesteps      | 8397005     |
| train/                  |             |
|    approx_kl            | 0.019905789 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.878      |
|    explained_variance   | 0.224       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0397      |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 206      |
|    time_elapsed    | 27259    |
|    total_timesteps | 8437760  |
---------------------------------
Eval num_timesteps=8437966, episode_reward=0.03 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0275      |
| time/                   |             |
|    total_timesteps      | 8437966     |
| train/                  |             |
|    approx_kl            | 0.020214181 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.881      |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0301      |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 207      |
|    time_elapsed    | 27391    |
|    total_timesteps | 8478720  |
---------------------------------
Eval num_timesteps=8478927, episode_reward=-0.06 +/- 0.98
Episode length: 29.99 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.055     |
| time/                   |            |
|    total_timesteps      | 8478927    |
| train/                  |            |
|    approx_kl            | 0.02025928 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.871     |
|    explained_variance   | 0.252      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0331     |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.0339    |
|    value_loss           | 0.226      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 208      |
|    time_elapsed    | 27524    |
|    total_timesteps | 8519680  |
---------------------------------
Eval num_timesteps=8519888, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.05      |
| time/                   |           |
|    total_timesteps      | 8519888   |
| train/                  |           |
|    approx_kl            | 0.0199314 |
|    clip_fraction        | 0.238     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.881    |
|    explained_variance   | 0.27      |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0413    |
|    n_updates            | 2080      |
|    policy_gradient_loss | -0.034    |
|    value_loss           | 0.223     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 209      |
|    time_elapsed    | 27657    |
|    total_timesteps | 8560640  |
---------------------------------
Eval num_timesteps=8560849, episode_reward=0.00 +/- 0.98
Episode length: 30.03 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0025      |
| time/                   |             |
|    total_timesteps      | 8560849     |
| train/                  |             |
|    approx_kl            | 0.020327555 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.879      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0345      |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 210      |
|    time_elapsed    | 27789    |
|    total_timesteps | 8601600  |
---------------------------------
Eval num_timesteps=8601810, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 1.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 8601810     |
| train/                  |             |
|    approx_kl            | 0.020635376 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.876      |
|    explained_variance   | 0.243       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0723      |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 211      |
|    time_elapsed    | 27922    |
|    total_timesteps | 8642560  |
---------------------------------
Eval num_timesteps=8642771, episode_reward=-0.10 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.105      |
| time/                   |             |
|    total_timesteps      | 8642771     |
| train/                  |             |
|    approx_kl            | 0.020145368 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.876      |
|    explained_variance   | 0.268       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0511      |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 212      |
|    time_elapsed    | 28055    |
|    total_timesteps | 8683520  |
---------------------------------
Eval num_timesteps=8683732, episode_reward=0.10 +/- 0.98
Episode length: 30.07 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.095       |
| time/                   |             |
|    total_timesteps      | 8683732     |
| train/                  |             |
|    approx_kl            | 0.020769024 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.871      |
|    explained_variance   | 0.282       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0403      |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 213      |
|    time_elapsed    | 28187    |
|    total_timesteps | 8724480  |
---------------------------------
Eval num_timesteps=8724693, episode_reward=0.12 +/- 0.98
Episode length: 30.04 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 8724693    |
| train/                  |            |
|    approx_kl            | 0.02078664 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.87      |
|    explained_variance   | 0.248      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0183     |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 214      |
|    time_elapsed    | 28320    |
|    total_timesteps | 8765440  |
---------------------------------
Eval num_timesteps=8765654, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.055       |
| time/                   |             |
|    total_timesteps      | 8765654     |
| train/                  |             |
|    approx_kl            | 0.020286426 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.873      |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0332      |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 215      |
|    time_elapsed    | 28452    |
|    total_timesteps | 8806400  |
---------------------------------
Eval num_timesteps=8806615, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 8806615    |
| train/                  |            |
|    approx_kl            | 0.02100579 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.88      |
|    explained_variance   | 0.24       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.075      |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0338    |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 216      |
|    time_elapsed    | 28585    |
|    total_timesteps | 8847360  |
---------------------------------
Eval num_timesteps=8847576, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.015      |
| time/                   |             |
|    total_timesteps      | 8847576     |
| train/                  |             |
|    approx_kl            | 0.020585652 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.873      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0337      |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 217      |
|    time_elapsed    | 28718    |
|    total_timesteps | 8888320  |
---------------------------------
Eval num_timesteps=8888537, episode_reward=0.01 +/- 0.98
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.005       |
| time/                   |             |
|    total_timesteps      | 8888537     |
| train/                  |             |
|    approx_kl            | 0.020391032 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.88       |
|    explained_variance   | 0.254       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.073       |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 218      |
|    time_elapsed    | 28850    |
|    total_timesteps | 8929280  |
---------------------------------
Eval num_timesteps=8929498, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 8929498     |
| train/                  |             |
|    approx_kl            | 0.020975765 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.881      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0615      |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 219      |
|    time_elapsed    | 28983    |
|    total_timesteps | 8970240  |
---------------------------------
Eval num_timesteps=8970459, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.03      |
| time/                   |            |
|    total_timesteps      | 8970459    |
| train/                  |            |
|    approx_kl            | 0.02095171 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.876     |
|    explained_variance   | 0.268      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0404     |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.0342    |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 220      |
|    time_elapsed    | 29115    |
|    total_timesteps | 9011200  |
---------------------------------
Eval num_timesteps=9011420, episode_reward=0.07 +/- 0.98
Episode length: 30.04 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.065       |
| time/                   |             |
|    total_timesteps      | 9011420     |
| train/                  |             |
|    approx_kl            | 0.020856429 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.875      |
|    explained_variance   | 0.25        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0292      |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 221      |
|    time_elapsed    | 29248    |
|    total_timesteps | 9052160  |
---------------------------------
Eval num_timesteps=9052381, episode_reward=0.09 +/- 0.97
Episode length: 30.04 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 9052381     |
| train/                  |             |
|    approx_kl            | 0.020343255 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.873      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.055       |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 222      |
|    time_elapsed    | 29380    |
|    total_timesteps | 9093120  |
---------------------------------
Eval num_timesteps=9093342, episode_reward=-0.03 +/- 0.99
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.025      |
| time/                   |             |
|    total_timesteps      | 9093342     |
| train/                  |             |
|    approx_kl            | 0.020206962 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.873      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0272      |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 223      |
|    time_elapsed    | 29513    |
|    total_timesteps | 9134080  |
---------------------------------
Eval num_timesteps=9134303, episode_reward=0.03 +/- 0.98
Episode length: 29.95 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 9134303     |
| train/                  |             |
|    approx_kl            | 0.020898024 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.872      |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0282      |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 224      |
|    time_elapsed    | 29645    |
|    total_timesteps | 9175040  |
---------------------------------
Eval num_timesteps=9175264, episode_reward=0.14 +/- 0.98
Episode length: 30.06 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.135       |
| time/                   |             |
|    total_timesteps      | 9175264     |
| train/                  |             |
|    approx_kl            | 0.020691102 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.863      |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0539      |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 225      |
|    time_elapsed    | 29778    |
|    total_timesteps | 9216000  |
---------------------------------
Eval num_timesteps=9216225, episode_reward=0.01 +/- 0.99
Episode length: 30.00 +/- 1.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0125      |
| time/                   |             |
|    total_timesteps      | 9216225     |
| train/                  |             |
|    approx_kl            | 0.021327134 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0626      |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 226      |
|    time_elapsed    | 29910    |
|    total_timesteps | 9256960  |
---------------------------------
Eval num_timesteps=9257186, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.065       |
| time/                   |             |
|    total_timesteps      | 9257186     |
| train/                  |             |
|    approx_kl            | 0.020969888 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.86       |
|    explained_variance   | 0.248       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0408      |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 227      |
|    time_elapsed    | 30043    |
|    total_timesteps | 9297920  |
---------------------------------
Eval num_timesteps=9298147, episode_reward=0.11 +/- 0.98
Episode length: 30.12 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.113       |
| time/                   |             |
|    total_timesteps      | 9298147     |
| train/                  |             |
|    approx_kl            | 0.021642646 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0539      |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 228      |
|    time_elapsed    | 30176    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9339108, episode_reward=0.11 +/- 0.98
Episode length: 30.05 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.107       |
| time/                   |             |
|    total_timesteps      | 9339108     |
| train/                  |             |
|    approx_kl            | 0.021314168 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.863      |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0492      |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 229      |
|    time_elapsed    | 30308    |
|    total_timesteps | 9379840  |
---------------------------------
Eval num_timesteps=9380069, episode_reward=0.01 +/- 0.98
Episode length: 30.04 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.005      |
| time/                   |            |
|    total_timesteps      | 9380069    |
| train/                  |            |
|    approx_kl            | 0.02101234 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.855     |
|    explained_variance   | 0.272      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.061      |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0339    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 230      |
|    time_elapsed    | 30441    |
|    total_timesteps | 9420800  |
---------------------------------
Eval num_timesteps=9421030, episode_reward=0.07 +/- 0.99
Episode length: 30.02 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 9421030     |
| train/                  |             |
|    approx_kl            | 0.021823518 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.861      |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0535      |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 231      |
|    time_elapsed    | 30573    |
|    total_timesteps | 9461760  |
---------------------------------
Eval num_timesteps=9461991, episode_reward=0.02 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0225      |
| time/                   |             |
|    total_timesteps      | 9461991     |
| train/                  |             |
|    approx_kl            | 0.021504307 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.864      |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0486      |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 232      |
|    time_elapsed    | 30706    |
|    total_timesteps | 9502720  |
---------------------------------
Eval num_timesteps=9502952, episode_reward=0.09 +/- 0.99
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0875      |
| time/                   |             |
|    total_timesteps      | 9502952     |
| train/                  |             |
|    approx_kl            | 0.021780908 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0362      |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 233      |
|    time_elapsed    | 30838    |
|    total_timesteps | 9543680  |
---------------------------------
Eval num_timesteps=9543913, episode_reward=0.12 +/- 0.97
Episode length: 30.06 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.117       |
| time/                   |             |
|    total_timesteps      | 9543913     |
| train/                  |             |
|    approx_kl            | 0.020310853 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.858      |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0561      |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 234      |
|    time_elapsed    | 30970    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9584874, episode_reward=0.10 +/- 0.98
Episode length: 30.07 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.105       |
| time/                   |             |
|    total_timesteps      | 9584874     |
| train/                  |             |
|    approx_kl            | 0.020677116 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0541      |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 235      |
|    time_elapsed    | 31103    |
|    total_timesteps | 9625600  |
---------------------------------
Eval num_timesteps=9625835, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 1.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.0175     |
| time/                   |            |
|    total_timesteps      | 9625835    |
| train/                  |            |
|    approx_kl            | 0.02157968 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.863     |
|    explained_variance   | 0.27       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0193     |
|    n_updates            | 2350       |
|    policy_gradient_loss | -0.0345    |
|    value_loss           | 0.221      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 236      |
|    time_elapsed    | 31235    |
|    total_timesteps | 9666560  |
---------------------------------
Eval num_timesteps=9666796, episode_reward=-0.01 +/- 0.98
Episode length: 30.04 +/- 0.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.015      |
| time/                   |             |
|    total_timesteps      | 9666796     |
| train/                  |             |
|    approx_kl            | 0.021566432 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.861      |
|    explained_variance   | 0.254       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0535      |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 237      |
|    time_elapsed    | 31368    |
|    total_timesteps | 9707520  |
---------------------------------
Eval num_timesteps=9707757, episode_reward=0.08 +/- 0.98
Episode length: 30.05 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.0825      |
| time/                   |             |
|    total_timesteps      | 9707757     |
| train/                  |             |
|    approx_kl            | 0.021200359 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.858      |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0331      |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 238      |
|    time_elapsed    | 31500    |
|    total_timesteps | 9748480  |
---------------------------------
Eval num_timesteps=9748718, episode_reward=0.07 +/- 0.98
Episode length: 29.87 +/- 2.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 9748718     |
| train/                  |             |
|    approx_kl            | 0.022067636 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.25        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0379      |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 239      |
|    time_elapsed    | 31632    |
|    total_timesteps | 9789440  |
---------------------------------
Eval num_timesteps=9789679, episode_reward=0.01 +/- 0.98
Episode length: 30.06 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.015       |
| time/                   |             |
|    total_timesteps      | 9789679     |
| train/                  |             |
|    approx_kl            | 0.021119077 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.858      |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0593      |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 240      |
|    time_elapsed    | 31765    |
|    total_timesteps | 9830400  |
---------------------------------
Eval num_timesteps=9830640, episode_reward=0.06 +/- 0.99
Episode length: 30.07 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 9830640     |
| train/                  |             |
|    approx_kl            | 0.022056798 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.857      |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0275      |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 241      |
|    time_elapsed    | 31897    |
|    total_timesteps | 9871360  |
---------------------------------
Eval num_timesteps=9871601, episode_reward=0.07 +/- 0.99
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0675      |
| time/                   |             |
|    total_timesteps      | 9871601     |
| train/                  |             |
|    approx_kl            | 0.021587254 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.855      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.036       |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 242      |
|    time_elapsed    | 32029    |
|    total_timesteps | 9912320  |
---------------------------------
Eval num_timesteps=9912562, episode_reward=-0.00 +/- 0.98
Episode length: 30.00 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.0025    |
| time/                   |            |
|    total_timesteps      | 9912562    |
| train/                  |            |
|    approx_kl            | 0.02153268 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.849     |
|    explained_variance   | 0.272      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.023      |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.0344    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 243      |
|    time_elapsed    | 32162    |
|    total_timesteps | 9953280  |
---------------------------------
Eval num_timesteps=9953523, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 9953523     |
| train/                  |             |
|    approx_kl            | 0.022477318 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.255       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0652      |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 244      |
|    time_elapsed    | 32294    |
|    total_timesteps | 9994240  |
---------------------------------
Eval num_timesteps=9994484, episode_reward=-0.01 +/- 0.99
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.0075     |
| time/                   |             |
|    total_timesteps      | 9994484     |
| train/                  |             |
|    approx_kl            | 0.022233194 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.848      |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0297      |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 245      |
|    time_elapsed    | 32427    |
|    total_timesteps | 10035200 |
---------------------------------
Eval num_timesteps=10035445, episode_reward=0.11 +/- 0.97
Episode length: 29.96 +/- 1.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.107       |
| time/                   |             |
|    total_timesteps      | 10035445    |
| train/                  |             |
|    approx_kl            | 0.022073545 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.849      |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0526      |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 246      |
|    time_elapsed    | 32559    |
|    total_timesteps | 10076160 |
---------------------------------
Eval num_timesteps=10076406, episode_reward=0.02 +/- 0.99
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0175      |
| time/                   |             |
|    total_timesteps      | 10076406    |
| train/                  |             |
|    approx_kl            | 0.021486906 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0499      |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 247      |
|    time_elapsed    | 32691    |
|    total_timesteps | 10117120 |
---------------------------------
Eval num_timesteps=10117367, episode_reward=0.06 +/- 0.99
Episode length: 30.08 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.0625      |
| time/                   |             |
|    total_timesteps      | 10117367    |
| train/                  |             |
|    approx_kl            | 0.021397246 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.251       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0557      |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 248      |
|    time_elapsed    | 32824    |
|    total_timesteps | 10158080 |
---------------------------------
Eval num_timesteps=10158328, episode_reward=0.07 +/- 0.98
Episode length: 30.09 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.075      |
| time/                   |            |
|    total_timesteps      | 10158328   |
| train/                  |            |
|    approx_kl            | 0.02133366 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.848     |
|    explained_variance   | 0.266      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.038      |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 249      |
|    time_elapsed    | 32956    |
|    total_timesteps | 10199040 |
---------------------------------
Eval num_timesteps=10199289, episode_reward=0.12 +/- 0.98
Episode length: 30.07 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.115       |
| time/                   |             |
|    total_timesteps      | 10199289    |
| train/                  |             |
|    approx_kl            | 0.021794805 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.026       |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 250      |
|    time_elapsed    | 33088    |
|    total_timesteps | 10240000 |
---------------------------------
Eval num_timesteps=10240250, episode_reward=0.11 +/- 0.97
Episode length: 30.00 +/- 1.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.113      |
| time/                   |            |
|    total_timesteps      | 10240250   |
| train/                  |            |
|    approx_kl            | 0.02230537 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.844     |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0327     |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0347    |
|    value_loss           | 0.227      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 251      |
|    time_elapsed    | 33221    |
|    total_timesteps | 10280960 |
---------------------------------
Eval num_timesteps=10281211, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 10281211    |
| train/                  |             |
|    approx_kl            | 0.021381097 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.848      |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0286      |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 252      |
|    time_elapsed    | 33353    |
|    total_timesteps | 10321920 |
---------------------------------
Eval num_timesteps=10322172, episode_reward=0.03 +/- 0.98
Episode length: 30.05 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.025       |
| time/                   |             |
|    total_timesteps      | 10322172    |
| train/                  |             |
|    approx_kl            | 0.021647519 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.846      |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0448      |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 253      |
|    time_elapsed    | 33486    |
|    total_timesteps | 10362880 |
---------------------------------
Eval num_timesteps=10363133, episode_reward=0.12 +/- 0.98
Episode length: 30.12 +/- 0.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.122      |
| time/                   |            |
|    total_timesteps      | 10363133   |
| train/                  |            |
|    approx_kl            | 0.02282567 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.842     |
|    explained_variance   | 0.254      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0345     |
|    n_updates            | 2530       |
|    policy_gradient_loss | -0.0348    |
|    value_loss           | 0.225      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 254      |
|    time_elapsed    | 33618    |
|    total_timesteps | 10403840 |
---------------------------------
Eval num_timesteps=10404094, episode_reward=0.12 +/- 0.98
Episode length: 30.05 +/- 1.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 10404094    |
| train/                  |             |
|    approx_kl            | 0.022715736 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.84       |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0373      |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 255      |
|    time_elapsed    | 33750    |
|    total_timesteps | 10444800 |
---------------------------------
Eval num_timesteps=10445055, episode_reward=0.06 +/- 0.98
Episode length: 30.05 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.055     |
| time/                   |           |
|    total_timesteps      | 10445055  |
| train/                  |           |
|    approx_kl            | 0.0221239 |
|    clip_fraction        | 0.243     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.845    |
|    explained_variance   | 0.278     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0144    |
|    n_updates            | 2550      |
|    policy_gradient_loss | -0.0346   |
|    value_loss           | 0.22      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 256      |
|    time_elapsed    | 33883    |
|    total_timesteps | 10485760 |
---------------------------------
Eval num_timesteps=10486016, episode_reward=0.10 +/- 0.97
Episode length: 29.96 +/- 1.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.105       |
| time/                   |             |
|    total_timesteps      | 10486016    |
| train/                  |             |
|    approx_kl            | 0.022181606 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.839      |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0513      |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 257      |
|    time_elapsed    | 34015    |
|    total_timesteps | 10526720 |
---------------------------------
Eval num_timesteps=10526977, episode_reward=0.14 +/- 0.97
Episode length: 30.07 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 10526977   |
| train/                  |            |
|    approx_kl            | 0.02232242 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.841     |
|    explained_variance   | 0.24       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0333     |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.0345    |
|    value_loss           | 0.227      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 258      |
|    time_elapsed    | 34147    |
|    total_timesteps | 10567680 |
---------------------------------
Eval num_timesteps=10567938, episode_reward=0.07 +/- 0.99
Episode length: 30.07 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.0675     |
| time/                   |            |
|    total_timesteps      | 10567938   |
| train/                  |            |
|    approx_kl            | 0.02264808 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.842     |
|    explained_variance   | 0.248      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0317     |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.034     |
|    value_loss           | 0.222      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 259      |
|    time_elapsed    | 34280    |
|    total_timesteps | 10608640 |
---------------------------------
Eval num_timesteps=10608899, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 1.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 10608899    |
| train/                  |             |
|    approx_kl            | 0.022100145 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0327      |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 260      |
|    time_elapsed    | 34412    |
|    total_timesteps | 10649600 |
---------------------------------
Eval num_timesteps=10649860, episode_reward=0.11 +/- 0.98
Episode length: 30.09 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.113       |
| time/                   |             |
|    total_timesteps      | 10649860    |
| train/                  |             |
|    approx_kl            | 0.022747548 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0668      |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 261      |
|    time_elapsed    | 34545    |
|    total_timesteps | 10690560 |
---------------------------------
Eval num_timesteps=10690821, episode_reward=0.14 +/- 0.98
Episode length: 30.11 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.145      |
| time/                   |            |
|    total_timesteps      | 10690821   |
| train/                  |            |
|    approx_kl            | 0.02210692 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.85      |
|    explained_variance   | 0.27       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0273     |
|    n_updates            | 2610       |
|    policy_gradient_loss | -0.0345    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 262      |
|    time_elapsed    | 34677    |
|    total_timesteps | 10731520 |
---------------------------------
Eval num_timesteps=10731782, episode_reward=0.08 +/- 0.99
Episode length: 30.13 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.0825      |
| time/                   |             |
|    total_timesteps      | 10731782    |
| train/                  |             |
|    approx_kl            | 0.022406274 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.836      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.024       |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 263      |
|    time_elapsed    | 34810    |
|    total_timesteps | 10772480 |
---------------------------------
Eval num_timesteps=10772743, episode_reward=0.09 +/- 0.98
Episode length: 30.05 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 10772743    |
| train/                  |             |
|    approx_kl            | 0.022991924 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.84       |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.038       |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 264      |
|    time_elapsed    | 34942    |
|    total_timesteps | 10813440 |
---------------------------------
Eval num_timesteps=10813704, episode_reward=0.10 +/- 0.99
Episode length: 30.06 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.095      |
| time/                   |            |
|    total_timesteps      | 10813704   |
| train/                  |            |
|    approx_kl            | 0.02255242 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.273      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0443     |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 0.225      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 265      |
|    time_elapsed    | 35074    |
|    total_timesteps | 10854400 |
---------------------------------
Eval num_timesteps=10854665, episode_reward=0.09 +/- 0.98
Episode length: 30.07 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.0925      |
| time/                   |             |
|    total_timesteps      | 10854665    |
| train/                  |             |
|    approx_kl            | 0.022625925 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.833      |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0308      |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 266      |
|    time_elapsed    | 35207    |
|    total_timesteps | 10895360 |
---------------------------------
Eval num_timesteps=10895626, episode_reward=0.13 +/- 0.98
Episode length: 30.11 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 10895626    |
| train/                  |             |
|    approx_kl            | 0.022450449 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0551      |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 267      |
|    time_elapsed    | 35339    |
|    total_timesteps | 10936320 |
---------------------------------
Eval num_timesteps=10936587, episode_reward=-0.01 +/- 0.99
Episode length: 30.00 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 10936587    |
| train/                  |             |
|    approx_kl            | 0.022727692 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0636      |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 268      |
|    time_elapsed    | 35472    |
|    total_timesteps | 10977280 |
---------------------------------
Eval num_timesteps=10977548, episode_reward=0.16 +/- 0.96
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.16        |
| time/                   |             |
|    total_timesteps      | 10977548    |
| train/                  |             |
|    approx_kl            | 0.021934753 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0555      |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 269      |
|    time_elapsed    | 35604    |
|    total_timesteps | 11018240 |
---------------------------------
Eval num_timesteps=11018509, episode_reward=0.06 +/- 0.99
Episode length: 30.05 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.055       |
| time/                   |             |
|    total_timesteps      | 11018509    |
| train/                  |             |
|    approx_kl            | 0.022362877 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0284      |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 270      |
|    time_elapsed    | 35736    |
|    total_timesteps | 11059200 |
---------------------------------
Eval num_timesteps=11059470, episode_reward=0.15 +/- 0.97
Episode length: 30.12 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.155       |
| time/                   |             |
|    total_timesteps      | 11059470    |
| train/                  |             |
|    approx_kl            | 0.022592694 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0358      |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 271      |
|    time_elapsed    | 35869    |
|    total_timesteps | 11100160 |
---------------------------------
Eval num_timesteps=11100431, episode_reward=0.12 +/- 0.99
Episode length: 29.97 +/- 1.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.12        |
| time/                   |             |
|    total_timesteps      | 11100431    |
| train/                  |             |
|    approx_kl            | 0.022306126 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0182      |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 272      |
|    time_elapsed    | 36002    |
|    total_timesteps | 11141120 |
---------------------------------
Eval num_timesteps=11141392, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 1.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.14        |
| time/                   |             |
|    total_timesteps      | 11141392    |
| train/                  |             |
|    approx_kl            | 0.022739362 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0377      |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 273      |
|    time_elapsed    | 36134    |
|    total_timesteps | 11182080 |
---------------------------------
Eval num_timesteps=11182353, episode_reward=0.17 +/- 0.97
Episode length: 30.09 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 11182353   |
| train/                  |            |
|    approx_kl            | 0.02233709 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.826     |
|    explained_variance   | 0.244      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0409     |
|    n_updates            | 2730       |
|    policy_gradient_loss | -0.0345    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 274      |
|    time_elapsed    | 36267    |
|    total_timesteps | 11223040 |
---------------------------------
Eval num_timesteps=11223314, episode_reward=0.04 +/- 0.99
Episode length: 30.06 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 11223314   |
| train/                  |            |
|    approx_kl            | 0.02311576 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.824     |
|    explained_variance   | 0.284      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0405     |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.0351    |
|    value_loss           | 0.22       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 275      |
|    time_elapsed    | 36399    |
|    total_timesteps | 11264000 |
---------------------------------
Eval num_timesteps=11264275, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 1.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0675      |
| time/                   |             |
|    total_timesteps      | 11264275    |
| train/                  |             |
|    approx_kl            | 0.023428049 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0547      |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 276      |
|    time_elapsed    | 36532    |
|    total_timesteps | 11304960 |
---------------------------------
Eval num_timesteps=11305236, episode_reward=0.15 +/- 0.97
Episode length: 30.09 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 11305236    |
| train/                  |             |
|    approx_kl            | 0.022445753 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.832      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0183      |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 277      |
|    time_elapsed    | 36664    |
|    total_timesteps | 11345920 |
---------------------------------
Eval num_timesteps=11346197, episode_reward=0.20 +/- 0.97
Episode length: 30.14 +/- 1.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.198       |
| time/                   |             |
|    total_timesteps      | 11346197    |
| train/                  |             |
|    approx_kl            | 0.022586893 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0391      |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 278      |
|    time_elapsed    | 36797    |
|    total_timesteps | 11386880 |
---------------------------------
Eval num_timesteps=11387158, episode_reward=0.10 +/- 0.98
Episode length: 30.10 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.095      |
| time/                   |            |
|    total_timesteps      | 11387158   |
| train/                  |            |
|    approx_kl            | 0.02252792 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.832     |
|    explained_variance   | 0.262      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0333     |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 279      |
|    time_elapsed    | 36929    |
|    total_timesteps | 11427840 |
---------------------------------
Eval num_timesteps=11428119, episode_reward=0.12 +/- 0.98
Episode length: 30.10 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 11428119    |
| train/                  |             |
|    approx_kl            | 0.023425322 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.268       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00959     |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 280      |
|    time_elapsed    | 37061    |
|    total_timesteps | 11468800 |
---------------------------------
Eval num_timesteps=11469080, episode_reward=0.10 +/- 0.98
Episode length: 30.11 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.105       |
| time/                   |             |
|    total_timesteps      | 11469080    |
| train/                  |             |
|    approx_kl            | 0.023539286 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0327      |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 281      |
|    time_elapsed    | 37194    |
|    total_timesteps | 11509760 |
---------------------------------
Eval num_timesteps=11510041, episode_reward=0.07 +/- 0.97
Episode length: 30.06 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.065       |
| time/                   |             |
|    total_timesteps      | 11510041    |
| train/                  |             |
|    approx_kl            | 0.023126218 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0457      |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 282      |
|    time_elapsed    | 37326    |
|    total_timesteps | 11550720 |
---------------------------------
Eval num_timesteps=11551002, episode_reward=0.18 +/- 0.97
Episode length: 30.09 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.182       |
| time/                   |             |
|    total_timesteps      | 11551002    |
| train/                  |             |
|    approx_kl            | 0.023394383 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0243      |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 283      |
|    time_elapsed    | 37458    |
|    total_timesteps | 11591680 |
---------------------------------
Eval num_timesteps=11591963, episode_reward=0.20 +/- 0.96
Episode length: 29.99 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 11591963    |
| train/                  |             |
|    approx_kl            | 0.022859361 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.033       |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 284      |
|    time_elapsed    | 37591    |
|    total_timesteps | 11632640 |
---------------------------------
Eval num_timesteps=11632924, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 1.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.12        |
| time/                   |             |
|    total_timesteps      | 11632924    |
| train/                  |             |
|    approx_kl            | 0.023127016 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0167      |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 285      |
|    time_elapsed    | 37724    |
|    total_timesteps | 11673600 |
---------------------------------
Eval num_timesteps=11673885, episode_reward=0.14 +/- 0.97
Episode length: 30.09 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.14        |
| time/                   |             |
|    total_timesteps      | 11673885    |
| train/                  |             |
|    approx_kl            | 0.023134895 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0427      |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 286      |
|    time_elapsed    | 37856    |
|    total_timesteps | 11714560 |
---------------------------------
Eval num_timesteps=11714846, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 1.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.055      |
| time/                   |            |
|    total_timesteps      | 11714846   |
| train/                  |            |
|    approx_kl            | 0.02340838 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.825     |
|    explained_variance   | 0.266      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0294     |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 0.222      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 287      |
|    time_elapsed    | 37989    |
|    total_timesteps | 11755520 |
---------------------------------
Eval num_timesteps=11755807, episode_reward=0.09 +/- 0.98
Episode length: 30.08 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 11755807   |
| train/                  |            |
|    approx_kl            | 0.02293552 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.823     |
|    explained_variance   | 0.25       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0339     |
|    n_updates            | 2870       |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 288      |
|    time_elapsed    | 38121    |
|    total_timesteps | 11796480 |
---------------------------------
Eval num_timesteps=11796768, episode_reward=0.20 +/- 0.96
Episode length: 29.94 +/- 1.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.195       |
| time/                   |             |
|    total_timesteps      | 11796768    |
| train/                  |             |
|    approx_kl            | 0.023320386 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.824      |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0353      |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 289      |
|    time_elapsed    | 38253    |
|    total_timesteps | 11837440 |
---------------------------------
Eval num_timesteps=11837729, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 1.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0975      |
| time/                   |             |
|    total_timesteps      | 11837729    |
| train/                  |             |
|    approx_kl            | 0.023714148 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.824      |
|    explained_variance   | 0.24        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0251      |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 290      |
|    time_elapsed    | 38386    |
|    total_timesteps | 11878400 |
---------------------------------
Eval num_timesteps=11878690, episode_reward=0.10 +/- 0.98
Episode length: 30.10 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.095       |
| time/                   |             |
|    total_timesteps      | 11878690    |
| train/                  |             |
|    approx_kl            | 0.023011371 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.824      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0411      |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 291      |
|    time_elapsed    | 38518    |
|    total_timesteps | 11919360 |
---------------------------------
Eval num_timesteps=11919651, episode_reward=0.13 +/- 0.97
Episode length: 30.07 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.13        |
| time/                   |             |
|    total_timesteps      | 11919651    |
| train/                  |             |
|    approx_kl            | 0.023352772 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0309      |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 292      |
|    time_elapsed    | 38651    |
|    total_timesteps | 11960320 |
---------------------------------
Eval num_timesteps=11960612, episode_reward=0.11 +/- 0.98
Episode length: 30.07 +/- 0.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 11960612    |
| train/                  |             |
|    approx_kl            | 0.023288956 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.824      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0536      |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 293      |
|    time_elapsed    | 38783    |
|    total_timesteps | 12001280 |
---------------------------------
Eval num_timesteps=12001573, episode_reward=0.16 +/- 0.97
Episode length: 30.14 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.16        |
| time/                   |             |
|    total_timesteps      | 12001573    |
| train/                  |             |
|    approx_kl            | 0.023846835 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0157      |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 294      |
|    time_elapsed    | 38915    |
|    total_timesteps | 12042240 |
---------------------------------
Eval num_timesteps=12042534, episode_reward=0.10 +/- 0.97
Episode length: 30.10 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 12042534    |
| train/                  |             |
|    approx_kl            | 0.023073714 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0401      |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 295      |
|    time_elapsed    | 39048    |
|    total_timesteps | 12083200 |
---------------------------------
Eval num_timesteps=12083495, episode_reward=0.07 +/- 0.98
Episode length: 30.09 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.0675      |
| time/                   |             |
|    total_timesteps      | 12083495    |
| train/                  |             |
|    approx_kl            | 0.024345305 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0184      |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 296      |
|    time_elapsed    | 39180    |
|    total_timesteps | 12124160 |
---------------------------------
Eval num_timesteps=12124456, episode_reward=0.11 +/- 0.98
Episode length: 30.09 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.107       |
| time/                   |             |
|    total_timesteps      | 12124456    |
| train/                  |             |
|    approx_kl            | 0.023314359 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0156      |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 297      |
|    time_elapsed    | 39313    |
|    total_timesteps | 12165120 |
---------------------------------
Eval num_timesteps=12165417, episode_reward=0.10 +/- 0.99
Episode length: 30.08 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 12165417  |
| train/                  |           |
|    approx_kl            | 0.0227204 |
|    clip_fraction        | 0.247     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.817    |
|    explained_variance   | 0.287     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.04      |
|    n_updates            | 2970      |
|    policy_gradient_loss | -0.0349   |
|    value_loss           | 0.221     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 298      |
|    time_elapsed    | 39445    |
|    total_timesteps | 12206080 |
---------------------------------
Eval num_timesteps=12206378, episode_reward=0.17 +/- 0.97
Episode length: 30.09 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.168       |
| time/                   |             |
|    total_timesteps      | 12206378    |
| train/                  |             |
|    approx_kl            | 0.023535965 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0289      |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 299      |
|    time_elapsed    | 39577    |
|    total_timesteps | 12247040 |
---------------------------------
Eval num_timesteps=12247339, episode_reward=0.17 +/- 0.98
Episode length: 30.10 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.165       |
| time/                   |             |
|    total_timesteps      | 12247339    |
| train/                  |             |
|    approx_kl            | 0.023336107 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0632      |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 300      |
|    time_elapsed    | 39709    |
|    total_timesteps | 12288000 |
---------------------------------
Eval num_timesteps=12288300, episode_reward=0.27 +/- 0.95
Episode length: 30.13 +/- 0.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 12288300   |
| train/                  |            |
|    approx_kl            | 0.02312714 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.262      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0289     |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.0343    |
|    value_loss           | 0.223      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 18
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 301      |
|    time_elapsed    | 39842    |
|    total_timesteps | 12328960 |
---------------------------------
Eval num_timesteps=12329261, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 12329261    |
| train/                  |             |
|    approx_kl            | 0.023282278 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0332      |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 302      |
|    time_elapsed    | 39975    |
|    total_timesteps | 12369920 |
---------------------------------
Eval num_timesteps=12370222, episode_reward=0.05 +/- 0.99
Episode length: 29.89 +/- 1.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 12370222    |
| train/                  |             |
|    approx_kl            | 0.023581868 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0348      |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 303      |
|    time_elapsed    | 40108    |
|    total_timesteps | 12410880 |
---------------------------------
Eval num_timesteps=12411183, episode_reward=0.00 +/- 0.99
Episode length: 30.05 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0025      |
| time/                   |             |
|    total_timesteps      | 12411183    |
| train/                  |             |
|    approx_kl            | 0.023272168 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0385      |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 304      |
|    time_elapsed    | 40241    |
|    total_timesteps | 12451840 |
---------------------------------
Eval num_timesteps=12452144, episode_reward=0.01 +/- 0.99
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.005       |
| time/                   |             |
|    total_timesteps      | 12452144    |
| train/                  |             |
|    approx_kl            | 0.022771023 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0446      |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 305      |
|    time_elapsed    | 40374    |
|    total_timesteps | 12492800 |
---------------------------------
Eval num_timesteps=12493105, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 12493105   |
| train/                  |            |
|    approx_kl            | 0.02342703 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.811     |
|    explained_variance   | 0.268      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0446     |
|    n_updates            | 3050       |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.227      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 306      |
|    time_elapsed    | 40507    |
|    total_timesteps | 12533760 |
---------------------------------
Eval num_timesteps=12534066, episode_reward=0.00 +/- 0.98
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0025      |
| time/                   |             |
|    total_timesteps      | 12534066    |
| train/                  |             |
|    approx_kl            | 0.023486305 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0254      |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 307      |
|    time_elapsed    | 40639    |
|    total_timesteps | 12574720 |
---------------------------------
Eval num_timesteps=12575027, episode_reward=0.04 +/- 0.99
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.035       |
| time/                   |             |
|    total_timesteps      | 12575027    |
| train/                  |             |
|    approx_kl            | 0.024137825 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0174      |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 308      |
|    time_elapsed    | 40772    |
|    total_timesteps | 12615680 |
---------------------------------
Eval num_timesteps=12615988, episode_reward=0.10 +/- 0.98
Episode length: 30.04 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.105      |
| time/                   |            |
|    total_timesteps      | 12615988   |
| train/                  |            |
|    approx_kl            | 0.02371047 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.809     |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.062      |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.0351    |
|    value_loss           | 0.226      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 309      |
|    time_elapsed    | 40905    |
|    total_timesteps | 12656640 |
---------------------------------
Eval num_timesteps=12656949, episode_reward=0.07 +/- 0.98
Episode length: 30.05 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.075       |
| time/                   |             |
|    total_timesteps      | 12656949    |
| train/                  |             |
|    approx_kl            | 0.023622323 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0299      |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 310      |
|    time_elapsed    | 41038    |
|    total_timesteps | 12697600 |
---------------------------------
Eval num_timesteps=12697910, episode_reward=0.02 +/- 0.99
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0175      |
| time/                   |             |
|    total_timesteps      | 12697910    |
| train/                  |             |
|    approx_kl            | 0.023604482 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.043       |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 311      |
|    time_elapsed    | 41171    |
|    total_timesteps | 12738560 |
---------------------------------
Eval num_timesteps=12738871, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0975      |
| time/                   |             |
|    total_timesteps      | 12738871    |
| train/                  |             |
|    approx_kl            | 0.024115594 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.064       |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 312      |
|    time_elapsed    | 41303    |
|    total_timesteps | 12779520 |
---------------------------------
Eval num_timesteps=12779832, episode_reward=0.04 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0425      |
| time/                   |             |
|    total_timesteps      | 12779832    |
| train/                  |             |
|    approx_kl            | 0.023153443 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0435      |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 313      |
|    time_elapsed    | 41436    |
|    total_timesteps | 12820480 |
---------------------------------
Eval num_timesteps=12820793, episode_reward=0.05 +/- 0.98
Episode length: 30.08 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.0475     |
| time/                   |            |
|    total_timesteps      | 12820793   |
| train/                  |            |
|    approx_kl            | 0.02401178 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.818     |
|    explained_variance   | 0.271      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0153     |
|    n_updates            | 3130       |
|    policy_gradient_loss | -0.0351    |
|    value_loss           | 0.224      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 314      |
|    time_elapsed    | 41568    |
|    total_timesteps | 12861440 |
---------------------------------
Eval num_timesteps=12861754, episode_reward=0.04 +/- 0.99
Episode length: 30.04 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.035      |
| time/                   |            |
|    total_timesteps      | 12861754   |
| train/                  |            |
|    approx_kl            | 0.02449039 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.821     |
|    explained_variance   | 0.263      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0252     |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 315      |
|    time_elapsed    | 41701    |
|    total_timesteps | 12902400 |
---------------------------------
Eval num_timesteps=12902715, episode_reward=0.09 +/- 0.98
Episode length: 30.06 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.085       |
| time/                   |             |
|    total_timesteps      | 12902715    |
| train/                  |             |
|    approx_kl            | 0.023995632 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0333      |
|    n_updates            | 3150        |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 316      |
|    time_elapsed    | 41834    |
|    total_timesteps | 12943360 |
---------------------------------
Eval num_timesteps=12943676, episode_reward=0.16 +/- 0.98
Episode length: 30.07 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.158       |
| time/                   |             |
|    total_timesteps      | 12943676    |
| train/                  |             |
|    approx_kl            | 0.024034683 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.000206    |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 317      |
|    time_elapsed    | 41967    |
|    total_timesteps | 12984320 |
---------------------------------
Eval num_timesteps=12984637, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.0525     |
| time/                   |            |
|    total_timesteps      | 12984637   |
| train/                  |            |
|    approx_kl            | 0.02409121 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.818     |
|    explained_variance   | 0.257      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0598     |
|    n_updates            | 3170       |
|    policy_gradient_loss | -0.035     |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 318      |
|    time_elapsed    | 42099    |
|    total_timesteps | 13025280 |
---------------------------------
Eval num_timesteps=13025598, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0675      |
| time/                   |             |
|    total_timesteps      | 13025598    |
| train/                  |             |
|    approx_kl            | 0.024658322 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.281       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.065       |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 319      |
|    time_elapsed    | 42232    |
|    total_timesteps | 13066240 |
---------------------------------
Eval num_timesteps=13066559, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 1.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.075      |
| time/                   |            |
|    total_timesteps      | 13066559   |
| train/                  |            |
|    approx_kl            | 0.02526306 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.295      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0404     |
|    n_updates            | 3190       |
|    policy_gradient_loss | -0.0352    |
|    value_loss           | 0.227      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 320      |
|    time_elapsed    | 42364    |
|    total_timesteps | 13107200 |
---------------------------------
Eval num_timesteps=13107520, episode_reward=0.02 +/- 0.99
Episode length: 30.04 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0225      |
| time/                   |             |
|    total_timesteps      | 13107520    |
| train/                  |             |
|    approx_kl            | 0.023468412 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0307      |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 321      |
|    time_elapsed    | 42497    |
|    total_timesteps | 13148160 |
---------------------------------
Eval num_timesteps=13148481, episode_reward=0.11 +/- 0.98
Episode length: 30.11 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.113       |
| time/                   |             |
|    total_timesteps      | 13148481    |
| train/                  |             |
|    approx_kl            | 0.023663886 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.051       |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 322      |
|    time_elapsed    | 42630    |
|    total_timesteps | 13189120 |
---------------------------------
Eval num_timesteps=13189442, episode_reward=-0.03 +/- 0.98
Episode length: 29.98 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 13189442    |
| train/                  |             |
|    approx_kl            | 0.023952367 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0321      |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 323      |
|    time_elapsed    | 42762    |
|    total_timesteps | 13230080 |
---------------------------------
Eval num_timesteps=13230403, episode_reward=0.06 +/- 0.98
Episode length: 30.05 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 13230403   |
| train/                  |            |
|    approx_kl            | 0.02359825 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.303      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0496     |
|    n_updates            | 3230       |
|    policy_gradient_loss | -0.035     |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 324      |
|    time_elapsed    | 42895    |
|    total_timesteps | 13271040 |
---------------------------------
Eval num_timesteps=13271364, episode_reward=0.05 +/- 0.99
Episode length: 30.04 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 13271364    |
| train/                  |             |
|    approx_kl            | 0.023812767 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0389      |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 325      |
|    time_elapsed    | 43027    |
|    total_timesteps | 13312000 |
---------------------------------
Eval num_timesteps=13312325, episode_reward=0.09 +/- 0.98
Episode length: 30.05 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.085       |
| time/                   |             |
|    total_timesteps      | 13312325    |
| train/                  |             |
|    approx_kl            | 0.023752194 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0466      |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 326      |
|    time_elapsed    | 43159    |
|    total_timesteps | 13352960 |
---------------------------------
Eval num_timesteps=13353286, episode_reward=0.15 +/- 0.97
Episode length: 30.04 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.147       |
| time/                   |             |
|    total_timesteps      | 13353286    |
| train/                  |             |
|    approx_kl            | 0.024820615 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0575      |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 327      |
|    time_elapsed    | 43292    |
|    total_timesteps | 13393920 |
---------------------------------
Eval num_timesteps=13394247, episode_reward=0.07 +/- 0.99
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 13394247    |
| train/                  |             |
|    approx_kl            | 0.023361122 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0111      |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 328      |
|    time_elapsed    | 43425    |
|    total_timesteps | 13434880 |
---------------------------------
Eval num_timesteps=13435208, episode_reward=0.03 +/- 0.98
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.025       |
| time/                   |             |
|    total_timesteps      | 13435208    |
| train/                  |             |
|    approx_kl            | 0.024764042 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0217      |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 309      |
|    iterations      | 329      |
|    time_elapsed    | 43557    |
|    total_timesteps | 13475840 |
---------------------------------
Eval num_timesteps=13476169, episode_reward=0.05 +/- 0.98
Episode length: 30.07 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.0525      |
| time/                   |             |
|    total_timesteps      | 13476169    |
| train/                  |             |
|    approx_kl            | 0.024272868 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0161      |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 330      |
|    time_elapsed    | 43690    |
|    total_timesteps | 13516800 |
---------------------------------
Eval num_timesteps=13517130, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 13517130   |
| train/                  |            |
|    approx_kl            | 0.02466422 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.827     |
|    explained_variance   | 0.27       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0648     |
|    n_updates            | 3300       |
|    policy_gradient_loss | -0.0351    |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 331      |
|    time_elapsed    | 43822    |
|    total_timesteps | 13557760 |
---------------------------------
Eval num_timesteps=13558091, episode_reward=0.12 +/- 0.99
Episode length: 30.07 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 13558091   |
| train/                  |            |
|    approx_kl            | 0.02420462 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.821     |
|    explained_variance   | 0.289      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0463     |
|    n_updates            | 3310       |
|    policy_gradient_loss | -0.0348    |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 309      |
|    iterations      | 332      |
|    time_elapsed    | 43955    |
|    total_timesteps | 13598720 |
---------------------------------
Eval num_timesteps=13599052, episode_reward=0.01 +/- 0.99
Episode length: 30.04 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0125      |
| time/                   |             |
|    total_timesteps      | 13599052    |
| train/                  |             |
|    approx_kl            | 0.024723848 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0442      |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 333      |
|    time_elapsed    | 44088    |
|    total_timesteps | 13639680 |
---------------------------------
Eval num_timesteps=13640013, episode_reward=0.11 +/- 0.98
Episode length: 30.07 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.107       |
| time/                   |             |
|    total_timesteps      | 13640013    |
| train/                  |             |
|    approx_kl            | 0.024509992 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0271      |
|    n_updates            | 3330        |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 334      |
|    time_elapsed    | 44220    |
|    total_timesteps | 13680640 |
---------------------------------
Eval num_timesteps=13680974, episode_reward=0.11 +/- 0.97
Episode length: 30.09 +/- 0.59
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30.1     |
|    mean_reward          | 0.11     |
| time/                   |          |
|    total_timesteps      | 13680974 |
| train/                  |          |
|    approx_kl            | 0.024593 |
|    clip_fraction        | 0.25     |
|    clip_range           | 0.15     |
|    entropy_loss         | -0.814   |
|    explained_variance   | 0.272    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.0252   |
|    n_updates            | 3340     |
|    policy_gradient_loss | -0.0351  |
|    value_loss           | 0.228    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 309      |
|    iterations      | 335      |
|    time_elapsed    | 44353    |
|    total_timesteps | 13721600 |
---------------------------------
Eval num_timesteps=13721935, episode_reward=0.14 +/- 0.97
Episode length: 30.06 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 13721935    |
| train/                  |             |
|    approx_kl            | 0.024139792 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0295      |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 336      |
|    time_elapsed    | 44486    |
|    total_timesteps | 13762560 |
---------------------------------
Eval num_timesteps=13762896, episode_reward=0.06 +/- 0.99
Episode length: 30.06 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 13762896    |
| train/                  |             |
|    approx_kl            | 0.025083343 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.291       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0451      |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.0357     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 337      |
|    time_elapsed    | 44618    |
|    total_timesteps | 13803520 |
---------------------------------
Eval num_timesteps=13803857, episode_reward=0.07 +/- 0.98
Episode length: 30.04 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.0725     |
| time/                   |            |
|    total_timesteps      | 13803857   |
| train/                  |            |
|    approx_kl            | 0.02451064 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.819     |
|    explained_variance   | 0.261      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0436     |
|    n_updates            | 3370       |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.23       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 338      |
|    time_elapsed    | 44751    |
|    total_timesteps | 13844480 |
---------------------------------
Eval num_timesteps=13844818, episode_reward=0.11 +/- 0.98
Episode length: 30.06 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 13844818    |
| train/                  |             |
|    approx_kl            | 0.024844302 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.031       |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.223       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 339      |
|    time_elapsed    | 44884    |
|    total_timesteps | 13885440 |
---------------------------------
Eval num_timesteps=13885779, episode_reward=0.11 +/- 0.98
Episode length: 30.08 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.107       |
| time/                   |             |
|    total_timesteps      | 13885779    |
| train/                  |             |
|    approx_kl            | 0.024599457 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0621      |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 340      |
|    time_elapsed    | 45016    |
|    total_timesteps | 13926400 |
---------------------------------
Eval num_timesteps=13926740, episode_reward=0.07 +/- 0.99
Episode length: 30.02 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.0725      |
| time/                   |             |
|    total_timesteps      | 13926740    |
| train/                  |             |
|    approx_kl            | 0.024716783 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.043       |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 341      |
|    time_elapsed    | 45149    |
|    total_timesteps | 13967360 |
---------------------------------
Eval num_timesteps=13967701, episode_reward=0.11 +/- 0.98
Episode length: 30.12 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.107       |
| time/                   |             |
|    total_timesteps      | 13967701    |
| train/                  |             |
|    approx_kl            | 0.024638923 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.813      |
|    explained_variance   | 0.281       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0086      |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 342      |
|    time_elapsed    | 45281    |
|    total_timesteps | 14008320 |
---------------------------------
Eval num_timesteps=14008662, episode_reward=0.06 +/- 0.99
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.0625     |
| time/                   |            |
|    total_timesteps      | 14008662   |
| train/                  |            |
|    approx_kl            | 0.02467163 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.813     |
|    explained_variance   | 0.271      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0571     |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 309      |
|    iterations      | 343      |
|    time_elapsed    | 45413    |
|    total_timesteps | 14049280 |
---------------------------------
Eval num_timesteps=14049623, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 1.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.085     |
| time/                   |           |
|    total_timesteps      | 14049623  |
| train/                  |           |
|    approx_kl            | 0.0250193 |
|    clip_fraction        | 0.253     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.812    |
|    explained_variance   | 0.299     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0493    |
|    n_updates            | 3430      |
|    policy_gradient_loss | -0.0353   |
|    value_loss           | 0.22      |
---------------------------------------
