CUDA Available: True
CPU Model: AMD EPYC 7313P 16-Core Processor
GPU Model: Tesla T4

num parallel processes: 4

CUDA available: True
net architecture - {'net_arch': {'pi': [128, 128, 128, 128, 128, 128, 128, 128], 'vf': [64, 64, 64, 64, 64, 64, 64, 64]}}
params: 
NUM_TIMESTEPS -500000
EVAL_FREQ=7681
EVAL_EPISODES=50
BEST_THRESHOLD=0.45
LOGDIR=scripts/rl/output/paral/base/
model params: 
 {'learning_rate': 0.0003, 'n_steps': 7680, 'n_epochs': 5, 'batch_size': 32, 'verbose': 1}
Using cuda device
starting model: scripts/rl/output/paral/base/random_start_model
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 692      |
|    iterations      | 1        |
|    time_elapsed    | 44       |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=30724, episode_reward=0.50 +/- 0.85
Episode length: 30.28 +/- 0.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.3        |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 30724       |
| train/                  |             |
|    approx_kl            | 0.019231983 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | -0.64       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00128    |
|    n_updates            | 5           |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.154       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5
SELFPLAY: new best model, bumping up generation to 1
Ep done - 500.
Ep done - 500.
Ep done - 500.
Ep done - 500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 2        |
|    time_elapsed    | 126      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61448, episode_reward=0.26 +/- 0.96
Episode length: 30.06 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.26        |
| time/                   |             |
|    total_timesteps      | 61448       |
| train/                  |             |
|    approx_kl            | 0.028117446 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.105       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00716    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0366     |
|    value_loss           | 0.163       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 466      |
|    iterations      | 3        |
|    time_elapsed    | 197      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92172, episode_reward=0.54 +/- 0.81
Episode length: 30.04 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.54        |
| time/                   |             |
|    total_timesteps      | 92172       |
| train/                  |             |
|    approx_kl            | 0.034634266 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.166       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00716    |
|    n_updates            | 15          |
|    policy_gradient_loss | -0.0371     |
|    value_loss           | 0.175       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.54
SELFPLAY: new best model, bumping up generation to 2
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 437      |
|    iterations      | 4        |
|    time_elapsed    | 281      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=122896, episode_reward=0.34 +/- 0.91
Episode length: 29.98 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.34        |
| time/                   |             |
|    total_timesteps      | 122896      |
| train/                  |             |
|    approx_kl            | 0.041806113 |
|    clip_fraction        | 0.403       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.119       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0106     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0375     |
|    value_loss           | 0.18        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 435      |
|    iterations      | 5        |
|    time_elapsed    | 352      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=153620, episode_reward=0.54 +/- 0.81
Episode length: 29.60 +/- 3.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.6        |
|    mean_reward          | 0.54        |
| time/                   |             |
|    total_timesteps      | 153620      |
| train/                  |             |
|    approx_kl            | 0.048248485 |
|    clip_fraction        | 0.431       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.149       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0226     |
|    n_updates            | 25          |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.171       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.54
SELFPLAY: new best model, bumping up generation to 3
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 426      |
|    iterations      | 6        |
|    time_elapsed    | 431      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184344, episode_reward=0.32 +/- 0.93
Episode length: 30.02 +/- 0.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.32        |
| time/                   |             |
|    total_timesteps      | 184344      |
| train/                  |             |
|    approx_kl            | 0.054574292 |
|    clip_fraction        | 0.451       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.166       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0189     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0379     |
|    value_loss           | 0.185       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 427      |
|    iterations      | 7        |
|    time_elapsed    | 503      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215068, episode_reward=0.52 +/- 0.85
Episode length: 30.06 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.52        |
| time/                   |             |
|    total_timesteps      | 215068      |
| train/                  |             |
|    approx_kl            | 0.062703855 |
|    clip_fraction        | 0.479       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.141       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00131     |
|    n_updates            | 35          |
|    policy_gradient_loss | -0.0356     |
|    value_loss           | 0.191       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.52
SELFPLAY: new best model, bumping up generation to 4
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 418      |
|    iterations      | 8        |
|    time_elapsed    | 587      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=245792, episode_reward=0.54 +/- 0.83
Episode length: 30.08 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.54       |
| time/                   |            |
|    total_timesteps      | 245792     |
| train/                  |            |
|    approx_kl            | 0.06722185 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | 0.123      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.024     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0355    |
|    value_loss           | 0.189      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.54
SELFPLAY: new best model, bumping up generation to 5
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 412      |
|    iterations      | 9        |
|    time_elapsed    | 670      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276516, episode_reward=0.40 +/- 0.92
Episode length: 29.98 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 276516      |
| train/                  |             |
|    approx_kl            | 0.074952036 |
|    clip_fraction        | 0.501       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.149       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0384      |
|    n_updates            | 45          |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.193       |
-----------------------------------------
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 414      |
|    iterations      | 10       |
|    time_elapsed    | 741      |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307240, episode_reward=0.62 +/- 0.77
Episode length: 30.14 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.62       |
| time/                   |            |
|    total_timesteps      | 307240     |
| train/                  |            |
|    approx_kl            | 0.07911266 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.165      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0115     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0332    |
|    value_loss           | 0.189      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.62
SELFPLAY: new best model, bumping up generation to 6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 409      |
|    iterations      | 11       |
|    time_elapsed    | 824      |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=337964, episode_reward=0.60 +/- 0.80
Episode length: 30.22 +/- 0.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.2       |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 337964     |
| train/                  |            |
|    approx_kl            | 0.08313169 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.69      |
|    explained_variance   | 0.145      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0321    |
|    n_updates            | 55         |
|    policy_gradient_loss | -0.0316    |
|    value_loss           | 0.204      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6
SELFPLAY: new best model, bumping up generation to 7
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 406      |
|    iterations      | 12       |
|    time_elapsed    | 907      |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=368688, episode_reward=0.52 +/- 0.85
Episode length: 30.08 +/- 0.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.52       |
| time/                   |            |
|    total_timesteps      | 368688     |
| train/                  |            |
|    approx_kl            | 0.09063151 |
|    clip_fraction        | 0.514      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.66      |
|    explained_variance   | 0.14       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0437     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0325    |
|    value_loss           | 0.192      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.52
SELFPLAY: new best model, bumping up generation to 8
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 404      |
|    iterations      | 13       |
|    time_elapsed    | 987      |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399412, episode_reward=0.28 +/- 0.94
Episode length: 30.08 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.28        |
| time/                   |             |
|    total_timesteps      | 399412      |
| train/                  |             |
|    approx_kl            | 0.094297744 |
|    clip_fraction        | 0.524       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.147       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00175    |
|    n_updates            | 65          |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.199       |
-----------------------------------------
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 406      |
|    iterations      | 14       |
|    time_elapsed    | 1059     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430136, episode_reward=0.54 +/- 0.83
Episode length: 30.02 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.54       |
| time/                   |            |
|    total_timesteps      | 430136     |
| train/                  |            |
|    approx_kl            | 0.09924863 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | 0.144      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0303    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.202      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.54
SELFPLAY: new best model, bumping up generation to 9
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 403      |
|    iterations      | 15       |
|    time_elapsed    | 1142     |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=460860, episode_reward=0.18 +/- 0.97
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.18        |
| time/                   |             |
|    total_timesteps      | 460860      |
| train/                  |             |
|    approx_kl            | 0.101611696 |
|    clip_fraction        | 0.521       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.152       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 75          |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.202       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 404      |
|    iterations      | 16       |
|    time_elapsed    | 1214     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=491584, episode_reward=0.46 +/- 0.88
Episode length: 30.10 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.46       |
| time/                   |            |
|    total_timesteps      | 491584     |
| train/                  |            |
|    approx_kl            | 0.11177973 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | 0.171      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00168   |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0278    |
|    value_loss           | 0.202      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.46
SELFPLAY: new best model, bumping up generation to 10
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 402      |
|    iterations      | 17       |
|    time_elapsed    | 1298     |
|    total_timesteps | 522240   |
---------------------------------
/home/student/pantrasa/project/venv2/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'scripts/rl/output/paral/base' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Elapsed time: 0h 22m 23s
