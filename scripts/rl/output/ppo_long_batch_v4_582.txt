CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
2024-06-09 23:20:37.654943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-09 23:20:37.797881: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-06-09 23:20:38.676456: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-09 23:20:38.676548: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-09 23:20:38.676559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
CUDA available: True
net architecture - {'net_arch': {'pi': [64, 64, 64, 64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64]}}
params: 
NUM_TIMESTEPS -100000000
EVAL_FREQ=61441
EVAL_EPISODES=500
BEST_THRESHOLD=0.14
LOGDIR=scripts/rl/output/v4/
model params: 
 {'learning_rate': <function linear_schedule.<locals>.func at 0x7fe9dfc9df70>, 'n_steps': 61440, 'n_epochs': 5, 'clip_range': 0.17, 'batch_size': 32, 'ent_coef': 0.01, 'gae_lambda': 0.95, 'verbose': 1}
Using cuda device
Wrapping the env in a DummyVecEnv.
starting model: scripts/rl/output/v4/random_start_model
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 1        |
|    time_elapsed    | 382      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61441, episode_reward=0.03 +/- 0.98
Episode length: 29.94 +/- 1.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 61441       |
| train/                  |             |
|    approx_kl            | 0.009344334 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.99       |
|    explained_variance   | -0.217      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0752      |
|    n_updates            | 5           |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.186       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 2        |
|    time_elapsed    | 677      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=122882, episode_reward=0.17 +/- 0.96
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.174       |
| time/                   |             |
|    total_timesteps      | 122882      |
| train/                  |             |
|    approx_kl            | 0.011653193 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.192       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.174
SELFPLAY: new best model, bumping up generation to 1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 3        |
|    time_elapsed    | 1054     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184323, episode_reward=0.11 +/- 0.97
Episode length: 30.01 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 184323      |
| train/                  |             |
|    approx_kl            | 0.012940647 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.226       |
|    learning_rate        | 0.000299    |
|    loss                 | 0.034       |
|    n_updates            | 15          |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.197       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 4        |
|    time_elapsed    | 1467     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=245764, episode_reward=0.05 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 245764      |
| train/                  |             |
|    approx_kl            | 0.015292327 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.000299    |
|    loss                 | 0.0584      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.197       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 5        |
|    time_elapsed    | 1893     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307205, episode_reward=0.24 +/- 0.95
Episode length: 29.94 +/- 1.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.242       |
| time/                   |             |
|    total_timesteps      | 307205      |
| train/                  |             |
|    approx_kl            | 0.016799781 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.000299    |
|    loss                 | 0.0535      |
|    n_updates            | 25          |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.198       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.242
SELFPLAY: new best model, bumping up generation to 2
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 6        |
|    time_elapsed    | 2319     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=368646, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 368646      |
| train/                  |             |
|    approx_kl            | 0.018659238 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.243       |
|    learning_rate        | 0.000299    |
|    loss                 | 0.0328      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.199       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 7        |
|    time_elapsed    | 2735     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430087, episode_reward=0.15 +/- 0.97
Episode length: 30.04 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.152       |
| time/                   |             |
|    total_timesteps      | 430087      |
| train/                  |             |
|    approx_kl            | 0.020913223 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.257       |
|    learning_rate        | 0.000299    |
|    loss                 | -0.00287    |
|    n_updates            | 35          |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.205       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.152
SELFPLAY: new best model, bumping up generation to 3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 8        |
|    time_elapsed    | 3158     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=491528, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 491528      |
| train/                  |             |
|    approx_kl            | 0.022004345 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.000299    |
|    loss                 | 0.0307      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.201       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 9        |
|    time_elapsed    | 3579     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=552969, episode_reward=0.10 +/- 0.97
Episode length: 29.96 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 552969     |
| train/                  |            |
|    approx_kl            | 0.02322592 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.85      |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.000298   |
|    loss                 | 0.00566    |
|    n_updates            | 45         |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.203      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 10       |
|    time_elapsed    | 4001     |
|    total_timesteps | 614400   |
---------------------------------
Eval num_timesteps=614410, episode_reward=0.15 +/- 0.97
Episode length: 29.97 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 614410      |
| train/                  |             |
|    approx_kl            | 0.025672236 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.000298    |
|    loss                 | 0.0526      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.199       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.15
SELFPLAY: new best model, bumping up generation to 4
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 11       |
|    time_elapsed    | 4420     |
|    total_timesteps | 675840   |
---------------------------------
Eval num_timesteps=675851, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 675851      |
| train/                  |             |
|    approx_kl            | 0.026759855 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.000298    |
|    loss                 | 0.0873      |
|    n_updates            | 55          |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.206       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 12       |
|    time_elapsed    | 4836     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=737292, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 737292      |
| train/                  |             |
|    approx_kl            | 0.027892912 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.000298    |
|    loss                 | 0.0299      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.213       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 13       |
|    time_elapsed    | 5257     |
|    total_timesteps | 798720   |
---------------------------------
Eval num_timesteps=798733, episode_reward=0.12 +/- 0.97
Episode length: 30.01 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 798733     |
| train/                  |            |
|    approx_kl            | 0.02963888 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.254      |
|    learning_rate        | 0.000298   |
|    loss                 | 0.117      |
|    n_updates            | 65         |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.21       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 14       |
|    time_elapsed    | 5673     |
|    total_timesteps | 860160   |
---------------------------------
Eval num_timesteps=860174, episode_reward=0.20 +/- 0.95
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 860174      |
| train/                  |             |
|    approx_kl            | 0.030409573 |
|    clip_fraction        | 0.369       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.000297    |
|    loss                 | 0.0348      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.202       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 5
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 15       |
|    time_elapsed    | 6090     |
|    total_timesteps | 921600   |
---------------------------------
Eval num_timesteps=921615, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 921615      |
| train/                  |             |
|    approx_kl            | 0.031944387 |
|    clip_fraction        | 0.371       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.00696    |
|    n_updates            | 75          |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 150      |
|    iterations      | 16       |
|    time_elapsed    | 6510     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=983056, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 983056     |
| train/                  |            |
|    approx_kl            | 0.03295717 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.279      |
|    learning_rate        | 0.000297   |
|    loss                 | -0.0108    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.211      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 17       |
|    time_elapsed    | 6926     |
|    total_timesteps | 1044480  |
---------------------------------
Eval num_timesteps=1044497, episode_reward=0.18 +/- 0.97
Episode length: 29.94 +/- 1.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.182       |
| time/                   |             |
|    total_timesteps      | 1044497     |
| train/                  |             |
|    approx_kl            | 0.034603264 |
|    clip_fraction        | 0.381       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.000297    |
|    loss                 | 0.0349      |
|    n_updates            | 85          |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.209       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.182
SELFPLAY: new best model, bumping up generation to 6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 18       |
|    time_elapsed    | 7342     |
|    total_timesteps | 1105920  |
---------------------------------
Eval num_timesteps=1105938, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 1105938     |
| train/                  |             |
|    approx_kl            | 0.034366027 |
|    clip_fraction        | 0.373       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.303       |
|    learning_rate        | 0.000297    |
|    loss                 | 0.114       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.209       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 150      |
|    iterations      | 19       |
|    time_elapsed    | 7763     |
|    total_timesteps | 1167360  |
---------------------------------
Eval num_timesteps=1167379, episode_reward=0.01 +/- 0.98
Episode length: 29.92 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 1167379     |
| train/                  |             |
|    approx_kl            | 0.036160216 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.000296    |
|    loss                 | 0.0935      |
|    n_updates            | 95          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 20       |
|    time_elapsed    | 8179     |
|    total_timesteps | 1228800  |
---------------------------------
Eval num_timesteps=1228820, episode_reward=0.11 +/- 0.97
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 1228820     |
| train/                  |             |
|    approx_kl            | 0.038567644 |
|    clip_fraction        | 0.388       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.000296    |
|    loss                 | 0.077       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 21       |
|    time_elapsed    | 8603     |
|    total_timesteps | 1290240  |
---------------------------------
Eval num_timesteps=1290261, episode_reward=0.19 +/- 0.97
Episode length: 30.02 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.192      |
| time/                   |            |
|    total_timesteps      | 1290261    |
| train/                  |            |
|    approx_kl            | 0.03951366 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.62      |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.000296   |
|    loss                 | 0.0166     |
|    n_updates            | 105        |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 0.216      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.192
SELFPLAY: new best model, bumping up generation to 7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 22       |
|    time_elapsed    | 9020     |
|    total_timesteps | 1351680  |
---------------------------------
Eval num_timesteps=1351702, episode_reward=0.10 +/- 0.98
Episode length: 30.03 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 1351702    |
| train/                  |            |
|    approx_kl            | 0.03850581 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.6       |
|    explained_variance   | 0.279      |
|    learning_rate        | 0.000296   |
|    loss                 | 0.0649     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 0.217      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 23       |
|    time_elapsed    | 9435     |
|    total_timesteps | 1413120  |
---------------------------------
Eval num_timesteps=1413143, episode_reward=0.13 +/- 0.97
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.132       |
| time/                   |             |
|    total_timesteps      | 1413143     |
| train/                  |             |
|    approx_kl            | 0.040032037 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.000296    |
|    loss                 | 0.0215      |
|    n_updates            | 115         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 24       |
|    time_elapsed    | 9855     |
|    total_timesteps | 1474560  |
---------------------------------
Eval num_timesteps=1474584, episode_reward=0.20 +/- 0.96
Episode length: 30.03 +/- 1.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.202       |
| time/                   |             |
|    total_timesteps      | 1474584     |
| train/                  |             |
|    approx_kl            | 0.040833607 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.257       |
|    learning_rate        | 0.000296    |
|    loss                 | 0.057       |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00983    |
|    value_loss           | 0.22        |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.202
SELFPLAY: new best model, bumping up generation to 8
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 25       |
|    time_elapsed    | 10268    |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1536025, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.018     |
| time/                   |            |
|    total_timesteps      | 1536025    |
| train/                  |            |
|    approx_kl            | 0.04192899 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.53      |
|    explained_variance   | 0.267      |
|    learning_rate        | 0.000295   |
|    loss                 | 0.0738     |
|    n_updates            | 125        |
|    policy_gradient_loss | -0.00867   |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 149      |
|    iterations      | 26       |
|    time_elapsed    | 10687    |
|    total_timesteps | 1597440  |
---------------------------------
Eval num_timesteps=1597466, episode_reward=-0.00 +/- 0.98
Episode length: 30.01 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 1597466    |
| train/                  |            |
|    approx_kl            | 0.04295319 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.52      |
|    explained_variance   | 0.283      |
|    learning_rate        | 0.000295   |
|    loss                 | 0.057      |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.00942   |
|    value_loss           | 0.227      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 27       |
|    time_elapsed    | 11110    |
|    total_timesteps | 1658880  |
---------------------------------
Eval num_timesteps=1658907, episode_reward=0.07 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 1658907     |
| train/                  |             |
|    approx_kl            | 0.042764843 |
|    clip_fraction        | 0.391       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.000295    |
|    loss                 | 0.148       |
|    n_updates            | 135         |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 149      |
|    iterations      | 28       |
|    time_elapsed    | 11525    |
|    total_timesteps | 1720320  |
---------------------------------
Eval num_timesteps=1720348, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 1720348     |
| train/                  |             |
|    approx_kl            | 0.042848688 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.000295    |
|    loss                 | 0.127       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 29       |
|    time_elapsed    | 11946    |
|    total_timesteps | 1781760  |
---------------------------------
Eval num_timesteps=1781789, episode_reward=0.12 +/- 0.98
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 1781789     |
| train/                  |             |
|    approx_kl            | 0.043147303 |
|    clip_fraction        | 0.386       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.301       |
|    learning_rate        | 0.000295    |
|    loss                 | -0.0177     |
|    n_updates            | 145         |
|    policy_gradient_loss | -0.00863    |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 30       |
|    time_elapsed    | 12368    |
|    total_timesteps | 1843200  |
---------------------------------
Eval num_timesteps=1843230, episode_reward=0.25 +/- 0.95
Episode length: 29.97 +/- 1.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.25        |
| time/                   |             |
|    total_timesteps      | 1843230     |
| train/                  |             |
|    approx_kl            | 0.044220146 |
|    clip_fraction        | 0.383       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.29        |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0513      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00754    |
|    value_loss           | 0.219       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.25
SELFPLAY: new best model, bumping up generation to 9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 31       |
|    time_elapsed    | 12788    |
|    total_timesteps | 1904640  |
---------------------------------
Eval num_timesteps=1904671, episode_reward=0.07 +/- 0.98
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 1904671     |
| train/                  |             |
|    approx_kl            | 0.042544708 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0451      |
|    n_updates            | 155         |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 32       |
|    time_elapsed    | 13199    |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1966112, episode_reward=0.08 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 1966112     |
| train/                  |             |
|    approx_kl            | 0.042061917 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0542      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 33       |
|    time_elapsed    | 13612    |
|    total_timesteps | 2027520  |
---------------------------------
Eval num_timesteps=2027553, episode_reward=0.10 +/- 0.99
Episode length: 29.92 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 2027553     |
| train/                  |             |
|    approx_kl            | 0.043467678 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0687      |
|    n_updates            | 165         |
|    policy_gradient_loss | -0.00819    |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 34       |
|    time_elapsed    | 14032    |
|    total_timesteps | 2088960  |
---------------------------------
Eval num_timesteps=2088994, episode_reward=0.20 +/- 0.96
Episode length: 30.03 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.202       |
| time/                   |             |
|    total_timesteps      | 2088994     |
| train/                  |             |
|    approx_kl            | 0.044686604 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0639      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00752    |
|    value_loss           | 0.228       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.202
SELFPLAY: new best model, bumping up generation to 10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 148      |
|    iterations      | 35       |
|    time_elapsed    | 14450    |
|    total_timesteps | 2150400  |
---------------------------------
Eval num_timesteps=2150435, episode_reward=-0.03 +/- 0.98
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.028     |
| time/                   |            |
|    total_timesteps      | 2150435    |
| train/                  |            |
|    approx_kl            | 0.04363891 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.319      |
|    learning_rate        | 0.000294   |
|    loss                 | 0.0214     |
|    n_updates            | 175        |
|    policy_gradient_loss | -0.00704   |
|    value_loss           | 0.23       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 36       |
|    time_elapsed    | 14869    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2211876, episode_reward=-0.01 +/- 0.98
Episode length: 29.92 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 2211876     |
| train/                  |             |
|    approx_kl            | 0.044692215 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.000293    |
|    loss                 | 0.0584      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00681    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 37       |
|    time_elapsed    | 15287    |
|    total_timesteps | 2273280  |
---------------------------------
Eval num_timesteps=2273317, episode_reward=0.10 +/- 0.97
Episode length: 30.01 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 2273317    |
| train/                  |            |
|    approx_kl            | 0.04577876 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000293   |
|    loss                 | 0.0635     |
|    n_updates            | 185        |
|    policy_gradient_loss | -0.00749   |
|    value_loss           | 0.22       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 38       |
|    time_elapsed    | 15703    |
|    total_timesteps | 2334720  |
---------------------------------
Eval num_timesteps=2334758, episode_reward=0.13 +/- 0.98
Episode length: 29.97 +/- 1.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.128      |
| time/                   |            |
|    total_timesteps      | 2334758    |
| train/                  |            |
|    approx_kl            | 0.04215718 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.318      |
|    learning_rate        | 0.000293   |
|    loss                 | 0.055      |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00601   |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 148      |
|    iterations      | 39       |
|    time_elapsed    | 16120    |
|    total_timesteps | 2396160  |
---------------------------------
Eval num_timesteps=2396199, episode_reward=0.16 +/- 0.96
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.156       |
| time/                   |             |
|    total_timesteps      | 2396199     |
| train/                  |             |
|    approx_kl            | 0.044522244 |
|    clip_fraction        | 0.374       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.000293    |
|    loss                 | 0.12        |
|    n_updates            | 195         |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 0.224       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.156
SELFPLAY: new best model, bumping up generation to 11
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 40       |
|    time_elapsed    | 16500    |
|    total_timesteps | 2457600  |
---------------------------------
Eval num_timesteps=2457640, episode_reward=-0.04 +/- 0.98
Episode length: 29.91 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.044      |
| time/                   |             |
|    total_timesteps      | 2457640     |
| train/                  |             |
|    approx_kl            | 0.042242393 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.000293    |
|    loss                 | 0.0381      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 41       |
|    time_elapsed    | 16881    |
|    total_timesteps | 2519040  |
---------------------------------
Eval num_timesteps=2519081, episode_reward=0.02 +/- 0.98
Episode length: 30.04 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.024      |
| time/                   |            |
|    total_timesteps      | 2519081    |
| train/                  |            |
|    approx_kl            | 0.04305813 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000292   |
|    loss                 | 0.0831     |
|    n_updates            | 205        |
|    policy_gradient_loss | -0.00729   |
|    value_loss           | 0.227      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 149      |
|    iterations      | 42       |
|    time_elapsed    | 17262    |
|    total_timesteps | 2580480  |
---------------------------------
Eval num_timesteps=2580522, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 2580522     |
| train/                  |             |
|    approx_kl            | 0.044493254 |
|    clip_fraction        | 0.366       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000292    |
|    loss                 | 0.131       |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 149      |
|    iterations      | 43       |
|    time_elapsed    | 17641    |
|    total_timesteps | 2641920  |
---------------------------------
Eval num_timesteps=2641963, episode_reward=0.14 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 2641963     |
| train/                  |             |
|    approx_kl            | 0.045018453 |
|    clip_fraction        | 0.371       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000292    |
|    loss                 | 0.0156      |
|    n_updates            | 215         |
|    policy_gradient_loss | -0.00631    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 44       |
|    time_elapsed    | 18023    |
|    total_timesteps | 2703360  |
---------------------------------
Eval num_timesteps=2703404, episode_reward=0.11 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 2703404     |
| train/                  |             |
|    approx_kl            | 0.045599524 |
|    clip_fraction        | 0.374       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.306       |
|    learning_rate        | 0.000292    |
|    loss                 | 0.017       |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00633    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 150      |
|    iterations      | 45       |
|    time_elapsed    | 18404    |
|    total_timesteps | 2764800  |
---------------------------------
Eval num_timesteps=2764845, episode_reward=0.06 +/- 0.98
Episode length: 30.01 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 2764845    |
| train/                  |            |
|    approx_kl            | 0.04534928 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.000292   |
|    loss                 | 0.106      |
|    n_updates            | 225        |
|    policy_gradient_loss | -0.00591   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 150      |
|    iterations      | 46       |
|    time_elapsed    | 18786    |
|    total_timesteps | 2826240  |
---------------------------------
Eval num_timesteps=2826286, episode_reward=0.16 +/- 0.97
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.162       |
| time/                   |             |
|    total_timesteps      | 2826286     |
| train/                  |             |
|    approx_kl            | 0.045214567 |
|    clip_fraction        | 0.364       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.000292    |
|    loss                 | 0.0715      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00578    |
|    value_loss           | 0.229       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.162
SELFPLAY: new best model, bumping up generation to 12
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 47       |
|    time_elapsed    | 19168    |
|    total_timesteps | 2887680  |
---------------------------------
Eval num_timesteps=2887727, episode_reward=0.01 +/- 0.99
Episode length: 29.94 +/- 1.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 2887727     |
| train/                  |             |
|    approx_kl            | 0.045312133 |
|    clip_fraction        | 0.368       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000291    |
|    loss                 | 0.000235    |
|    n_updates            | 235         |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 48       |
|    time_elapsed    | 19545    |
|    total_timesteps | 2949120  |
---------------------------------
Eval num_timesteps=2949168, episode_reward=0.09 +/- 0.98
Episode length: 29.96 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 2949168     |
| train/                  |             |
|    approx_kl            | 0.042594068 |
|    clip_fraction        | 0.36        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.000291    |
|    loss                 | 0.0868      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 49       |
|    time_elapsed    | 19923    |
|    total_timesteps | 3010560  |
---------------------------------
Eval num_timesteps=3010609, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.096     |
| time/                   |           |
|    total_timesteps      | 3010609   |
| train/                  |           |
|    approx_kl            | 0.0436664 |
|    clip_fraction        | 0.358     |
|    clip_range           | 0.17      |
|    entropy_loss         | -1.26     |
|    explained_variance   | 0.338     |
|    learning_rate        | 0.000291  |
|    loss                 | 0.157     |
|    n_updates            | 245       |
|    policy_gradient_loss | -0.00576  |
|    value_loss           | 0.227     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 50       |
|    time_elapsed    | 20306    |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3072050, episode_reward=-0.00 +/- 0.99
Episode length: 29.93 +/- 1.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 3072050     |
| train/                  |             |
|    approx_kl            | 0.044346906 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000291    |
|    loss                 | 0.0225      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 51       |
|    time_elapsed    | 20686    |
|    total_timesteps | 3133440  |
---------------------------------
Eval num_timesteps=3133491, episode_reward=0.16 +/- 0.97
Episode length: 30.00 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.158      |
| time/                   |            |
|    total_timesteps      | 3133491    |
| train/                  |            |
|    approx_kl            | 0.04571503 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000291   |
|    loss                 | 0.0862     |
|    n_updates            | 255        |
|    policy_gradient_loss | -0.00466   |
|    value_loss           | 0.231      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.158
SELFPLAY: new best model, bumping up generation to 13
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 151      |
|    iterations      | 52       |
|    time_elapsed    | 21065    |
|    total_timesteps | 3194880  |
---------------------------------
Eval num_timesteps=3194932, episode_reward=-0.03 +/- 0.99
Episode length: 29.98 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 3194932     |
| train/                  |             |
|    approx_kl            | 0.046573162 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.00029     |
|    loss                 | 0.035       |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 151      |
|    iterations      | 53       |
|    time_elapsed    | 21446    |
|    total_timesteps | 3256320  |
---------------------------------
Eval num_timesteps=3256373, episode_reward=-0.03 +/- 0.99
Episode length: 29.93 +/- 1.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.028     |
| time/                   |            |
|    total_timesteps      | 3256373    |
| train/                  |            |
|    approx_kl            | 0.04792881 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.00029    |
|    loss                 | 0.116      |
|    n_updates            | 265        |
|    policy_gradient_loss | -0.00442   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 54       |
|    time_elapsed    | 21824    |
|    total_timesteps | 3317760  |
---------------------------------
Eval num_timesteps=3317814, episode_reward=0.00 +/- 0.99
Episode length: 29.93 +/- 1.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 3317814     |
| train/                  |             |
|    approx_kl            | 0.047870018 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.00029     |
|    loss                 | 0.0393      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00488    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 152      |
|    iterations      | 55       |
|    time_elapsed    | 22202    |
|    total_timesteps | 3379200  |
---------------------------------
Eval num_timesteps=3379255, episode_reward=0.01 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 3379255     |
| train/                  |             |
|    approx_kl            | 0.049138777 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.00029     |
|    loss                 | 0.0575      |
|    n_updates            | 275         |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 56       |
|    time_elapsed    | 22584    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3440696, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 3440696    |
| train/                  |            |
|    approx_kl            | 0.04871285 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.00029    |
|    loss                 | 0.102      |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.00458   |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 57       |
|    time_elapsed    | 22962    |
|    total_timesteps | 3502080  |
---------------------------------
Eval num_timesteps=3502137, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 3502137     |
| train/                  |             |
|    approx_kl            | 0.048890267 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000289    |
|    loss                 | 0.106       |
|    n_updates            | 285         |
|    policy_gradient_loss | -0.00456    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 58       |
|    time_elapsed    | 23339    |
|    total_timesteps | 3563520  |
---------------------------------
Eval num_timesteps=3563578, episode_reward=0.01 +/- 0.99
Episode length: 29.95 +/- 0.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 3563578    |
| train/                  |            |
|    approx_kl            | 0.04828674 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.000289   |
|    loss                 | 0.0881     |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00486   |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 59       |
|    time_elapsed    | 23721    |
|    total_timesteps | 3624960  |
---------------------------------
Eval num_timesteps=3625019, episode_reward=0.11 +/- 0.98
Episode length: 30.06 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 3625019     |
| train/                  |             |
|    approx_kl            | 0.047334902 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000289    |
|    loss                 | 0.0951      |
|    n_updates            | 295         |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 60       |
|    time_elapsed    | 24098    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3686460, episode_reward=0.07 +/- 0.99
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 3686460     |
| train/                  |             |
|    approx_kl            | 0.047668424 |
|    clip_fraction        | 0.359       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000289    |
|    loss                 | 0.142       |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 61       |
|    time_elapsed    | 24474    |
|    total_timesteps | 3747840  |
---------------------------------
Eval num_timesteps=3747901, episode_reward=0.11 +/- 0.98
Episode length: 30.07 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 3747901     |
| train/                  |             |
|    approx_kl            | 0.048686154 |
|    clip_fraction        | 0.36        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000289    |
|    loss                 | 0.132       |
|    n_updates            | 305         |
|    policy_gradient_loss | -0.00466    |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 62       |
|    time_elapsed    | 24857    |
|    total_timesteps | 3809280  |
---------------------------------
Eval num_timesteps=3809342, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 3809342    |
| train/                  |            |
|    approx_kl            | 0.04885006 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.000289   |
|    loss                 | 0.233      |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.00422   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 63       |
|    time_elapsed    | 25235    |
|    total_timesteps | 3870720  |
---------------------------------
Eval num_timesteps=3870783, episode_reward=0.05 +/- 0.98
Episode length: 29.93 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 3870783     |
| train/                  |             |
|    approx_kl            | 0.049648747 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.049       |
|    n_updates            | 315         |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 64       |
|    time_elapsed    | 25616    |
|    total_timesteps | 3932160  |
---------------------------------
Eval num_timesteps=3932224, episode_reward=0.17 +/- 0.98
Episode length: 29.96 +/- 1.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.17        |
| time/                   |             |
|    total_timesteps      | 3932224     |
| train/                  |             |
|    approx_kl            | 0.047213856 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.0671      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 0.225       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.17
SELFPLAY: new best model, bumping up generation to 14
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 65       |
|    time_elapsed    | 25993    |
|    total_timesteps | 3993600  |
---------------------------------
Eval num_timesteps=3993665, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 3993665     |
| train/                  |             |
|    approx_kl            | 0.046078824 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.0286      |
|    n_updates            | 325         |
|    policy_gradient_loss | -0.00428    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 66       |
|    time_elapsed    | 26373    |
|    total_timesteps | 4055040  |
---------------------------------
Eval num_timesteps=4055106, episode_reward=0.02 +/- 0.99
Episode length: 29.86 +/- 1.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 4055106     |
| train/                  |             |
|    approx_kl            | 0.048261605 |
|    clip_fraction        | 0.357       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.082       |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 67       |
|    time_elapsed    | 26753    |
|    total_timesteps | 4116480  |
---------------------------------
Eval num_timesteps=4116547, episode_reward=0.02 +/- 0.99
Episode length: 29.89 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 4116547     |
| train/                  |             |
|    approx_kl            | 0.046538305 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.0954      |
|    n_updates            | 335         |
|    policy_gradient_loss | -0.00442    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 68       |
|    time_elapsed    | 27128    |
|    total_timesteps | 4177920  |
---------------------------------
Eval num_timesteps=4177988, episode_reward=0.04 +/- 0.99
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 4177988    |
| train/                  |            |
|    approx_kl            | 0.04716132 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000287   |
|    loss                 | 0.152      |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.00396   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 69       |
|    time_elapsed    | 27508    |
|    total_timesteps | 4239360  |
---------------------------------
Eval num_timesteps=4239429, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 4239429     |
| train/                  |             |
|    approx_kl            | 0.048003443 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0601      |
|    n_updates            | 345         |
|    policy_gradient_loss | -0.00404    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 70       |
|    time_elapsed    | 27856    |
|    total_timesteps | 4300800  |
---------------------------------
Eval num_timesteps=4300870, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 4300870     |
| train/                  |             |
|    approx_kl            | 0.052191656 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0639      |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00397    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 71       |
|    time_elapsed    | 28204    |
|    total_timesteps | 4362240  |
---------------------------------
Eval num_timesteps=4362311, episode_reward=0.08 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 4362311     |
| train/                  |             |
|    approx_kl            | 0.047780372 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.124       |
|    n_updates            | 355         |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 154      |
|    iterations      | 72       |
|    time_elapsed    | 28551    |
|    total_timesteps | 4423680  |
---------------------------------
Eval num_timesteps=4423752, episode_reward=0.05 +/- 0.98
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 4423752     |
| train/                  |             |
|    approx_kl            | 0.047638156 |
|    clip_fraction        | 0.347       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0865      |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 73       |
|    time_elapsed    | 28899    |
|    total_timesteps | 4485120  |
---------------------------------
Eval num_timesteps=4485193, episode_reward=0.10 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 4485193     |
| train/                  |             |
|    approx_kl            | 0.048688844 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0338      |
|    n_updates            | 365         |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 74       |
|    time_elapsed    | 29248    |
|    total_timesteps | 4546560  |
---------------------------------
Eval num_timesteps=4546634, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 4546634     |
| train/                  |             |
|    approx_kl            | 0.046071593 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000286    |
|    loss                 | 0.0666      |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 75       |
|    time_elapsed    | 29599    |
|    total_timesteps | 4608000  |
---------------------------------
Eval num_timesteps=4608075, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 4608075     |
| train/                  |             |
|    approx_kl            | 0.046900924 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000286    |
|    loss                 | 0.149       |
|    n_updates            | 375         |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 76       |
|    time_elapsed    | 29949    |
|    total_timesteps | 4669440  |
---------------------------------
Eval num_timesteps=4669516, episode_reward=0.18 +/- 0.97
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.182       |
| time/                   |             |
|    total_timesteps      | 4669516     |
| train/                  |             |
|    approx_kl            | 0.048151013 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000286    |
|    loss                 | 0.146       |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00407    |
|    value_loss           | 0.234       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.182
SELFPLAY: new best model, bumping up generation to 15
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 77       |
|    time_elapsed    | 30298    |
|    total_timesteps | 4730880  |
---------------------------------
Eval num_timesteps=4730957, episode_reward=-0.06 +/- 0.98
Episode length: 30.00 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.06      |
| time/                   |            |
|    total_timesteps      | 4730957    |
| train/                  |            |
|    approx_kl            | 0.05025997 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000286   |
|    loss                 | 0.073      |
|    n_updates            | 385        |
|    policy_gradient_loss | -0.00256   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 78       |
|    time_elapsed    | 30646    |
|    total_timesteps | 4792320  |
---------------------------------
Eval num_timesteps=4792398, episode_reward=0.09 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 4792398     |
| train/                  |             |
|    approx_kl            | 0.049117204 |
|    clip_fraction        | 0.347       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000286    |
|    loss                 | 0.0569      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 79       |
|    time_elapsed    | 30995    |
|    total_timesteps | 4853760  |
---------------------------------
Eval num_timesteps=4853839, episode_reward=0.02 +/- 0.99
Episode length: 29.93 +/- 1.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 4853839     |
| train/                  |             |
|    approx_kl            | 0.047663327 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000285    |
|    loss                 | 0.0596      |
|    n_updates            | 395         |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 80       |
|    time_elapsed    | 31343    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4915280, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 4915280     |
| train/                  |             |
|    approx_kl            | 0.048022985 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000285    |
|    loss                 | 0.158       |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 81       |
|    time_elapsed    | 31693    |
|    total_timesteps | 4976640  |
---------------------------------
Eval num_timesteps=4976721, episode_reward=0.05 +/- 0.98
Episode length: 29.90 +/- 1.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 4976721    |
| train/                  |            |
|    approx_kl            | 0.04995929 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.000285   |
|    loss                 | 0.0882     |
|    n_updates            | 405        |
|    policy_gradient_loss | -0.00293   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 82       |
|    time_elapsed    | 32040    |
|    total_timesteps | 5038080  |
---------------------------------
Eval num_timesteps=5038162, episode_reward=-0.03 +/- 0.99
Episode length: 29.91 +/- 1.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 5038162    |
| train/                  |            |
|    approx_kl            | 0.04772352 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.324      |
|    learning_rate        | 0.000285   |
|    loss                 | 0.0688     |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.00329   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 83       |
|    time_elapsed    | 32388    |
|    total_timesteps | 5099520  |
---------------------------------
Eval num_timesteps=5099603, episode_reward=0.02 +/- 0.98
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 5099603     |
| train/                  |             |
|    approx_kl            | 0.048069775 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000285    |
|    loss                 | 0.0216      |
|    n_updates            | 415         |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 84       |
|    time_elapsed    | 32736    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5161044, episode_reward=0.00 +/- 0.98
Episode length: 29.89 +/- 1.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5161044    |
| train/                  |            |
|    approx_kl            | 0.05061628 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.000285   |
|    loss                 | 0.107      |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.00337   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 85       |
|    time_elapsed    | 33085    |
|    total_timesteps | 5222400  |
---------------------------------
Eval num_timesteps=5222485, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 5222485     |
| train/                  |             |
|    approx_kl            | 0.049609866 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000284    |
|    loss                 | 0.0724      |
|    n_updates            | 425         |
|    policy_gradient_loss | -0.00265    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 86       |
|    time_elapsed    | 33436    |
|    total_timesteps | 5283840  |
---------------------------------
Eval num_timesteps=5283926, episode_reward=0.04 +/- 0.99
Episode length: 29.94 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 5283926     |
| train/                  |             |
|    approx_kl            | 0.050042104 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000284    |
|    loss                 | 0.0225      |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 87       |
|    time_elapsed    | 33785    |
|    total_timesteps | 5345280  |
---------------------------------
Eval num_timesteps=5345367, episode_reward=0.07 +/- 0.99
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.072      |
| time/                   |            |
|    total_timesteps      | 5345367    |
| train/                  |            |
|    approx_kl            | 0.04908633 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.000284   |
|    loss                 | 0.15       |
|    n_updates            | 435        |
|    policy_gradient_loss | -0.00286   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 88       |
|    time_elapsed    | 34133    |
|    total_timesteps | 5406720  |
---------------------------------
Eval num_timesteps=5406808, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 5406808     |
| train/                  |             |
|    approx_kl            | 0.048031583 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.000284    |
|    loss                 | 0.094       |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 89       |
|    time_elapsed    | 34481    |
|    total_timesteps | 5468160  |
---------------------------------
Eval num_timesteps=5468249, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 5468249     |
| train/                  |             |
|    approx_kl            | 0.049083855 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.000284    |
|    loss                 | 0.139       |
|    n_updates            | 445         |
|    policy_gradient_loss | -0.00204    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 90       |
|    time_elapsed    | 34830    |
|    total_timesteps | 5529600  |
---------------------------------
Eval num_timesteps=5529690, episode_reward=0.06 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 5529690     |
| train/                  |             |
|    approx_kl            | 0.049577627 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000283    |
|    loss                 | 0.101       |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 91       |
|    time_elapsed    | 35179    |
|    total_timesteps | 5591040  |
---------------------------------
Eval num_timesteps=5591131, episode_reward=0.08 +/- 0.99
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 5591131     |
| train/                  |             |
|    approx_kl            | 0.049852517 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000283    |
|    loss                 | 0.0857      |
|    n_updates            | 455         |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 92       |
|    time_elapsed    | 35527    |
|    total_timesteps | 5652480  |
---------------------------------
Eval num_timesteps=5652572, episode_reward=0.01 +/- 0.99
Episode length: 29.92 +/- 0.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.006      |
| time/                   |            |
|    total_timesteps      | 5652572    |
| train/                  |            |
|    approx_kl            | 0.04994645 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000283   |
|    loss                 | 0.0632     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.00189   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 93       |
|    time_elapsed    | 35874    |
|    total_timesteps | 5713920  |
---------------------------------
Eval num_timesteps=5714013, episode_reward=0.09 +/- 0.99
Episode length: 29.96 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.092      |
| time/                   |            |
|    total_timesteps      | 5714013    |
| train/                  |            |
|    approx_kl            | 0.05191389 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.000283   |
|    loss                 | 0.111      |
|    n_updates            | 465        |
|    policy_gradient_loss | -0.0019    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 94       |
|    time_elapsed    | 36221    |
|    total_timesteps | 5775360  |
---------------------------------
Eval num_timesteps=5775454, episode_reward=0.06 +/- 0.99
Episode length: 29.95 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 5775454    |
| train/                  |            |
|    approx_kl            | 0.04956732 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.000283   |
|    loss                 | 0.0475     |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.00094   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 95       |
|    time_elapsed    | 36570    |
|    total_timesteps | 5836800  |
---------------------------------
Eval num_timesteps=5836895, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 5836895   |
| train/                  |           |
|    approx_kl            | 0.0497388 |
|    clip_fraction        | 0.335     |
|    clip_range           | 0.17      |
|    entropy_loss         | -1.03     |
|    explained_variance   | 0.326     |
|    learning_rate        | 0.000282  |
|    loss                 | 0.0261    |
|    n_updates            | 475       |
|    policy_gradient_loss | -0.00214  |
|    value_loss           | 0.234     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 96       |
|    time_elapsed    | 36920    |
|    total_timesteps | 5898240  |
---------------------------------
Eval num_timesteps=5898336, episode_reward=0.08 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 5898336     |
| train/                  |             |
|    approx_kl            | 0.049487755 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000282    |
|    loss                 | 0.0399      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 97       |
|    time_elapsed    | 37269    |
|    total_timesteps | 5959680  |
---------------------------------
Eval num_timesteps=5959777, episode_reward=0.10 +/- 0.98
Episode length: 29.94 +/- 1.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 5959777    |
| train/                  |            |
|    approx_kl            | 0.05155673 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.000282   |
|    loss                 | 0.0699     |
|    n_updates            | 485        |
|    policy_gradient_loss | -0.00207   |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 98       |
|    time_elapsed    | 37618    |
|    total_timesteps | 6021120  |
---------------------------------
Eval num_timesteps=6021218, episode_reward=0.14 +/- 0.98
Episode length: 29.95 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 6021218     |
| train/                  |             |
|    approx_kl            | 0.050100293 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000282    |
|    loss                 | 0.144       |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 99       |
|    time_elapsed    | 37967    |
|    total_timesteps | 6082560  |
---------------------------------
Eval num_timesteps=6082659, episode_reward=0.07 +/- 0.98
Episode length: 29.95 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.074      |
| time/                   |            |
|    total_timesteps      | 6082659    |
| train/                  |            |
|    approx_kl            | 0.04993778 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000282   |
|    loss                 | 0.047      |
|    n_updates            | 495        |
|    policy_gradient_loss | -0.00231   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 100      |
|    time_elapsed    | 38316    |
|    total_timesteps | 6144000  |
---------------------------------
Eval num_timesteps=6144100, episode_reward=0.10 +/- 0.97
Episode length: 29.96 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 6144100     |
| train/                  |             |
|    approx_kl            | 0.052101128 |
|    clip_fraction        | 0.336       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000282    |
|    loss                 | 0.0674      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 101      |
|    time_elapsed    | 38664    |
|    total_timesteps | 6205440  |
---------------------------------
Eval num_timesteps=6205541, episode_reward=0.11 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 6205541     |
| train/                  |             |
|    approx_kl            | 0.051935975 |
|    clip_fraction        | 0.337       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.0432      |
|    n_updates            | 505         |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 102      |
|    time_elapsed    | 39013    |
|    total_timesteps | 6266880  |
---------------------------------
Eval num_timesteps=6266982, episode_reward=0.11 +/- 0.99
Episode length: 29.92 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 6266982     |
| train/                  |             |
|    approx_kl            | 0.051995516 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.12        |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 103      |
|    time_elapsed    | 39360    |
|    total_timesteps | 6328320  |
---------------------------------
Eval num_timesteps=6328423, episode_reward=0.12 +/- 0.98
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 6328423     |
| train/                  |             |
|    approx_kl            | 0.050829735 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.0143      |
|    n_updates            | 515         |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 104      |
|    time_elapsed    | 39708    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6389864, episode_reward=0.10 +/- 0.98
Episode length: 29.91 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 6389864     |
| train/                  |             |
|    approx_kl            | 0.049996626 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.0954      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00244    |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 105      |
|    time_elapsed    | 40055    |
|    total_timesteps | 6451200  |
---------------------------------
Eval num_timesteps=6451305, episode_reward=0.17 +/- 0.97
Episode length: 29.96 +/- 1.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.168       |
| time/                   |             |
|    total_timesteps      | 6451305     |
| train/                  |             |
|    approx_kl            | 0.050470512 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.141       |
|    n_updates            | 525         |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 0.228       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.168
SELFPLAY: new best model, bumping up generation to 16
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 106      |
|    time_elapsed    | 40405    |
|    total_timesteps | 6512640  |
---------------------------------
Eval num_timesteps=6512746, episode_reward=-0.05 +/- 0.99
Episode length: 29.94 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.046      |
| time/                   |             |
|    total_timesteps      | 6512746     |
| train/                  |             |
|    approx_kl            | 0.050866835 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.00028     |
|    loss                 | 0.0978      |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 107      |
|    time_elapsed    | 40757    |
|    total_timesteps | 6574080  |
---------------------------------
Eval num_timesteps=6574187, episode_reward=0.07 +/- 0.99
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 6574187     |
| train/                  |             |
|    approx_kl            | 0.052280605 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.00028     |
|    loss                 | 0.167       |
|    n_updates            | 535         |
|    policy_gradient_loss | -0.00239    |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 108      |
|    time_elapsed    | 41107    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6635628, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 6635628    |
| train/                  |            |
|    approx_kl            | 0.04994194 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.32       |
|    learning_rate        | 0.00028    |
|    loss                 | 0.144      |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.00271   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 109      |
|    time_elapsed    | 41456    |
|    total_timesteps | 6696960  |
---------------------------------
Eval num_timesteps=6697069, episode_reward=0.05 +/- 0.98
Episode length: 29.95 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 6697069     |
| train/                  |             |
|    approx_kl            | 0.050597064 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.00028     |
|    loss                 | 0.0458      |
|    n_updates            | 545         |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 110      |
|    time_elapsed    | 41805    |
|    total_timesteps | 6758400  |
---------------------------------
Eval num_timesteps=6758510, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 6758510    |
| train/                  |            |
|    approx_kl            | 0.05235827 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.00028    |
|    loss                 | 0.111      |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.00158   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 111      |
|    time_elapsed    | 42154    |
|    total_timesteps | 6819840  |
---------------------------------
Eval num_timesteps=6819951, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 6819951     |
| train/                  |             |
|    approx_kl            | 0.054454073 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.00028     |
|    loss                 | 0.0468      |
|    n_updates            | 555         |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 112      |
|    time_elapsed    | 42502    |
|    total_timesteps | 6881280  |
---------------------------------
Eval num_timesteps=6881392, episode_reward=0.02 +/- 0.99
Episode length: 29.99 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.024      |
| time/                   |            |
|    total_timesteps      | 6881392    |
| train/                  |            |
|    approx_kl            | 0.05079849 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000279   |
|    loss                 | 0.046      |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.00211   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 113      |
|    time_elapsed    | 42851    |
|    total_timesteps | 6942720  |
---------------------------------
Eval num_timesteps=6942833, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 6942833     |
| train/                  |             |
|    approx_kl            | 0.050179943 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000279    |
|    loss                 | 0.0626      |
|    n_updates            | 565         |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 114      |
|    time_elapsed    | 43198    |
|    total_timesteps | 7004160  |
---------------------------------
Eval num_timesteps=7004274, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.024      |
| time/                   |            |
|    total_timesteps      | 7004274    |
| train/                  |            |
|    approx_kl            | 0.05032196 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.331      |
|    learning_rate        | 0.000279   |
|    loss                 | 0.0812     |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.00241   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 115      |
|    time_elapsed    | 43545    |
|    total_timesteps | 7065600  |
---------------------------------
Eval num_timesteps=7065715, episode_reward=0.02 +/- 0.98
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 7065715     |
| train/                  |             |
|    approx_kl            | 0.050247792 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000279    |
|    loss                 | 0.101       |
|    n_updates            | 575         |
|    policy_gradient_loss | -0.000647   |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 116      |
|    time_elapsed    | 43894    |
|    total_timesteps | 7127040  |
---------------------------------
Eval num_timesteps=7127156, episode_reward=0.12 +/- 0.98
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 7127156     |
| train/                  |             |
|    approx_kl            | 0.048456557 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000279    |
|    loss                 | 0.0921      |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 117      |
|    time_elapsed    | 44243    |
|    total_timesteps | 7188480  |
---------------------------------
Eval num_timesteps=7188597, episode_reward=0.03 +/- 0.99
Episode length: 29.94 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 7188597     |
| train/                  |             |
|    approx_kl            | 0.050960217 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000278    |
|    loss                 | 0.0207      |
|    n_updates            | 585         |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 118      |
|    time_elapsed    | 44594    |
|    total_timesteps | 7249920  |
---------------------------------
Eval num_timesteps=7250038, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.092      |
| time/                   |            |
|    total_timesteps      | 7250038    |
| train/                  |            |
|    approx_kl            | 0.05230332 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.000278   |
|    loss                 | 0.149      |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.00082   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 119      |
|    time_elapsed    | 44943    |
|    total_timesteps | 7311360  |
---------------------------------
Eval num_timesteps=7311479, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.036      |
| time/                   |            |
|    total_timesteps      | 7311479    |
| train/                  |            |
|    approx_kl            | 0.05174977 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000278   |
|    loss                 | 0.0707     |
|    n_updates            | 595        |
|    policy_gradient_loss | -0.000882  |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 120      |
|    time_elapsed    | 45291    |
|    total_timesteps | 7372800  |
---------------------------------
Eval num_timesteps=7372920, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 7372920     |
| train/                  |             |
|    approx_kl            | 0.050689515 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000278    |
|    loss                 | 0.119       |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 121      |
|    time_elapsed    | 45640    |
|    total_timesteps | 7434240  |
---------------------------------
Eval num_timesteps=7434361, episode_reward=0.11 +/- 0.98
Episode length: 29.95 +/- 0.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 7434361     |
| train/                  |             |
|    approx_kl            | 0.050224368 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000278    |
|    loss                 | 0.0471      |
|    n_updates            | 605         |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 122      |
|    time_elapsed    | 45989    |
|    total_timesteps | 7495680  |
---------------------------------
Eval num_timesteps=7495802, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 7495802     |
| train/                  |             |
|    approx_kl            | 0.048598994 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.99       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000278    |
|    loss                 | 0.0619      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 123      |
|    time_elapsed    | 46338    |
|    total_timesteps | 7557120  |
---------------------------------
Eval num_timesteps=7557243, episode_reward=0.09 +/- 0.98
Episode length: 29.87 +/- 1.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 7557243     |
| train/                  |             |
|    approx_kl            | 0.049232494 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.987      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.000277    |
|    loss                 | 0.0607      |
|    n_updates            | 615         |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 124      |
|    time_elapsed    | 46687    |
|    total_timesteps | 7618560  |
---------------------------------
Eval num_timesteps=7618684, episode_reward=0.19 +/- 0.97
Episode length: 29.94 +/- 1.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.188      |
| time/                   |            |
|    total_timesteps      | 7618684    |
| train/                  |            |
|    approx_kl            | 0.04978234 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.98      |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.000277   |
|    loss                 | 0.0741     |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.00234   |
|    value_loss           | 0.231      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.188
SELFPLAY: new best model, bumping up generation to 17
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 125      |
|    time_elapsed    | 47035    |
|    total_timesteps | 7680000  |
---------------------------------
Eval num_timesteps=7680125, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 7680125     |
| train/                  |             |
|    approx_kl            | 0.049237784 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.997      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000277    |
|    loss                 | 0.0596      |
|    n_updates            | 625         |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 126      |
|    time_elapsed    | 47382    |
|    total_timesteps | 7741440  |
---------------------------------
Eval num_timesteps=7741566, episode_reward=0.07 +/- 0.99
Episode length: 29.90 +/- 1.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 7741566     |
| train/                  |             |
|    approx_kl            | 0.051179696 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000277    |
|    loss                 | 0.113       |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00077    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 127      |
|    time_elapsed    | 47731    |
|    total_timesteps | 7802880  |
---------------------------------
Eval num_timesteps=7803007, episode_reward=0.08 +/- 0.97
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 7803007     |
| train/                  |             |
|    approx_kl            | 0.051764686 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000277    |
|    loss                 | 0.0727      |
|    n_updates            | 635         |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 128      |
|    time_elapsed    | 48082    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7864448, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 7864448     |
| train/                  |             |
|    approx_kl            | 0.051543973 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000276    |
|    loss                 | 0.0923      |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 129      |
|    time_elapsed    | 48432    |
|    total_timesteps | 7925760  |
---------------------------------
Eval num_timesteps=7925889, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 7925889     |
| train/                  |             |
|    approx_kl            | 0.048342194 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.000276    |
|    loss                 | 0.152       |
|    n_updates            | 645         |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 130      |
|    time_elapsed    | 48781    |
|    total_timesteps | 7987200  |
---------------------------------
Eval num_timesteps=7987330, episode_reward=-0.07 +/- 0.99
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.066      |
| time/                   |             |
|    total_timesteps      | 7987330     |
| train/                  |             |
|    approx_kl            | 0.051977586 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.983      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000276    |
|    loss                 | 0.107       |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00059    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 131      |
|    time_elapsed    | 49130    |
|    total_timesteps | 8048640  |
---------------------------------
Eval num_timesteps=8048771, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 8048771     |
| train/                  |             |
|    approx_kl            | 0.052328892 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.984      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000276    |
|    loss                 | 0.0568      |
|    n_updates            | 655         |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 132      |
|    time_elapsed    | 49478    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8110212, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 8110212     |
| train/                  |             |
|    approx_kl            | 0.052933652 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.978      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000276    |
|    loss                 | 0.105       |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00219    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 133      |
|    time_elapsed    | 49826    |
|    total_timesteps | 8171520  |
---------------------------------
Eval num_timesteps=8171653, episode_reward=0.02 +/- 0.99
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 8171653     |
| train/                  |             |
|    approx_kl            | 0.051866487 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.971      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.184       |
|    n_updates            | 665         |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 134      |
|    time_elapsed    | 50175    |
|    total_timesteps | 8232960  |
---------------------------------
Eval num_timesteps=8233094, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.042      |
| time/                   |            |
|    total_timesteps      | 8233094    |
| train/                  |            |
|    approx_kl            | 0.05146311 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.968     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000275   |
|    loss                 | 0.0225     |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.00107   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 135      |
|    time_elapsed    | 50523    |
|    total_timesteps | 8294400  |
---------------------------------
Eval num_timesteps=8294535, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 8294535     |
| train/                  |             |
|    approx_kl            | 0.052856836 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.962      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.0688      |
|    n_updates            | 675         |
|    policy_gradient_loss | -0.000277   |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 136      |
|    time_elapsed    | 50870    |
|    total_timesteps | 8355840  |
---------------------------------
Eval num_timesteps=8355976, episode_reward=0.08 +/- 0.99
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 8355976     |
| train/                  |             |
|    approx_kl            | 0.050089154 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.954      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.049       |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00197    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 137      |
|    time_elapsed    | 51218    |
|    total_timesteps | 8417280  |
---------------------------------
Eval num_timesteps=8417417, episode_reward=-0.02 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 8417417     |
| train/                  |             |
|    approx_kl            | 0.053909052 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.94       |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.0542      |
|    n_updates            | 685         |
|    policy_gradient_loss | -5.59e-05   |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 138      |
|    time_elapsed    | 51567    |
|    total_timesteps | 8478720  |
---------------------------------
Eval num_timesteps=8478858, episode_reward=0.04 +/- 0.98
Episode length: 29.94 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 8478858     |
| train/                  |             |
|    approx_kl            | 0.050597485 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.937      |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.0644      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00138    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 139      |
|    time_elapsed    | 51917    |
|    total_timesteps | 8540160  |
---------------------------------
Eval num_timesteps=8540299, episode_reward=0.06 +/- 0.99
Episode length: 29.99 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.064     |
| time/                   |           |
|    total_timesteps      | 8540299   |
| train/                  |           |
|    approx_kl            | 0.0521381 |
|    clip_fraction        | 0.313     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.923    |
|    explained_variance   | 0.341     |
|    learning_rate        | 0.000274  |
|    loss                 | 0.112     |
|    n_updates            | 695       |
|    policy_gradient_loss | -0.00132  |
|    value_loss           | 0.233     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 140      |
|    time_elapsed    | 52267    |
|    total_timesteps | 8601600  |
---------------------------------
Eval num_timesteps=8601740, episode_reward=0.14 +/- 0.97
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 8601740     |
| train/                  |             |
|    approx_kl            | 0.051480357 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.918      |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000274    |
|    loss                 | 0.131       |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.000707   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 141      |
|    time_elapsed    | 52615    |
|    total_timesteps | 8663040  |
---------------------------------
Eval num_timesteps=8663181, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.044      |
| time/                   |            |
|    total_timesteps      | 8663181    |
| train/                  |            |
|    approx_kl            | 0.05112425 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.91      |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.000274   |
|    loss                 | 0.163      |
|    n_updates            | 705        |
|    policy_gradient_loss | -0.00057   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 142      |
|    time_elapsed    | 52964    |
|    total_timesteps | 8724480  |
---------------------------------
Eval num_timesteps=8724622, episode_reward=0.11 +/- 0.98
Episode length: 29.94 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 8724622     |
| train/                  |             |
|    approx_kl            | 0.050801966 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000274    |
|    loss                 | 0.102       |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.000504    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 143      |
|    time_elapsed    | 53312    |
|    total_timesteps | 8785920  |
---------------------------------
Eval num_timesteps=8786063, episode_reward=0.12 +/- 0.98
Episode length: 29.93 +/- 1.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 8786063     |
| train/                  |             |
|    approx_kl            | 0.051432632 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.896      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000274    |
|    loss                 | 0.0967      |
|    n_updates            | 715         |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 144      |
|    time_elapsed    | 53661    |
|    total_timesteps | 8847360  |
---------------------------------
Eval num_timesteps=8847504, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 8847504     |
| train/                  |             |
|    approx_kl            | 0.051297497 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.000273    |
|    loss                 | 0.095       |
|    n_updates            | 720         |
|    policy_gradient_loss | -3.58e-05   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 145      |
|    time_elapsed    | 54010    |
|    total_timesteps | 8908800  |
---------------------------------
Eval num_timesteps=8908945, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 8908945    |
| train/                  |            |
|    approx_kl            | 0.05909117 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.9       |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.000273   |
|    loss                 | 0.129      |
|    n_updates            | 725        |
|    policy_gradient_loss | -9.41e-05  |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 146      |
|    time_elapsed    | 54357    |
|    total_timesteps | 8970240  |
---------------------------------
Eval num_timesteps=8970386, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 8970386     |
| train/                  |             |
|    approx_kl            | 0.051360495 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.889      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000273    |
|    loss                 | 0.0541      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00059    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 147      |
|    time_elapsed    | 54704    |
|    total_timesteps | 9031680  |
---------------------------------
Eval num_timesteps=9031827, episode_reward=0.08 +/- 0.99
Episode length: 29.90 +/- 1.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 9031827    |
| train/                  |            |
|    approx_kl            | 0.05070056 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.889     |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000273   |
|    loss                 | 0.0856     |
|    n_updates            | 735        |
|    policy_gradient_loss | -0.000792  |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 148      |
|    time_elapsed    | 55052    |
|    total_timesteps | 9093120  |
---------------------------------
Eval num_timesteps=9093268, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 9093268     |
| train/                  |             |
|    approx_kl            | 0.051085625 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.888      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000273    |
|    loss                 | 0.117       |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.000112   |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 149      |
|    time_elapsed    | 55402    |
|    total_timesteps | 9154560  |
---------------------------------
Eval num_timesteps=9154709, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 9154709     |
| train/                  |             |
|    approx_kl            | 0.049044795 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.884      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000273    |
|    loss                 | 0.0702      |
|    n_updates            | 745         |
|    policy_gradient_loss | -0.000119   |
|    value_loss           | 0.232       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 18
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 150      |
|    time_elapsed    | 55752    |
|    total_timesteps | 9216000  |
---------------------------------
Eval num_timesteps=9216150, episode_reward=0.07 +/- 0.99
Episode length: 29.92 +/- 1.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 9216150    |
| train/                  |            |
|    approx_kl            | 0.05250733 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.917     |
|    explained_variance   | 0.315      |
|    learning_rate        | 0.000272   |
|    loss                 | 0.0411     |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.000162  |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 151      |
|    time_elapsed    | 56101    |
|    total_timesteps | 9277440  |
---------------------------------
Eval num_timesteps=9277591, episode_reward=0.03 +/- 0.99
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 9277591     |
| train/                  |             |
|    approx_kl            | 0.052868415 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.904      |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.000272    |
|    loss                 | 0.0463      |
|    n_updates            | 755         |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 152      |
|    time_elapsed    | 56449    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9339032, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.102      |
| time/                   |            |
|    total_timesteps      | 9339032    |
| train/                  |            |
|    approx_kl            | 0.05164603 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.892     |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000272   |
|    loss                 | 0.0668     |
|    n_updates            | 760        |
|    policy_gradient_loss | 0.000498   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 153      |
|    time_elapsed    | 56797    |
|    total_timesteps | 9400320  |
---------------------------------
Eval num_timesteps=9400473, episode_reward=0.03 +/- 0.98
Episode length: 29.92 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 9400473     |
| train/                  |             |
|    approx_kl            | 0.052176993 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.889      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000272    |
|    loss                 | 0.158       |
|    n_updates            | 765         |
|    policy_gradient_loss | 0.000466    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 154      |
|    time_elapsed    | 57145    |
|    total_timesteps | 9461760  |
---------------------------------
Eval num_timesteps=9461914, episode_reward=0.11 +/- 0.98
Episode length: 29.97 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 9461914    |
| train/                  |            |
|    approx_kl            | 0.05203374 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.888     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000272   |
|    loss                 | 0.105      |
|    n_updates            | 770        |
|    policy_gradient_loss | 0.00133    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 155      |
|    time_elapsed    | 57494    |
|    total_timesteps | 9523200  |
---------------------------------
Eval num_timesteps=9523355, episode_reward=-0.02 +/- 0.99
Episode length: 29.92 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 9523355     |
| train/                  |             |
|    approx_kl            | 0.050379883 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.882      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000271    |
|    loss                 | 0.119       |
|    n_updates            | 775         |
|    policy_gradient_loss | 0.000156    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 156      |
|    time_elapsed    | 57843    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9584796, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 9584796     |
| train/                  |             |
|    approx_kl            | 0.052643012 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.878      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000271    |
|    loss                 | 0.035       |
|    n_updates            | 780         |
|    policy_gradient_loss | -6.1e-05    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 157      |
|    time_elapsed    | 58190    |
|    total_timesteps | 9646080  |
---------------------------------
Eval num_timesteps=9646237, episode_reward=0.05 +/- 0.99
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 9646237     |
| train/                  |             |
|    approx_kl            | 0.053432755 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000271    |
|    loss                 | 0.15        |
|    n_updates            | 785         |
|    policy_gradient_loss | -0.000861   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 158      |
|    time_elapsed    | 58537    |
|    total_timesteps | 9707520  |
---------------------------------
Eval num_timesteps=9707678, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 9707678     |
| train/                  |             |
|    approx_kl            | 0.052856497 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000271    |
|    loss                 | 0.134       |
|    n_updates            | 790         |
|    policy_gradient_loss | -2.57e-05   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 159      |
|    time_elapsed    | 58886    |
|    total_timesteps | 9768960  |
---------------------------------
Eval num_timesteps=9769119, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 9769119     |
| train/                  |             |
|    approx_kl            | 0.054349914 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000271    |
|    loss                 | -0.015      |
|    n_updates            | 795         |
|    policy_gradient_loss | 0.000487    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 160      |
|    time_elapsed    | 59237    |
|    total_timesteps | 9830400  |
---------------------------------
Eval num_timesteps=9830560, episode_reward=0.15 +/- 0.98
Episode length: 29.93 +/- 0.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.148      |
| time/                   |            |
|    total_timesteps      | 9830560    |
| train/                  |            |
|    approx_kl            | 0.05323671 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.871     |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.000271   |
|    loss                 | 0.112      |
|    n_updates            | 800        |
|    policy_gradient_loss | 0.000446   |
|    value_loss           | 0.24       |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 19
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 161      |
|    time_elapsed    | 59586    |
|    total_timesteps | 9891840  |
---------------------------------
Eval num_timesteps=9892001, episode_reward=0.05 +/- 0.98
Episode length: 29.92 +/- 1.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 9892001     |
| train/                  |             |
|    approx_kl            | 0.052299812 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.883      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.00027     |
|    loss                 | 0.0691      |
|    n_updates            | 805         |
|    policy_gradient_loss | 0.00051     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 162      |
|    time_elapsed    | 59934    |
|    total_timesteps | 9953280  |
---------------------------------
Eval num_timesteps=9953442, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 9953442     |
| train/                  |             |
|    approx_kl            | 0.054179065 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.00027     |
|    loss                 | 0.112       |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.000247   |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 163      |
|    time_elapsed    | 60283    |
|    total_timesteps | 10014720 |
---------------------------------
Eval num_timesteps=10014883, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 10014883   |
| train/                  |            |
|    approx_kl            | 0.05356594 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.88      |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.00027    |
|    loss                 | 0.0185     |
|    n_updates            | 815        |
|    policy_gradient_loss | 0.000164   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 164      |
|    time_elapsed    | 60632    |
|    total_timesteps | 10076160 |
---------------------------------
Eval num_timesteps=10076324, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.03       |
| time/                   |            |
|    total_timesteps      | 10076324   |
| train/                  |            |
|    approx_kl            | 0.05639391 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.875     |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.00027    |
|    loss                 | 0.142      |
|    n_updates            | 820        |
|    policy_gradient_loss | 0.00237    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 165      |
|    time_elapsed    | 60981    |
|    total_timesteps | 10137600 |
---------------------------------
Eval num_timesteps=10137765, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 10137765    |
| train/                  |             |
|    approx_kl            | 0.052819762 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.00027     |
|    loss                 | 0.0205      |
|    n_updates            | 825         |
|    policy_gradient_loss | 0.00159     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 166      |
|    time_elapsed    | 61330    |
|    total_timesteps | 10199040 |
---------------------------------
Eval num_timesteps=10199206, episode_reward=0.01 +/- 0.98
Episode length: 29.92 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 10199206    |
| train/                  |             |
|    approx_kl            | 0.052552424 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.855      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000269    |
|    loss                 | 0.0615      |
|    n_updates            | 830         |
|    policy_gradient_loss | 0.00118     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 167      |
|    time_elapsed    | 61678    |
|    total_timesteps | 10260480 |
---------------------------------
Eval num_timesteps=10260647, episode_reward=0.03 +/- 0.99
Episode length: 29.94 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.026      |
| time/                   |            |
|    total_timesteps      | 10260647   |
| train/                  |            |
|    approx_kl            | 0.05498185 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.852     |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.000269   |
|    loss                 | 0.143      |
|    n_updates            | 835        |
|    policy_gradient_loss | 0.00148    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 168      |
|    time_elapsed    | 62025    |
|    total_timesteps | 10321920 |
---------------------------------
Eval num_timesteps=10322088, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 10322088    |
| train/                  |             |
|    approx_kl            | 0.052464955 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000269    |
|    loss                 | 0.0893      |
|    n_updates            | 840         |
|    policy_gradient_loss | 0.00087     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 169      |
|    time_elapsed    | 62372    |
|    total_timesteps | 10383360 |
---------------------------------
Eval num_timesteps=10383529, episode_reward=-0.00 +/- 0.99
Episode length: 29.95 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.004     |
| time/                   |            |
|    total_timesteps      | 10383529   |
| train/                  |            |
|    approx_kl            | 0.05429857 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.836     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000269   |
|    loss                 | 0.15       |
|    n_updates            | 845        |
|    policy_gradient_loss | 0.00038    |
|    value_loss           | 0.232      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 170      |
|    time_elapsed    | 62722    |
|    total_timesteps | 10444800 |
---------------------------------
Eval num_timesteps=10444970, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 10444970   |
| train/                  |            |
|    approx_kl            | 0.05201452 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.838     |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.000269   |
|    loss                 | 0.098      |
|    n_updates            | 850        |
|    policy_gradient_loss | 0.000237   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 171      |
|    time_elapsed    | 63073    |
|    total_timesteps | 10506240 |
---------------------------------
Eval num_timesteps=10506411, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 10506411    |
| train/                  |             |
|    approx_kl            | 0.053384826 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.84       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000268    |
|    loss                 | 0.065       |
|    n_updates            | 855         |
|    policy_gradient_loss | 0.000144    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 172      |
|    time_elapsed    | 63422    |
|    total_timesteps | 10567680 |
---------------------------------
Eval num_timesteps=10567852, episode_reward=0.04 +/- 0.99
Episode length: 29.95 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 10567852    |
| train/                  |             |
|    approx_kl            | 0.054142594 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000268    |
|    loss                 | 0.0662      |
|    n_updates            | 860         |
|    policy_gradient_loss | 0.00286     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 173      |
|    time_elapsed    | 63771    |
|    total_timesteps | 10629120 |
---------------------------------
Eval num_timesteps=10629293, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 10629293    |
| train/                  |             |
|    approx_kl            | 0.050159276 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.839      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000268    |
|    loss                 | 0.0846      |
|    n_updates            | 865         |
|    policy_gradient_loss | -0.000139   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 174      |
|    time_elapsed    | 64120    |
|    total_timesteps | 10690560 |
---------------------------------
Eval num_timesteps=10690734, episode_reward=0.12 +/- 0.98
Episode length: 29.94 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 10690734    |
| train/                  |             |
|    approx_kl            | 0.052588023 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.834      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000268    |
|    loss                 | 0.0512      |
|    n_updates            | 870         |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 175      |
|    time_elapsed    | 64469    |
|    total_timesteps | 10752000 |
---------------------------------
Eval num_timesteps=10752175, episode_reward=0.07 +/- 0.98
Episode length: 29.94 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.074      |
| time/                   |            |
|    total_timesteps      | 10752175   |
| train/                  |            |
|    approx_kl            | 0.05217673 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.833     |
|    explained_variance   | 0.313      |
|    learning_rate        | 0.000268   |
|    loss                 | 0.163      |
|    n_updates            | 875        |
|    policy_gradient_loss | 0.000926   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 176      |
|    time_elapsed    | 64829    |
|    total_timesteps | 10813440 |
---------------------------------
Eval num_timesteps=10813616, episode_reward=0.06 +/- 0.99
Episode length: 29.91 +/- 1.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 10813616   |
| train/                  |            |
|    approx_kl            | 0.05121283 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.83      |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000268   |
|    loss                 | 0.068      |
|    n_updates            | 880        |
|    policy_gradient_loss | 0.00107    |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 177      |
|    time_elapsed    | 65206    |
|    total_timesteps | 10874880 |
---------------------------------
Eval num_timesteps=10875057, episode_reward=0.11 +/- 0.98
Episode length: 29.91 +/- 1.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 10875057   |
| train/                  |            |
|    approx_kl            | 0.05072441 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.814     |
|    explained_variance   | 0.324      |
|    learning_rate        | 0.000267   |
|    loss                 | 0.0647     |
|    n_updates            | 885        |
|    policy_gradient_loss | 0.000209   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 178      |
|    time_elapsed    | 65569    |
|    total_timesteps | 10936320 |
---------------------------------
Eval num_timesteps=10936498, episode_reward=0.05 +/- 0.98
Episode length: 29.94 +/- 0.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 10936498    |
| train/                  |             |
|    approx_kl            | 0.050128628 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000267    |
|    loss                 | 0.0712      |
|    n_updates            | 890         |
|    policy_gradient_loss | 0.000138    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 179      |
|    time_elapsed    | 65939    |
|    total_timesteps | 10997760 |
---------------------------------
Eval num_timesteps=10997939, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 10997939    |
| train/                  |             |
|    approx_kl            | 0.050062988 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.809      |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.000267    |
|    loss                 | 0.0564      |
|    n_updates            | 895         |
|    policy_gradient_loss | 4.3e-05     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 180      |
|    time_elapsed    | 66306    |
|    total_timesteps | 11059200 |
---------------------------------
Eval num_timesteps=11059380, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.09      |
| time/                   |           |
|    total_timesteps      | 11059380  |
| train/                  |           |
|    approx_kl            | 0.0501482 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.803    |
|    explained_variance   | 0.336     |
|    learning_rate        | 0.000267  |
|    loss                 | 0.0606    |
|    n_updates            | 900       |
|    policy_gradient_loss | 0.00138   |
|    value_loss           | 0.231     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 181      |
|    time_elapsed    | 66686    |
|    total_timesteps | 11120640 |
---------------------------------
Eval num_timesteps=11120821, episode_reward=0.06 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 11120821   |
| train/                  |            |
|    approx_kl            | 0.04985797 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.000267   |
|    loss                 | 0.0481     |
|    n_updates            | 905        |
|    policy_gradient_loss | 0.00014    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 182      |
|    time_elapsed    | 67069    |
|    total_timesteps | 11182080 |
---------------------------------
Eval num_timesteps=11182262, episode_reward=0.15 +/- 0.97
Episode length: 29.93 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.148       |
| time/                   |             |
|    total_timesteps      | 11182262    |
| train/                  |             |
|    approx_kl            | 0.050884653 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000266    |
|    loss                 | 0.132       |
|    n_updates            | 910         |
|    policy_gradient_loss | 0.000511    |
|    value_loss           | 0.232       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 20
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 183      |
|    time_elapsed    | 67481    |
|    total_timesteps | 11243520 |
---------------------------------
Eval num_timesteps=11243703, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 1.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 11243703    |
| train/                  |             |
|    approx_kl            | 0.051204212 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000266    |
|    loss                 | 0.0658      |
|    n_updates            | 915         |
|    policy_gradient_loss | -0.000169   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 184      |
|    time_elapsed    | 67899    |
|    total_timesteps | 11304960 |
---------------------------------
Eval num_timesteps=11305144, episode_reward=0.02 +/- 0.97
Episode length: 30.01 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 11305144    |
| train/                  |             |
|    approx_kl            | 0.050767608 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.000266    |
|    loss                 | 0.0978      |
|    n_updates            | 920         |
|    policy_gradient_loss | 0.00113     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 185      |
|    time_elapsed    | 68313    |
|    total_timesteps | 11366400 |
---------------------------------
Eval num_timesteps=11366585, episode_reward=-0.01 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.008     |
| time/                   |            |
|    total_timesteps      | 11366585   |
| train/                  |            |
|    approx_kl            | 0.05210582 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.791     |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.000266   |
|    loss                 | 0.174      |
|    n_updates            | 925        |
|    policy_gradient_loss | 0.000372   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 186      |
|    time_elapsed    | 68735    |
|    total_timesteps | 11427840 |
---------------------------------
Eval num_timesteps=11428026, episode_reward=0.02 +/- 0.99
Episode length: 29.89 +/- 0.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 11428026    |
| train/                  |             |
|    approx_kl            | 0.054353997 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.794      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000266    |
|    loss                 | 0.0965      |
|    n_updates            | 930         |
|    policy_gradient_loss | 0.000621    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 187      |
|    time_elapsed    | 69155    |
|    total_timesteps | 11489280 |
---------------------------------
Eval num_timesteps=11489467, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.014     |
| time/                   |            |
|    total_timesteps      | 11489467   |
| train/                  |            |
|    approx_kl            | 0.04839297 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.785     |
|    explained_variance   | 0.328      |
|    learning_rate        | 0.000266   |
|    loss                 | 0.0874     |
|    n_updates            | 935        |
|    policy_gradient_loss | 0.000716   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 188      |
|    time_elapsed    | 69567    |
|    total_timesteps | 11550720 |
---------------------------------
Eval num_timesteps=11550908, episode_reward=0.13 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 11550908    |
| train/                  |             |
|    approx_kl            | 0.051875427 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000265    |
|    loss                 | 0.0272      |
|    n_updates            | 940         |
|    policy_gradient_loss | 0.00217     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 189      |
|    time_elapsed    | 69986    |
|    total_timesteps | 11612160 |
---------------------------------
Eval num_timesteps=11612349, episode_reward=0.11 +/- 0.97
Episode length: 30.03 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total_timesteps      | 11612349   |
| train/                  |            |
|    approx_kl            | 0.05107017 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.778     |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000265   |
|    loss                 | 0.185      |
|    n_updates            | 945        |
|    policy_gradient_loss | 0.000296   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 190      |
|    time_elapsed    | 70407    |
|    total_timesteps | 11673600 |
---------------------------------
Eval num_timesteps=11673790, episode_reward=0.07 +/- 0.98
Episode length: 29.87 +/- 1.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 11673790    |
| train/                  |             |
|    approx_kl            | 0.051645655 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.771      |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.000265    |
|    loss                 | 0.0712      |
|    n_updates            | 950         |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 191      |
|    time_elapsed    | 70821    |
|    total_timesteps | 11735040 |
---------------------------------
Eval num_timesteps=11735231, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 11735231   |
| train/                  |            |
|    approx_kl            | 0.05027796 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.762     |
|    explained_variance   | 0.314      |
|    learning_rate        | 0.000265   |
|    loss                 | 0.0965     |
|    n_updates            | 955        |
|    policy_gradient_loss | 0.00128    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 192      |
|    time_elapsed    | 71240    |
|    total_timesteps | 11796480 |
---------------------------------
Eval num_timesteps=11796672, episode_reward=0.07 +/- 0.98
Episode length: 29.93 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 11796672    |
| train/                  |             |
|    approx_kl            | 0.049402975 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000265    |
|    loss                 | 0.181       |
|    n_updates            | 960         |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 193      |
|    time_elapsed    | 71661    |
|    total_timesteps | 11857920 |
---------------------------------
Eval num_timesteps=11858113, episode_reward=0.04 +/- 0.99
Episode length: 29.94 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 11858113    |
| train/                  |             |
|    approx_kl            | 0.048640918 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.3         |
|    learning_rate        | 0.000264    |
|    loss                 | 0.123       |
|    n_updates            | 965         |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 194      |
|    time_elapsed    | 72075    |
|    total_timesteps | 11919360 |
---------------------------------
Eval num_timesteps=11919554, episode_reward=-0.01 +/- 0.98
Episode length: 29.90 +/- 1.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 11919554    |
| train/                  |             |
|    approx_kl            | 0.048092175 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000264    |
|    loss                 | 0.0765      |
|    n_updates            | 970         |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 195      |
|    time_elapsed    | 72493    |
|    total_timesteps | 11980800 |
---------------------------------
Eval num_timesteps=11980995, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 11980995    |
| train/                  |             |
|    approx_kl            | 0.048563663 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000264    |
|    loss                 | 0.0344      |
|    n_updates            | 975         |
|    policy_gradient_loss | 0.00128     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 196      |
|    time_elapsed    | 72915    |
|    total_timesteps | 12042240 |
---------------------------------
Eval num_timesteps=12042436, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 12042436    |
| train/                  |             |
|    approx_kl            | 0.049808588 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000264    |
|    loss                 | 0.0913      |
|    n_updates            | 980         |
|    policy_gradient_loss | 0.00213     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 197      |
|    time_elapsed    | 73329    |
|    total_timesteps | 12103680 |
---------------------------------
Eval num_timesteps=12103877, episode_reward=0.01 +/- 0.99
Episode length: 29.95 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 12103877   |
| train/                  |            |
|    approx_kl            | 0.04914079 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.000264   |
|    loss                 | 0.239      |
|    n_updates            | 985        |
|    policy_gradient_loss | 0.00115    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 198      |
|    time_elapsed    | 73860    |
|    total_timesteps | 12165120 |
---------------------------------
Eval num_timesteps=12165318, episode_reward=0.03 +/- 0.99
Episode length: 29.93 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 12165318    |
| train/                  |             |
|    approx_kl            | 0.049365655 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.000264    |
|    loss                 | 0.116       |
|    n_updates            | 990         |
|    policy_gradient_loss | 0.000313    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 199      |
|    time_elapsed    | 74327    |
|    total_timesteps | 12226560 |
---------------------------------
Eval num_timesteps=12226759, episode_reward=0.06 +/- 0.99
Episode length: 29.94 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 12226759    |
| train/                  |             |
|    approx_kl            | 0.049169816 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.000263    |
|    loss                 | 0.165       |
|    n_updates            | 995         |
|    policy_gradient_loss | 0.000272    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 200      |
|    time_elapsed    | 74745    |
|    total_timesteps | 12288000 |
---------------------------------
Eval num_timesteps=12288200, episode_reward=-0.02 +/- 0.98
Episode length: 29.95 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.024      |
| time/                   |             |
|    total_timesteps      | 12288200    |
| train/                  |             |
|    approx_kl            | 0.049972188 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000263    |
|    loss                 | 0.0694      |
|    n_updates            | 1000        |
|    policy_gradient_loss | 0.000289    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 201      |
|    time_elapsed    | 75164    |
|    total_timesteps | 12349440 |
---------------------------------
Eval num_timesteps=12349641, episode_reward=0.08 +/- 0.98
Episode length: 29.94 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 12349641   |
| train/                  |            |
|    approx_kl            | 0.05030901 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.000263   |
|    loss                 | 0.106      |
|    n_updates            | 1005       |
|    policy_gradient_loss | 0.00102    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 202      |
|    time_elapsed    | 75583    |
|    total_timesteps | 12410880 |
---------------------------------
Eval num_timesteps=12411082, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 12411082    |
| train/                  |             |
|    approx_kl            | 0.048959803 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.761      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000263    |
|    loss                 | 0.0955      |
|    n_updates            | 1010        |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 203      |
|    time_elapsed    | 75998    |
|    total_timesteps | 12472320 |
---------------------------------
Eval num_timesteps=12472523, episode_reward=0.11 +/- 0.98
Episode length: 29.93 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total_timesteps      | 12472523   |
| train/                  |            |
|    approx_kl            | 0.04668287 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000263   |
|    loss                 | 0.11       |
|    n_updates            | 1015       |
|    policy_gradient_loss | -0.00059   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 204      |
|    time_elapsed    | 76417    |
|    total_timesteps | 12533760 |
---------------------------------
Eval num_timesteps=12533964, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.122      |
| time/                   |            |
|    total_timesteps      | 12533964   |
| train/                  |            |
|    approx_kl            | 0.04822796 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.000262   |
|    loss                 | 0.117      |
|    n_updates            | 1020       |
|    policy_gradient_loss | 0.000181   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 163      |
|    iterations      | 205      |
|    time_elapsed    | 76840    |
|    total_timesteps | 12595200 |
---------------------------------
Eval num_timesteps=12595405, episode_reward=0.09 +/- 0.98
Episode length: 29.94 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.092      |
| time/                   |            |
|    total_timesteps      | 12595405   |
| train/                  |            |
|    approx_kl            | 0.04820556 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.749     |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.000262   |
|    loss                 | 0.136      |
|    n_updates            | 1025       |
|    policy_gradient_loss | 0.000545   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 206      |
|    time_elapsed    | 77254    |
|    total_timesteps | 12656640 |
---------------------------------
Eval num_timesteps=12656846, episode_reward=0.13 +/- 0.97
Episode length: 30.05 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.134      |
| time/                   |            |
|    total_timesteps      | 12656846   |
| train/                  |            |
|    approx_kl            | 0.04807297 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.32       |
|    learning_rate        | 0.000262   |
|    loss                 | 0.0691     |
|    n_updates            | 1030       |
|    policy_gradient_loss | 0.000906   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 207      |
|    time_elapsed    | 77671    |
|    total_timesteps | 12718080 |
---------------------------------
Eval num_timesteps=12718287, episode_reward=0.13 +/- 0.98
Episode length: 29.96 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.134      |
| time/                   |            |
|    total_timesteps      | 12718287   |
| train/                  |            |
|    approx_kl            | 0.04812071 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.731     |
|    explained_variance   | 0.317      |
|    learning_rate        | 0.000262   |
|    loss                 | 0.129      |
|    n_updates            | 1035       |
|    policy_gradient_loss | 0.000761   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 208      |
|    time_elapsed    | 78092    |
|    total_timesteps | 12779520 |
---------------------------------
Eval num_timesteps=12779728, episode_reward=0.06 +/- 0.98
Episode length: 29.90 +/- 0.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 12779728   |
| train/                  |            |
|    approx_kl            | 0.04876877 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.735     |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000262   |
|    loss                 | 0.069      |
|    n_updates            | 1040       |
|    policy_gradient_loss | 0.000188   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 209      |
|    time_elapsed    | 78502    |
|    total_timesteps | 12840960 |
---------------------------------
Eval num_timesteps=12841169, episode_reward=0.10 +/- 0.98
Episode length: 29.91 +/- 0.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 12841169    |
| train/                  |             |
|    approx_kl            | 0.047662318 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.735      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.0852      |
|    n_updates            | 1045        |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 210      |
|    time_elapsed    | 78921    |
|    total_timesteps | 12902400 |
---------------------------------
Eval num_timesteps=12902610, episode_reward=0.10 +/- 0.98
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 12902610    |
| train/                  |             |
|    approx_kl            | 0.047354944 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.737      |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.113       |
|    n_updates            | 1050        |
|    policy_gradient_loss | 0.000748    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 211      |
|    time_elapsed    | 79344    |
|    total_timesteps | 12963840 |
---------------------------------
Eval num_timesteps=12964051, episode_reward=0.15 +/- 0.98
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 12964051    |
| train/                  |             |
|    approx_kl            | 0.048275676 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.101       |
|    n_updates            | 1055        |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 0.238       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.15
SELFPLAY: new best model, bumping up generation to 21
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 212      |
|    time_elapsed    | 79755    |
|    total_timesteps | 13025280 |
---------------------------------
Eval num_timesteps=13025492, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 13025492    |
| train/                  |             |
|    approx_kl            | 0.045843147 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.104       |
|    n_updates            | 1060        |
|    policy_gradient_loss | 0.00149     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 213      |
|    time_elapsed    | 80174    |
|    total_timesteps | 13086720 |
---------------------------------
Eval num_timesteps=13086933, episode_reward=-0.07 +/- 0.98
Episode length: 29.92 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.068      |
| time/                   |             |
|    total_timesteps      | 13086933    |
| train/                  |             |
|    approx_kl            | 0.046086214 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.128       |
|    n_updates            | 1065        |
|    policy_gradient_loss | 0.000533    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 214      |
|    time_elapsed    | 80661    |
|    total_timesteps | 13148160 |
---------------------------------
Eval num_timesteps=13148374, episode_reward=0.00 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.004      |
| time/                   |            |
|    total_timesteps      | 13148374   |
| train/                  |            |
|    approx_kl            | 0.04595549 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.000261   |
|    loss                 | 0.0655     |
|    n_updates            | 1070       |
|    policy_gradient_loss | 0.00042    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 215      |
|    time_elapsed    | 81175    |
|    total_timesteps | 13209600 |
---------------------------------
Eval num_timesteps=13209815, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.082    |
| time/                   |          |
|    total_timesteps      | 13209815 |
| train/                  |          |
|    approx_kl            | 0.047565 |
|    clip_fraction        | 0.27     |
|    clip_range           | 0.17     |
|    entropy_loss         | -0.768   |
|    explained_variance   | 0.345    |
|    learning_rate        | 0.00026  |
|    loss                 | 0.124    |
|    n_updates            | 1075     |
|    policy_gradient_loss | 0.00125  |
|    value_loss           | 0.235    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 216      |
|    time_elapsed    | 81594    |
|    total_timesteps | 13271040 |
---------------------------------
Eval num_timesteps=13271256, episode_reward=0.09 +/- 0.98
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 13271256    |
| train/                  |             |
|    approx_kl            | 0.048157826 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.00026     |
|    loss                 | 0.0629      |
|    n_updates            | 1080        |
|    policy_gradient_loss | 0.00175     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 217      |
|    time_elapsed    | 82011    |
|    total_timesteps | 13332480 |
---------------------------------
Eval num_timesteps=13332697, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 13332697    |
| train/                  |             |
|    approx_kl            | 0.044958718 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.00026     |
|    loss                 | 0.136       |
|    n_updates            | 1085        |
|    policy_gradient_loss | 0.000707    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.22    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 218      |
|    time_elapsed    | 82427    |
|    total_timesteps | 13393920 |
---------------------------------
Eval num_timesteps=13394138, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 13394138    |
| train/                  |             |
|    approx_kl            | 0.047148965 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.00026     |
|    loss                 | 0.164       |
|    n_updates            | 1090        |
|    policy_gradient_loss | 0.00026     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 219      |
|    time_elapsed    | 82844    |
|    total_timesteps | 13455360 |
---------------------------------
Eval num_timesteps=13455579, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 13455579    |
| train/                  |             |
|    approx_kl            | 0.047688905 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.731      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.00026     |
|    loss                 | 0.149       |
|    n_updates            | 1095        |
|    policy_gradient_loss | 0.00117     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 220      |
|    time_elapsed    | 83264    |
|    total_timesteps | 13516800 |
---------------------------------
Eval num_timesteps=13517020, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 13517020    |
| train/                  |             |
|    approx_kl            | 0.045455858 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.732      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.0891      |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.000874   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 221      |
|    time_elapsed    | 83682    |
|    total_timesteps | 13578240 |
---------------------------------
Eval num_timesteps=13578461, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 13578461    |
| train/                  |             |
|    approx_kl            | 0.045287423 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.0928      |
|    n_updates            | 1105        |
|    policy_gradient_loss | 0.000938    |
|    value_loss           | 0.246       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 222      |
|    time_elapsed    | 84098    |
|    total_timesteps | 13639680 |
---------------------------------
Eval num_timesteps=13639902, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 13639902    |
| train/                  |             |
|    approx_kl            | 0.045926936 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.12        |
|    n_updates            | 1110        |
|    policy_gradient_loss | 0.000573    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 223      |
|    time_elapsed    | 84520    |
|    total_timesteps | 13701120 |
---------------------------------
Eval num_timesteps=13701343, episode_reward=0.15 +/- 0.97
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 13701343    |
| train/                  |             |
|    approx_kl            | 0.046483494 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.0795      |
|    n_updates            | 1115        |
|    policy_gradient_loss | 0.000224    |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.15
SELFPLAY: new best model, bumping up generation to 22
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 224      |
|    time_elapsed    | 84938    |
|    total_timesteps | 13762560 |
---------------------------------
Eval num_timesteps=13762784, episode_reward=-0.02 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.024      |
| time/                   |             |
|    total_timesteps      | 13762784    |
| train/                  |             |
|    approx_kl            | 0.045767855 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.0147      |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.000455   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 225      |
|    time_elapsed    | 85353    |
|    total_timesteps | 13824000 |
---------------------------------
Eval num_timesteps=13824225, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 13824225    |
| train/                  |             |
|    approx_kl            | 0.046351776 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.117       |
|    n_updates            | 1125        |
|    policy_gradient_loss | -2.4e-05    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 226      |
|    time_elapsed    | 85776    |
|    total_timesteps | 13885440 |
---------------------------------
Eval num_timesteps=13885666, episode_reward=0.02 +/- 0.99
Episode length: 29.94 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 13885666   |
| train/                  |            |
|    approx_kl            | 0.04832792 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.741     |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.000258   |
|    loss                 | 0.133      |
|    n_updates            | 1130       |
|    policy_gradient_loss | 0.000252   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 227      |
|    time_elapsed    | 86192    |
|    total_timesteps | 13946880 |
---------------------------------
Eval num_timesteps=13947107, episode_reward=0.03 +/- 0.99
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.028      |
| time/                   |            |
|    total_timesteps      | 13947107   |
| train/                  |            |
|    approx_kl            | 0.04498787 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.74      |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000258   |
|    loss                 | 0.0763     |
|    n_updates            | 1135       |
|    policy_gradient_loss | 0.000655   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 228      |
|    time_elapsed    | 86555    |
|    total_timesteps | 14008320 |
---------------------------------
Eval num_timesteps=14008548, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 14008548    |
| train/                  |             |
|    approx_kl            | 0.046107627 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000258    |
|    loss                 | 0.0656      |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.000382   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 229      |
|    time_elapsed    | 86956    |
|    total_timesteps | 14069760 |
---------------------------------
Eval num_timesteps=14069989, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 1.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.058      |
| time/                   |            |
|    total_timesteps      | 14069989   |
| train/                  |            |
|    approx_kl            | 0.04528184 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.732     |
|    explained_variance   | 0.313      |
|    learning_rate        | 0.000258   |
|    loss                 | 0.115      |
|    n_updates            | 1145       |
|    policy_gradient_loss | 0.000549   |
|    value_loss           | 0.245      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 230      |
|    time_elapsed    | 87370    |
|    total_timesteps | 14131200 |
---------------------------------
Eval num_timesteps=14131430, episode_reward=-0.05 +/- 0.99
Episode length: 29.95 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | -0.05     |
| time/                   |           |
|    total_timesteps      | 14131430  |
| train/                  |           |
|    approx_kl            | 0.0457685 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.737    |
|    explained_variance   | 0.326     |
|    learning_rate        | 0.000258  |
|    loss                 | 0.144     |
|    n_updates            | 1150      |
|    policy_gradient_loss | 0.000314  |
|    value_loss           | 0.235     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 231      |
|    time_elapsed    | 87779    |
|    total_timesteps | 14192640 |
---------------------------------
Eval num_timesteps=14192871, episode_reward=0.06 +/- 0.99
Episode length: 29.94 +/- 0.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.062      |
| time/                   |            |
|    total_timesteps      | 14192871   |
| train/                  |            |
|    approx_kl            | 0.04399213 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.000257   |
|    loss                 | 0.121      |
|    n_updates            | 1155       |
|    policy_gradient_loss | 0.00133    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 161      |
|    iterations      | 232      |
|    time_elapsed    | 88194    |
|    total_timesteps | 14254080 |
---------------------------------
Eval num_timesteps=14254312, episode_reward=0.05 +/- 0.99
Episode length: 29.95 +/- 0.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 14254312   |
| train/                  |            |
|    approx_kl            | 0.04609213 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.726     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000257   |
|    loss                 | 0.0721     |
|    n_updates            | 1160       |
|    policy_gradient_loss | 0.000579   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 233      |
|    time_elapsed    | 88607    |
|    total_timesteps | 14315520 |
---------------------------------
Eval num_timesteps=14315753, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 14315753    |
| train/                  |             |
|    approx_kl            | 0.046624694 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000257    |
|    loss                 | 0.112       |
|    n_updates            | 1165        |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 234      |
|    time_elapsed    | 89016    |
|    total_timesteps | 14376960 |
---------------------------------
Eval num_timesteps=14377194, episode_reward=0.01 +/- 0.99
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 14377194    |
| train/                  |             |
|    approx_kl            | 0.048045658 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000257    |
|    loss                 | 0.117       |
|    n_updates            | 1170        |
|    policy_gradient_loss | 0.000275    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 235      |
|    time_elapsed    | 89543    |
|    total_timesteps | 14438400 |
---------------------------------
Eval num_timesteps=14438635, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 14438635   |
| train/                  |            |
|    approx_kl            | 0.04742663 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000257   |
|    loss                 | 0.099      |
|    n_updates            | 1175       |
|    policy_gradient_loss | 0.00148    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 236      |
|    time_elapsed    | 89974    |
|    total_timesteps | 14499840 |
---------------------------------
Eval num_timesteps=14500076, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 14500076    |
| train/                  |             |
|    approx_kl            | 0.046400607 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000257    |
|    loss                 | 0.0862      |
|    n_updates            | 1180        |
|    policy_gradient_loss | 0.000524    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 237      |
|    time_elapsed    | 90386    |
|    total_timesteps | 14561280 |
---------------------------------
Eval num_timesteps=14561517, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 14561517   |
| train/                  |            |
|    approx_kl            | 0.04937272 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.000256   |
|    loss                 | 0.103      |
|    n_updates            | 1185       |
|    policy_gradient_loss | 0.002      |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 238      |
|    time_elapsed    | 90795    |
|    total_timesteps | 14622720 |
---------------------------------
Eval num_timesteps=14622958, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 14622958    |
| train/                  |             |
|    approx_kl            | 0.047908034 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000256    |
|    loss                 | 0.0948      |
|    n_updates            | 1190        |
|    policy_gradient_loss | 0.0016      |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 239      |
|    time_elapsed    | 91207    |
|    total_timesteps | 14684160 |
---------------------------------
Eval num_timesteps=14684399, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 14684399    |
| train/                  |             |
|    approx_kl            | 0.046897624 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000256    |
|    loss                 | 0.153       |
|    n_updates            | 1195        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 240      |
|    time_elapsed    | 91618    |
|    total_timesteps | 14745600 |
---------------------------------
Eval num_timesteps=14745840, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.044      |
| time/                   |            |
|    total_timesteps      | 14745840   |
| train/                  |            |
|    approx_kl            | 0.04682951 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.000256   |
|    loss                 | 0.0492     |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.00032   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 241      |
|    time_elapsed    | 92044    |
|    total_timesteps | 14807040 |
---------------------------------
Eval num_timesteps=14807281, episode_reward=0.08 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 14807281    |
| train/                  |             |
|    approx_kl            | 0.047038075 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000256    |
|    loss                 | 0.174       |
|    n_updates            | 1205        |
|    policy_gradient_loss | 0.000508    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 242      |
|    time_elapsed    | 92548    |
|    total_timesteps | 14868480 |
---------------------------------
Eval num_timesteps=14868722, episode_reward=0.11 +/- 0.97
Episode length: 29.95 +/- 0.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 14868722    |
| train/                  |             |
|    approx_kl            | 0.048427112 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000255    |
|    loss                 | 0.167       |
|    n_updates            | 1210        |
|    policy_gradient_loss | 0.000392    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 243      |
|    time_elapsed    | 92993    |
|    total_timesteps | 14929920 |
---------------------------------
Eval num_timesteps=14930163, episode_reward=0.18 +/- 0.97
Episode length: 30.05 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.176       |
| time/                   |             |
|    total_timesteps      | 14930163    |
| train/                  |             |
|    approx_kl            | 0.048629943 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000255    |
|    loss                 | 0.0791      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.000737   |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.176
SELFPLAY: new best model, bumping up generation to 23
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 244      |
|    time_elapsed    | 93404    |
|    total_timesteps | 14991360 |
---------------------------------
Eval num_timesteps=14991604, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 14991604    |
| train/                  |             |
|    approx_kl            | 0.048646133 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.000255    |
|    loss                 | 0.0539      |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.000894   |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 245      |
|    time_elapsed    | 93816    |
|    total_timesteps | 15052800 |
---------------------------------
Eval num_timesteps=15053045, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 15053045    |
| train/                  |             |
|    approx_kl            | 0.049899746 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.000255    |
|    loss                 | 0.136       |
|    n_updates            | 1225        |
|    policy_gradient_loss | 0.000608    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 246      |
|    time_elapsed    | 94228    |
|    total_timesteps | 15114240 |
---------------------------------
Eval num_timesteps=15114486, episode_reward=0.09 +/- 0.98
Episode length: 30.05 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 15114486   |
| train/                  |            |
|    approx_kl            | 0.04735537 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.386      |
|    learning_rate        | 0.000255   |
|    loss                 | 0.114      |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.000138  |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 247      |
|    time_elapsed    | 94635    |
|    total_timesteps | 15175680 |
---------------------------------
Eval num_timesteps=15175927, episode_reward=0.04 +/- 0.98
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 15175927    |
| train/                  |             |
|    approx_kl            | 0.047917064 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.396       |
|    learning_rate        | 0.000254    |
|    loss                 | 0.0449      |
|    n_updates            | 1235        |
|    policy_gradient_loss | -0.000926   |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 248      |
|    time_elapsed    | 95090    |
|    total_timesteps | 15237120 |
---------------------------------
Eval num_timesteps=15237368, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 15237368    |
| train/                  |             |
|    approx_kl            | 0.047389213 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.000254    |
|    loss                 | 0.0603      |
|    n_updates            | 1240        |
|    policy_gradient_loss | 0.000334    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 249      |
|    time_elapsed    | 95565    |
|    total_timesteps | 15298560 |
---------------------------------
Eval num_timesteps=15298809, episode_reward=0.13 +/- 0.98
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.132      |
| time/                   |            |
|    total_timesteps      | 15298809   |
| train/                  |            |
|    approx_kl            | 0.04883963 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.375      |
|    learning_rate        | 0.000254   |
|    loss                 | 0.053      |
|    n_updates            | 1245       |
|    policy_gradient_loss | 0.000393   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 250      |
|    time_elapsed    | 96018    |
|    total_timesteps | 15360000 |
---------------------------------
Eval num_timesteps=15360250, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.072     |
| time/                   |           |
|    total_timesteps      | 15360250  |
| train/                  |           |
|    approx_kl            | 0.0505821 |
|    clip_fraction        | 0.273     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.757    |
|    explained_variance   | 0.378     |
|    learning_rate        | 0.000254  |
|    loss                 | 0.0629    |
|    n_updates            | 1250      |
|    policy_gradient_loss | 0.000918  |
|    value_loss           | 0.227     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 251      |
|    time_elapsed    | 96428    |
|    total_timesteps | 15421440 |
---------------------------------
Eval num_timesteps=15421691, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.172       |
| time/                   |             |
|    total_timesteps      | 15421691    |
| train/                  |             |
|    approx_kl            | 0.048247736 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.000254    |
|    loss                 | 0.112       |
|    n_updates            | 1255        |
|    policy_gradient_loss | 0.00132     |
|    value_loss           | 0.229       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.172
SELFPLAY: new best model, bumping up generation to 24
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 252      |
|    time_elapsed    | 96841    |
|    total_timesteps | 15482880 |
---------------------------------
Eval num_timesteps=15483132, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 15483132    |
| train/                  |             |
|    approx_kl            | 0.057802223 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.793      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000254    |
|    loss                 | 0.0886      |
|    n_updates            | 1260        |
|    policy_gradient_loss | 0.000355    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 253      |
|    time_elapsed    | 97256    |
|    total_timesteps | 15544320 |
---------------------------------
Eval num_timesteps=15544573, episode_reward=0.09 +/- 0.97
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 15544573    |
| train/                  |             |
|    approx_kl            | 0.047779024 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000253    |
|    loss                 | 0.0235      |
|    n_updates            | 1265        |
|    policy_gradient_loss | 0.000234    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 254      |
|    time_elapsed    | 97665    |
|    total_timesteps | 15605760 |
---------------------------------
Eval num_timesteps=15606014, episode_reward=-0.00 +/- 0.99
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 15606014    |
| train/                  |             |
|    approx_kl            | 0.050678086 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.000253    |
|    loss                 | 0.0947      |
|    n_updates            | 1270        |
|    policy_gradient_loss | 0.000559    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 255      |
|    time_elapsed    | 98131    |
|    total_timesteps | 15667200 |
---------------------------------
Eval num_timesteps=15667455, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 15667455    |
| train/                  |             |
|    approx_kl            | 0.050612696 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000253    |
|    loss                 | 0.0879      |
|    n_updates            | 1275        |
|    policy_gradient_loss | -6.06e-05   |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 256      |
|    time_elapsed    | 98577    |
|    total_timesteps | 15728640 |
---------------------------------
Eval num_timesteps=15728896, episode_reward=0.11 +/- 0.97
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 15728896    |
| train/                  |             |
|    approx_kl            | 0.049446035 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000253    |
|    loss                 | 0.0951      |
|    n_updates            | 1280        |
|    policy_gradient_loss | 9.62e-05    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 257      |
|    time_elapsed    | 99039    |
|    total_timesteps | 15790080 |
---------------------------------
Eval num_timesteps=15790337, episode_reward=0.07 +/- 0.98
Episode length: 29.96 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 15790337    |
| train/                  |             |
|    approx_kl            | 0.048653807 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.823      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000253    |
|    loss                 | 0.111       |
|    n_updates            | 1285        |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 258      |
|    time_elapsed    | 99449    |
|    total_timesteps | 15851520 |
---------------------------------
Eval num_timesteps=15851778, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.124      |
| time/                   |            |
|    total_timesteps      | 15851778   |
| train/                  |            |
|    approx_kl            | 0.04892148 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.822     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000252   |
|    loss                 | 0.006      |
|    n_updates            | 1290       |
|    policy_gradient_loss | 0.000618   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 259      |
|    time_elapsed    | 99860    |
|    total_timesteps | 15912960 |
---------------------------------
Eval num_timesteps=15913219, episode_reward=0.25 +/- 0.96
Episode length: 30.05 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.246       |
| time/                   |             |
|    total_timesteps      | 15913219    |
| train/                  |             |
|    approx_kl            | 0.048869092 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000252    |
|    loss                 | 0.0897      |
|    n_updates            | 1295        |
|    policy_gradient_loss | 0.000713    |
|    value_loss           | 0.224       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.246
SELFPLAY: new best model, bumping up generation to 25
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 260      |
|    time_elapsed    | 100273   |
|    total_timesteps | 15974400 |
---------------------------------
Eval num_timesteps=15974660, episode_reward=0.03 +/- 0.98
Episode length: 29.94 +/- 1.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.034     |
| time/                   |           |
|    total_timesteps      | 15974660  |
| train/                  |           |
|    approx_kl            | 0.0512748 |
|    clip_fraction        | 0.298     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.863    |
|    explained_variance   | 0.351     |
|    learning_rate        | 0.000252  |
|    loss                 | 0.146     |
|    n_updates            | 1300      |
|    policy_gradient_loss | 0.00183   |
|    value_loss           | 0.231     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 261      |
|    time_elapsed    | 100707   |
|    total_timesteps | 16035840 |
---------------------------------
Eval num_timesteps=16036101, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 16036101    |
| train/                  |             |
|    approx_kl            | 0.052107785 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000252    |
|    loss                 | 0.109       |
|    n_updates            | 1305        |
|    policy_gradient_loss | 0.000149    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 262      |
|    time_elapsed    | 101149   |
|    total_timesteps | 16097280 |
---------------------------------
Eval num_timesteps=16097542, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 16097542    |
| train/                  |             |
|    approx_kl            | 0.050573006 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.862      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000252    |
|    loss                 | 0.0704      |
|    n_updates            | 1310        |
|    policy_gradient_loss | 0.00176     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 263      |
|    time_elapsed    | 101596   |
|    total_timesteps | 16158720 |
---------------------------------
Eval num_timesteps=16158983, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 16158983    |
| train/                  |             |
|    approx_kl            | 0.049735203 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.859      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000252    |
|    loss                 | 0.0993      |
|    n_updates            | 1315        |
|    policy_gradient_loss | 0.00173     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 264      |
|    time_elapsed    | 102067   |
|    total_timesteps | 16220160 |
---------------------------------
Eval num_timesteps=16220424, episode_reward=0.14 +/- 0.98
Episode length: 29.97 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 16220424    |
| train/                  |             |
|    approx_kl            | 0.050939575 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.866      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.13        |
|    n_updates            | 1320        |
|    policy_gradient_loss | 8.22e-05    |
|    value_loss           | 0.237       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.142
SELFPLAY: new best model, bumping up generation to 26
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 265      |
|    time_elapsed    | 102476   |
|    total_timesteps | 16281600 |
---------------------------------
Eval num_timesteps=16281865, episode_reward=-0.02 +/- 0.99
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 16281865    |
| train/                  |             |
|    approx_kl            | 0.050582282 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.879      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.0498      |
|    n_updates            | 1325        |
|    policy_gradient_loss | -0.000155   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 266      |
|    time_elapsed    | 102889   |
|    total_timesteps | 16343040 |
---------------------------------
Eval num_timesteps=16343306, episode_reward=-0.03 +/- 0.99
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 16343306    |
| train/                  |             |
|    approx_kl            | 0.050807945 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.882      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.12        |
|    n_updates            | 1330        |
|    policy_gradient_loss | 0.000362    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 267      |
|    time_elapsed    | 103300   |
|    total_timesteps | 16404480 |
---------------------------------
Eval num_timesteps=16404747, episode_reward=-0.08 +/- 0.98
Episode length: 29.94 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.076      |
| time/                   |             |
|    total_timesteps      | 16404747    |
| train/                  |             |
|    approx_kl            | 0.048988834 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.882      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.0443      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -8.13e-05   |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 268      |
|    time_elapsed    | 103769   |
|    total_timesteps | 16465920 |
---------------------------------
Eval num_timesteps=16466188, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 16466188    |
| train/                  |             |
|    approx_kl            | 0.052054893 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.879      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.0668      |
|    n_updates            | 1340        |
|    policy_gradient_loss | 0.000718    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 269      |
|    time_elapsed    | 104181   |
|    total_timesteps | 16527360 |
---------------------------------
Eval num_timesteps=16527629, episode_reward=0.03 +/- 0.98
Episode length: 29.95 +/- 0.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 16527629    |
| train/                  |             |
|    approx_kl            | 0.049703024 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.00025     |
|    loss                 | 0.0957      |
|    n_updates            | 1345        |
|    policy_gradient_loss | -0.00056    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 270      |
|    time_elapsed    | 104635   |
|    total_timesteps | 16588800 |
---------------------------------
Eval num_timesteps=16589070, episode_reward=0.03 +/- 0.98
Episode length: 30.01 +/- 0.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.032     |
| time/                   |           |
|    total_timesteps      | 16589070  |
| train/                  |           |
|    approx_kl            | 0.0508831 |
|    clip_fraction        | 0.302     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.89     |
|    explained_variance   | 0.353     |
|    learning_rate        | 0.00025   |
|    loss                 | 0.0781    |
|    n_updates            | 1350      |
|    policy_gradient_loss | 0.000686  |
|    value_loss           | 0.234     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 271      |
|    time_elapsed    | 105095   |
|    total_timesteps | 16650240 |
---------------------------------
Eval num_timesteps=16650511, episode_reward=0.04 +/- 0.98
Episode length: 29.91 +/- 1.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.036      |
| time/                   |            |
|    total_timesteps      | 16650511   |
| train/                  |            |
|    approx_kl            | 0.04962189 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.885     |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.00025    |
|    loss                 | 0.106      |
|    n_updates            | 1355       |
|    policy_gradient_loss | 0.000254   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 272      |
|    time_elapsed    | 105504   |
|    total_timesteps | 16711680 |
---------------------------------
Eval num_timesteps=16711952, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 16711952    |
| train/                  |             |
|    approx_kl            | 0.053513367 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.885      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.00025     |
|    loss                 | 0.125       |
|    n_updates            | 1360        |
|    policy_gradient_loss | 0.00184     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 273      |
|    time_elapsed    | 105919   |
|    total_timesteps | 16773120 |
---------------------------------
Eval num_timesteps=16773393, episode_reward=0.13 +/- 0.98
Episode length: 29.97 +/- 0.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.132       |
| time/                   |             |
|    total_timesteps      | 16773393    |
| train/                  |             |
|    approx_kl            | 0.050878372 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.885      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.00025     |
|    loss                 | 0.2         |
|    n_updates            | 1365        |
|    policy_gradient_loss | 0.000898    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 274      |
|    time_elapsed    | 106360   |
|    total_timesteps | 16834560 |
---------------------------------
Eval num_timesteps=16834834, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.098      |
| time/                   |            |
|    total_timesteps      | 16834834   |
| train/                  |            |
|    approx_kl            | 0.05080249 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.891     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.000249   |
|    loss                 | 0.0105     |
|    n_updates            | 1370       |
|    policy_gradient_loss | 5.18e-05   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 275      |
|    time_elapsed    | 106795   |
|    total_timesteps | 16896000 |
---------------------------------
Eval num_timesteps=16896275, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.088      |
| time/                   |            |
|    total_timesteps      | 16896275   |
| train/                  |            |
|    approx_kl            | 0.04990828 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.879     |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000249   |
|    loss                 | 0.146      |
|    n_updates            | 1375       |
|    policy_gradient_loss | 0.000285   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 276      |
|    time_elapsed    | 107206   |
|    total_timesteps | 16957440 |
---------------------------------
Eval num_timesteps=16957716, episode_reward=0.07 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 16957716    |
| train/                  |             |
|    approx_kl            | 0.049550246 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000249    |
|    loss                 | 0.0892      |
|    n_updates            | 1380        |
|    policy_gradient_loss | 0.000302    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 277      |
|    time_elapsed    | 107649   |
|    total_timesteps | 17018880 |
---------------------------------
Eval num_timesteps=17019157, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 17019157   |
| train/                  |            |
|    approx_kl            | 0.04906197 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.872     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000249   |
|    loss                 | 0.0662     |
|    n_updates            | 1385       |
|    policy_gradient_loss | -0.000196  |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 278      |
|    time_elapsed    | 108117   |
|    total_timesteps | 17080320 |
---------------------------------
Eval num_timesteps=17080598, episode_reward=0.12 +/- 0.98
Episode length: 30.03 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 17080598    |
| train/                  |             |
|    approx_kl            | 0.049191173 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000249    |
|    loss                 | 0.0736      |
|    n_updates            | 1390        |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 279      |
|    time_elapsed    | 108528   |
|    total_timesteps | 17141760 |
---------------------------------
Eval num_timesteps=17142039, episode_reward=0.04 +/- 0.99
Episode length: 29.88 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 17142039    |
| train/                  |             |
|    approx_kl            | 0.048418775 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000249    |
|    loss                 | 0.0867      |
|    n_updates            | 1395        |
|    policy_gradient_loss | 0.000329    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 280      |
|    time_elapsed    | 108938   |
|    total_timesteps | 17203200 |
---------------------------------
Eval num_timesteps=17203480, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.042      |
| time/                   |            |
|    total_timesteps      | 17203480   |
| train/                  |            |
|    approx_kl            | 0.05086371 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.871     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000248   |
|    loss                 | 0.059      |
|    n_updates            | 1400       |
|    policy_gradient_loss | 0.000306   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 281      |
|    time_elapsed    | 109413   |
|    total_timesteps | 17264640 |
---------------------------------
Eval num_timesteps=17264921, episode_reward=0.15 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.154       |
| time/                   |             |
|    total_timesteps      | 17264921    |
| train/                  |             |
|    approx_kl            | 0.050545573 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000248    |
|    loss                 | 0.12        |
|    n_updates            | 1405        |
|    policy_gradient_loss | 0.000751    |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.154
SELFPLAY: new best model, bumping up generation to 27
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 282      |
|    time_elapsed    | 109822   |
|    total_timesteps | 17326080 |
---------------------------------
Eval num_timesteps=17326362, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 17326362   |
| train/                  |            |
|    approx_kl            | 0.04997533 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.88      |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000248   |
|    loss                 | 0.074      |
|    n_updates            | 1410       |
|    policy_gradient_loss | -7.07e-05  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 283      |
|    time_elapsed    | 110235   |
|    total_timesteps | 17387520 |
---------------------------------
Eval num_timesteps=17387803, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.032     |
| time/                   |            |
|    total_timesteps      | 17387803   |
| train/                  |            |
|    approx_kl            | 0.04764305 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.875     |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.000248   |
|    loss                 | 0.03       |
|    n_updates            | 1415       |
|    policy_gradient_loss | 0.000838   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 284      |
|    time_elapsed    | 110685   |
|    total_timesteps | 17448960 |
---------------------------------
Eval num_timesteps=17449244, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 17449244   |
| train/                  |            |
|    approx_kl            | 0.04832197 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.876     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000248   |
|    loss                 | 0.0318     |
|    n_updates            | 1420       |
|    policy_gradient_loss | 0.00133    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 157      |
|    iterations      | 285      |
|    time_elapsed    | 111153   |
|    total_timesteps | 17510400 |
---------------------------------
Eval num_timesteps=17510685, episode_reward=0.01 +/- 0.98
Episode length: 29.92 +/- 1.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 17510685    |
| train/                  |             |
|    approx_kl            | 0.047753118 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000247    |
|    loss                 | 0.102       |
|    n_updates            | 1425        |
|    policy_gradient_loss | 0.000739    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 157      |
|    iterations      | 286      |
|    time_elapsed    | 111561   |
|    total_timesteps | 17571840 |
---------------------------------
Eval num_timesteps=17572126, episode_reward=-0.04 +/- 0.99
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.044     |
| time/                   |            |
|    total_timesteps      | 17572126   |
| train/                  |            |
|    approx_kl            | 0.04870475 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.882     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000247   |
|    loss                 | 0.0742     |
|    n_updates            | 1430       |
|    policy_gradient_loss | 0.00111    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 287      |
|    time_elapsed    | 111996   |
|    total_timesteps | 17633280 |
---------------------------------
Eval num_timesteps=17633567, episode_reward=-0.01 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.014     |
| time/                   |            |
|    total_timesteps      | 17633567   |
| train/                  |            |
|    approx_kl            | 0.04945692 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.879     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000247   |
|    loss                 | 0.0707     |
|    n_updates            | 1435       |
|    policy_gradient_loss | 0.000383   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 288      |
|    time_elapsed    | 112439   |
|    total_timesteps | 17694720 |
---------------------------------
Eval num_timesteps=17695008, episode_reward=-0.11 +/- 0.98
Episode length: 29.91 +/- 0.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.11       |
| time/                   |             |
|    total_timesteps      | 17695008    |
| train/                  |             |
|    approx_kl            | 0.047817778 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000247    |
|    loss                 | 0.0775      |
|    n_updates            | 1440        |
|    policy_gradient_loss | 0.000336    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 289      |
|    time_elapsed    | 112848   |
|    total_timesteps | 17756160 |
---------------------------------
Eval num_timesteps=17756449, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 1.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 17756449   |
| train/                  |            |
|    approx_kl            | 0.04969721 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.866     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000247   |
|    loss                 | 0.0905     |
|    n_updates            | 1445       |
|    policy_gradient_loss | 6.27e-05   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 290      |
|    time_elapsed    | 113261   |
|    total_timesteps | 17817600 |
---------------------------------
Eval num_timesteps=17817890, episode_reward=0.04 +/- 0.99
Episode length: 29.95 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.036      |
| time/                   |            |
|    total_timesteps      | 17817890   |
| train/                  |            |
|    approx_kl            | 0.04727805 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.858     |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000247   |
|    loss                 | 0.131      |
|    n_updates            | 1450       |
|    policy_gradient_loss | 0.000825   |
|    value_loss           | 0.244      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 291      |
|    time_elapsed    | 113704   |
|    total_timesteps | 17879040 |
---------------------------------
Eval num_timesteps=17879331, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 17879331    |
| train/                  |             |
|    approx_kl            | 0.048176147 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000246    |
|    loss                 | 0.0867      |
|    n_updates            | 1455        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 292      |
|    time_elapsed    | 114174   |
|    total_timesteps | 17940480 |
---------------------------------
Eval num_timesteps=17940772, episode_reward=0.00 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17940772    |
| train/                  |             |
|    approx_kl            | 0.048379507 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.86       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000246    |
|    loss                 | 0.07        |
|    n_updates            | 1460        |
|    policy_gradient_loss | 0.000744    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 293      |
|    time_elapsed    | 114583   |
|    total_timesteps | 18001920 |
---------------------------------
Eval num_timesteps=18002213, episode_reward=0.04 +/- 0.98
Episode length: 29.92 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 18002213    |
| train/                  |             |
|    approx_kl            | 0.046867624 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.856      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000246    |
|    loss                 | 0.0759      |
|    n_updates            | 1465        |
|    policy_gradient_loss | -0.000256   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 294      |
|    time_elapsed    | 115052   |
|    total_timesteps | 18063360 |
---------------------------------
Eval num_timesteps=18063654, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 18063654   |
| train/                  |            |
|    approx_kl            | 0.04719299 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.853     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000246   |
|    loss                 | 0.0769     |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.000366  |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 295      |
|    time_elapsed    | 115464   |
|    total_timesteps | 18124800 |
---------------------------------
Eval num_timesteps=18125095, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 18125095    |
| train/                  |             |
|    approx_kl            | 0.044133745 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.847      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000246    |
|    loss                 | 0.191       |
|    n_updates            | 1475        |
|    policy_gradient_loss | 0.000329    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 296      |
|    time_elapsed    | 115871   |
|    total_timesteps | 18186240 |
---------------------------------
Eval num_timesteps=18186536, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 18186536   |
| train/                  |            |
|    approx_kl            | 0.04514707 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.853     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000245   |
|    loss                 | 0.0709     |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.00075   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 297      |
|    time_elapsed    | 116284   |
|    total_timesteps | 18247680 |
---------------------------------
Eval num_timesteps=18247977, episode_reward=0.09 +/- 0.99
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 18247977    |
| train/                  |             |
|    approx_kl            | 0.049477797 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000245    |
|    loss                 | 0.121       |
|    n_updates            | 1485        |
|    policy_gradient_loss | 0.000366    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 298      |
|    time_elapsed    | 116725   |
|    total_timesteps | 18309120 |
---------------------------------
Eval num_timesteps=18309418, episode_reward=-0.03 +/- 0.98
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 18309418   |
| train/                  |            |
|    approx_kl            | 0.04699533 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.835     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000245   |
|    loss                 | 0.0626     |
|    n_updates            | 1490       |
|    policy_gradient_loss | 0.000213   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 299      |
|    time_elapsed    | 117203   |
|    total_timesteps | 18370560 |
---------------------------------
Eval num_timesteps=18370859, episode_reward=0.00 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.004      |
| time/                   |            |
|    total_timesteps      | 18370859   |
| train/                  |            |
|    approx_kl            | 0.04866848 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.836     |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.000245   |
|    loss                 | 0.0402     |
|    n_updates            | 1495       |
|    policy_gradient_loss | -0.00031   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 300      |
|    time_elapsed    | 117634   |
|    total_timesteps | 18432000 |
---------------------------------
Eval num_timesteps=18432300, episode_reward=0.01 +/- 0.99
Episode length: 29.95 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 18432300    |
| train/                  |             |
|    approx_kl            | 0.046889145 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000245    |
|    loss                 | 0.085       |
|    n_updates            | 1500        |
|    policy_gradient_loss | 9.22e-05    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 301      |
|    time_elapsed    | 118082   |
|    total_timesteps | 18493440 |
---------------------------------
Eval num_timesteps=18493741, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 18493741    |
| train/                  |             |
|    approx_kl            | 0.047663264 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000245    |
|    loss                 | 0.094       |
|    n_updates            | 1505        |
|    policy_gradient_loss | 0.000236    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 302      |
|    time_elapsed    | 118496   |
|    total_timesteps | 18554880 |
---------------------------------
Eval num_timesteps=18555182, episode_reward=0.08 +/- 0.99
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 18555182   |
| train/                  |            |
|    approx_kl            | 0.04600654 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.838     |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.000244   |
|    loss                 | 0.0428     |
|    n_updates            | 1510       |
|    policy_gradient_loss | 0.000505   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 303      |
|    time_elapsed    | 118906   |
|    total_timesteps | 18616320 |
---------------------------------
Eval num_timesteps=18616623, episode_reward=0.08 +/- 0.98
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 18616623   |
| train/                  |            |
|    approx_kl            | 0.04864274 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.836     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000244   |
|    loss                 | 0.118      |
|    n_updates            | 1515       |
|    policy_gradient_loss | 0.000136   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 304      |
|    time_elapsed    | 119320   |
|    total_timesteps | 18677760 |
---------------------------------
Eval num_timesteps=18678064, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 18678064    |
| train/                  |             |
|    approx_kl            | 0.050084922 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.834      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000244    |
|    loss                 | 0.0596      |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.000222   |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 305      |
|    time_elapsed    | 119752   |
|    total_timesteps | 18739200 |
---------------------------------
Eval num_timesteps=18739505, episode_reward=0.10 +/- 0.98
Episode length: 30.03 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.102      |
| time/                   |            |
|    total_timesteps      | 18739505   |
| train/                  |            |
|    approx_kl            | 0.04878829 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.831     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000244   |
|    loss                 | 0.0447     |
|    n_updates            | 1525       |
|    policy_gradient_loss | 0.00119    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 306      |
|    time_elapsed    | 120227   |
|    total_timesteps | 18800640 |
---------------------------------
Eval num_timesteps=18800946, episode_reward=0.09 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 18800946    |
| train/                  |             |
|    approx_kl            | 0.047312055 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.831      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000244    |
|    loss                 | 0.14        |
|    n_updates            | 1530        |
|    policy_gradient_loss | 0.000397    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 307      |
|    time_elapsed    | 120692   |
|    total_timesteps | 18862080 |
---------------------------------
Eval num_timesteps=18862387, episode_reward=0.11 +/- 0.99
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 18862387   |
| train/                  |            |
|    approx_kl            | 0.04826851 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.828     |
|    explained_variance   | 0.328      |
|    learning_rate        | 0.000243   |
|    loss                 | 0.107      |
|    n_updates            | 1535       |
|    policy_gradient_loss | -0.000502  |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 308      |
|    time_elapsed    | 121104   |
|    total_timesteps | 18923520 |
---------------------------------
Eval num_timesteps=18923828, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.066      |
| time/                   |            |
|    total_timesteps      | 18923828   |
| train/                  |            |
|    approx_kl            | 0.04633663 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000243   |
|    loss                 | 0.0548     |
|    n_updates            | 1540       |
|    policy_gradient_loss | 0.000103   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 309      |
|    time_elapsed    | 121519   |
|    total_timesteps | 18984960 |
---------------------------------
Eval num_timesteps=18985269, episode_reward=0.17 +/- 0.96
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.174       |
| time/                   |             |
|    total_timesteps      | 18985269    |
| train/                  |             |
|    approx_kl            | 0.047583412 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000243    |
|    loss                 | 0.106       |
|    n_updates            | 1545        |
|    policy_gradient_loss | 0.000448    |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.174
SELFPLAY: new best model, bumping up generation to 28
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 310      |
|    time_elapsed    | 121929   |
|    total_timesteps | 19046400 |
---------------------------------
Eval num_timesteps=19046710, episode_reward=0.01 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 19046710    |
| train/                  |             |
|    approx_kl            | 0.045909032 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.851      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000243    |
|    loss                 | 0.104       |
|    n_updates            | 1550        |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 156      |
|    iterations      | 311      |
|    time_elapsed    | 122342   |
|    total_timesteps | 19107840 |
---------------------------------
Eval num_timesteps=19108151, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 19108151    |
| train/                  |             |
|    approx_kl            | 0.045931317 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.847      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000243    |
|    loss                 | 0.134       |
|    n_updates            | 1555        |
|    policy_gradient_loss | 0.000549    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 312      |
|    time_elapsed    | 122777   |
|    total_timesteps | 19169280 |
---------------------------------
Eval num_timesteps=19169592, episode_reward=0.06 +/- 0.98
Episode length: 29.90 +/- 1.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 19169592    |
| train/                  |             |
|    approx_kl            | 0.047331873 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000242    |
|    loss                 | 0.103       |
|    n_updates            | 1560        |
|    policy_gradient_loss | 0.000328    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 313      |
|    time_elapsed    | 123143   |
|    total_timesteps | 19230720 |
---------------------------------
Eval num_timesteps=19231033, episode_reward=-0.02 +/- 0.99
Episode length: 29.90 +/- 1.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.018     |
| time/                   |            |
|    total_timesteps      | 19231033   |
| train/                  |            |
|    approx_kl            | 0.04504438 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.856     |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.000242   |
|    loss                 | 0.135      |
|    n_updates            | 1565       |
|    policy_gradient_loss | 0.00206    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 314      |
|    time_elapsed    | 123490   |
|    total_timesteps | 19292160 |
---------------------------------
Eval num_timesteps=19292474, episode_reward=-0.01 +/- 0.99
Episode length: 29.94 +/- 1.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.01      |
| time/                   |            |
|    total_timesteps      | 19292474   |
| train/                  |            |
|    approx_kl            | 0.04412482 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.842     |
|    explained_variance   | 0.328      |
|    learning_rate        | 0.000242   |
|    loss                 | 0.0768     |
|    n_updates            | 1570       |
|    policy_gradient_loss | 0.00142    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 315      |
|    time_elapsed    | 123838   |
|    total_timesteps | 19353600 |
---------------------------------
Eval num_timesteps=19353915, episode_reward=-0.05 +/- 0.99
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.046      |
| time/                   |             |
|    total_timesteps      | 19353915    |
| train/                  |             |
|    approx_kl            | 0.045280196 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000242    |
|    loss                 | 0.0894      |
|    n_updates            | 1575        |
|    policy_gradient_loss | 0.000714    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 156      |
|    iterations      | 316      |
|    time_elapsed    | 124187   |
|    total_timesteps | 19415040 |
---------------------------------
Eval num_timesteps=19415356, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 19415356   |
| train/                  |            |
|    approx_kl            | 0.04888149 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.838     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000242   |
|    loss                 | 0.0368     |
|    n_updates            | 1580       |
|    policy_gradient_loss | 0.000285   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 317      |
|    time_elapsed    | 124537   |
|    total_timesteps | 19476480 |
---------------------------------
Eval num_timesteps=19476797, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.078      |
| time/                   |            |
|    total_timesteps      | 19476797   |
| train/                  |            |
|    approx_kl            | 0.04595685 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.825     |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000242   |
|    loss                 | 0.0854     |
|    n_updates            | 1585       |
|    policy_gradient_loss | 0.000369   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 318      |
|    time_elapsed    | 124888   |
|    total_timesteps | 19537920 |
---------------------------------
Eval num_timesteps=19538238, episode_reward=0.11 +/- 0.98
Episode length: 29.89 +/- 1.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.114      |
| time/                   |            |
|    total_timesteps      | 19538238   |
| train/                  |            |
|    approx_kl            | 0.04681004 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.822     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000241   |
|    loss                 | 0.0535     |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.000134  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 319      |
|    time_elapsed    | 125237   |
|    total_timesteps | 19599360 |
---------------------------------
Eval num_timesteps=19599679, episode_reward=0.00 +/- 0.99
Episode length: 29.95 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 19599679    |
| train/                  |             |
|    approx_kl            | 0.046338223 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000241    |
|    loss                 | 0.05        |
|    n_updates            | 1595        |
|    policy_gradient_loss | 0.000986    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 320      |
|    time_elapsed    | 125586   |
|    total_timesteps | 19660800 |
---------------------------------
Eval num_timesteps=19661120, episode_reward=0.08 +/- 0.99
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 19661120    |
| train/                  |             |
|    approx_kl            | 0.045861676 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.813      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000241    |
|    loss                 | 0.0741      |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.000343    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 321      |
|    time_elapsed    | 125935   |
|    total_timesteps | 19722240 |
---------------------------------
Eval num_timesteps=19722561, episode_reward=-0.06 +/- 0.99
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.064     |
| time/                   |            |
|    total_timesteps      | 19722561   |
| train/                  |            |
|    approx_kl            | 0.04655061 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.818     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000241   |
|    loss                 | 0.153      |
|    n_updates            | 1605       |
|    policy_gradient_loss | -0.000272  |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 322      |
|    time_elapsed    | 126284   |
|    total_timesteps | 19783680 |
---------------------------------
Eval num_timesteps=19784002, episode_reward=0.00 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 19784002    |
| train/                  |             |
|    approx_kl            | 0.046211313 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000241    |
|    loss                 | 0.117       |
|    n_updates            | 1610        |
|    policy_gradient_loss | 0.000958    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 323      |
|    time_elapsed    | 126633   |
|    total_timesteps | 19845120 |
---------------------------------
Eval num_timesteps=19845443, episode_reward=-0.01 +/- 0.99
Episode length: 29.85 +/- 1.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 19845443    |
| train/                  |             |
|    approx_kl            | 0.044088934 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.00024     |
|    loss                 | 0.0954      |
|    n_updates            | 1615        |
|    policy_gradient_loss | 0.00161     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 324      |
|    time_elapsed    | 126981   |
|    total_timesteps | 19906560 |
---------------------------------
Eval num_timesteps=19906884, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 1.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.066      |
| time/                   |            |
|    total_timesteps      | 19906884   |
| train/                  |            |
|    approx_kl            | 0.04591934 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.814     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.00024    |
|    loss                 | 0.0826     |
|    n_updates            | 1620       |
|    policy_gradient_loss | 0.000911   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 325      |
|    time_elapsed    | 127329   |
|    total_timesteps | 19968000 |
---------------------------------
Eval num_timesteps=19968325, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 19968325    |
| train/                  |             |
|    approx_kl            | 0.045499165 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.00024     |
|    loss                 | 0.0842      |
|    n_updates            | 1625        |
|    policy_gradient_loss | 0.000472    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 326      |
|    time_elapsed    | 127676   |
|    total_timesteps | 20029440 |
---------------------------------
Eval num_timesteps=20029766, episode_reward=0.08 +/- 0.99
Episode length: 30.02 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 20029766   |
| train/                  |            |
|    approx_kl            | 0.04835862 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.82      |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.00024    |
|    loss                 | 0.145      |
|    n_updates            | 1630       |
|    policy_gradient_loss | 0.00181    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 327      |
|    time_elapsed    | 128026   |
|    total_timesteps | 20090880 |
---------------------------------
Eval num_timesteps=20091207, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 0.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.006      |
| time/                   |            |
|    total_timesteps      | 20091207   |
| train/                  |            |
|    approx_kl            | 0.04420797 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.82      |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.00024    |
|    loss                 | 0.0541     |
|    n_updates            | 1635       |
|    policy_gradient_loss | 0.000235   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 328      |
|    time_elapsed    | 128377   |
|    total_timesteps | 20152320 |
---------------------------------
Eval num_timesteps=20152648, episode_reward=0.02 +/- 0.99
Episode length: 29.91 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 20152648    |
| train/                  |             |
|    approx_kl            | 0.043717306 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.00024     |
|    loss                 | 0.147       |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00028    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 329      |
|    time_elapsed    | 128727   |
|    total_timesteps | 20213760 |
---------------------------------
Eval num_timesteps=20214089, episode_reward=0.02 +/- 0.99
Episode length: 29.95 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 20214089    |
| train/                  |             |
|    approx_kl            | 0.044434715 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.809      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000239    |
|    loss                 | 0.125       |
|    n_updates            | 1645        |
|    policy_gradient_loss | -0.000345   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 330      |
|    time_elapsed    | 129075   |
|    total_timesteps | 20275200 |
---------------------------------
Eval num_timesteps=20275530, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 1.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.01       |
| time/                   |            |
|    total_timesteps      | 20275530   |
| train/                  |            |
|    approx_kl            | 0.04610551 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000239   |
|    loss                 | 0.0728     |
|    n_updates            | 1650       |
|    policy_gradient_loss | 8.5e-05    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 331      |
|    time_elapsed    | 129423   |
|    total_timesteps | 20336640 |
---------------------------------
Eval num_timesteps=20336971, episode_reward=-0.02 +/- 0.99
Episode length: 29.99 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 20336971    |
| train/                  |             |
|    approx_kl            | 0.042966556 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.799      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000239    |
|    loss                 | 0.104       |
|    n_updates            | 1655        |
|    policy_gradient_loss | 6.31e-05    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 332      |
|    time_elapsed    | 129772   |
|    total_timesteps | 20398080 |
---------------------------------
Eval num_timesteps=20398412, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 20398412    |
| train/                  |             |
|    approx_kl            | 0.043077223 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.802      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.000239    |
|    loss                 | 0.101       |
|    n_updates            | 1660        |
|    policy_gradient_loss | 0.0015      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 333      |
|    time_elapsed    | 130122   |
|    total_timesteps | 20459520 |
---------------------------------
Eval num_timesteps=20459853, episode_reward=0.01 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 20459853    |
| train/                  |             |
|    approx_kl            | 0.044165455 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000239    |
|    loss                 | 0.0803      |
|    n_updates            | 1665        |
|    policy_gradient_loss | 0.000622    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 334      |
|    time_elapsed    | 130503   |
|    total_timesteps | 20520960 |
---------------------------------
Eval num_timesteps=20521294, episode_reward=0.04 +/- 0.98
Episode length: 30.01 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 20521294    |
| train/                  |             |
|    approx_kl            | 0.045182664 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000238    |
|    loss                 | 0.0429      |
|    n_updates            | 1670        |
|    policy_gradient_loss | 5.25e-06    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 335      |
|    time_elapsed    | 130946   |
|    total_timesteps | 20582400 |
---------------------------------
Eval num_timesteps=20582735, episode_reward=0.04 +/- 0.99
Episode length: 29.99 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 20582735   |
| train/                  |            |
|    approx_kl            | 0.04705655 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.8       |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.000238   |
|    loss                 | 0.103      |
|    n_updates            | 1675       |
|    policy_gradient_loss | 8.03e-05   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 336      |
|    time_elapsed    | 131352   |
|    total_timesteps | 20643840 |
---------------------------------
Eval num_timesteps=20644176, episode_reward=0.04 +/- 0.99
Episode length: 30.01 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 20644176    |
| train/                  |             |
|    approx_kl            | 0.042768326 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.805      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000238    |
|    loss                 | 0.0803      |
|    n_updates            | 1680        |
|    policy_gradient_loss | 0.000212    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 337      |
|    time_elapsed    | 131802   |
|    total_timesteps | 20705280 |
---------------------------------
Eval num_timesteps=20705617, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 20705617    |
| train/                  |             |
|    approx_kl            | 0.042830095 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.802      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000238    |
|    loss                 | 0.045       |
|    n_updates            | 1685        |
|    policy_gradient_loss | 0.000367    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 338      |
|    time_elapsed    | 132207   |
|    total_timesteps | 20766720 |
---------------------------------
Eval num_timesteps=20767058, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 20767058    |
| train/                  |             |
|    approx_kl            | 0.043666527 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.808      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000238    |
|    loss                 | 0.061       |
|    n_updates            | 1690        |
|    policy_gradient_loss | 0.000631    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 339      |
|    time_elapsed    | 132698   |
|    total_timesteps | 20828160 |
---------------------------------
Eval num_timesteps=20828499, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.054     |
| time/                   |           |
|    total_timesteps      | 20828499  |
| train/                  |           |
|    approx_kl            | 0.0450747 |
|    clip_fraction        | 0.27      |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.807    |
|    explained_variance   | 0.344     |
|    learning_rate        | 0.000238  |
|    loss                 | 0.198     |
|    n_updates            | 1695      |
|    policy_gradient_loss | -4.03e-05 |
|    value_loss           | 0.235     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 340      |
|    time_elapsed    | 133191   |
|    total_timesteps | 20889600 |
---------------------------------
Eval num_timesteps=20889940, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 20889940    |
| train/                  |             |
|    approx_kl            | 0.043256488 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.803      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.1         |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.000439   |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 341      |
|    time_elapsed    | 133662   |
|    total_timesteps | 20951040 |
---------------------------------
Eval num_timesteps=20951381, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 20951381    |
| train/                  |             |
|    approx_kl            | 0.042602252 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.808      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.088       |
|    n_updates            | 1705        |
|    policy_gradient_loss | 0.000182    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 342      |
|    time_elapsed    | 134167   |
|    total_timesteps | 21012480 |
---------------------------------
Eval num_timesteps=21012822, episode_reward=0.02 +/- 0.98
Episode length: 29.88 +/- 1.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 21012822    |
| train/                  |             |
|    approx_kl            | 0.045286004 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0794      |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 343      |
|    time_elapsed    | 134627   |
|    total_timesteps | 21073920 |
---------------------------------
Eval num_timesteps=21074263, episode_reward=0.07 +/- 0.99
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 21074263    |
| train/                  |             |
|    approx_kl            | 0.042775113 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.801      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0786      |
|    n_updates            | 1715        |
|    policy_gradient_loss | -0.000458   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 344      |
|    time_elapsed    | 135150   |
|    total_timesteps | 21135360 |
---------------------------------
Eval num_timesteps=21135704, episode_reward=0.05 +/- 0.99
Episode length: 29.93 +/- 0.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 21135704   |
| train/                  |            |
|    approx_kl            | 0.04182488 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.795     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000237   |
|    loss                 | 0.0744     |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.000588  |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 345      |
|    time_elapsed    | 135607   |
|    total_timesteps | 21196800 |
---------------------------------
Eval num_timesteps=21197145, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 21197145    |
| train/                  |             |
|    approx_kl            | 0.044838555 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000236    |
|    loss                 | 0.142       |
|    n_updates            | 1725        |
|    policy_gradient_loss | 0.000415    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 346      |
|    time_elapsed    | 136118   |
|    total_timesteps | 21258240 |
---------------------------------
Eval num_timesteps=21258586, episode_reward=-0.04 +/- 0.99
Episode length: 29.90 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.044     |
| time/                   |            |
|    total_timesteps      | 21258586   |
| train/                  |            |
|    approx_kl            | 0.04398156 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.807     |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.000236   |
|    loss                 | 0.0852     |
|    n_updates            | 1730       |
|    policy_gradient_loss | 0.000717   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 347      |
|    time_elapsed    | 136579   |
|    total_timesteps | 21319680 |
---------------------------------
Eval num_timesteps=21320027, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 21320027   |
| train/                  |            |
|    approx_kl            | 0.04500394 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000236   |
|    loss                 | 0.107      |
|    n_updates            | 1735       |
|    policy_gradient_loss | 0.000321   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 348      |
|    time_elapsed    | 137097   |
|    total_timesteps | 21381120 |
---------------------------------
Eval num_timesteps=21381468, episode_reward=-0.00 +/- 0.98
Episode length: 29.91 +/- 1.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 21381468   |
| train/                  |            |
|    approx_kl            | 0.04332367 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.805     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000236   |
|    loss                 | 0.103      |
|    n_updates            | 1740       |
|    policy_gradient_loss | 0.000202   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 349      |
|    time_elapsed    | 137542   |
|    total_timesteps | 21442560 |
---------------------------------
Eval num_timesteps=21442909, episode_reward=0.03 +/- 0.99
Episode length: 29.94 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 21442909    |
| train/                  |             |
|    approx_kl            | 0.042595513 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000236    |
|    loss                 | 0.113       |
|    n_updates            | 1745        |
|    policy_gradient_loss | -0.000181   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 350      |
|    time_elapsed    | 138076   |
|    total_timesteps | 21504000 |
---------------------------------
Eval num_timesteps=21504350, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 21504350    |
| train/                  |             |
|    approx_kl            | 0.043595076 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.796      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000235    |
|    loss                 | 0.0912      |
|    n_updates            | 1750        |
|    policy_gradient_loss | 0.000175    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 351      |
|    time_elapsed    | 138534   |
|    total_timesteps | 21565440 |
---------------------------------
Eval num_timesteps=21565791, episode_reward=0.03 +/- 0.99
Episode length: 29.93 +/- 1.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.026      |
| time/                   |            |
|    total_timesteps      | 21565791   |
| train/                  |            |
|    approx_kl            | 0.04161292 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.786     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000235   |
|    loss                 | 0.152      |
|    n_updates            | 1755       |
|    policy_gradient_loss | -0.00135   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 352      |
|    time_elapsed    | 139040   |
|    total_timesteps | 21626880 |
---------------------------------
Eval num_timesteps=21627232, episode_reward=0.01 +/- 0.98
Episode length: 29.91 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 21627232    |
| train/                  |             |
|    approx_kl            | 0.045661457 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000235    |
|    loss                 | 0.129       |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.000627   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 353      |
|    time_elapsed    | 139516   |
|    total_timesteps | 21688320 |
---------------------------------
Eval num_timesteps=21688673, episode_reward=0.05 +/- 0.99
Episode length: 29.96 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 21688673    |
| train/                  |             |
|    approx_kl            | 0.045213956 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.79       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000235    |
|    loss                 | 0.082       |
|    n_updates            | 1765        |
|    policy_gradient_loss | 0.000405    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 354      |
|    time_elapsed    | 140003   |
|    total_timesteps | 21749760 |
---------------------------------
Eval num_timesteps=21750114, episode_reward=0.04 +/- 0.99
Episode length: 29.92 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 21750114   |
| train/                  |            |
|    approx_kl            | 0.04327585 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.777     |
|    explained_variance   | 0.339      |
|    learning_rate        | 0.000235   |
|    loss                 | 0.0924     |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.00106   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 355      |
|    time_elapsed    | 140493   |
|    total_timesteps | 21811200 |
---------------------------------
Eval num_timesteps=21811555, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 21811555    |
| train/                  |             |
|    approx_kl            | 0.044804342 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.772      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000235    |
|    loss                 | 0.0632      |
|    n_updates            | 1775        |
|    policy_gradient_loss | -0.000156   |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 356      |
|    time_elapsed    | 140997   |
|    total_timesteps | 21872640 |
---------------------------------
Eval num_timesteps=21872996, episode_reward=0.06 +/- 0.98
Episode length: 29.92 +/- 0.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.058      |
| time/                   |            |
|    total_timesteps      | 21872996   |
| train/                  |            |
|    approx_kl            | 0.04512446 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.000234   |
|    loss                 | 0.0686     |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.00018   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 357      |
|    time_elapsed    | 141457   |
|    total_timesteps | 21934080 |
---------------------------------
Eval num_timesteps=21934437, episode_reward=0.05 +/- 0.99
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 21934437    |
| train/                  |             |
|    approx_kl            | 0.044348065 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000234    |
|    loss                 | 0.175       |
|    n_updates            | 1785        |
|    policy_gradient_loss | 0.000743    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 358      |
|    time_elapsed    | 141987   |
|    total_timesteps | 21995520 |
---------------------------------
Eval num_timesteps=21995878, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.12        |
| time/                   |             |
|    total_timesteps      | 21995878    |
| train/                  |             |
|    approx_kl            | 0.043563925 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.77       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000234    |
|    loss                 | 0.182       |
|    n_updates            | 1790        |
|    policy_gradient_loss | 0.000543    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 359      |
|    time_elapsed    | 142427   |
|    total_timesteps | 22056960 |
---------------------------------
Eval num_timesteps=22057319, episode_reward=0.03 +/- 0.99
Episode length: 29.95 +/- 1.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.034     |
| time/                   |           |
|    total_timesteps      | 22057319  |
| train/                  |           |
|    approx_kl            | 0.0457573 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.768    |
|    explained_variance   | 0.33      |
|    learning_rate        | 0.000234  |
|    loss                 | 0.0807    |
|    n_updates            | 1795      |
|    policy_gradient_loss | 0.000705  |
|    value_loss           | 0.242     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 360      |
|    time_elapsed    | 142953   |
|    total_timesteps | 22118400 |
---------------------------------
Eval num_timesteps=22118760, episode_reward=0.04 +/- 0.99
Episode length: 29.93 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 22118760    |
| train/                  |             |
|    approx_kl            | 0.047426157 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000234    |
|    loss                 | 0.127       |
|    n_updates            | 1800        |
|    policy_gradient_loss | 0.000158    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 361      |
|    time_elapsed    | 143395   |
|    total_timesteps | 22179840 |
---------------------------------
Eval num_timesteps=22180201, episode_reward=0.04 +/- 0.99
Episode length: 29.89 +/- 1.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 22180201    |
| train/                  |             |
|    approx_kl            | 0.044170894 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000233    |
|    loss                 | 0.117       |
|    n_updates            | 1805        |
|    policy_gradient_loss | -0.000556   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 362      |
|    time_elapsed    | 143922   |
|    total_timesteps | 22241280 |
---------------------------------
Eval num_timesteps=22241642, episode_reward=0.07 +/- 0.98
Episode length: 29.95 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 22241642    |
| train/                  |             |
|    approx_kl            | 0.045717828 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000233    |
|    loss                 | 0.149       |
|    n_updates            | 1810        |
|    policy_gradient_loss | 0.000633    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 363      |
|    time_elapsed    | 144389   |
|    total_timesteps | 22302720 |
---------------------------------
Eval num_timesteps=22303083, episode_reward=0.17 +/- 0.98
Episode length: 29.86 +/- 1.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.166       |
| time/                   |             |
|    total_timesteps      | 22303083    |
| train/                  |             |
|    approx_kl            | 0.045404587 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.000233    |
|    loss                 | 0.114       |
|    n_updates            | 1815        |
|    policy_gradient_loss | 0.000368    |
|    value_loss           | 0.242       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.166
SELFPLAY: new best model, bumping up generation to 29
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 364      |
|    time_elapsed    | 144892   |
|    total_timesteps | 22364160 |
---------------------------------
Eval num_timesteps=22364524, episode_reward=-0.03 +/- 0.98
Episode length: 29.93 +/- 1.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 22364524    |
| train/                  |             |
|    approx_kl            | 0.045995213 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000233    |
|    loss                 | 0.0463      |
|    n_updates            | 1820        |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 365      |
|    time_elapsed    | 145371   |
|    total_timesteps | 22425600 |
---------------------------------
Eval num_timesteps=22425965, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.006      |
| time/                   |            |
|    total_timesteps      | 22425965   |
| train/                  |            |
|    approx_kl            | 0.04609533 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.806     |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.000233   |
|    loss                 | 0.111      |
|    n_updates            | 1825       |
|    policy_gradient_loss | 0.00123    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 366      |
|    time_elapsed    | 145866   |
|    total_timesteps | 22487040 |
---------------------------------
Eval num_timesteps=22487406, episode_reward=-0.01 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 22487406    |
| train/                  |             |
|    approx_kl            | 0.046561185 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000233    |
|    loss                 | 0.0816      |
|    n_updates            | 1830        |
|    policy_gradient_loss | 0.00134     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 367      |
|    time_elapsed    | 146351   |
|    total_timesteps | 22548480 |
---------------------------------
Eval num_timesteps=22548847, episode_reward=-0.00 +/- 0.99
Episode length: 29.84 +/- 1.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 22548847    |
| train/                  |             |
|    approx_kl            | 0.045318965 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000232    |
|    loss                 | 0.108       |
|    n_updates            | 1835        |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 153      |
|    iterations      | 368      |
|    time_elapsed    | 146855   |
|    total_timesteps | 22609920 |
---------------------------------
Eval num_timesteps=22610288, episode_reward=0.02 +/- 0.99
Episode length: 29.94 +/- 1.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 22610288   |
| train/                  |            |
|    approx_kl            | 0.04860954 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.000232   |
|    loss                 | 0.107      |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.000158  |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 369      |
|    time_elapsed    | 147315   |
|    total_timesteps | 22671360 |
---------------------------------
Eval num_timesteps=22671729, episode_reward=0.06 +/- 0.99
Episode length: 29.87 +/- 1.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 22671729    |
| train/                  |             |
|    approx_kl            | 0.043790955 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.787      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000232    |
|    loss                 | 0.141       |
|    n_updates            | 1845        |
|    policy_gradient_loss | 0.000462    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 153      |
|    iterations      | 370      |
|    time_elapsed    | 147844   |
|    total_timesteps | 22732800 |
---------------------------------
Eval num_timesteps=22733170, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.066      |
| time/                   |            |
|    total_timesteps      | 22733170   |
| train/                  |            |
|    approx_kl            | 0.04480883 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.781     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.000232   |
|    loss                 | 0.0471     |
|    n_updates            | 1850       |
|    policy_gradient_loss | 0.000179   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 371      |
|    time_elapsed    | 148226   |
|    total_timesteps | 22794240 |
---------------------------------
Eval num_timesteps=22794611, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 22794611    |
| train/                  |             |
|    approx_kl            | 0.044888306 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000232    |
|    loss                 | 0.104       |
|    n_updates            | 1855        |
|    policy_gradient_loss | -8.94e-05   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 372      |
|    time_elapsed    | 148574   |
|    total_timesteps | 22855680 |
---------------------------------
Eval num_timesteps=22856052, episode_reward=0.06 +/- 0.99
Episode length: 29.82 +/- 1.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 22856052    |
| train/                  |             |
|    approx_kl            | 0.044504274 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000231    |
|    loss                 | 0.157       |
|    n_updates            | 1860        |
|    policy_gradient_loss | 0.000712    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 373      |
|    time_elapsed    | 148923   |
|    total_timesteps | 22917120 |
---------------------------------
Eval num_timesteps=22917493, episode_reward=0.00 +/- 0.99
Episode length: 29.88 +/- 1.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 22917493   |
| train/                  |            |
|    approx_kl            | 0.04451551 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.776     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000231   |
|    loss                 | 0.0931     |
|    n_updates            | 1865       |
|    policy_gradient_loss | -0.000393  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 374      |
|    time_elapsed    | 149273   |
|    total_timesteps | 22978560 |
---------------------------------
Eval num_timesteps=22978934, episode_reward=-0.00 +/- 0.98
Episode length: 29.84 +/- 1.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 22978934    |
| train/                  |             |
|    approx_kl            | 0.042202514 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000231    |
|    loss                 | 0.0691      |
|    n_updates            | 1870        |
|    policy_gradient_loss | -3.99e-05   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 153      |
|    iterations      | 375      |
|    time_elapsed    | 149623   |
|    total_timesteps | 23040000 |
---------------------------------
Eval num_timesteps=23040375, episode_reward=0.10 +/- 0.98
Episode length: 29.86 +/- 1.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.098      |
| time/                   |            |
|    total_timesteps      | 23040375   |
| train/                  |            |
|    approx_kl            | 0.04435331 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000231   |
|    loss                 | 0.108      |
|    n_updates            | 1875       |
|    policy_gradient_loss | 0.000403   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 376      |
|    time_elapsed    | 149971   |
|    total_timesteps | 23101440 |
---------------------------------
Eval num_timesteps=23101816, episode_reward=0.05 +/- 0.99
Episode length: 29.89 +/- 1.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 23101816    |
| train/                  |             |
|    approx_kl            | 0.045592487 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000231    |
|    loss                 | 0.0795      |
|    n_updates            | 1880        |
|    policy_gradient_loss | 0.00096     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 377      |
|    time_elapsed    | 150319   |
|    total_timesteps | 23162880 |
---------------------------------
Eval num_timesteps=23163257, episode_reward=0.04 +/- 0.98
Episode length: 29.91 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 23163257    |
| train/                  |             |
|    approx_kl            | 0.045128416 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000231    |
|    loss                 | 0.0745      |
|    n_updates            | 1885        |
|    policy_gradient_loss | 0.000551    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 378      |
|    time_elapsed    | 150667   |
|    total_timesteps | 23224320 |
---------------------------------
Eval num_timesteps=23224698, episode_reward=0.10 +/- 0.98
Episode length: 29.94 +/- 1.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 23224698   |
| train/                  |            |
|    approx_kl            | 0.04383942 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.761     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.00023    |
|    loss                 | 0.121      |
|    n_updates            | 1890       |
|    policy_gradient_loss | 7.34e-05   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 379      |
|    time_elapsed    | 151016   |
|    total_timesteps | 23285760 |
---------------------------------
Eval num_timesteps=23286139, episode_reward=0.08 +/- 0.99
Episode length: 29.85 +/- 1.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 23286139    |
| train/                  |             |
|    approx_kl            | 0.043371923 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.00023     |
|    loss                 | 0.16        |
|    n_updates            | 1895        |
|    policy_gradient_loss | 0.0004      |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 380      |
|    time_elapsed    | 151365   |
|    total_timesteps | 23347200 |
---------------------------------
Eval num_timesteps=23347580, episode_reward=0.06 +/- 0.99
Episode length: 29.92 +/- 1.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.056     |
| time/                   |           |
|    total_timesteps      | 23347580  |
| train/                  |           |
|    approx_kl            | 0.0436092 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.752    |
|    explained_variance   | 0.361     |
|    learning_rate        | 0.00023   |
|    loss                 | 0.142     |
|    n_updates            | 1900      |
|    policy_gradient_loss | 0.000196  |
|    value_loss           | 0.233     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 154      |
|    iterations      | 381      |
|    time_elapsed    | 151713   |
|    total_timesteps | 23408640 |
---------------------------------
Eval num_timesteps=23409021, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 1.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.104     |
| time/                   |           |
|    total_timesteps      | 23409021  |
| train/                  |           |
|    approx_kl            | 0.0436467 |
|    clip_fraction        | 0.25      |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.748    |
|    explained_variance   | 0.342     |
|    learning_rate        | 0.00023   |
|    loss                 | 0.0577    |
|    n_updates            | 1905      |
|    policy_gradient_loss | 0.00188   |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 382      |
|    time_elapsed    | 152060   |
|    total_timesteps | 23470080 |
---------------------------------
Eval num_timesteps=23470462, episode_reward=0.02 +/- 0.98
Episode length: 29.93 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 23470462    |
| train/                  |             |
|    approx_kl            | 0.043808457 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.00023     |
|    loss                 | 0.1         |
|    n_updates            | 1910        |
|    policy_gradient_loss | 0.000227    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 154      |
|    iterations      | 383      |
|    time_elapsed    | 152408   |
|    total_timesteps | 23531520 |
---------------------------------
Eval num_timesteps=23531903, episode_reward=0.05 +/- 0.99
Episode length: 29.83 +/- 1.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.8       |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 23531903   |
| train/                  |            |
|    approx_kl            | 0.04182813 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000229   |
|    loss                 | 0.0294     |
|    n_updates            | 1915       |
|    policy_gradient_loss | 0.000642   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 384      |
|    time_elapsed    | 152757   |
|    total_timesteps | 23592960 |
---------------------------------
Eval num_timesteps=23593344, episode_reward=0.03 +/- 0.98
Episode length: 29.83 +/- 1.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 23593344    |
| train/                  |             |
|    approx_kl            | 0.042161204 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000229    |
|    loss                 | 0.0741      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.000584   |
|    value_loss           | 0.229       |
-----------------------------------------
