CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
2024-06-09 23:20:37.654943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-09 23:20:37.797881: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-06-09 23:20:38.676456: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-09 23:20:38.676548: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-09 23:20:38.676559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
CUDA available: True
net architecture - {'net_arch': {'pi': [64, 64, 64, 64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64]}}
params: 
NUM_TIMESTEPS -100000000
EVAL_FREQ=61441
EVAL_EPISODES=500
BEST_THRESHOLD=0.14
LOGDIR=scripts/rl/output/v4/
model params: 
 {'learning_rate': <function linear_schedule.<locals>.func at 0x7fe9dfc9df70>, 'n_steps': 61440, 'n_epochs': 5, 'clip_range': 0.17, 'batch_size': 32, 'ent_coef': 0.01, 'gae_lambda': 0.95, 'verbose': 1}
Using cuda device
Wrapping the env in a DummyVecEnv.
starting model: scripts/rl/output/v4/random_start_model
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 1        |
|    time_elapsed    | 382      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61441, episode_reward=0.03 +/- 0.98
Episode length: 29.94 +/- 1.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 61441       |
| train/                  |             |
|    approx_kl            | 0.009344334 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.99       |
|    explained_variance   | -0.217      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0752      |
|    n_updates            | 5           |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.186       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 2        |
|    time_elapsed    | 677      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=122882, episode_reward=0.17 +/- 0.96
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.174       |
| time/                   |             |
|    total_timesteps      | 122882      |
| train/                  |             |
|    approx_kl            | 0.011653193 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.192       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.174
SELFPLAY: new best model, bumping up generation to 1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 3        |
|    time_elapsed    | 1054     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184323, episode_reward=0.11 +/- 0.97
Episode length: 30.01 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 184323      |
| train/                  |             |
|    approx_kl            | 0.012940647 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.226       |
|    learning_rate        | 0.000299    |
|    loss                 | 0.034       |
|    n_updates            | 15          |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.197       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 4        |
|    time_elapsed    | 1467     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=245764, episode_reward=0.05 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 245764      |
| train/                  |             |
|    approx_kl            | 0.015292327 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.000299    |
|    loss                 | 0.0584      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.197       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 5        |
|    time_elapsed    | 1893     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307205, episode_reward=0.24 +/- 0.95
Episode length: 29.94 +/- 1.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.242       |
| time/                   |             |
|    total_timesteps      | 307205      |
| train/                  |             |
|    approx_kl            | 0.016799781 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.000299    |
|    loss                 | 0.0535      |
|    n_updates            | 25          |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.198       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.242
SELFPLAY: new best model, bumping up generation to 2
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 6        |
|    time_elapsed    | 2319     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=368646, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 368646      |
| train/                  |             |
|    approx_kl            | 0.018659238 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.243       |
|    learning_rate        | 0.000299    |
|    loss                 | 0.0328      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.199       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 7        |
|    time_elapsed    | 2735     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430087, episode_reward=0.15 +/- 0.97
Episode length: 30.04 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.152       |
| time/                   |             |
|    total_timesteps      | 430087      |
| train/                  |             |
|    approx_kl            | 0.020913223 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.257       |
|    learning_rate        | 0.000299    |
|    loss                 | -0.00287    |
|    n_updates            | 35          |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.205       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.152
SELFPLAY: new best model, bumping up generation to 3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 8        |
|    time_elapsed    | 3158     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=491528, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 491528      |
| train/                  |             |
|    approx_kl            | 0.022004345 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.000299    |
|    loss                 | 0.0307      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.201       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 9        |
|    time_elapsed    | 3579     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=552969, episode_reward=0.10 +/- 0.97
Episode length: 29.96 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 552969     |
| train/                  |            |
|    approx_kl            | 0.02322592 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.85      |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.000298   |
|    loss                 | 0.00566    |
|    n_updates            | 45         |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.203      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 10       |
|    time_elapsed    | 4001     |
|    total_timesteps | 614400   |
---------------------------------
Eval num_timesteps=614410, episode_reward=0.15 +/- 0.97
Episode length: 29.97 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 614410      |
| train/                  |             |
|    approx_kl            | 0.025672236 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.000298    |
|    loss                 | 0.0526      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.199       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.15
SELFPLAY: new best model, bumping up generation to 4
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 11       |
|    time_elapsed    | 4420     |
|    total_timesteps | 675840   |
---------------------------------
Eval num_timesteps=675851, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 675851      |
| train/                  |             |
|    approx_kl            | 0.026759855 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.000298    |
|    loss                 | 0.0873      |
|    n_updates            | 55          |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.206       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 12       |
|    time_elapsed    | 4836     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=737292, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 737292      |
| train/                  |             |
|    approx_kl            | 0.027892912 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.000298    |
|    loss                 | 0.0299      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.213       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 13       |
|    time_elapsed    | 5257     |
|    total_timesteps | 798720   |
---------------------------------
Eval num_timesteps=798733, episode_reward=0.12 +/- 0.97
Episode length: 30.01 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 798733     |
| train/                  |            |
|    approx_kl            | 0.02963888 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.254      |
|    learning_rate        | 0.000298   |
|    loss                 | 0.117      |
|    n_updates            | 65         |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.21       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 14       |
|    time_elapsed    | 5673     |
|    total_timesteps | 860160   |
---------------------------------
Eval num_timesteps=860174, episode_reward=0.20 +/- 0.95
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 860174      |
| train/                  |             |
|    approx_kl            | 0.030409573 |
|    clip_fraction        | 0.369       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.000297    |
|    loss                 | 0.0348      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.202       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 5
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 15       |
|    time_elapsed    | 6090     |
|    total_timesteps | 921600   |
---------------------------------
Eval num_timesteps=921615, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 921615      |
| train/                  |             |
|    approx_kl            | 0.031944387 |
|    clip_fraction        | 0.371       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.00696    |
|    n_updates            | 75          |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 150      |
|    iterations      | 16       |
|    time_elapsed    | 6510     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=983056, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 983056     |
| train/                  |            |
|    approx_kl            | 0.03295717 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.279      |
|    learning_rate        | 0.000297   |
|    loss                 | -0.0108    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.211      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 17       |
|    time_elapsed    | 6926     |
|    total_timesteps | 1044480  |
---------------------------------
Eval num_timesteps=1044497, episode_reward=0.18 +/- 0.97
Episode length: 29.94 +/- 1.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.182       |
| time/                   |             |
|    total_timesteps      | 1044497     |
| train/                  |             |
|    approx_kl            | 0.034603264 |
|    clip_fraction        | 0.381       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.000297    |
|    loss                 | 0.0349      |
|    n_updates            | 85          |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.209       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.182
SELFPLAY: new best model, bumping up generation to 6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 18       |
|    time_elapsed    | 7342     |
|    total_timesteps | 1105920  |
---------------------------------
Eval num_timesteps=1105938, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 1105938     |
| train/                  |             |
|    approx_kl            | 0.034366027 |
|    clip_fraction        | 0.373       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.303       |
|    learning_rate        | 0.000297    |
|    loss                 | 0.114       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.209       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 150      |
|    iterations      | 19       |
|    time_elapsed    | 7763     |
|    total_timesteps | 1167360  |
---------------------------------
Eval num_timesteps=1167379, episode_reward=0.01 +/- 0.98
Episode length: 29.92 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 1167379     |
| train/                  |             |
|    approx_kl            | 0.036160216 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.000296    |
|    loss                 | 0.0935      |
|    n_updates            | 95          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.22        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 20       |
|    time_elapsed    | 8179     |
|    total_timesteps | 1228800  |
---------------------------------
Eval num_timesteps=1228820, episode_reward=0.11 +/- 0.97
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 1228820     |
| train/                  |             |
|    approx_kl            | 0.038567644 |
|    clip_fraction        | 0.388       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.000296    |
|    loss                 | 0.077       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.218       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 21       |
|    time_elapsed    | 8603     |
|    total_timesteps | 1290240  |
---------------------------------
Eval num_timesteps=1290261, episode_reward=0.19 +/- 0.97
Episode length: 30.02 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.192      |
| time/                   |            |
|    total_timesteps      | 1290261    |
| train/                  |            |
|    approx_kl            | 0.03951366 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.62      |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.000296   |
|    loss                 | 0.0166     |
|    n_updates            | 105        |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 0.216      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.192
SELFPLAY: new best model, bumping up generation to 7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 22       |
|    time_elapsed    | 9020     |
|    total_timesteps | 1351680  |
---------------------------------
Eval num_timesteps=1351702, episode_reward=0.10 +/- 0.98
Episode length: 30.03 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 1351702    |
| train/                  |            |
|    approx_kl            | 0.03850581 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.6       |
|    explained_variance   | 0.279      |
|    learning_rate        | 0.000296   |
|    loss                 | 0.0649     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 0.217      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 23       |
|    time_elapsed    | 9435     |
|    total_timesteps | 1413120  |
---------------------------------
Eval num_timesteps=1413143, episode_reward=0.13 +/- 0.97
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.132       |
| time/                   |             |
|    total_timesteps      | 1413143     |
| train/                  |             |
|    approx_kl            | 0.040032037 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.000296    |
|    loss                 | 0.0215      |
|    n_updates            | 115         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 24       |
|    time_elapsed    | 9855     |
|    total_timesteps | 1474560  |
---------------------------------
Eval num_timesteps=1474584, episode_reward=0.20 +/- 0.96
Episode length: 30.03 +/- 1.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.202       |
| time/                   |             |
|    total_timesteps      | 1474584     |
| train/                  |             |
|    approx_kl            | 0.040833607 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.257       |
|    learning_rate        | 0.000296    |
|    loss                 | 0.057       |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00983    |
|    value_loss           | 0.22        |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.202
SELFPLAY: new best model, bumping up generation to 8
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 25       |
|    time_elapsed    | 10268    |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1536025, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.018     |
| time/                   |            |
|    total_timesteps      | 1536025    |
| train/                  |            |
|    approx_kl            | 0.04192899 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.53      |
|    explained_variance   | 0.267      |
|    learning_rate        | 0.000295   |
|    loss                 | 0.0738     |
|    n_updates            | 125        |
|    policy_gradient_loss | -0.00867   |
|    value_loss           | 0.223      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 149      |
|    iterations      | 26       |
|    time_elapsed    | 10687    |
|    total_timesteps | 1597440  |
---------------------------------
Eval num_timesteps=1597466, episode_reward=-0.00 +/- 0.98
Episode length: 30.01 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 1597466    |
| train/                  |            |
|    approx_kl            | 0.04295319 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.52      |
|    explained_variance   | 0.283      |
|    learning_rate        | 0.000295   |
|    loss                 | 0.057      |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.00942   |
|    value_loss           | 0.227      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 27       |
|    time_elapsed    | 11110    |
|    total_timesteps | 1658880  |
---------------------------------
Eval num_timesteps=1658907, episode_reward=0.07 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 1658907     |
| train/                  |             |
|    approx_kl            | 0.042764843 |
|    clip_fraction        | 0.391       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.000295    |
|    loss                 | 0.148       |
|    n_updates            | 135         |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 0.222       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 149      |
|    iterations      | 28       |
|    time_elapsed    | 11525    |
|    total_timesteps | 1720320  |
---------------------------------
Eval num_timesteps=1720348, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 1720348     |
| train/                  |             |
|    approx_kl            | 0.042848688 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.000295    |
|    loss                 | 0.127       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 29       |
|    time_elapsed    | 11946    |
|    total_timesteps | 1781760  |
---------------------------------
Eval num_timesteps=1781789, episode_reward=0.12 +/- 0.98
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 1781789     |
| train/                  |             |
|    approx_kl            | 0.043147303 |
|    clip_fraction        | 0.386       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.301       |
|    learning_rate        | 0.000295    |
|    loss                 | -0.0177     |
|    n_updates            | 145         |
|    policy_gradient_loss | -0.00863    |
|    value_loss           | 0.216       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 30       |
|    time_elapsed    | 12368    |
|    total_timesteps | 1843200  |
---------------------------------
Eval num_timesteps=1843230, episode_reward=0.25 +/- 0.95
Episode length: 29.97 +/- 1.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.25        |
| time/                   |             |
|    total_timesteps      | 1843230     |
| train/                  |             |
|    approx_kl            | 0.044220146 |
|    clip_fraction        | 0.383       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.29        |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0513      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00754    |
|    value_loss           | 0.219       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.25
SELFPLAY: new best model, bumping up generation to 9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 31       |
|    time_elapsed    | 12788    |
|    total_timesteps | 1904640  |
---------------------------------
Eval num_timesteps=1904671, episode_reward=0.07 +/- 0.98
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 1904671     |
| train/                  |             |
|    approx_kl            | 0.042544708 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0451      |
|    n_updates            | 155         |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 32       |
|    time_elapsed    | 13199    |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1966112, episode_reward=0.08 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 1966112     |
| train/                  |             |
|    approx_kl            | 0.042061917 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0542      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 33       |
|    time_elapsed    | 13612    |
|    total_timesteps | 2027520  |
---------------------------------
Eval num_timesteps=2027553, episode_reward=0.10 +/- 0.99
Episode length: 29.92 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 2027553     |
| train/                  |             |
|    approx_kl            | 0.043467678 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0687      |
|    n_updates            | 165         |
|    policy_gradient_loss | -0.00819    |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 34       |
|    time_elapsed    | 14032    |
|    total_timesteps | 2088960  |
---------------------------------
Eval num_timesteps=2088994, episode_reward=0.20 +/- 0.96
Episode length: 30.03 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.202       |
| time/                   |             |
|    total_timesteps      | 2088994     |
| train/                  |             |
|    approx_kl            | 0.044686604 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0639      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00752    |
|    value_loss           | 0.228       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.202
SELFPLAY: new best model, bumping up generation to 10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 148      |
|    iterations      | 35       |
|    time_elapsed    | 14450    |
|    total_timesteps | 2150400  |
---------------------------------
Eval num_timesteps=2150435, episode_reward=-0.03 +/- 0.98
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.028     |
| time/                   |            |
|    total_timesteps      | 2150435    |
| train/                  |            |
|    approx_kl            | 0.04363891 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.319      |
|    learning_rate        | 0.000294   |
|    loss                 | 0.0214     |
|    n_updates            | 175        |
|    policy_gradient_loss | -0.00704   |
|    value_loss           | 0.23       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 36       |
|    time_elapsed    | 14869    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2211876, episode_reward=-0.01 +/- 0.98
Episode length: 29.92 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 2211876     |
| train/                  |             |
|    approx_kl            | 0.044692215 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.000293    |
|    loss                 | 0.0584      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00681    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 37       |
|    time_elapsed    | 15287    |
|    total_timesteps | 2273280  |
---------------------------------
Eval num_timesteps=2273317, episode_reward=0.10 +/- 0.97
Episode length: 30.01 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 2273317    |
| train/                  |            |
|    approx_kl            | 0.04577876 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000293   |
|    loss                 | 0.0635     |
|    n_updates            | 185        |
|    policy_gradient_loss | -0.00749   |
|    value_loss           | 0.22       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 38       |
|    time_elapsed    | 15703    |
|    total_timesteps | 2334720  |
---------------------------------
Eval num_timesteps=2334758, episode_reward=0.13 +/- 0.98
Episode length: 29.97 +/- 1.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.128      |
| time/                   |            |
|    total_timesteps      | 2334758    |
| train/                  |            |
|    approx_kl            | 0.04215718 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.318      |
|    learning_rate        | 0.000293   |
|    loss                 | 0.055      |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00601   |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 148      |
|    iterations      | 39       |
|    time_elapsed    | 16120    |
|    total_timesteps | 2396160  |
---------------------------------
Eval num_timesteps=2396199, episode_reward=0.16 +/- 0.96
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.156       |
| time/                   |             |
|    total_timesteps      | 2396199     |
| train/                  |             |
|    approx_kl            | 0.044522244 |
|    clip_fraction        | 0.374       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.000293    |
|    loss                 | 0.12        |
|    n_updates            | 195         |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 0.224       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.156
SELFPLAY: new best model, bumping up generation to 11
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 40       |
|    time_elapsed    | 16500    |
|    total_timesteps | 2457600  |
---------------------------------
Eval num_timesteps=2457640, episode_reward=-0.04 +/- 0.98
Episode length: 29.91 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.044      |
| time/                   |             |
|    total_timesteps      | 2457640     |
| train/                  |             |
|    approx_kl            | 0.042242393 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.000293    |
|    loss                 | 0.0381      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00689    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 41       |
|    time_elapsed    | 16881    |
|    total_timesteps | 2519040  |
---------------------------------
Eval num_timesteps=2519081, episode_reward=0.02 +/- 0.98
Episode length: 30.04 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.024      |
| time/                   |            |
|    total_timesteps      | 2519081    |
| train/                  |            |
|    approx_kl            | 0.04305813 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000292   |
|    loss                 | 0.0831     |
|    n_updates            | 205        |
|    policy_gradient_loss | -0.00729   |
|    value_loss           | 0.227      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 149      |
|    iterations      | 42       |
|    time_elapsed    | 17262    |
|    total_timesteps | 2580480  |
---------------------------------
Eval num_timesteps=2580522, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 2580522     |
| train/                  |             |
|    approx_kl            | 0.044493254 |
|    clip_fraction        | 0.366       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000292    |
|    loss                 | 0.131       |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 149      |
|    iterations      | 43       |
|    time_elapsed    | 17641    |
|    total_timesteps | 2641920  |
---------------------------------
Eval num_timesteps=2641963, episode_reward=0.14 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 2641963     |
| train/                  |             |
|    approx_kl            | 0.045018453 |
|    clip_fraction        | 0.371       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000292    |
|    loss                 | 0.0156      |
|    n_updates            | 215         |
|    policy_gradient_loss | -0.00631    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 44       |
|    time_elapsed    | 18023    |
|    total_timesteps | 2703360  |
---------------------------------
Eval num_timesteps=2703404, episode_reward=0.11 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 2703404     |
| train/                  |             |
|    approx_kl            | 0.045599524 |
|    clip_fraction        | 0.374       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.306       |
|    learning_rate        | 0.000292    |
|    loss                 | 0.017       |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00633    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 150      |
|    iterations      | 45       |
|    time_elapsed    | 18404    |
|    total_timesteps | 2764800  |
---------------------------------
Eval num_timesteps=2764845, episode_reward=0.06 +/- 0.98
Episode length: 30.01 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 2764845    |
| train/                  |            |
|    approx_kl            | 0.04534928 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.000292   |
|    loss                 | 0.106      |
|    n_updates            | 225        |
|    policy_gradient_loss | -0.00591   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 150      |
|    iterations      | 46       |
|    time_elapsed    | 18786    |
|    total_timesteps | 2826240  |
---------------------------------
Eval num_timesteps=2826286, episode_reward=0.16 +/- 0.97
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.162       |
| time/                   |             |
|    total_timesteps      | 2826286     |
| train/                  |             |
|    approx_kl            | 0.045214567 |
|    clip_fraction        | 0.364       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.000292    |
|    loss                 | 0.0715      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00578    |
|    value_loss           | 0.229       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.162
SELFPLAY: new best model, bumping up generation to 12
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 47       |
|    time_elapsed    | 19168    |
|    total_timesteps | 2887680  |
---------------------------------
Eval num_timesteps=2887727, episode_reward=0.01 +/- 0.99
Episode length: 29.94 +/- 1.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 2887727     |
| train/                  |             |
|    approx_kl            | 0.045312133 |
|    clip_fraction        | 0.368       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000291    |
|    loss                 | 0.000235    |
|    n_updates            | 235         |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 48       |
|    time_elapsed    | 19545    |
|    total_timesteps | 2949120  |
---------------------------------
Eval num_timesteps=2949168, episode_reward=0.09 +/- 0.98
Episode length: 29.96 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 2949168     |
| train/                  |             |
|    approx_kl            | 0.042594068 |
|    clip_fraction        | 0.36        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.000291    |
|    loss                 | 0.0868      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 49       |
|    time_elapsed    | 19923    |
|    total_timesteps | 3010560  |
---------------------------------
Eval num_timesteps=3010609, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.096     |
| time/                   |           |
|    total_timesteps      | 3010609   |
| train/                  |           |
|    approx_kl            | 0.0436664 |
|    clip_fraction        | 0.358     |
|    clip_range           | 0.17      |
|    entropy_loss         | -1.26     |
|    explained_variance   | 0.338     |
|    learning_rate        | 0.000291  |
|    loss                 | 0.157     |
|    n_updates            | 245       |
|    policy_gradient_loss | -0.00576  |
|    value_loss           | 0.227     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 50       |
|    time_elapsed    | 20306    |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3072050, episode_reward=-0.00 +/- 0.99
Episode length: 29.93 +/- 1.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 3072050     |
| train/                  |             |
|    approx_kl            | 0.044346906 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000291    |
|    loss                 | 0.0225      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 51       |
|    time_elapsed    | 20686    |
|    total_timesteps | 3133440  |
---------------------------------
Eval num_timesteps=3133491, episode_reward=0.16 +/- 0.97
Episode length: 30.00 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.158      |
| time/                   |            |
|    total_timesteps      | 3133491    |
| train/                  |            |
|    approx_kl            | 0.04571503 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000291   |
|    loss                 | 0.0862     |
|    n_updates            | 255        |
|    policy_gradient_loss | -0.00466   |
|    value_loss           | 0.231      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.158
SELFPLAY: new best model, bumping up generation to 13
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 151      |
|    iterations      | 52       |
|    time_elapsed    | 21065    |
|    total_timesteps | 3194880  |
---------------------------------
Eval num_timesteps=3194932, episode_reward=-0.03 +/- 0.99
Episode length: 29.98 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 3194932     |
| train/                  |             |
|    approx_kl            | 0.046573162 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.00029     |
|    loss                 | 0.035       |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 151      |
|    iterations      | 53       |
|    time_elapsed    | 21446    |
|    total_timesteps | 3256320  |
---------------------------------
Eval num_timesteps=3256373, episode_reward=-0.03 +/- 0.99
Episode length: 29.93 +/- 1.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.028     |
| time/                   |            |
|    total_timesteps      | 3256373    |
| train/                  |            |
|    approx_kl            | 0.04792881 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.00029    |
|    loss                 | 0.116      |
|    n_updates            | 265        |
|    policy_gradient_loss | -0.00442   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 54       |
|    time_elapsed    | 21824    |
|    total_timesteps | 3317760  |
---------------------------------
Eval num_timesteps=3317814, episode_reward=0.00 +/- 0.99
Episode length: 29.93 +/- 1.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 3317814     |
| train/                  |             |
|    approx_kl            | 0.047870018 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.00029     |
|    loss                 | 0.0393      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00488    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 152      |
|    iterations      | 55       |
|    time_elapsed    | 22202    |
|    total_timesteps | 3379200  |
---------------------------------
Eval num_timesteps=3379255, episode_reward=0.01 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 3379255     |
| train/                  |             |
|    approx_kl            | 0.049138777 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.00029     |
|    loss                 | 0.0575      |
|    n_updates            | 275         |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 56       |
|    time_elapsed    | 22584    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3440696, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 3440696    |
| train/                  |            |
|    approx_kl            | 0.04871285 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.00029    |
|    loss                 | 0.102      |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.00458   |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 57       |
|    time_elapsed    | 22962    |
|    total_timesteps | 3502080  |
---------------------------------
Eval num_timesteps=3502137, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 3502137     |
| train/                  |             |
|    approx_kl            | 0.048890267 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000289    |
|    loss                 | 0.106       |
|    n_updates            | 285         |
|    policy_gradient_loss | -0.00456    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 58       |
|    time_elapsed    | 23339    |
|    total_timesteps | 3563520  |
---------------------------------
Eval num_timesteps=3563578, episode_reward=0.01 +/- 0.99
Episode length: 29.95 +/- 0.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 3563578    |
| train/                  |            |
|    approx_kl            | 0.04828674 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.000289   |
|    loss                 | 0.0881     |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00486   |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 59       |
|    time_elapsed    | 23721    |
|    total_timesteps | 3624960  |
---------------------------------
Eval num_timesteps=3625019, episode_reward=0.11 +/- 0.98
Episode length: 30.06 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 3625019     |
| train/                  |             |
|    approx_kl            | 0.047334902 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000289    |
|    loss                 | 0.0951      |
|    n_updates            | 295         |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 60       |
|    time_elapsed    | 24098    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3686460, episode_reward=0.07 +/- 0.99
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 3686460     |
| train/                  |             |
|    approx_kl            | 0.047668424 |
|    clip_fraction        | 0.359       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000289    |
|    loss                 | 0.142       |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 61       |
|    time_elapsed    | 24474    |
|    total_timesteps | 3747840  |
---------------------------------
Eval num_timesteps=3747901, episode_reward=0.11 +/- 0.98
Episode length: 30.07 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 3747901     |
| train/                  |             |
|    approx_kl            | 0.048686154 |
|    clip_fraction        | 0.36        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000289    |
|    loss                 | 0.132       |
|    n_updates            | 305         |
|    policy_gradient_loss | -0.00466    |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 62       |
|    time_elapsed    | 24857    |
|    total_timesteps | 3809280  |
---------------------------------
Eval num_timesteps=3809342, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 3809342    |
| train/                  |            |
|    approx_kl            | 0.04885006 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.000289   |
|    loss                 | 0.233      |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.00422   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 63       |
|    time_elapsed    | 25235    |
|    total_timesteps | 3870720  |
---------------------------------
Eval num_timesteps=3870783, episode_reward=0.05 +/- 0.98
Episode length: 29.93 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 3870783     |
| train/                  |             |
|    approx_kl            | 0.049648747 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.049       |
|    n_updates            | 315         |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 64       |
|    time_elapsed    | 25616    |
|    total_timesteps | 3932160  |
---------------------------------
Eval num_timesteps=3932224, episode_reward=0.17 +/- 0.98
Episode length: 29.96 +/- 1.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.17        |
| time/                   |             |
|    total_timesteps      | 3932224     |
| train/                  |             |
|    approx_kl            | 0.047213856 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.0671      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 0.225       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.17
SELFPLAY: new best model, bumping up generation to 14
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 65       |
|    time_elapsed    | 25993    |
|    total_timesteps | 3993600  |
---------------------------------
Eval num_timesteps=3993665, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 3993665     |
| train/                  |             |
|    approx_kl            | 0.046078824 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.0286      |
|    n_updates            | 325         |
|    policy_gradient_loss | -0.00428    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 66       |
|    time_elapsed    | 26373    |
|    total_timesteps | 4055040  |
---------------------------------
Eval num_timesteps=4055106, episode_reward=0.02 +/- 0.99
Episode length: 29.86 +/- 1.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 4055106     |
| train/                  |             |
|    approx_kl            | 0.048261605 |
|    clip_fraction        | 0.357       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.082       |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 67       |
|    time_elapsed    | 26753    |
|    total_timesteps | 4116480  |
---------------------------------
Eval num_timesteps=4116547, episode_reward=0.02 +/- 0.99
Episode length: 29.89 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 4116547     |
| train/                  |             |
|    approx_kl            | 0.046538305 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000288    |
|    loss                 | 0.0954      |
|    n_updates            | 335         |
|    policy_gradient_loss | -0.00442    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 68       |
|    time_elapsed    | 27128    |
|    total_timesteps | 4177920  |
---------------------------------
Eval num_timesteps=4177988, episode_reward=0.04 +/- 0.99
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 4177988    |
| train/                  |            |
|    approx_kl            | 0.04716132 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000287   |
|    loss                 | 0.152      |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.00396   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 69       |
|    time_elapsed    | 27508    |
|    total_timesteps | 4239360  |
---------------------------------
Eval num_timesteps=4239429, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 4239429     |
| train/                  |             |
|    approx_kl            | 0.048003443 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0601      |
|    n_updates            | 345         |
|    policy_gradient_loss | -0.00404    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 70       |
|    time_elapsed    | 27856    |
|    total_timesteps | 4300800  |
---------------------------------
Eval num_timesteps=4300870, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 4300870     |
| train/                  |             |
|    approx_kl            | 0.052191656 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0639      |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00397    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 71       |
|    time_elapsed    | 28204    |
|    total_timesteps | 4362240  |
---------------------------------
Eval num_timesteps=4362311, episode_reward=0.08 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 4362311     |
| train/                  |             |
|    approx_kl            | 0.047780372 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.124       |
|    n_updates            | 355         |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 154      |
|    iterations      | 72       |
|    time_elapsed    | 28551    |
|    total_timesteps | 4423680  |
---------------------------------
Eval num_timesteps=4423752, episode_reward=0.05 +/- 0.98
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 4423752     |
| train/                  |             |
|    approx_kl            | 0.047638156 |
|    clip_fraction        | 0.347       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0865      |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 73       |
|    time_elapsed    | 28899    |
|    total_timesteps | 4485120  |
---------------------------------
Eval num_timesteps=4485193, episode_reward=0.10 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 4485193     |
| train/                  |             |
|    approx_kl            | 0.048688844 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0338      |
|    n_updates            | 365         |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 74       |
|    time_elapsed    | 29248    |
|    total_timesteps | 4546560  |
---------------------------------
Eval num_timesteps=4546634, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 4546634     |
| train/                  |             |
|    approx_kl            | 0.046071593 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000286    |
|    loss                 | 0.0666      |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 75       |
|    time_elapsed    | 29599    |
|    total_timesteps | 4608000  |
---------------------------------
Eval num_timesteps=4608075, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 4608075     |
| train/                  |             |
|    approx_kl            | 0.046900924 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000286    |
|    loss                 | 0.149       |
|    n_updates            | 375         |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 76       |
|    time_elapsed    | 29949    |
|    total_timesteps | 4669440  |
---------------------------------
Eval num_timesteps=4669516, episode_reward=0.18 +/- 0.97
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.182       |
| time/                   |             |
|    total_timesteps      | 4669516     |
| train/                  |             |
|    approx_kl            | 0.048151013 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000286    |
|    loss                 | 0.146       |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00407    |
|    value_loss           | 0.234       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.182
SELFPLAY: new best model, bumping up generation to 15
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 77       |
|    time_elapsed    | 30298    |
|    total_timesteps | 4730880  |
---------------------------------
Eval num_timesteps=4730957, episode_reward=-0.06 +/- 0.98
Episode length: 30.00 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.06      |
| time/                   |            |
|    total_timesteps      | 4730957    |
| train/                  |            |
|    approx_kl            | 0.05025997 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000286   |
|    loss                 | 0.073      |
|    n_updates            | 385        |
|    policy_gradient_loss | -0.00256   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 78       |
|    time_elapsed    | 30646    |
|    total_timesteps | 4792320  |
---------------------------------
Eval num_timesteps=4792398, episode_reward=0.09 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 4792398     |
| train/                  |             |
|    approx_kl            | 0.049117204 |
|    clip_fraction        | 0.347       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000286    |
|    loss                 | 0.0569      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 79       |
|    time_elapsed    | 30995    |
|    total_timesteps | 4853760  |
---------------------------------
Eval num_timesteps=4853839, episode_reward=0.02 +/- 0.99
Episode length: 29.93 +/- 1.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 4853839     |
| train/                  |             |
|    approx_kl            | 0.047663327 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000285    |
|    loss                 | 0.0596      |
|    n_updates            | 395         |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 80       |
|    time_elapsed    | 31343    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4915280, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 4915280     |
| train/                  |             |
|    approx_kl            | 0.048022985 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000285    |
|    loss                 | 0.158       |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 81       |
|    time_elapsed    | 31693    |
|    total_timesteps | 4976640  |
---------------------------------
Eval num_timesteps=4976721, episode_reward=0.05 +/- 0.98
Episode length: 29.90 +/- 1.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 4976721    |
| train/                  |            |
|    approx_kl            | 0.04995929 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.000285   |
|    loss                 | 0.0882     |
|    n_updates            | 405        |
|    policy_gradient_loss | -0.00293   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 82       |
|    time_elapsed    | 32040    |
|    total_timesteps | 5038080  |
---------------------------------
Eval num_timesteps=5038162, episode_reward=-0.03 +/- 0.99
Episode length: 29.91 +/- 1.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 5038162    |
| train/                  |            |
|    approx_kl            | 0.04772352 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.324      |
|    learning_rate        | 0.000285   |
|    loss                 | 0.0688     |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.00329   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 83       |
|    time_elapsed    | 32388    |
|    total_timesteps | 5099520  |
---------------------------------
Eval num_timesteps=5099603, episode_reward=0.02 +/- 0.98
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 5099603     |
| train/                  |             |
|    approx_kl            | 0.048069775 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000285    |
|    loss                 | 0.0216      |
|    n_updates            | 415         |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 84       |
|    time_elapsed    | 32736    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5161044, episode_reward=0.00 +/- 0.98
Episode length: 29.89 +/- 1.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5161044    |
| train/                  |            |
|    approx_kl            | 0.05061628 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.000285   |
|    loss                 | 0.107      |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.00337   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 85       |
|    time_elapsed    | 33085    |
|    total_timesteps | 5222400  |
---------------------------------
Eval num_timesteps=5222485, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 5222485     |
| train/                  |             |
|    approx_kl            | 0.049609866 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000284    |
|    loss                 | 0.0724      |
|    n_updates            | 425         |
|    policy_gradient_loss | -0.00265    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 86       |
|    time_elapsed    | 33436    |
|    total_timesteps | 5283840  |
---------------------------------
Eval num_timesteps=5283926, episode_reward=0.04 +/- 0.99
Episode length: 29.94 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 5283926     |
| train/                  |             |
|    approx_kl            | 0.050042104 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000284    |
|    loss                 | 0.0225      |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 87       |
|    time_elapsed    | 33785    |
|    total_timesteps | 5345280  |
---------------------------------
Eval num_timesteps=5345367, episode_reward=0.07 +/- 0.99
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.072      |
| time/                   |            |
|    total_timesteps      | 5345367    |
| train/                  |            |
|    approx_kl            | 0.04908633 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.000284   |
|    loss                 | 0.15       |
|    n_updates            | 435        |
|    policy_gradient_loss | -0.00286   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 88       |
|    time_elapsed    | 34133    |
|    total_timesteps | 5406720  |
---------------------------------
Eval num_timesteps=5406808, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 5406808     |
| train/                  |             |
|    approx_kl            | 0.048031583 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.000284    |
|    loss                 | 0.094       |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 89       |
|    time_elapsed    | 34481    |
|    total_timesteps | 5468160  |
---------------------------------
Eval num_timesteps=5468249, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 5468249     |
| train/                  |             |
|    approx_kl            | 0.049083855 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.000284    |
|    loss                 | 0.139       |
|    n_updates            | 445         |
|    policy_gradient_loss | -0.00204    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 90       |
|    time_elapsed    | 34830    |
|    total_timesteps | 5529600  |
---------------------------------
Eval num_timesteps=5529690, episode_reward=0.06 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 5529690     |
| train/                  |             |
|    approx_kl            | 0.049577627 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000283    |
|    loss                 | 0.101       |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 91       |
|    time_elapsed    | 35179    |
|    total_timesteps | 5591040  |
---------------------------------
Eval num_timesteps=5591131, episode_reward=0.08 +/- 0.99
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 5591131     |
| train/                  |             |
|    approx_kl            | 0.049852517 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000283    |
|    loss                 | 0.0857      |
|    n_updates            | 455         |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 92       |
|    time_elapsed    | 35527    |
|    total_timesteps | 5652480  |
---------------------------------
Eval num_timesteps=5652572, episode_reward=0.01 +/- 0.99
Episode length: 29.92 +/- 0.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.006      |
| time/                   |            |
|    total_timesteps      | 5652572    |
| train/                  |            |
|    approx_kl            | 0.04994645 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000283   |
|    loss                 | 0.0632     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.00189   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 93       |
|    time_elapsed    | 35874    |
|    total_timesteps | 5713920  |
---------------------------------
Eval num_timesteps=5714013, episode_reward=0.09 +/- 0.99
Episode length: 29.96 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.092      |
| time/                   |            |
|    total_timesteps      | 5714013    |
| train/                  |            |
|    approx_kl            | 0.05191389 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.000283   |
|    loss                 | 0.111      |
|    n_updates            | 465        |
|    policy_gradient_loss | -0.0019    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 94       |
|    time_elapsed    | 36221    |
|    total_timesteps | 5775360  |
---------------------------------
Eval num_timesteps=5775454, episode_reward=0.06 +/- 0.99
Episode length: 29.95 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 5775454    |
| train/                  |            |
|    approx_kl            | 0.04956732 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.000283   |
|    loss                 | 0.0475     |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.00094   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 95       |
|    time_elapsed    | 36570    |
|    total_timesteps | 5836800  |
---------------------------------
Eval num_timesteps=5836895, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 5836895   |
| train/                  |           |
|    approx_kl            | 0.0497388 |
|    clip_fraction        | 0.335     |
|    clip_range           | 0.17      |
|    entropy_loss         | -1.03     |
|    explained_variance   | 0.326     |
|    learning_rate        | 0.000282  |
|    loss                 | 0.0261    |
|    n_updates            | 475       |
|    policy_gradient_loss | -0.00214  |
|    value_loss           | 0.234     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 96       |
|    time_elapsed    | 36920    |
|    total_timesteps | 5898240  |
---------------------------------
Eval num_timesteps=5898336, episode_reward=0.08 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 5898336     |
| train/                  |             |
|    approx_kl            | 0.049487755 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000282    |
|    loss                 | 0.0399      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 97       |
|    time_elapsed    | 37269    |
|    total_timesteps | 5959680  |
---------------------------------
Eval num_timesteps=5959777, episode_reward=0.10 +/- 0.98
Episode length: 29.94 +/- 1.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 5959777    |
| train/                  |            |
|    approx_kl            | 0.05155673 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.000282   |
|    loss                 | 0.0699     |
|    n_updates            | 485        |
|    policy_gradient_loss | -0.00207   |
|    value_loss           | 0.228      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 98       |
|    time_elapsed    | 37618    |
|    total_timesteps | 6021120  |
---------------------------------
Eval num_timesteps=6021218, episode_reward=0.14 +/- 0.98
Episode length: 29.95 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 6021218     |
| train/                  |             |
|    approx_kl            | 0.050100293 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000282    |
|    loss                 | 0.144       |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 99       |
|    time_elapsed    | 37967    |
|    total_timesteps | 6082560  |
---------------------------------
Eval num_timesteps=6082659, episode_reward=0.07 +/- 0.98
Episode length: 29.95 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.074      |
| time/                   |            |
|    total_timesteps      | 6082659    |
| train/                  |            |
|    approx_kl            | 0.04993778 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000282   |
|    loss                 | 0.047      |
|    n_updates            | 495        |
|    policy_gradient_loss | -0.00231   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 100      |
|    time_elapsed    | 38316    |
|    total_timesteps | 6144000  |
---------------------------------
Eval num_timesteps=6144100, episode_reward=0.10 +/- 0.97
Episode length: 29.96 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 6144100     |
| train/                  |             |
|    approx_kl            | 0.052101128 |
|    clip_fraction        | 0.336       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000282    |
|    loss                 | 0.0674      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 101      |
|    time_elapsed    | 38664    |
|    total_timesteps | 6205440  |
---------------------------------
Eval num_timesteps=6205541, episode_reward=0.11 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 6205541     |
| train/                  |             |
|    approx_kl            | 0.051935975 |
|    clip_fraction        | 0.337       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.0432      |
|    n_updates            | 505         |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 102      |
|    time_elapsed    | 39013    |
|    total_timesteps | 6266880  |
---------------------------------
Eval num_timesteps=6266982, episode_reward=0.11 +/- 0.99
Episode length: 29.92 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 6266982     |
| train/                  |             |
|    approx_kl            | 0.051995516 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.12        |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 103      |
|    time_elapsed    | 39360    |
|    total_timesteps | 6328320  |
---------------------------------
Eval num_timesteps=6328423, episode_reward=0.12 +/- 0.98
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 6328423     |
| train/                  |             |
|    approx_kl            | 0.050829735 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.0143      |
|    n_updates            | 515         |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 104      |
|    time_elapsed    | 39708    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6389864, episode_reward=0.10 +/- 0.98
Episode length: 29.91 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 6389864     |
| train/                  |             |
|    approx_kl            | 0.049996626 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.0954      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00244    |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 105      |
|    time_elapsed    | 40055    |
|    total_timesteps | 6451200  |
---------------------------------
Eval num_timesteps=6451305, episode_reward=0.17 +/- 0.97
Episode length: 29.96 +/- 1.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.168       |
| time/                   |             |
|    total_timesteps      | 6451305     |
| train/                  |             |
|    approx_kl            | 0.050470512 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000281    |
|    loss                 | 0.141       |
|    n_updates            | 525         |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 0.228       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.168
SELFPLAY: new best model, bumping up generation to 16
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 106      |
|    time_elapsed    | 40405    |
|    total_timesteps | 6512640  |
---------------------------------
Eval num_timesteps=6512746, episode_reward=-0.05 +/- 0.99
Episode length: 29.94 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.046      |
| time/                   |             |
|    total_timesteps      | 6512746     |
| train/                  |             |
|    approx_kl            | 0.050866835 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.00028     |
|    loss                 | 0.0978      |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 107      |
|    time_elapsed    | 40757    |
|    total_timesteps | 6574080  |
---------------------------------
Eval num_timesteps=6574187, episode_reward=0.07 +/- 0.99
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 6574187     |
| train/                  |             |
|    approx_kl            | 0.052280605 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.00028     |
|    loss                 | 0.167       |
|    n_updates            | 535         |
|    policy_gradient_loss | -0.00239    |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 108      |
|    time_elapsed    | 41107    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6635628, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 6635628    |
| train/                  |            |
|    approx_kl            | 0.04994194 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.32       |
|    learning_rate        | 0.00028    |
|    loss                 | 0.144      |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.00271   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 109      |
|    time_elapsed    | 41456    |
|    total_timesteps | 6696960  |
---------------------------------
Eval num_timesteps=6697069, episode_reward=0.05 +/- 0.98
Episode length: 29.95 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 6697069     |
| train/                  |             |
|    approx_kl            | 0.050597064 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.00028     |
|    loss                 | 0.0458      |
|    n_updates            | 545         |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 110      |
|    time_elapsed    | 41805    |
|    total_timesteps | 6758400  |
---------------------------------
Eval num_timesteps=6758510, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 6758510    |
| train/                  |            |
|    approx_kl            | 0.05235827 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.00028    |
|    loss                 | 0.111      |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.00158   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 111      |
|    time_elapsed    | 42154    |
|    total_timesteps | 6819840  |
---------------------------------
Eval num_timesteps=6819951, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 6819951     |
| train/                  |             |
|    approx_kl            | 0.054454073 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.00028     |
|    loss                 | 0.0468      |
|    n_updates            | 555         |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 112      |
|    time_elapsed    | 42502    |
|    total_timesteps | 6881280  |
---------------------------------
Eval num_timesteps=6881392, episode_reward=0.02 +/- 0.99
Episode length: 29.99 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.024      |
| time/                   |            |
|    total_timesteps      | 6881392    |
| train/                  |            |
|    approx_kl            | 0.05079849 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000279   |
|    loss                 | 0.046      |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.00211   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 113      |
|    time_elapsed    | 42851    |
|    total_timesteps | 6942720  |
---------------------------------
Eval num_timesteps=6942833, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 6942833     |
| train/                  |             |
|    approx_kl            | 0.050179943 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000279    |
|    loss                 | 0.0626      |
|    n_updates            | 565         |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 114      |
|    time_elapsed    | 43198    |
|    total_timesteps | 7004160  |
---------------------------------
Eval num_timesteps=7004274, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.024      |
| time/                   |            |
|    total_timesteps      | 7004274    |
| train/                  |            |
|    approx_kl            | 0.05032196 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.331      |
|    learning_rate        | 0.000279   |
|    loss                 | 0.0812     |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.00241   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 115      |
|    time_elapsed    | 43545    |
|    total_timesteps | 7065600  |
---------------------------------
Eval num_timesteps=7065715, episode_reward=0.02 +/- 0.98
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 7065715     |
| train/                  |             |
|    approx_kl            | 0.050247792 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000279    |
|    loss                 | 0.101       |
|    n_updates            | 575         |
|    policy_gradient_loss | -0.000647   |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 116      |
|    time_elapsed    | 43894    |
|    total_timesteps | 7127040  |
---------------------------------
Eval num_timesteps=7127156, episode_reward=0.12 +/- 0.98
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 7127156     |
| train/                  |             |
|    approx_kl            | 0.048456557 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000279    |
|    loss                 | 0.0921      |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 117      |
|    time_elapsed    | 44243    |
|    total_timesteps | 7188480  |
---------------------------------
Eval num_timesteps=7188597, episode_reward=0.03 +/- 0.99
Episode length: 29.94 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 7188597     |
| train/                  |             |
|    approx_kl            | 0.050960217 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000278    |
|    loss                 | 0.0207      |
|    n_updates            | 585         |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 118      |
|    time_elapsed    | 44594    |
|    total_timesteps | 7249920  |
---------------------------------
Eval num_timesteps=7250038, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.092      |
| time/                   |            |
|    total_timesteps      | 7250038    |
| train/                  |            |
|    approx_kl            | 0.05230332 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.000278   |
|    loss                 | 0.149      |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.00082   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 119      |
|    time_elapsed    | 44943    |
|    total_timesteps | 7311360  |
---------------------------------
Eval num_timesteps=7311479, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.036      |
| time/                   |            |
|    total_timesteps      | 7311479    |
| train/                  |            |
|    approx_kl            | 0.05174977 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.17       |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000278   |
|    loss                 | 0.0707     |
|    n_updates            | 595        |
|    policy_gradient_loss | -0.000882  |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 120      |
|    time_elapsed    | 45291    |
|    total_timesteps | 7372800  |
---------------------------------
Eval num_timesteps=7372920, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 7372920     |
| train/                  |             |
|    approx_kl            | 0.050689515 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000278    |
|    loss                 | 0.119       |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 121      |
|    time_elapsed    | 45640    |
|    total_timesteps | 7434240  |
---------------------------------
Eval num_timesteps=7434361, episode_reward=0.11 +/- 0.98
Episode length: 29.95 +/- 0.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 7434361     |
| train/                  |             |
|    approx_kl            | 0.050224368 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000278    |
|    loss                 | 0.0471      |
|    n_updates            | 605         |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 122      |
|    time_elapsed    | 45989    |
|    total_timesteps | 7495680  |
---------------------------------
Eval num_timesteps=7495802, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 7495802     |
| train/                  |             |
|    approx_kl            | 0.048598994 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.99       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000278    |
|    loss                 | 0.0619      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 123      |
|    time_elapsed    | 46338    |
|    total_timesteps | 7557120  |
---------------------------------
Eval num_timesteps=7557243, episode_reward=0.09 +/- 0.98
Episode length: 29.87 +/- 1.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 7557243     |
| train/                  |             |
|    approx_kl            | 0.049232494 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.987      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.000277    |
|    loss                 | 0.0607      |
|    n_updates            | 615         |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 124      |
|    time_elapsed    | 46687    |
|    total_timesteps | 7618560  |
---------------------------------
Eval num_timesteps=7618684, episode_reward=0.19 +/- 0.97
Episode length: 29.94 +/- 1.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.188      |
| time/                   |            |
|    total_timesteps      | 7618684    |
| train/                  |            |
|    approx_kl            | 0.04978234 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.98      |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.000277   |
|    loss                 | 0.0741     |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.00234   |
|    value_loss           | 0.231      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.188
SELFPLAY: new best model, bumping up generation to 17
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 125      |
|    time_elapsed    | 47035    |
|    total_timesteps | 7680000  |
---------------------------------
Eval num_timesteps=7680125, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 7680125     |
| train/                  |             |
|    approx_kl            | 0.049237784 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.997      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000277    |
|    loss                 | 0.0596      |
|    n_updates            | 625         |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 126      |
|    time_elapsed    | 47382    |
|    total_timesteps | 7741440  |
---------------------------------
Eval num_timesteps=7741566, episode_reward=0.07 +/- 0.99
Episode length: 29.90 +/- 1.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 7741566     |
| train/                  |             |
|    approx_kl            | 0.051179696 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000277    |
|    loss                 | 0.113       |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00077    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 127      |
|    time_elapsed    | 47731    |
|    total_timesteps | 7802880  |
---------------------------------
Eval num_timesteps=7803007, episode_reward=0.08 +/- 0.97
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 7803007     |
| train/                  |             |
|    approx_kl            | 0.051764686 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000277    |
|    loss                 | 0.0727      |
|    n_updates            | 635         |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 128      |
|    time_elapsed    | 48082    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7864448, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 7864448     |
| train/                  |             |
|    approx_kl            | 0.051543973 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000276    |
|    loss                 | 0.0923      |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 129      |
|    time_elapsed    | 48432    |
|    total_timesteps | 7925760  |
---------------------------------
Eval num_timesteps=7925889, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 7925889     |
| train/                  |             |
|    approx_kl            | 0.048342194 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.17        |
|    entropy_loss         | -1          |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.000276    |
|    loss                 | 0.152       |
|    n_updates            | 645         |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 130      |
|    time_elapsed    | 48781    |
|    total_timesteps | 7987200  |
---------------------------------
Eval num_timesteps=7987330, episode_reward=-0.07 +/- 0.99
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.066      |
| time/                   |             |
|    total_timesteps      | 7987330     |
| train/                  |             |
|    approx_kl            | 0.051977586 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.983      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000276    |
|    loss                 | 0.107       |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00059    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 131      |
|    time_elapsed    | 49130    |
|    total_timesteps | 8048640  |
---------------------------------
Eval num_timesteps=8048771, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 8048771     |
| train/                  |             |
|    approx_kl            | 0.052328892 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.984      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000276    |
|    loss                 | 0.0568      |
|    n_updates            | 655         |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 132      |
|    time_elapsed    | 49478    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8110212, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 8110212     |
| train/                  |             |
|    approx_kl            | 0.052933652 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.978      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000276    |
|    loss                 | 0.105       |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00219    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 133      |
|    time_elapsed    | 49826    |
|    total_timesteps | 8171520  |
---------------------------------
Eval num_timesteps=8171653, episode_reward=0.02 +/- 0.99
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 8171653     |
| train/                  |             |
|    approx_kl            | 0.051866487 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.971      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.184       |
|    n_updates            | 665         |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 134      |
|    time_elapsed    | 50175    |
|    total_timesteps | 8232960  |
---------------------------------
Eval num_timesteps=8233094, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.042      |
| time/                   |            |
|    total_timesteps      | 8233094    |
| train/                  |            |
|    approx_kl            | 0.05146311 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.968     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000275   |
|    loss                 | 0.0225     |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.00107   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 135      |
|    time_elapsed    | 50523    |
|    total_timesteps | 8294400  |
---------------------------------
Eval num_timesteps=8294535, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 8294535     |
| train/                  |             |
|    approx_kl            | 0.052856836 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.962      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.0688      |
|    n_updates            | 675         |
|    policy_gradient_loss | -0.000277   |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 136      |
|    time_elapsed    | 50870    |
|    total_timesteps | 8355840  |
---------------------------------
Eval num_timesteps=8355976, episode_reward=0.08 +/- 0.99
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 8355976     |
| train/                  |             |
|    approx_kl            | 0.050089154 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.954      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.049       |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00197    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 137      |
|    time_elapsed    | 51218    |
|    total_timesteps | 8417280  |
---------------------------------
Eval num_timesteps=8417417, episode_reward=-0.02 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 8417417     |
| train/                  |             |
|    approx_kl            | 0.053909052 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.94       |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.0542      |
|    n_updates            | 685         |
|    policy_gradient_loss | -5.59e-05   |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 138      |
|    time_elapsed    | 51567    |
|    total_timesteps | 8478720  |
---------------------------------
Eval num_timesteps=8478858, episode_reward=0.04 +/- 0.98
Episode length: 29.94 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 8478858     |
| train/                  |             |
|    approx_kl            | 0.050597485 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.937      |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.000275    |
|    loss                 | 0.0644      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00138    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 139      |
|    time_elapsed    | 51917    |
|    total_timesteps | 8540160  |
---------------------------------
Eval num_timesteps=8540299, episode_reward=0.06 +/- 0.99
Episode length: 29.99 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.064     |
| time/                   |           |
|    total_timesteps      | 8540299   |
| train/                  |           |
|    approx_kl            | 0.0521381 |
|    clip_fraction        | 0.313     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.923    |
|    explained_variance   | 0.341     |
|    learning_rate        | 0.000274  |
|    loss                 | 0.112     |
|    n_updates            | 695       |
|    policy_gradient_loss | -0.00132  |
|    value_loss           | 0.233     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 140      |
|    time_elapsed    | 52267    |
|    total_timesteps | 8601600  |
---------------------------------
Eval num_timesteps=8601740, episode_reward=0.14 +/- 0.97
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 8601740     |
| train/                  |             |
|    approx_kl            | 0.051480357 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.918      |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000274    |
|    loss                 | 0.131       |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.000707   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 141      |
|    time_elapsed    | 52615    |
|    total_timesteps | 8663040  |
---------------------------------
Eval num_timesteps=8663181, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.044      |
| time/                   |            |
|    total_timesteps      | 8663181    |
| train/                  |            |
|    approx_kl            | 0.05112425 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.91      |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.000274   |
|    loss                 | 0.163      |
|    n_updates            | 705        |
|    policy_gradient_loss | -0.00057   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 142      |
|    time_elapsed    | 52964    |
|    total_timesteps | 8724480  |
---------------------------------
Eval num_timesteps=8724622, episode_reward=0.11 +/- 0.98
Episode length: 29.94 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 8724622     |
| train/                  |             |
|    approx_kl            | 0.050801966 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000274    |
|    loss                 | 0.102       |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.000504    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 143      |
|    time_elapsed    | 53312    |
|    total_timesteps | 8785920  |
---------------------------------
Eval num_timesteps=8786063, episode_reward=0.12 +/- 0.98
Episode length: 29.93 +/- 1.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 8786063     |
| train/                  |             |
|    approx_kl            | 0.051432632 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.896      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000274    |
|    loss                 | 0.0967      |
|    n_updates            | 715         |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 144      |
|    time_elapsed    | 53661    |
|    total_timesteps | 8847360  |
---------------------------------
Eval num_timesteps=8847504, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 8847504     |
| train/                  |             |
|    approx_kl            | 0.051297497 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.000273    |
|    loss                 | 0.095       |
|    n_updates            | 720         |
|    policy_gradient_loss | -3.58e-05   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 145      |
|    time_elapsed    | 54010    |
|    total_timesteps | 8908800  |
---------------------------------
Eval num_timesteps=8908945, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 8908945    |
| train/                  |            |
|    approx_kl            | 0.05909117 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.9       |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.000273   |
|    loss                 | 0.129      |
|    n_updates            | 725        |
|    policy_gradient_loss | -9.41e-05  |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 146      |
|    time_elapsed    | 54357    |
|    total_timesteps | 8970240  |
---------------------------------
Eval num_timesteps=8970386, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 8970386     |
| train/                  |             |
|    approx_kl            | 0.051360495 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.889      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000273    |
|    loss                 | 0.0541      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00059    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 147      |
|    time_elapsed    | 54704    |
|    total_timesteps | 9031680  |
---------------------------------
Eval num_timesteps=9031827, episode_reward=0.08 +/- 0.99
Episode length: 29.90 +/- 1.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 9031827    |
| train/                  |            |
|    approx_kl            | 0.05070056 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.889     |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000273   |
|    loss                 | 0.0856     |
|    n_updates            | 735        |
|    policy_gradient_loss | -0.000792  |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 148      |
|    time_elapsed    | 55052    |
|    total_timesteps | 9093120  |
---------------------------------
Eval num_timesteps=9093268, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 9093268     |
| train/                  |             |
|    approx_kl            | 0.051085625 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.888      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000273    |
|    loss                 | 0.117       |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.000112   |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 149      |
|    time_elapsed    | 55402    |
|    total_timesteps | 9154560  |
---------------------------------
Eval num_timesteps=9154709, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 9154709     |
| train/                  |             |
|    approx_kl            | 0.049044795 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.884      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000273    |
|    loss                 | 0.0702      |
|    n_updates            | 745         |
|    policy_gradient_loss | -0.000119   |
|    value_loss           | 0.232       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 18
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 150      |
|    time_elapsed    | 55752    |
|    total_timesteps | 9216000  |
---------------------------------
Eval num_timesteps=9216150, episode_reward=0.07 +/- 0.99
Episode length: 29.92 +/- 1.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 9216150    |
| train/                  |            |
|    approx_kl            | 0.05250733 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.917     |
|    explained_variance   | 0.315      |
|    learning_rate        | 0.000272   |
|    loss                 | 0.0411     |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.000162  |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 151      |
|    time_elapsed    | 56101    |
|    total_timesteps | 9277440  |
---------------------------------
Eval num_timesteps=9277591, episode_reward=0.03 +/- 0.99
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 9277591     |
| train/                  |             |
|    approx_kl            | 0.052868415 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.904      |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.000272    |
|    loss                 | 0.0463      |
|    n_updates            | 755         |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 152      |
|    time_elapsed    | 56449    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9339032, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.102      |
| time/                   |            |
|    total_timesteps      | 9339032    |
| train/                  |            |
|    approx_kl            | 0.05164603 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.892     |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000272   |
|    loss                 | 0.0668     |
|    n_updates            | 760        |
|    policy_gradient_loss | 0.000498   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 153      |
|    time_elapsed    | 56797    |
|    total_timesteps | 9400320  |
---------------------------------
Eval num_timesteps=9400473, episode_reward=0.03 +/- 0.98
Episode length: 29.92 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 9400473     |
| train/                  |             |
|    approx_kl            | 0.052176993 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.889      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000272    |
|    loss                 | 0.158       |
|    n_updates            | 765         |
|    policy_gradient_loss | 0.000466    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 154      |
|    time_elapsed    | 57145    |
|    total_timesteps | 9461760  |
---------------------------------
Eval num_timesteps=9461914, episode_reward=0.11 +/- 0.98
Episode length: 29.97 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 9461914    |
| train/                  |            |
|    approx_kl            | 0.05203374 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.888     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000272   |
|    loss                 | 0.105      |
|    n_updates            | 770        |
|    policy_gradient_loss | 0.00133    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 155      |
|    time_elapsed    | 57494    |
|    total_timesteps | 9523200  |
---------------------------------
Eval num_timesteps=9523355, episode_reward=-0.02 +/- 0.99
Episode length: 29.92 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 9523355     |
| train/                  |             |
|    approx_kl            | 0.050379883 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.882      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000271    |
|    loss                 | 0.119       |
|    n_updates            | 775         |
|    policy_gradient_loss | 0.000156    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 156      |
|    time_elapsed    | 57843    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9584796, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 9584796     |
| train/                  |             |
|    approx_kl            | 0.052643012 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.878      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000271    |
|    loss                 | 0.035       |
|    n_updates            | 780         |
|    policy_gradient_loss | -6.1e-05    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 157      |
|    time_elapsed    | 58190    |
|    total_timesteps | 9646080  |
---------------------------------
Eval num_timesteps=9646237, episode_reward=0.05 +/- 0.99
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 9646237     |
| train/                  |             |
|    approx_kl            | 0.053432755 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000271    |
|    loss                 | 0.15        |
|    n_updates            | 785         |
|    policy_gradient_loss | -0.000861   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 158      |
|    time_elapsed    | 58537    |
|    total_timesteps | 9707520  |
---------------------------------
Eval num_timesteps=9707678, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 9707678     |
| train/                  |             |
|    approx_kl            | 0.052856497 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000271    |
|    loss                 | 0.134       |
|    n_updates            | 790         |
|    policy_gradient_loss | -2.57e-05   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 159      |
|    time_elapsed    | 58886    |
|    total_timesteps | 9768960  |
---------------------------------
Eval num_timesteps=9769119, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 9769119     |
| train/                  |             |
|    approx_kl            | 0.054349914 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000271    |
|    loss                 | -0.015      |
|    n_updates            | 795         |
|    policy_gradient_loss | 0.000487    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 160      |
|    time_elapsed    | 59237    |
|    total_timesteps | 9830400  |
---------------------------------
Eval num_timesteps=9830560, episode_reward=0.15 +/- 0.98
Episode length: 29.93 +/- 0.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.148      |
| time/                   |            |
|    total_timesteps      | 9830560    |
| train/                  |            |
|    approx_kl            | 0.05323671 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.871     |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.000271   |
|    loss                 | 0.112      |
|    n_updates            | 800        |
|    policy_gradient_loss | 0.000446   |
|    value_loss           | 0.24       |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 19
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 161      |
|    time_elapsed    | 59586    |
|    total_timesteps | 9891840  |
---------------------------------
Eval num_timesteps=9892001, episode_reward=0.05 +/- 0.98
Episode length: 29.92 +/- 1.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 9892001     |
| train/                  |             |
|    approx_kl            | 0.052299812 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.883      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.00027     |
|    loss                 | 0.0691      |
|    n_updates            | 805         |
|    policy_gradient_loss | 0.00051     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 162      |
|    time_elapsed    | 59934    |
|    total_timesteps | 9953280  |
---------------------------------
Eval num_timesteps=9953442, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 9953442     |
| train/                  |             |
|    approx_kl            | 0.054179065 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.00027     |
|    loss                 | 0.112       |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.000247   |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 163      |
|    time_elapsed    | 60283    |
|    total_timesteps | 10014720 |
---------------------------------
Eval num_timesteps=10014883, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 10014883   |
| train/                  |            |
|    approx_kl            | 0.05356594 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.88      |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.00027    |
|    loss                 | 0.0185     |
|    n_updates            | 815        |
|    policy_gradient_loss | 0.000164   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 164      |
|    time_elapsed    | 60632    |
|    total_timesteps | 10076160 |
---------------------------------
Eval num_timesteps=10076324, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.03       |
| time/                   |            |
|    total_timesteps      | 10076324   |
| train/                  |            |
|    approx_kl            | 0.05639391 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.875     |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.00027    |
|    loss                 | 0.142      |
|    n_updates            | 820        |
|    policy_gradient_loss | 0.00237    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 165      |
|    time_elapsed    | 60981    |
|    total_timesteps | 10137600 |
---------------------------------
Eval num_timesteps=10137765, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 10137765    |
| train/                  |             |
|    approx_kl            | 0.052819762 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.00027     |
|    loss                 | 0.0205      |
|    n_updates            | 825         |
|    policy_gradient_loss | 0.00159     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 166      |
|    time_elapsed    | 61330    |
|    total_timesteps | 10199040 |
---------------------------------
Eval num_timesteps=10199206, episode_reward=0.01 +/- 0.98
Episode length: 29.92 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 10199206    |
| train/                  |             |
|    approx_kl            | 0.052552424 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.855      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000269    |
|    loss                 | 0.0615      |
|    n_updates            | 830         |
|    policy_gradient_loss | 0.00118     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 167      |
|    time_elapsed    | 61678    |
|    total_timesteps | 10260480 |
---------------------------------
Eval num_timesteps=10260647, episode_reward=0.03 +/- 0.99
Episode length: 29.94 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.026      |
| time/                   |            |
|    total_timesteps      | 10260647   |
| train/                  |            |
|    approx_kl            | 0.05498185 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.852     |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.000269   |
|    loss                 | 0.143      |
|    n_updates            | 835        |
|    policy_gradient_loss | 0.00148    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 168      |
|    time_elapsed    | 62025    |
|    total_timesteps | 10321920 |
---------------------------------
Eval num_timesteps=10322088, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 10322088    |
| train/                  |             |
|    approx_kl            | 0.052464955 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000269    |
|    loss                 | 0.0893      |
|    n_updates            | 840         |
|    policy_gradient_loss | 0.00087     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 169      |
|    time_elapsed    | 62372    |
|    total_timesteps | 10383360 |
---------------------------------
Eval num_timesteps=10383529, episode_reward=-0.00 +/- 0.99
Episode length: 29.95 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.004     |
| time/                   |            |
|    total_timesteps      | 10383529   |
| train/                  |            |
|    approx_kl            | 0.05429857 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.836     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000269   |
|    loss                 | 0.15       |
|    n_updates            | 845        |
|    policy_gradient_loss | 0.00038    |
|    value_loss           | 0.232      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 170      |
|    time_elapsed    | 62722    |
|    total_timesteps | 10444800 |
---------------------------------
Eval num_timesteps=10444970, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 10444970   |
| train/                  |            |
|    approx_kl            | 0.05201452 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.838     |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.000269   |
|    loss                 | 0.098      |
|    n_updates            | 850        |
|    policy_gradient_loss | 0.000237   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 171      |
|    time_elapsed    | 63073    |
|    total_timesteps | 10506240 |
---------------------------------
Eval num_timesteps=10506411, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 10506411    |
| train/                  |             |
|    approx_kl            | 0.053384826 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.84       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000268    |
|    loss                 | 0.065       |
|    n_updates            | 855         |
|    policy_gradient_loss | 0.000144    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 172      |
|    time_elapsed    | 63422    |
|    total_timesteps | 10567680 |
---------------------------------
Eval num_timesteps=10567852, episode_reward=0.04 +/- 0.99
Episode length: 29.95 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 10567852    |
| train/                  |             |
|    approx_kl            | 0.054142594 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000268    |
|    loss                 | 0.0662      |
|    n_updates            | 860         |
|    policy_gradient_loss | 0.00286     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 173      |
|    time_elapsed    | 63771    |
|    total_timesteps | 10629120 |
---------------------------------
Eval num_timesteps=10629293, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 10629293    |
| train/                  |             |
|    approx_kl            | 0.050159276 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.839      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000268    |
|    loss                 | 0.0846      |
|    n_updates            | 865         |
|    policy_gradient_loss | -0.000139   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 174      |
|    time_elapsed    | 64120    |
|    total_timesteps | 10690560 |
---------------------------------
Eval num_timesteps=10690734, episode_reward=0.12 +/- 0.98
Episode length: 29.94 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 10690734    |
| train/                  |             |
|    approx_kl            | 0.052588023 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.834      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000268    |
|    loss                 | 0.0512      |
|    n_updates            | 870         |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 175      |
|    time_elapsed    | 64469    |
|    total_timesteps | 10752000 |
---------------------------------
Eval num_timesteps=10752175, episode_reward=0.07 +/- 0.98
Episode length: 29.94 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.074      |
| time/                   |            |
|    total_timesteps      | 10752175   |
| train/                  |            |
|    approx_kl            | 0.05217673 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.833     |
|    explained_variance   | 0.313      |
|    learning_rate        | 0.000268   |
|    loss                 | 0.163      |
|    n_updates            | 875        |
|    policy_gradient_loss | 0.000926   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 176      |
|    time_elapsed    | 64829    |
|    total_timesteps | 10813440 |
---------------------------------
Eval num_timesteps=10813616, episode_reward=0.06 +/- 0.99
Episode length: 29.91 +/- 1.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 10813616   |
| train/                  |            |
|    approx_kl            | 0.05121283 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.83      |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000268   |
|    loss                 | 0.068      |
|    n_updates            | 880        |
|    policy_gradient_loss | 0.00107    |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 177      |
|    time_elapsed    | 65206    |
|    total_timesteps | 10874880 |
---------------------------------
Eval num_timesteps=10875057, episode_reward=0.11 +/- 0.98
Episode length: 29.91 +/- 1.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 10875057   |
| train/                  |            |
|    approx_kl            | 0.05072441 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.814     |
|    explained_variance   | 0.324      |
|    learning_rate        | 0.000267   |
|    loss                 | 0.0647     |
|    n_updates            | 885        |
|    policy_gradient_loss | 0.000209   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 178      |
|    time_elapsed    | 65569    |
|    total_timesteps | 10936320 |
---------------------------------
Eval num_timesteps=10936498, episode_reward=0.05 +/- 0.98
Episode length: 29.94 +/- 0.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 10936498    |
| train/                  |             |
|    approx_kl            | 0.050128628 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000267    |
|    loss                 | 0.0712      |
|    n_updates            | 890         |
|    policy_gradient_loss | 0.000138    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 179      |
|    time_elapsed    | 65939    |
|    total_timesteps | 10997760 |
---------------------------------
Eval num_timesteps=10997939, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 10997939    |
| train/                  |             |
|    approx_kl            | 0.050062988 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.809      |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.000267    |
|    loss                 | 0.0564      |
|    n_updates            | 895         |
|    policy_gradient_loss | 4.3e-05     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 180      |
|    time_elapsed    | 66306    |
|    total_timesteps | 11059200 |
---------------------------------
Eval num_timesteps=11059380, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.09      |
| time/                   |           |
|    total_timesteps      | 11059380  |
| train/                  |           |
|    approx_kl            | 0.0501482 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.803    |
|    explained_variance   | 0.336     |
|    learning_rate        | 0.000267  |
|    loss                 | 0.0606    |
|    n_updates            | 900       |
|    policy_gradient_loss | 0.00138   |
|    value_loss           | 0.231     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 181      |
|    time_elapsed    | 66686    |
|    total_timesteps | 11120640 |
---------------------------------
Eval num_timesteps=11120821, episode_reward=0.06 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 11120821   |
| train/                  |            |
|    approx_kl            | 0.04985797 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.000267   |
|    loss                 | 0.0481     |
|    n_updates            | 905        |
|    policy_gradient_loss | 0.00014    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 182      |
|    time_elapsed    | 67069    |
|    total_timesteps | 11182080 |
---------------------------------
Eval num_timesteps=11182262, episode_reward=0.15 +/- 0.97
Episode length: 29.93 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.148       |
| time/                   |             |
|    total_timesteps      | 11182262    |
| train/                  |             |
|    approx_kl            | 0.050884653 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000266    |
|    loss                 | 0.132       |
|    n_updates            | 910         |
|    policy_gradient_loss | 0.000511    |
|    value_loss           | 0.232       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 20
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 183      |
|    time_elapsed    | 67481    |
|    total_timesteps | 11243520 |
---------------------------------
Eval num_timesteps=11243703, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 1.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 11243703    |
| train/                  |             |
|    approx_kl            | 0.051204212 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000266    |
|    loss                 | 0.0658      |
|    n_updates            | 915         |
|    policy_gradient_loss | -0.000169   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 184      |
|    time_elapsed    | 67899    |
|    total_timesteps | 11304960 |
---------------------------------
Eval num_timesteps=11305144, episode_reward=0.02 +/- 0.97
Episode length: 30.01 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 11305144    |
| train/                  |             |
|    approx_kl            | 0.050767608 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.000266    |
|    loss                 | 0.0978      |
|    n_updates            | 920         |
|    policy_gradient_loss | 0.00113     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 185      |
|    time_elapsed    | 68313    |
|    total_timesteps | 11366400 |
---------------------------------
Eval num_timesteps=11366585, episode_reward=-0.01 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.008     |
| time/                   |            |
|    total_timesteps      | 11366585   |
| train/                  |            |
|    approx_kl            | 0.05210582 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.791     |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.000266   |
|    loss                 | 0.174      |
|    n_updates            | 925        |
|    policy_gradient_loss | 0.000372   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 186      |
|    time_elapsed    | 68735    |
|    total_timesteps | 11427840 |
---------------------------------
Eval num_timesteps=11428026, episode_reward=0.02 +/- 0.99
Episode length: 29.89 +/- 0.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 11428026    |
| train/                  |             |
|    approx_kl            | 0.054353997 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.794      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000266    |
|    loss                 | 0.0965      |
|    n_updates            | 930         |
|    policy_gradient_loss | 0.000621    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 187      |
|    time_elapsed    | 69155    |
|    total_timesteps | 11489280 |
---------------------------------
Eval num_timesteps=11489467, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.014     |
| time/                   |            |
|    total_timesteps      | 11489467   |
| train/                  |            |
|    approx_kl            | 0.04839297 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.785     |
|    explained_variance   | 0.328      |
|    learning_rate        | 0.000266   |
|    loss                 | 0.0874     |
|    n_updates            | 935        |
|    policy_gradient_loss | 0.000716   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 188      |
|    time_elapsed    | 69567    |
|    total_timesteps | 11550720 |
---------------------------------
Eval num_timesteps=11550908, episode_reward=0.13 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 11550908    |
| train/                  |             |
|    approx_kl            | 0.051875427 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000265    |
|    loss                 | 0.0272      |
|    n_updates            | 940         |
|    policy_gradient_loss | 0.00217     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 189      |
|    time_elapsed    | 69986    |
|    total_timesteps | 11612160 |
---------------------------------
Eval num_timesteps=11612349, episode_reward=0.11 +/- 0.97
Episode length: 30.03 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total_timesteps      | 11612349   |
| train/                  |            |
|    approx_kl            | 0.05107017 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.778     |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000265   |
|    loss                 | 0.185      |
|    n_updates            | 945        |
|    policy_gradient_loss | 0.000296   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 190      |
|    time_elapsed    | 70407    |
|    total_timesteps | 11673600 |
---------------------------------
Eval num_timesteps=11673790, episode_reward=0.07 +/- 0.98
Episode length: 29.87 +/- 1.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 11673790    |
| train/                  |             |
|    approx_kl            | 0.051645655 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.771      |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.000265    |
|    loss                 | 0.0712      |
|    n_updates            | 950         |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 191      |
|    time_elapsed    | 70821    |
|    total_timesteps | 11735040 |
---------------------------------
Eval num_timesteps=11735231, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 11735231   |
| train/                  |            |
|    approx_kl            | 0.05027796 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.762     |
|    explained_variance   | 0.314      |
|    learning_rate        | 0.000265   |
|    loss                 | 0.0965     |
|    n_updates            | 955        |
|    policy_gradient_loss | 0.00128    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 192      |
|    time_elapsed    | 71240    |
|    total_timesteps | 11796480 |
---------------------------------
Eval num_timesteps=11796672, episode_reward=0.07 +/- 0.98
Episode length: 29.93 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 11796672    |
| train/                  |             |
|    approx_kl            | 0.049402975 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000265    |
|    loss                 | 0.181       |
|    n_updates            | 960         |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 193      |
|    time_elapsed    | 71661    |
|    total_timesteps | 11857920 |
---------------------------------
Eval num_timesteps=11858113, episode_reward=0.04 +/- 0.99
Episode length: 29.94 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 11858113    |
| train/                  |             |
|    approx_kl            | 0.048640918 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.3         |
|    learning_rate        | 0.000264    |
|    loss                 | 0.123       |
|    n_updates            | 965         |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 194      |
|    time_elapsed    | 72075    |
|    total_timesteps | 11919360 |
---------------------------------
Eval num_timesteps=11919554, episode_reward=-0.01 +/- 0.98
Episode length: 29.90 +/- 1.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 11919554    |
| train/                  |             |
|    approx_kl            | 0.048092175 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000264    |
|    loss                 | 0.0765      |
|    n_updates            | 970         |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 195      |
|    time_elapsed    | 72493    |
|    total_timesteps | 11980800 |
---------------------------------
Eval num_timesteps=11980995, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 11980995    |
| train/                  |             |
|    approx_kl            | 0.048563663 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000264    |
|    loss                 | 0.0344      |
|    n_updates            | 975         |
|    policy_gradient_loss | 0.00128     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 196      |
|    time_elapsed    | 72915    |
|    total_timesteps | 12042240 |
---------------------------------
Eval num_timesteps=12042436, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 12042436    |
| train/                  |             |
|    approx_kl            | 0.049808588 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000264    |
|    loss                 | 0.0913      |
|    n_updates            | 980         |
|    policy_gradient_loss | 0.00213     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 197      |
|    time_elapsed    | 73329    |
|    total_timesteps | 12103680 |
---------------------------------
Eval num_timesteps=12103877, episode_reward=0.01 +/- 0.99
Episode length: 29.95 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 12103877   |
| train/                  |            |
|    approx_kl            | 0.04914079 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.000264   |
|    loss                 | 0.239      |
|    n_updates            | 985        |
|    policy_gradient_loss | 0.00115    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 198      |
|    time_elapsed    | 73860    |
|    total_timesteps | 12165120 |
---------------------------------
Eval num_timesteps=12165318, episode_reward=0.03 +/- 0.99
Episode length: 29.93 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 12165318    |
| train/                  |             |
|    approx_kl            | 0.049365655 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.000264    |
|    loss                 | 0.116       |
|    n_updates            | 990         |
|    policy_gradient_loss | 0.000313    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 199      |
|    time_elapsed    | 74327    |
|    total_timesteps | 12226560 |
---------------------------------
Eval num_timesteps=12226759, episode_reward=0.06 +/- 0.99
Episode length: 29.94 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 12226759    |
| train/                  |             |
|    approx_kl            | 0.049169816 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.000263    |
|    loss                 | 0.165       |
|    n_updates            | 995         |
|    policy_gradient_loss | 0.000272    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 200      |
|    time_elapsed    | 74745    |
|    total_timesteps | 12288000 |
---------------------------------
Eval num_timesteps=12288200, episode_reward=-0.02 +/- 0.98
Episode length: 29.95 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.024      |
| time/                   |             |
|    total_timesteps      | 12288200    |
| train/                  |             |
|    approx_kl            | 0.049972188 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000263    |
|    loss                 | 0.0694      |
|    n_updates            | 1000        |
|    policy_gradient_loss | 0.000289    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 201      |
|    time_elapsed    | 75164    |
|    total_timesteps | 12349440 |
---------------------------------
Eval num_timesteps=12349641, episode_reward=0.08 +/- 0.98
Episode length: 29.94 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 12349641   |
| train/                  |            |
|    approx_kl            | 0.05030901 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.000263   |
|    loss                 | 0.106      |
|    n_updates            | 1005       |
|    policy_gradient_loss | 0.00102    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 202      |
|    time_elapsed    | 75583    |
|    total_timesteps | 12410880 |
---------------------------------
Eval num_timesteps=12411082, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 12411082    |
| train/                  |             |
|    approx_kl            | 0.048959803 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.761      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000263    |
|    loss                 | 0.0955      |
|    n_updates            | 1010        |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 203      |
|    time_elapsed    | 75998    |
|    total_timesteps | 12472320 |
---------------------------------
Eval num_timesteps=12472523, episode_reward=0.11 +/- 0.98
Episode length: 29.93 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total_timesteps      | 12472523   |
| train/                  |            |
|    approx_kl            | 0.04668287 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000263   |
|    loss                 | 0.11       |
|    n_updates            | 1015       |
|    policy_gradient_loss | -0.00059   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 204      |
|    time_elapsed    | 76417    |
|    total_timesteps | 12533760 |
---------------------------------
Eval num_timesteps=12533964, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.122      |
| time/                   |            |
|    total_timesteps      | 12533964   |
| train/                  |            |
|    approx_kl            | 0.04822796 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.000262   |
|    loss                 | 0.117      |
|    n_updates            | 1020       |
|    policy_gradient_loss | 0.000181   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 163      |
|    iterations      | 205      |
|    time_elapsed    | 76840    |
|    total_timesteps | 12595200 |
---------------------------------
Eval num_timesteps=12595405, episode_reward=0.09 +/- 0.98
Episode length: 29.94 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.092      |
| time/                   |            |
|    total_timesteps      | 12595405   |
| train/                  |            |
|    approx_kl            | 0.04820556 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.749     |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.000262   |
|    loss                 | 0.136      |
|    n_updates            | 1025       |
|    policy_gradient_loss | 0.000545   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 206      |
|    time_elapsed    | 77254    |
|    total_timesteps | 12656640 |
---------------------------------
Eval num_timesteps=12656846, episode_reward=0.13 +/- 0.97
Episode length: 30.05 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.134      |
| time/                   |            |
|    total_timesteps      | 12656846   |
| train/                  |            |
|    approx_kl            | 0.04807297 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.32       |
|    learning_rate        | 0.000262   |
|    loss                 | 0.0691     |
|    n_updates            | 1030       |
|    policy_gradient_loss | 0.000906   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 207      |
|    time_elapsed    | 77671    |
|    total_timesteps | 12718080 |
---------------------------------
Eval num_timesteps=12718287, episode_reward=0.13 +/- 0.98
Episode length: 29.96 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.134      |
| time/                   |            |
|    total_timesteps      | 12718287   |
| train/                  |            |
|    approx_kl            | 0.04812071 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.731     |
|    explained_variance   | 0.317      |
|    learning_rate        | 0.000262   |
|    loss                 | 0.129      |
|    n_updates            | 1035       |
|    policy_gradient_loss | 0.000761   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 208      |
|    time_elapsed    | 78092    |
|    total_timesteps | 12779520 |
---------------------------------
Eval num_timesteps=12779728, episode_reward=0.06 +/- 0.98
Episode length: 29.90 +/- 0.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 12779728   |
| train/                  |            |
|    approx_kl            | 0.04876877 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.735     |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000262   |
|    loss                 | 0.069      |
|    n_updates            | 1040       |
|    policy_gradient_loss | 0.000188   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 209      |
|    time_elapsed    | 78502    |
|    total_timesteps | 12840960 |
---------------------------------
Eval num_timesteps=12841169, episode_reward=0.10 +/- 0.98
Episode length: 29.91 +/- 0.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 12841169    |
| train/                  |             |
|    approx_kl            | 0.047662318 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.735      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.0852      |
|    n_updates            | 1045        |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 210      |
|    time_elapsed    | 78921    |
|    total_timesteps | 12902400 |
---------------------------------
Eval num_timesteps=12902610, episode_reward=0.10 +/- 0.98
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 12902610    |
| train/                  |             |
|    approx_kl            | 0.047354944 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.737      |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.113       |
|    n_updates            | 1050        |
|    policy_gradient_loss | 0.000748    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 211      |
|    time_elapsed    | 79344    |
|    total_timesteps | 12963840 |
---------------------------------
Eval num_timesteps=12964051, episode_reward=0.15 +/- 0.98
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 12964051    |
| train/                  |             |
|    approx_kl            | 0.048275676 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.101       |
|    n_updates            | 1055        |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 0.238       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.15
SELFPLAY: new best model, bumping up generation to 21
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 212      |
|    time_elapsed    | 79755    |
|    total_timesteps | 13025280 |
---------------------------------
Eval num_timesteps=13025492, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 13025492    |
| train/                  |             |
|    approx_kl            | 0.045843147 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.104       |
|    n_updates            | 1060        |
|    policy_gradient_loss | 0.00149     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 213      |
|    time_elapsed    | 80174    |
|    total_timesteps | 13086720 |
---------------------------------
Eval num_timesteps=13086933, episode_reward=-0.07 +/- 0.98
Episode length: 29.92 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.068      |
| time/                   |             |
|    total_timesteps      | 13086933    |
| train/                  |             |
|    approx_kl            | 0.046086214 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000261    |
|    loss                 | 0.128       |
|    n_updates            | 1065        |
|    policy_gradient_loss | 0.000533    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 214      |
|    time_elapsed    | 80661    |
|    total_timesteps | 13148160 |
---------------------------------
Eval num_timesteps=13148374, episode_reward=0.00 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.004      |
| time/                   |            |
|    total_timesteps      | 13148374   |
| train/                  |            |
|    approx_kl            | 0.04595549 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.000261   |
|    loss                 | 0.0655     |
|    n_updates            | 1070       |
|    policy_gradient_loss | 0.00042    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 215      |
|    time_elapsed    | 81175    |
|    total_timesteps | 13209600 |
---------------------------------
Eval num_timesteps=13209815, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.082    |
| time/                   |          |
|    total_timesteps      | 13209815 |
| train/                  |          |
|    approx_kl            | 0.047565 |
|    clip_fraction        | 0.27     |
|    clip_range           | 0.17     |
|    entropy_loss         | -0.768   |
|    explained_variance   | 0.345    |
|    learning_rate        | 0.00026  |
|    loss                 | 0.124    |
|    n_updates            | 1075     |
|    policy_gradient_loss | 0.00125  |
|    value_loss           | 0.235    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 216      |
|    time_elapsed    | 81594    |
|    total_timesteps | 13271040 |
---------------------------------
Eval num_timesteps=13271256, episode_reward=0.09 +/- 0.98
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 13271256    |
| train/                  |             |
|    approx_kl            | 0.048157826 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.00026     |
|    loss                 | 0.0629      |
|    n_updates            | 1080        |
|    policy_gradient_loss | 0.00175     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 217      |
|    time_elapsed    | 82011    |
|    total_timesteps | 13332480 |
---------------------------------
Eval num_timesteps=13332697, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 13332697    |
| train/                  |             |
|    approx_kl            | 0.044958718 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.00026     |
|    loss                 | 0.136       |
|    n_updates            | 1085        |
|    policy_gradient_loss | 0.000707    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.22    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 218      |
|    time_elapsed    | 82427    |
|    total_timesteps | 13393920 |
---------------------------------
Eval num_timesteps=13394138, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 13394138    |
| train/                  |             |
|    approx_kl            | 0.047148965 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.00026     |
|    loss                 | 0.164       |
|    n_updates            | 1090        |
|    policy_gradient_loss | 0.00026     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 219      |
|    time_elapsed    | 82844    |
|    total_timesteps | 13455360 |
---------------------------------
Eval num_timesteps=13455579, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 13455579    |
| train/                  |             |
|    approx_kl            | 0.047688905 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.731      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.00026     |
|    loss                 | 0.149       |
|    n_updates            | 1095        |
|    policy_gradient_loss | 0.00117     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 220      |
|    time_elapsed    | 83264    |
|    total_timesteps | 13516800 |
---------------------------------
Eval num_timesteps=13517020, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 13517020    |
| train/                  |             |
|    approx_kl            | 0.045455858 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.732      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.0891      |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.000874   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 221      |
|    time_elapsed    | 83682    |
|    total_timesteps | 13578240 |
---------------------------------
Eval num_timesteps=13578461, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 13578461    |
| train/                  |             |
|    approx_kl            | 0.045287423 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.0928      |
|    n_updates            | 1105        |
|    policy_gradient_loss | 0.000938    |
|    value_loss           | 0.246       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 222      |
|    time_elapsed    | 84098    |
|    total_timesteps | 13639680 |
---------------------------------
Eval num_timesteps=13639902, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 13639902    |
| train/                  |             |
|    approx_kl            | 0.045926936 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.12        |
|    n_updates            | 1110        |
|    policy_gradient_loss | 0.000573    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 223      |
|    time_elapsed    | 84520    |
|    total_timesteps | 13701120 |
---------------------------------
Eval num_timesteps=13701343, episode_reward=0.15 +/- 0.97
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 13701343    |
| train/                  |             |
|    approx_kl            | 0.046483494 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.0795      |
|    n_updates            | 1115        |
|    policy_gradient_loss | 0.000224    |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.15
SELFPLAY: new best model, bumping up generation to 22
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 224      |
|    time_elapsed    | 84938    |
|    total_timesteps | 13762560 |
---------------------------------
Eval num_timesteps=13762784, episode_reward=-0.02 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.024      |
| time/                   |             |
|    total_timesteps      | 13762784    |
| train/                  |             |
|    approx_kl            | 0.045767855 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.0147      |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.000455   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 225      |
|    time_elapsed    | 85353    |
|    total_timesteps | 13824000 |
---------------------------------
Eval num_timesteps=13824225, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 13824225    |
| train/                  |             |
|    approx_kl            | 0.046351776 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000259    |
|    loss                 | 0.117       |
|    n_updates            | 1125        |
|    policy_gradient_loss | -2.4e-05    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 226      |
|    time_elapsed    | 85776    |
|    total_timesteps | 13885440 |
---------------------------------
Eval num_timesteps=13885666, episode_reward=0.02 +/- 0.99
Episode length: 29.94 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 13885666   |
| train/                  |            |
|    approx_kl            | 0.04832792 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.741     |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.000258   |
|    loss                 | 0.133      |
|    n_updates            | 1130       |
|    policy_gradient_loss | 0.000252   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 227      |
|    time_elapsed    | 86192    |
|    total_timesteps | 13946880 |
---------------------------------
Eval num_timesteps=13947107, episode_reward=0.03 +/- 0.99
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.028      |
| time/                   |            |
|    total_timesteps      | 13947107   |
| train/                  |            |
|    approx_kl            | 0.04498787 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.74      |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000258   |
|    loss                 | 0.0763     |
|    n_updates            | 1135       |
|    policy_gradient_loss | 0.000655   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 228      |
|    time_elapsed    | 86555    |
|    total_timesteps | 14008320 |
---------------------------------
Eval num_timesteps=14008548, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 14008548    |
| train/                  |             |
|    approx_kl            | 0.046107627 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000258    |
|    loss                 | 0.0656      |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.000382   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 229      |
|    time_elapsed    | 86956    |
|    total_timesteps | 14069760 |
---------------------------------
Eval num_timesteps=14069989, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 1.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.058      |
| time/                   |            |
|    total_timesteps      | 14069989   |
| train/                  |            |
|    approx_kl            | 0.04528184 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.732     |
|    explained_variance   | 0.313      |
|    learning_rate        | 0.000258   |
|    loss                 | 0.115      |
|    n_updates            | 1145       |
|    policy_gradient_loss | 0.000549   |
|    value_loss           | 0.245      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 230      |
|    time_elapsed    | 87370    |
|    total_timesteps | 14131200 |
---------------------------------
Eval num_timesteps=14131430, episode_reward=-0.05 +/- 0.99
Episode length: 29.95 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | -0.05     |
| time/                   |           |
|    total_timesteps      | 14131430  |
| train/                  |           |
|    approx_kl            | 0.0457685 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.737    |
|    explained_variance   | 0.326     |
|    learning_rate        | 0.000258  |
|    loss                 | 0.144     |
|    n_updates            | 1150      |
|    policy_gradient_loss | 0.000314  |
|    value_loss           | 0.235     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 231      |
|    time_elapsed    | 87779    |
|    total_timesteps | 14192640 |
---------------------------------
Eval num_timesteps=14192871, episode_reward=0.06 +/- 0.99
Episode length: 29.94 +/- 0.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.062      |
| time/                   |            |
|    total_timesteps      | 14192871   |
| train/                  |            |
|    approx_kl            | 0.04399213 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.000257   |
|    loss                 | 0.121      |
|    n_updates            | 1155       |
|    policy_gradient_loss | 0.00133    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 161      |
|    iterations      | 232      |
|    time_elapsed    | 88194    |
|    total_timesteps | 14254080 |
---------------------------------
Eval num_timesteps=14254312, episode_reward=0.05 +/- 0.99
Episode length: 29.95 +/- 0.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 14254312   |
| train/                  |            |
|    approx_kl            | 0.04609213 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.726     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000257   |
|    loss                 | 0.0721     |
|    n_updates            | 1160       |
|    policy_gradient_loss | 0.000579   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 233      |
|    time_elapsed    | 88607    |
|    total_timesteps | 14315520 |
---------------------------------
Eval num_timesteps=14315753, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 14315753    |
| train/                  |             |
|    approx_kl            | 0.046624694 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000257    |
|    loss                 | 0.112       |
|    n_updates            | 1165        |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 234      |
|    time_elapsed    | 89016    |
|    total_timesteps | 14376960 |
---------------------------------
Eval num_timesteps=14377194, episode_reward=0.01 +/- 0.99
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 14377194    |
| train/                  |             |
|    approx_kl            | 0.048045658 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000257    |
|    loss                 | 0.117       |
|    n_updates            | 1170        |
|    policy_gradient_loss | 0.000275    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 235      |
|    time_elapsed    | 89543    |
|    total_timesteps | 14438400 |
---------------------------------
Eval num_timesteps=14438635, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 14438635   |
| train/                  |            |
|    approx_kl            | 0.04742663 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000257   |
|    loss                 | 0.099      |
|    n_updates            | 1175       |
|    policy_gradient_loss | 0.00148    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 236      |
|    time_elapsed    | 89974    |
|    total_timesteps | 14499840 |
---------------------------------
Eval num_timesteps=14500076, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 14500076    |
| train/                  |             |
|    approx_kl            | 0.046400607 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000257    |
|    loss                 | 0.0862      |
|    n_updates            | 1180        |
|    policy_gradient_loss | 0.000524    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 237      |
|    time_elapsed    | 90386    |
|    total_timesteps | 14561280 |
---------------------------------
Eval num_timesteps=14561517, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 14561517   |
| train/                  |            |
|    approx_kl            | 0.04937272 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.000256   |
|    loss                 | 0.103      |
|    n_updates            | 1185       |
|    policy_gradient_loss | 0.002      |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 238      |
|    time_elapsed    | 90795    |
|    total_timesteps | 14622720 |
---------------------------------
Eval num_timesteps=14622958, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 14622958    |
| train/                  |             |
|    approx_kl            | 0.047908034 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000256    |
|    loss                 | 0.0948      |
|    n_updates            | 1190        |
|    policy_gradient_loss | 0.0016      |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 239      |
|    time_elapsed    | 91207    |
|    total_timesteps | 14684160 |
---------------------------------
Eval num_timesteps=14684399, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 14684399    |
| train/                  |             |
|    approx_kl            | 0.046897624 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000256    |
|    loss                 | 0.153       |
|    n_updates            | 1195        |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 240      |
|    time_elapsed    | 91618    |
|    total_timesteps | 14745600 |
---------------------------------
Eval num_timesteps=14745840, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.044      |
| time/                   |            |
|    total_timesteps      | 14745840   |
| train/                  |            |
|    approx_kl            | 0.04682951 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.000256   |
|    loss                 | 0.0492     |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.00032   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 241      |
|    time_elapsed    | 92044    |
|    total_timesteps | 14807040 |
---------------------------------
Eval num_timesteps=14807281, episode_reward=0.08 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 14807281    |
| train/                  |             |
|    approx_kl            | 0.047038075 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000256    |
|    loss                 | 0.174       |
|    n_updates            | 1205        |
|    policy_gradient_loss | 0.000508    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 242      |
|    time_elapsed    | 92548    |
|    total_timesteps | 14868480 |
---------------------------------
Eval num_timesteps=14868722, episode_reward=0.11 +/- 0.97
Episode length: 29.95 +/- 0.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 14868722    |
| train/                  |             |
|    approx_kl            | 0.048427112 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000255    |
|    loss                 | 0.167       |
|    n_updates            | 1210        |
|    policy_gradient_loss | 0.000392    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 243      |
|    time_elapsed    | 92993    |
|    total_timesteps | 14929920 |
---------------------------------
Eval num_timesteps=14930163, episode_reward=0.18 +/- 0.97
Episode length: 30.05 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.176       |
| time/                   |             |
|    total_timesteps      | 14930163    |
| train/                  |             |
|    approx_kl            | 0.048629943 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000255    |
|    loss                 | 0.0791      |
|    n_updates            | 1215        |
|    policy_gradient_loss | -0.000737   |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.176
SELFPLAY: new best model, bumping up generation to 23
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 244      |
|    time_elapsed    | 93404    |
|    total_timesteps | 14991360 |
---------------------------------
Eval num_timesteps=14991604, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 14991604    |
| train/                  |             |
|    approx_kl            | 0.048646133 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.000255    |
|    loss                 | 0.0539      |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.000894   |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 245      |
|    time_elapsed    | 93816    |
|    total_timesteps | 15052800 |
---------------------------------
Eval num_timesteps=15053045, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 15053045    |
| train/                  |             |
|    approx_kl            | 0.049899746 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.000255    |
|    loss                 | 0.136       |
|    n_updates            | 1225        |
|    policy_gradient_loss | 0.000608    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 246      |
|    time_elapsed    | 94228    |
|    total_timesteps | 15114240 |
---------------------------------
Eval num_timesteps=15114486, episode_reward=0.09 +/- 0.98
Episode length: 30.05 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 15114486   |
| train/                  |            |
|    approx_kl            | 0.04735537 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.386      |
|    learning_rate        | 0.000255   |
|    loss                 | 0.114      |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.000138  |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 247      |
|    time_elapsed    | 94635    |
|    total_timesteps | 15175680 |
---------------------------------
Eval num_timesteps=15175927, episode_reward=0.04 +/- 0.98
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 15175927    |
| train/                  |             |
|    approx_kl            | 0.047917064 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.396       |
|    learning_rate        | 0.000254    |
|    loss                 | 0.0449      |
|    n_updates            | 1235        |
|    policy_gradient_loss | -0.000926   |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 248      |
|    time_elapsed    | 95090    |
|    total_timesteps | 15237120 |
---------------------------------
Eval num_timesteps=15237368, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 15237368    |
| train/                  |             |
|    approx_kl            | 0.047389213 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.000254    |
|    loss                 | 0.0603      |
|    n_updates            | 1240        |
|    policy_gradient_loss | 0.000334    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 249      |
|    time_elapsed    | 95565    |
|    total_timesteps | 15298560 |
---------------------------------
Eval num_timesteps=15298809, episode_reward=0.13 +/- 0.98
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.132      |
| time/                   |            |
|    total_timesteps      | 15298809   |
| train/                  |            |
|    approx_kl            | 0.04883963 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.375      |
|    learning_rate        | 0.000254   |
|    loss                 | 0.053      |
|    n_updates            | 1245       |
|    policy_gradient_loss | 0.000393   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 250      |
|    time_elapsed    | 96018    |
|    total_timesteps | 15360000 |
---------------------------------
Eval num_timesteps=15360250, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.072     |
| time/                   |           |
|    total_timesteps      | 15360250  |
| train/                  |           |
|    approx_kl            | 0.0505821 |
|    clip_fraction        | 0.273     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.757    |
|    explained_variance   | 0.378     |
|    learning_rate        | 0.000254  |
|    loss                 | 0.0629    |
|    n_updates            | 1250      |
|    policy_gradient_loss | 0.000918  |
|    value_loss           | 0.227     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 251      |
|    time_elapsed    | 96428    |
|    total_timesteps | 15421440 |
---------------------------------
Eval num_timesteps=15421691, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.172       |
| time/                   |             |
|    total_timesteps      | 15421691    |
| train/                  |             |
|    approx_kl            | 0.048247736 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.000254    |
|    loss                 | 0.112       |
|    n_updates            | 1255        |
|    policy_gradient_loss | 0.00132     |
|    value_loss           | 0.229       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.172
SELFPLAY: new best model, bumping up generation to 24
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 252      |
|    time_elapsed    | 96841    |
|    total_timesteps | 15482880 |
---------------------------------
Eval num_timesteps=15483132, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 15483132    |
| train/                  |             |
|    approx_kl            | 0.057802223 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.793      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000254    |
|    loss                 | 0.0886      |
|    n_updates            | 1260        |
|    policy_gradient_loss | 0.000355    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 253      |
|    time_elapsed    | 97256    |
|    total_timesteps | 15544320 |
---------------------------------
Eval num_timesteps=15544573, episode_reward=0.09 +/- 0.97
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 15544573    |
| train/                  |             |
|    approx_kl            | 0.047779024 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000253    |
|    loss                 | 0.0235      |
|    n_updates            | 1265        |
|    policy_gradient_loss | 0.000234    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 254      |
|    time_elapsed    | 97665    |
|    total_timesteps | 15605760 |
---------------------------------
Eval num_timesteps=15606014, episode_reward=-0.00 +/- 0.99
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 15606014    |
| train/                  |             |
|    approx_kl            | 0.050678086 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.000253    |
|    loss                 | 0.0947      |
|    n_updates            | 1270        |
|    policy_gradient_loss | 0.000559    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 255      |
|    time_elapsed    | 98131    |
|    total_timesteps | 15667200 |
---------------------------------
Eval num_timesteps=15667455, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 15667455    |
| train/                  |             |
|    approx_kl            | 0.050612696 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000253    |
|    loss                 | 0.0879      |
|    n_updates            | 1275        |
|    policy_gradient_loss | -6.06e-05   |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 256      |
|    time_elapsed    | 98577    |
|    total_timesteps | 15728640 |
---------------------------------
Eval num_timesteps=15728896, episode_reward=0.11 +/- 0.97
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 15728896    |
| train/                  |             |
|    approx_kl            | 0.049446035 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000253    |
|    loss                 | 0.0951      |
|    n_updates            | 1280        |
|    policy_gradient_loss | 9.62e-05    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 257      |
|    time_elapsed    | 99039    |
|    total_timesteps | 15790080 |
---------------------------------
Eval num_timesteps=15790337, episode_reward=0.07 +/- 0.98
Episode length: 29.96 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 15790337    |
| train/                  |             |
|    approx_kl            | 0.048653807 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.823      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000253    |
|    loss                 | 0.111       |
|    n_updates            | 1285        |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 258      |
|    time_elapsed    | 99449    |
|    total_timesteps | 15851520 |
---------------------------------
Eval num_timesteps=15851778, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.124      |
| time/                   |            |
|    total_timesteps      | 15851778   |
| train/                  |            |
|    approx_kl            | 0.04892148 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.822     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000252   |
|    loss                 | 0.006      |
|    n_updates            | 1290       |
|    policy_gradient_loss | 0.000618   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 259      |
|    time_elapsed    | 99860    |
|    total_timesteps | 15912960 |
---------------------------------
Eval num_timesteps=15913219, episode_reward=0.25 +/- 0.96
Episode length: 30.05 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.246       |
| time/                   |             |
|    total_timesteps      | 15913219    |
| train/                  |             |
|    approx_kl            | 0.048869092 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000252    |
|    loss                 | 0.0897      |
|    n_updates            | 1295        |
|    policy_gradient_loss | 0.000713    |
|    value_loss           | 0.224       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.246
SELFPLAY: new best model, bumping up generation to 25
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 260      |
|    time_elapsed    | 100273   |
|    total_timesteps | 15974400 |
---------------------------------
Eval num_timesteps=15974660, episode_reward=0.03 +/- 0.98
Episode length: 29.94 +/- 1.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.034     |
| time/                   |           |
|    total_timesteps      | 15974660  |
| train/                  |           |
|    approx_kl            | 0.0512748 |
|    clip_fraction        | 0.298     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.863    |
|    explained_variance   | 0.351     |
|    learning_rate        | 0.000252  |
|    loss                 | 0.146     |
|    n_updates            | 1300      |
|    policy_gradient_loss | 0.00183   |
|    value_loss           | 0.231     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 261      |
|    time_elapsed    | 100707   |
|    total_timesteps | 16035840 |
---------------------------------
Eval num_timesteps=16036101, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 16036101    |
| train/                  |             |
|    approx_kl            | 0.052107785 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000252    |
|    loss                 | 0.109       |
|    n_updates            | 1305        |
|    policy_gradient_loss | 0.000149    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 262      |
|    time_elapsed    | 101149   |
|    total_timesteps | 16097280 |
---------------------------------
Eval num_timesteps=16097542, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 16097542    |
| train/                  |             |
|    approx_kl            | 0.050573006 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.862      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000252    |
|    loss                 | 0.0704      |
|    n_updates            | 1310        |
|    policy_gradient_loss | 0.00176     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 263      |
|    time_elapsed    | 101596   |
|    total_timesteps | 16158720 |
---------------------------------
Eval num_timesteps=16158983, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 16158983    |
| train/                  |             |
|    approx_kl            | 0.049735203 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.859      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000252    |
|    loss                 | 0.0993      |
|    n_updates            | 1315        |
|    policy_gradient_loss | 0.00173     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 264      |
|    time_elapsed    | 102067   |
|    total_timesteps | 16220160 |
---------------------------------
Eval num_timesteps=16220424, episode_reward=0.14 +/- 0.98
Episode length: 29.97 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 16220424    |
| train/                  |             |
|    approx_kl            | 0.050939575 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.866      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.13        |
|    n_updates            | 1320        |
|    policy_gradient_loss | 8.22e-05    |
|    value_loss           | 0.237       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.142
SELFPLAY: new best model, bumping up generation to 26
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 265      |
|    time_elapsed    | 102476   |
|    total_timesteps | 16281600 |
---------------------------------
Eval num_timesteps=16281865, episode_reward=-0.02 +/- 0.99
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 16281865    |
| train/                  |             |
|    approx_kl            | 0.050582282 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.879      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.0498      |
|    n_updates            | 1325        |
|    policy_gradient_loss | -0.000155   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 266      |
|    time_elapsed    | 102889   |
|    total_timesteps | 16343040 |
---------------------------------
Eval num_timesteps=16343306, episode_reward=-0.03 +/- 0.99
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 16343306    |
| train/                  |             |
|    approx_kl            | 0.050807945 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.882      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.12        |
|    n_updates            | 1330        |
|    policy_gradient_loss | 0.000362    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 267      |
|    time_elapsed    | 103300   |
|    total_timesteps | 16404480 |
---------------------------------
Eval num_timesteps=16404747, episode_reward=-0.08 +/- 0.98
Episode length: 29.94 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.076      |
| time/                   |             |
|    total_timesteps      | 16404747    |
| train/                  |             |
|    approx_kl            | 0.048988834 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.882      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.0443      |
|    n_updates            | 1335        |
|    policy_gradient_loss | -8.13e-05   |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 268      |
|    time_elapsed    | 103769   |
|    total_timesteps | 16465920 |
---------------------------------
Eval num_timesteps=16466188, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 16466188    |
| train/                  |             |
|    approx_kl            | 0.052054893 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.879      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000251    |
|    loss                 | 0.0668      |
|    n_updates            | 1340        |
|    policy_gradient_loss | 0.000718    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 269      |
|    time_elapsed    | 104181   |
|    total_timesteps | 16527360 |
---------------------------------
Eval num_timesteps=16527629, episode_reward=0.03 +/- 0.98
Episode length: 29.95 +/- 0.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 16527629    |
| train/                  |             |
|    approx_kl            | 0.049703024 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.00025     |
|    loss                 | 0.0957      |
|    n_updates            | 1345        |
|    policy_gradient_loss | -0.00056    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 270      |
|    time_elapsed    | 104635   |
|    total_timesteps | 16588800 |
---------------------------------
Eval num_timesteps=16589070, episode_reward=0.03 +/- 0.98
Episode length: 30.01 +/- 0.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.032     |
| time/                   |           |
|    total_timesteps      | 16589070  |
| train/                  |           |
|    approx_kl            | 0.0508831 |
|    clip_fraction        | 0.302     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.89     |
|    explained_variance   | 0.353     |
|    learning_rate        | 0.00025   |
|    loss                 | 0.0781    |
|    n_updates            | 1350      |
|    policy_gradient_loss | 0.000686  |
|    value_loss           | 0.234     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 271      |
|    time_elapsed    | 105095   |
|    total_timesteps | 16650240 |
---------------------------------
Eval num_timesteps=16650511, episode_reward=0.04 +/- 0.98
Episode length: 29.91 +/- 1.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.036      |
| time/                   |            |
|    total_timesteps      | 16650511   |
| train/                  |            |
|    approx_kl            | 0.04962189 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.885     |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.00025    |
|    loss                 | 0.106      |
|    n_updates            | 1355       |
|    policy_gradient_loss | 0.000254   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 272      |
|    time_elapsed    | 105504   |
|    total_timesteps | 16711680 |
---------------------------------
Eval num_timesteps=16711952, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 16711952    |
| train/                  |             |
|    approx_kl            | 0.053513367 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.885      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.00025     |
|    loss                 | 0.125       |
|    n_updates            | 1360        |
|    policy_gradient_loss | 0.00184     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 273      |
|    time_elapsed    | 105919   |
|    total_timesteps | 16773120 |
---------------------------------
Eval num_timesteps=16773393, episode_reward=0.13 +/- 0.98
Episode length: 29.97 +/- 0.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.132       |
| time/                   |             |
|    total_timesteps      | 16773393    |
| train/                  |             |
|    approx_kl            | 0.050878372 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.885      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.00025     |
|    loss                 | 0.2         |
|    n_updates            | 1365        |
|    policy_gradient_loss | 0.000898    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 274      |
|    time_elapsed    | 106360   |
|    total_timesteps | 16834560 |
---------------------------------
Eval num_timesteps=16834834, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.098      |
| time/                   |            |
|    total_timesteps      | 16834834   |
| train/                  |            |
|    approx_kl            | 0.05080249 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.891     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.000249   |
|    loss                 | 0.0105     |
|    n_updates            | 1370       |
|    policy_gradient_loss | 5.18e-05   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 275      |
|    time_elapsed    | 106795   |
|    total_timesteps | 16896000 |
---------------------------------
Eval num_timesteps=16896275, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.088      |
| time/                   |            |
|    total_timesteps      | 16896275   |
| train/                  |            |
|    approx_kl            | 0.04990828 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.879     |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000249   |
|    loss                 | 0.146      |
|    n_updates            | 1375       |
|    policy_gradient_loss | 0.000285   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 276      |
|    time_elapsed    | 107206   |
|    total_timesteps | 16957440 |
---------------------------------
Eval num_timesteps=16957716, episode_reward=0.07 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 16957716    |
| train/                  |             |
|    approx_kl            | 0.049550246 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000249    |
|    loss                 | 0.0892      |
|    n_updates            | 1380        |
|    policy_gradient_loss | 0.000302    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 277      |
|    time_elapsed    | 107649   |
|    total_timesteps | 17018880 |
---------------------------------
Eval num_timesteps=17019157, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 17019157   |
| train/                  |            |
|    approx_kl            | 0.04906197 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.872     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000249   |
|    loss                 | 0.0662     |
|    n_updates            | 1385       |
|    policy_gradient_loss | -0.000196  |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 278      |
|    time_elapsed    | 108117   |
|    total_timesteps | 17080320 |
---------------------------------
Eval num_timesteps=17080598, episode_reward=0.12 +/- 0.98
Episode length: 30.03 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 17080598    |
| train/                  |             |
|    approx_kl            | 0.049191173 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000249    |
|    loss                 | 0.0736      |
|    n_updates            | 1390        |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 279      |
|    time_elapsed    | 108528   |
|    total_timesteps | 17141760 |
---------------------------------
Eval num_timesteps=17142039, episode_reward=0.04 +/- 0.99
Episode length: 29.88 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 17142039    |
| train/                  |             |
|    approx_kl            | 0.048418775 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000249    |
|    loss                 | 0.0867      |
|    n_updates            | 1395        |
|    policy_gradient_loss | 0.000329    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 280      |
|    time_elapsed    | 108938   |
|    total_timesteps | 17203200 |
---------------------------------
Eval num_timesteps=17203480, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.042      |
| time/                   |            |
|    total_timesteps      | 17203480   |
| train/                  |            |
|    approx_kl            | 0.05086371 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.871     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000248   |
|    loss                 | 0.059      |
|    n_updates            | 1400       |
|    policy_gradient_loss | 0.000306   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 281      |
|    time_elapsed    | 109413   |
|    total_timesteps | 17264640 |
---------------------------------
Eval num_timesteps=17264921, episode_reward=0.15 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.154       |
| time/                   |             |
|    total_timesteps      | 17264921    |
| train/                  |             |
|    approx_kl            | 0.050545573 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000248    |
|    loss                 | 0.12        |
|    n_updates            | 1405        |
|    policy_gradient_loss | 0.000751    |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.154
SELFPLAY: new best model, bumping up generation to 27
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 282      |
|    time_elapsed    | 109822   |
|    total_timesteps | 17326080 |
---------------------------------
Eval num_timesteps=17326362, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 17326362   |
| train/                  |            |
|    approx_kl            | 0.04997533 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.88      |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000248   |
|    loss                 | 0.074      |
|    n_updates            | 1410       |
|    policy_gradient_loss | -7.07e-05  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 283      |
|    time_elapsed    | 110235   |
|    total_timesteps | 17387520 |
---------------------------------
Eval num_timesteps=17387803, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.032     |
| time/                   |            |
|    total_timesteps      | 17387803   |
| train/                  |            |
|    approx_kl            | 0.04764305 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.875     |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.000248   |
|    loss                 | 0.03       |
|    n_updates            | 1415       |
|    policy_gradient_loss | 0.000838   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 284      |
|    time_elapsed    | 110685   |
|    total_timesteps | 17448960 |
---------------------------------
Eval num_timesteps=17449244, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 17449244   |
| train/                  |            |
|    approx_kl            | 0.04832197 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.876     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000248   |
|    loss                 | 0.0318     |
|    n_updates            | 1420       |
|    policy_gradient_loss | 0.00133    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 157      |
|    iterations      | 285      |
|    time_elapsed    | 111153   |
|    total_timesteps | 17510400 |
---------------------------------
Eval num_timesteps=17510685, episode_reward=0.01 +/- 0.98
Episode length: 29.92 +/- 1.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 17510685    |
| train/                  |             |
|    approx_kl            | 0.047753118 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000247    |
|    loss                 | 0.102       |
|    n_updates            | 1425        |
|    policy_gradient_loss | 0.000739    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 157      |
|    iterations      | 286      |
|    time_elapsed    | 111561   |
|    total_timesteps | 17571840 |
---------------------------------
Eval num_timesteps=17572126, episode_reward=-0.04 +/- 0.99
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.044     |
| time/                   |            |
|    total_timesteps      | 17572126   |
| train/                  |            |
|    approx_kl            | 0.04870475 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.882     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000247   |
|    loss                 | 0.0742     |
|    n_updates            | 1430       |
|    policy_gradient_loss | 0.00111    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 287      |
|    time_elapsed    | 111996   |
|    total_timesteps | 17633280 |
---------------------------------
Eval num_timesteps=17633567, episode_reward=-0.01 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.014     |
| time/                   |            |
|    total_timesteps      | 17633567   |
| train/                  |            |
|    approx_kl            | 0.04945692 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.879     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000247   |
|    loss                 | 0.0707     |
|    n_updates            | 1435       |
|    policy_gradient_loss | 0.000383   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 288      |
|    time_elapsed    | 112439   |
|    total_timesteps | 17694720 |
---------------------------------
Eval num_timesteps=17695008, episode_reward=-0.11 +/- 0.98
Episode length: 29.91 +/- 0.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.11       |
| time/                   |             |
|    total_timesteps      | 17695008    |
| train/                  |             |
|    approx_kl            | 0.047817778 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000247    |
|    loss                 | 0.0775      |
|    n_updates            | 1440        |
|    policy_gradient_loss | 0.000336    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 289      |
|    time_elapsed    | 112848   |
|    total_timesteps | 17756160 |
---------------------------------
Eval num_timesteps=17756449, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 1.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 17756449   |
| train/                  |            |
|    approx_kl            | 0.04969721 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.866     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000247   |
|    loss                 | 0.0905     |
|    n_updates            | 1445       |
|    policy_gradient_loss | 6.27e-05   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 290      |
|    time_elapsed    | 113261   |
|    total_timesteps | 17817600 |
---------------------------------
Eval num_timesteps=17817890, episode_reward=0.04 +/- 0.99
Episode length: 29.95 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.036      |
| time/                   |            |
|    total_timesteps      | 17817890   |
| train/                  |            |
|    approx_kl            | 0.04727805 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.858     |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.000247   |
|    loss                 | 0.131      |
|    n_updates            | 1450       |
|    policy_gradient_loss | 0.000825   |
|    value_loss           | 0.244      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 291      |
|    time_elapsed    | 113704   |
|    total_timesteps | 17879040 |
---------------------------------
Eval num_timesteps=17879331, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 17879331    |
| train/                  |             |
|    approx_kl            | 0.048176147 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000246    |
|    loss                 | 0.0867      |
|    n_updates            | 1455        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 292      |
|    time_elapsed    | 114174   |
|    total_timesteps | 17940480 |
---------------------------------
Eval num_timesteps=17940772, episode_reward=0.00 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17940772    |
| train/                  |             |
|    approx_kl            | 0.048379507 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.86       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000246    |
|    loss                 | 0.07        |
|    n_updates            | 1460        |
|    policy_gradient_loss | 0.000744    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 293      |
|    time_elapsed    | 114583   |
|    total_timesteps | 18001920 |
---------------------------------
Eval num_timesteps=18002213, episode_reward=0.04 +/- 0.98
Episode length: 29.92 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 18002213    |
| train/                  |             |
|    approx_kl            | 0.046867624 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.856      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000246    |
|    loss                 | 0.0759      |
|    n_updates            | 1465        |
|    policy_gradient_loss | -0.000256   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 294      |
|    time_elapsed    | 115052   |
|    total_timesteps | 18063360 |
---------------------------------
Eval num_timesteps=18063654, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 18063654   |
| train/                  |            |
|    approx_kl            | 0.04719299 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.853     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000246   |
|    loss                 | 0.0769     |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.000366  |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 295      |
|    time_elapsed    | 115464   |
|    total_timesteps | 18124800 |
---------------------------------
Eval num_timesteps=18125095, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 18125095    |
| train/                  |             |
|    approx_kl            | 0.044133745 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.847      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000246    |
|    loss                 | 0.191       |
|    n_updates            | 1475        |
|    policy_gradient_loss | 0.000329    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 296      |
|    time_elapsed    | 115871   |
|    total_timesteps | 18186240 |
---------------------------------
Eval num_timesteps=18186536, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 18186536   |
| train/                  |            |
|    approx_kl            | 0.04514707 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.853     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000245   |
|    loss                 | 0.0709     |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.00075   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 297      |
|    time_elapsed    | 116284   |
|    total_timesteps | 18247680 |
---------------------------------
Eval num_timesteps=18247977, episode_reward=0.09 +/- 0.99
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 18247977    |
| train/                  |             |
|    approx_kl            | 0.049477797 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000245    |
|    loss                 | 0.121       |
|    n_updates            | 1485        |
|    policy_gradient_loss | 0.000366    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 298      |
|    time_elapsed    | 116725   |
|    total_timesteps | 18309120 |
---------------------------------
Eval num_timesteps=18309418, episode_reward=-0.03 +/- 0.98
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 18309418   |
| train/                  |            |
|    approx_kl            | 0.04699533 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.835     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000245   |
|    loss                 | 0.0626     |
|    n_updates            | 1490       |
|    policy_gradient_loss | 0.000213   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 299      |
|    time_elapsed    | 117203   |
|    total_timesteps | 18370560 |
---------------------------------
Eval num_timesteps=18370859, episode_reward=0.00 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.004      |
| time/                   |            |
|    total_timesteps      | 18370859   |
| train/                  |            |
|    approx_kl            | 0.04866848 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.836     |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.000245   |
|    loss                 | 0.0402     |
|    n_updates            | 1495       |
|    policy_gradient_loss | -0.00031   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 300      |
|    time_elapsed    | 117634   |
|    total_timesteps | 18432000 |
---------------------------------
Eval num_timesteps=18432300, episode_reward=0.01 +/- 0.99
Episode length: 29.95 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 18432300    |
| train/                  |             |
|    approx_kl            | 0.046889145 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000245    |
|    loss                 | 0.085       |
|    n_updates            | 1500        |
|    policy_gradient_loss | 9.22e-05    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 301      |
|    time_elapsed    | 118082   |
|    total_timesteps | 18493440 |
---------------------------------
Eval num_timesteps=18493741, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 18493741    |
| train/                  |             |
|    approx_kl            | 0.047663264 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000245    |
|    loss                 | 0.094       |
|    n_updates            | 1505        |
|    policy_gradient_loss | 0.000236    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 302      |
|    time_elapsed    | 118496   |
|    total_timesteps | 18554880 |
---------------------------------
Eval num_timesteps=18555182, episode_reward=0.08 +/- 0.99
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 18555182   |
| train/                  |            |
|    approx_kl            | 0.04600654 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.838     |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.000244   |
|    loss                 | 0.0428     |
|    n_updates            | 1510       |
|    policy_gradient_loss | 0.000505   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 303      |
|    time_elapsed    | 118906   |
|    total_timesteps | 18616320 |
---------------------------------
Eval num_timesteps=18616623, episode_reward=0.08 +/- 0.98
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 18616623   |
| train/                  |            |
|    approx_kl            | 0.04864274 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.836     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000244   |
|    loss                 | 0.118      |
|    n_updates            | 1515       |
|    policy_gradient_loss | 0.000136   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 304      |
|    time_elapsed    | 119320   |
|    total_timesteps | 18677760 |
---------------------------------
Eval num_timesteps=18678064, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 18678064    |
| train/                  |             |
|    approx_kl            | 0.050084922 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.834      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000244    |
|    loss                 | 0.0596      |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.000222   |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 305      |
|    time_elapsed    | 119752   |
|    total_timesteps | 18739200 |
---------------------------------
Eval num_timesteps=18739505, episode_reward=0.10 +/- 0.98
Episode length: 30.03 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.102      |
| time/                   |            |
|    total_timesteps      | 18739505   |
| train/                  |            |
|    approx_kl            | 0.04878829 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.831     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000244   |
|    loss                 | 0.0447     |
|    n_updates            | 1525       |
|    policy_gradient_loss | 0.00119    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 306      |
|    time_elapsed    | 120227   |
|    total_timesteps | 18800640 |
---------------------------------
Eval num_timesteps=18800946, episode_reward=0.09 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 18800946    |
| train/                  |             |
|    approx_kl            | 0.047312055 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.831      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000244    |
|    loss                 | 0.14        |
|    n_updates            | 1530        |
|    policy_gradient_loss | 0.000397    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 307      |
|    time_elapsed    | 120692   |
|    total_timesteps | 18862080 |
---------------------------------
Eval num_timesteps=18862387, episode_reward=0.11 +/- 0.99
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 18862387   |
| train/                  |            |
|    approx_kl            | 0.04826851 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.828     |
|    explained_variance   | 0.328      |
|    learning_rate        | 0.000243   |
|    loss                 | 0.107      |
|    n_updates            | 1535       |
|    policy_gradient_loss | -0.000502  |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 308      |
|    time_elapsed    | 121104   |
|    total_timesteps | 18923520 |
---------------------------------
Eval num_timesteps=18923828, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.066      |
| time/                   |            |
|    total_timesteps      | 18923828   |
| train/                  |            |
|    approx_kl            | 0.04633663 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000243   |
|    loss                 | 0.0548     |
|    n_updates            | 1540       |
|    policy_gradient_loss | 0.000103   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 309      |
|    time_elapsed    | 121519   |
|    total_timesteps | 18984960 |
---------------------------------
Eval num_timesteps=18985269, episode_reward=0.17 +/- 0.96
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.174       |
| time/                   |             |
|    total_timesteps      | 18985269    |
| train/                  |             |
|    approx_kl            | 0.047583412 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000243    |
|    loss                 | 0.106       |
|    n_updates            | 1545        |
|    policy_gradient_loss | 0.000448    |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.174
SELFPLAY: new best model, bumping up generation to 28
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 310      |
|    time_elapsed    | 121929   |
|    total_timesteps | 19046400 |
---------------------------------
Eval num_timesteps=19046710, episode_reward=0.01 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 19046710    |
| train/                  |             |
|    approx_kl            | 0.045909032 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.851      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000243    |
|    loss                 | 0.104       |
|    n_updates            | 1550        |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 156      |
|    iterations      | 311      |
|    time_elapsed    | 122342   |
|    total_timesteps | 19107840 |
---------------------------------
Eval num_timesteps=19108151, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 19108151    |
| train/                  |             |
|    approx_kl            | 0.045931317 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.847      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000243    |
|    loss                 | 0.134       |
|    n_updates            | 1555        |
|    policy_gradient_loss | 0.000549    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 312      |
|    time_elapsed    | 122777   |
|    total_timesteps | 19169280 |
---------------------------------
Eval num_timesteps=19169592, episode_reward=0.06 +/- 0.98
Episode length: 29.90 +/- 1.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 19169592    |
| train/                  |             |
|    approx_kl            | 0.047331873 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000242    |
|    loss                 | 0.103       |
|    n_updates            | 1560        |
|    policy_gradient_loss | 0.000328    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 313      |
|    time_elapsed    | 123143   |
|    total_timesteps | 19230720 |
---------------------------------
Eval num_timesteps=19231033, episode_reward=-0.02 +/- 0.99
Episode length: 29.90 +/- 1.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.018     |
| time/                   |            |
|    total_timesteps      | 19231033   |
| train/                  |            |
|    approx_kl            | 0.04504438 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.856     |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.000242   |
|    loss                 | 0.135      |
|    n_updates            | 1565       |
|    policy_gradient_loss | 0.00206    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 314      |
|    time_elapsed    | 123490   |
|    total_timesteps | 19292160 |
---------------------------------
Eval num_timesteps=19292474, episode_reward=-0.01 +/- 0.99
Episode length: 29.94 +/- 1.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.01      |
| time/                   |            |
|    total_timesteps      | 19292474   |
| train/                  |            |
|    approx_kl            | 0.04412482 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.842     |
|    explained_variance   | 0.328      |
|    learning_rate        | 0.000242   |
|    loss                 | 0.0768     |
|    n_updates            | 1570       |
|    policy_gradient_loss | 0.00142    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 315      |
|    time_elapsed    | 123838   |
|    total_timesteps | 19353600 |
---------------------------------
Eval num_timesteps=19353915, episode_reward=-0.05 +/- 0.99
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.046      |
| time/                   |             |
|    total_timesteps      | 19353915    |
| train/                  |             |
|    approx_kl            | 0.045280196 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000242    |
|    loss                 | 0.0894      |
|    n_updates            | 1575        |
|    policy_gradient_loss | 0.000714    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 156      |
|    iterations      | 316      |
|    time_elapsed    | 124187   |
|    total_timesteps | 19415040 |
---------------------------------
Eval num_timesteps=19415356, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 19415356   |
| train/                  |            |
|    approx_kl            | 0.04888149 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.838     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000242   |
|    loss                 | 0.0368     |
|    n_updates            | 1580       |
|    policy_gradient_loss | 0.000285   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 317      |
|    time_elapsed    | 124537   |
|    total_timesteps | 19476480 |
---------------------------------
Eval num_timesteps=19476797, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.078      |
| time/                   |            |
|    total_timesteps      | 19476797   |
| train/                  |            |
|    approx_kl            | 0.04595685 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.825     |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.000242   |
|    loss                 | 0.0854     |
|    n_updates            | 1585       |
|    policy_gradient_loss | 0.000369   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 318      |
|    time_elapsed    | 124888   |
|    total_timesteps | 19537920 |
---------------------------------
Eval num_timesteps=19538238, episode_reward=0.11 +/- 0.98
Episode length: 29.89 +/- 1.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.114      |
| time/                   |            |
|    total_timesteps      | 19538238   |
| train/                  |            |
|    approx_kl            | 0.04681004 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.822     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000241   |
|    loss                 | 0.0535     |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.000134  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 319      |
|    time_elapsed    | 125237   |
|    total_timesteps | 19599360 |
---------------------------------
Eval num_timesteps=19599679, episode_reward=0.00 +/- 0.99
Episode length: 29.95 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 19599679    |
| train/                  |             |
|    approx_kl            | 0.046338223 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000241    |
|    loss                 | 0.05        |
|    n_updates            | 1595        |
|    policy_gradient_loss | 0.000986    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 320      |
|    time_elapsed    | 125586   |
|    total_timesteps | 19660800 |
---------------------------------
Eval num_timesteps=19661120, episode_reward=0.08 +/- 0.99
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 19661120    |
| train/                  |             |
|    approx_kl            | 0.045861676 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.813      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000241    |
|    loss                 | 0.0741      |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.000343    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 321      |
|    time_elapsed    | 125935   |
|    total_timesteps | 19722240 |
---------------------------------
Eval num_timesteps=19722561, episode_reward=-0.06 +/- 0.99
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.064     |
| time/                   |            |
|    total_timesteps      | 19722561   |
| train/                  |            |
|    approx_kl            | 0.04655061 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.818     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000241   |
|    loss                 | 0.153      |
|    n_updates            | 1605       |
|    policy_gradient_loss | -0.000272  |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 322      |
|    time_elapsed    | 126284   |
|    total_timesteps | 19783680 |
---------------------------------
Eval num_timesteps=19784002, episode_reward=0.00 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 19784002    |
| train/                  |             |
|    approx_kl            | 0.046211313 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000241    |
|    loss                 | 0.117       |
|    n_updates            | 1610        |
|    policy_gradient_loss | 0.000958    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 323      |
|    time_elapsed    | 126633   |
|    total_timesteps | 19845120 |
---------------------------------
Eval num_timesteps=19845443, episode_reward=-0.01 +/- 0.99
Episode length: 29.85 +/- 1.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 19845443    |
| train/                  |             |
|    approx_kl            | 0.044088934 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.00024     |
|    loss                 | 0.0954      |
|    n_updates            | 1615        |
|    policy_gradient_loss | 0.00161     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 324      |
|    time_elapsed    | 126981   |
|    total_timesteps | 19906560 |
---------------------------------
Eval num_timesteps=19906884, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 1.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.066      |
| time/                   |            |
|    total_timesteps      | 19906884   |
| train/                  |            |
|    approx_kl            | 0.04591934 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.814     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.00024    |
|    loss                 | 0.0826     |
|    n_updates            | 1620       |
|    policy_gradient_loss | 0.000911   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 325      |
|    time_elapsed    | 127329   |
|    total_timesteps | 19968000 |
---------------------------------
Eval num_timesteps=19968325, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 19968325    |
| train/                  |             |
|    approx_kl            | 0.045499165 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.00024     |
|    loss                 | 0.0842      |
|    n_updates            | 1625        |
|    policy_gradient_loss | 0.000472    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 326      |
|    time_elapsed    | 127676   |
|    total_timesteps | 20029440 |
---------------------------------
Eval num_timesteps=20029766, episode_reward=0.08 +/- 0.99
Episode length: 30.02 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 20029766   |
| train/                  |            |
|    approx_kl            | 0.04835862 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.82      |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.00024    |
|    loss                 | 0.145      |
|    n_updates            | 1630       |
|    policy_gradient_loss | 0.00181    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 327      |
|    time_elapsed    | 128026   |
|    total_timesteps | 20090880 |
---------------------------------
Eval num_timesteps=20091207, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 0.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.006      |
| time/                   |            |
|    total_timesteps      | 20091207   |
| train/                  |            |
|    approx_kl            | 0.04420797 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.82      |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.00024    |
|    loss                 | 0.0541     |
|    n_updates            | 1635       |
|    policy_gradient_loss | 0.000235   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 328      |
|    time_elapsed    | 128377   |
|    total_timesteps | 20152320 |
---------------------------------
Eval num_timesteps=20152648, episode_reward=0.02 +/- 0.99
Episode length: 29.91 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 20152648    |
| train/                  |             |
|    approx_kl            | 0.043717306 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.00024     |
|    loss                 | 0.147       |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00028    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 329      |
|    time_elapsed    | 128727   |
|    total_timesteps | 20213760 |
---------------------------------
Eval num_timesteps=20214089, episode_reward=0.02 +/- 0.99
Episode length: 29.95 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 20214089    |
| train/                  |             |
|    approx_kl            | 0.044434715 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.809      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000239    |
|    loss                 | 0.125       |
|    n_updates            | 1645        |
|    policy_gradient_loss | -0.000345   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 330      |
|    time_elapsed    | 129075   |
|    total_timesteps | 20275200 |
---------------------------------
Eval num_timesteps=20275530, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 1.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.01       |
| time/                   |            |
|    total_timesteps      | 20275530   |
| train/                  |            |
|    approx_kl            | 0.04610551 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000239   |
|    loss                 | 0.0728     |
|    n_updates            | 1650       |
|    policy_gradient_loss | 8.5e-05    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 331      |
|    time_elapsed    | 129423   |
|    total_timesteps | 20336640 |
---------------------------------
Eval num_timesteps=20336971, episode_reward=-0.02 +/- 0.99
Episode length: 29.99 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 20336971    |
| train/                  |             |
|    approx_kl            | 0.042966556 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.799      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000239    |
|    loss                 | 0.104       |
|    n_updates            | 1655        |
|    policy_gradient_loss | 6.31e-05    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 332      |
|    time_elapsed    | 129772   |
|    total_timesteps | 20398080 |
---------------------------------
Eval num_timesteps=20398412, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 20398412    |
| train/                  |             |
|    approx_kl            | 0.043077223 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.802      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.000239    |
|    loss                 | 0.101       |
|    n_updates            | 1660        |
|    policy_gradient_loss | 0.0015      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 333      |
|    time_elapsed    | 130122   |
|    total_timesteps | 20459520 |
---------------------------------
Eval num_timesteps=20459853, episode_reward=0.01 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 20459853    |
| train/                  |             |
|    approx_kl            | 0.044165455 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000239    |
|    loss                 | 0.0803      |
|    n_updates            | 1665        |
|    policy_gradient_loss | 0.000622    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 334      |
|    time_elapsed    | 130503   |
|    total_timesteps | 20520960 |
---------------------------------
Eval num_timesteps=20521294, episode_reward=0.04 +/- 0.98
Episode length: 30.01 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 20521294    |
| train/                  |             |
|    approx_kl            | 0.045182664 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000238    |
|    loss                 | 0.0429      |
|    n_updates            | 1670        |
|    policy_gradient_loss | 5.25e-06    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 335      |
|    time_elapsed    | 130946   |
|    total_timesteps | 20582400 |
---------------------------------
Eval num_timesteps=20582735, episode_reward=0.04 +/- 0.99
Episode length: 29.99 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 20582735   |
| train/                  |            |
|    approx_kl            | 0.04705655 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.8       |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.000238   |
|    loss                 | 0.103      |
|    n_updates            | 1675       |
|    policy_gradient_loss | 8.03e-05   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 336      |
|    time_elapsed    | 131352   |
|    total_timesteps | 20643840 |
---------------------------------
Eval num_timesteps=20644176, episode_reward=0.04 +/- 0.99
Episode length: 30.01 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 20644176    |
| train/                  |             |
|    approx_kl            | 0.042768326 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.805      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000238    |
|    loss                 | 0.0803      |
|    n_updates            | 1680        |
|    policy_gradient_loss | 0.000212    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 337      |
|    time_elapsed    | 131802   |
|    total_timesteps | 20705280 |
---------------------------------
Eval num_timesteps=20705617, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 20705617    |
| train/                  |             |
|    approx_kl            | 0.042830095 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.802      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000238    |
|    loss                 | 0.045       |
|    n_updates            | 1685        |
|    policy_gradient_loss | 0.000367    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 338      |
|    time_elapsed    | 132207   |
|    total_timesteps | 20766720 |
---------------------------------
Eval num_timesteps=20767058, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 20767058    |
| train/                  |             |
|    approx_kl            | 0.043666527 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.808      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000238    |
|    loss                 | 0.061       |
|    n_updates            | 1690        |
|    policy_gradient_loss | 0.000631    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 339      |
|    time_elapsed    | 132698   |
|    total_timesteps | 20828160 |
---------------------------------
Eval num_timesteps=20828499, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.054     |
| time/                   |           |
|    total_timesteps      | 20828499  |
| train/                  |           |
|    approx_kl            | 0.0450747 |
|    clip_fraction        | 0.27      |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.807    |
|    explained_variance   | 0.344     |
|    learning_rate        | 0.000238  |
|    loss                 | 0.198     |
|    n_updates            | 1695      |
|    policy_gradient_loss | -4.03e-05 |
|    value_loss           | 0.235     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 340      |
|    time_elapsed    | 133191   |
|    total_timesteps | 20889600 |
---------------------------------
Eval num_timesteps=20889940, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 20889940    |
| train/                  |             |
|    approx_kl            | 0.043256488 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.803      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.1         |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.000439   |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 341      |
|    time_elapsed    | 133662   |
|    total_timesteps | 20951040 |
---------------------------------
Eval num_timesteps=20951381, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 20951381    |
| train/                  |             |
|    approx_kl            | 0.042602252 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.808      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.088       |
|    n_updates            | 1705        |
|    policy_gradient_loss | 0.000182    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 342      |
|    time_elapsed    | 134167   |
|    total_timesteps | 21012480 |
---------------------------------
Eval num_timesteps=21012822, episode_reward=0.02 +/- 0.98
Episode length: 29.88 +/- 1.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 21012822    |
| train/                  |             |
|    approx_kl            | 0.045286004 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0794      |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 343      |
|    time_elapsed    | 134627   |
|    total_timesteps | 21073920 |
---------------------------------
Eval num_timesteps=21074263, episode_reward=0.07 +/- 0.99
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 21074263    |
| train/                  |             |
|    approx_kl            | 0.042775113 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.801      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0786      |
|    n_updates            | 1715        |
|    policy_gradient_loss | -0.000458   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 344      |
|    time_elapsed    | 135150   |
|    total_timesteps | 21135360 |
---------------------------------
Eval num_timesteps=21135704, episode_reward=0.05 +/- 0.99
Episode length: 29.93 +/- 0.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 21135704   |
| train/                  |            |
|    approx_kl            | 0.04182488 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.795     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000237   |
|    loss                 | 0.0744     |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.000588  |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 345      |
|    time_elapsed    | 135607   |
|    total_timesteps | 21196800 |
---------------------------------
Eval num_timesteps=21197145, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 21197145    |
| train/                  |             |
|    approx_kl            | 0.044838555 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000236    |
|    loss                 | 0.142       |
|    n_updates            | 1725        |
|    policy_gradient_loss | 0.000415    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 346      |
|    time_elapsed    | 136118   |
|    total_timesteps | 21258240 |
---------------------------------
Eval num_timesteps=21258586, episode_reward=-0.04 +/- 0.99
Episode length: 29.90 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.044     |
| time/                   |            |
|    total_timesteps      | 21258586   |
| train/                  |            |
|    approx_kl            | 0.04398156 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.807     |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.000236   |
|    loss                 | 0.0852     |
|    n_updates            | 1730       |
|    policy_gradient_loss | 0.000717   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 347      |
|    time_elapsed    | 136579   |
|    total_timesteps | 21319680 |
---------------------------------
Eval num_timesteps=21320027, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.104      |
| time/                   |            |
|    total_timesteps      | 21320027   |
| train/                  |            |
|    approx_kl            | 0.04500394 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000236   |
|    loss                 | 0.107      |
|    n_updates            | 1735       |
|    policy_gradient_loss | 0.000321   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 348      |
|    time_elapsed    | 137097   |
|    total_timesteps | 21381120 |
---------------------------------
Eval num_timesteps=21381468, episode_reward=-0.00 +/- 0.98
Episode length: 29.91 +/- 1.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 21381468   |
| train/                  |            |
|    approx_kl            | 0.04332367 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.805     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000236   |
|    loss                 | 0.103      |
|    n_updates            | 1740       |
|    policy_gradient_loss | 0.000202   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 349      |
|    time_elapsed    | 137542   |
|    total_timesteps | 21442560 |
---------------------------------
Eval num_timesteps=21442909, episode_reward=0.03 +/- 0.99
Episode length: 29.94 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 21442909    |
| train/                  |             |
|    approx_kl            | 0.042595513 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000236    |
|    loss                 | 0.113       |
|    n_updates            | 1745        |
|    policy_gradient_loss | -0.000181   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 350      |
|    time_elapsed    | 138076   |
|    total_timesteps | 21504000 |
---------------------------------
Eval num_timesteps=21504350, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 21504350    |
| train/                  |             |
|    approx_kl            | 0.043595076 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.796      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000235    |
|    loss                 | 0.0912      |
|    n_updates            | 1750        |
|    policy_gradient_loss | 0.000175    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 351      |
|    time_elapsed    | 138534   |
|    total_timesteps | 21565440 |
---------------------------------
Eval num_timesteps=21565791, episode_reward=0.03 +/- 0.99
Episode length: 29.93 +/- 1.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.026      |
| time/                   |            |
|    total_timesteps      | 21565791   |
| train/                  |            |
|    approx_kl            | 0.04161292 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.786     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000235   |
|    loss                 | 0.152      |
|    n_updates            | 1755       |
|    policy_gradient_loss | -0.00135   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 352      |
|    time_elapsed    | 139040   |
|    total_timesteps | 21626880 |
---------------------------------
Eval num_timesteps=21627232, episode_reward=0.01 +/- 0.98
Episode length: 29.91 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 21627232    |
| train/                  |             |
|    approx_kl            | 0.045661457 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000235    |
|    loss                 | 0.129       |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.000627   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 353      |
|    time_elapsed    | 139516   |
|    total_timesteps | 21688320 |
---------------------------------
Eval num_timesteps=21688673, episode_reward=0.05 +/- 0.99
Episode length: 29.96 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 21688673    |
| train/                  |             |
|    approx_kl            | 0.045213956 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.79       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000235    |
|    loss                 | 0.082       |
|    n_updates            | 1765        |
|    policy_gradient_loss | 0.000405    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 354      |
|    time_elapsed    | 140003   |
|    total_timesteps | 21749760 |
---------------------------------
Eval num_timesteps=21750114, episode_reward=0.04 +/- 0.99
Episode length: 29.92 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 21750114   |
| train/                  |            |
|    approx_kl            | 0.04327585 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.777     |
|    explained_variance   | 0.339      |
|    learning_rate        | 0.000235   |
|    loss                 | 0.0924     |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.00106   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 355      |
|    time_elapsed    | 140493   |
|    total_timesteps | 21811200 |
---------------------------------
Eval num_timesteps=21811555, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 21811555    |
| train/                  |             |
|    approx_kl            | 0.044804342 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.772      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000235    |
|    loss                 | 0.0632      |
|    n_updates            | 1775        |
|    policy_gradient_loss | -0.000156   |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 356      |
|    time_elapsed    | 140997   |
|    total_timesteps | 21872640 |
---------------------------------
Eval num_timesteps=21872996, episode_reward=0.06 +/- 0.98
Episode length: 29.92 +/- 0.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.058      |
| time/                   |            |
|    total_timesteps      | 21872996   |
| train/                  |            |
|    approx_kl            | 0.04512446 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.000234   |
|    loss                 | 0.0686     |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.00018   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 357      |
|    time_elapsed    | 141457   |
|    total_timesteps | 21934080 |
---------------------------------
Eval num_timesteps=21934437, episode_reward=0.05 +/- 0.99
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 21934437    |
| train/                  |             |
|    approx_kl            | 0.044348065 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000234    |
|    loss                 | 0.175       |
|    n_updates            | 1785        |
|    policy_gradient_loss | 0.000743    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 358      |
|    time_elapsed    | 141987   |
|    total_timesteps | 21995520 |
---------------------------------
Eval num_timesteps=21995878, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.12        |
| time/                   |             |
|    total_timesteps      | 21995878    |
| train/                  |             |
|    approx_kl            | 0.043563925 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.77       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000234    |
|    loss                 | 0.182       |
|    n_updates            | 1790        |
|    policy_gradient_loss | 0.000543    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 359      |
|    time_elapsed    | 142427   |
|    total_timesteps | 22056960 |
---------------------------------
Eval num_timesteps=22057319, episode_reward=0.03 +/- 0.99
Episode length: 29.95 +/- 1.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.034     |
| time/                   |           |
|    total_timesteps      | 22057319  |
| train/                  |           |
|    approx_kl            | 0.0457573 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.768    |
|    explained_variance   | 0.33      |
|    learning_rate        | 0.000234  |
|    loss                 | 0.0807    |
|    n_updates            | 1795      |
|    policy_gradient_loss | 0.000705  |
|    value_loss           | 0.242     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 360      |
|    time_elapsed    | 142953   |
|    total_timesteps | 22118400 |
---------------------------------
Eval num_timesteps=22118760, episode_reward=0.04 +/- 0.99
Episode length: 29.93 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 22118760    |
| train/                  |             |
|    approx_kl            | 0.047426157 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000234    |
|    loss                 | 0.127       |
|    n_updates            | 1800        |
|    policy_gradient_loss | 0.000158    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 361      |
|    time_elapsed    | 143395   |
|    total_timesteps | 22179840 |
---------------------------------
Eval num_timesteps=22180201, episode_reward=0.04 +/- 0.99
Episode length: 29.89 +/- 1.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 22180201    |
| train/                  |             |
|    approx_kl            | 0.044170894 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000233    |
|    loss                 | 0.117       |
|    n_updates            | 1805        |
|    policy_gradient_loss | -0.000556   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 362      |
|    time_elapsed    | 143922   |
|    total_timesteps | 22241280 |
---------------------------------
Eval num_timesteps=22241642, episode_reward=0.07 +/- 0.98
Episode length: 29.95 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 22241642    |
| train/                  |             |
|    approx_kl            | 0.045717828 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000233    |
|    loss                 | 0.149       |
|    n_updates            | 1810        |
|    policy_gradient_loss | 0.000633    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 363      |
|    time_elapsed    | 144389   |
|    total_timesteps | 22302720 |
---------------------------------
Eval num_timesteps=22303083, episode_reward=0.17 +/- 0.98
Episode length: 29.86 +/- 1.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.166       |
| time/                   |             |
|    total_timesteps      | 22303083    |
| train/                  |             |
|    approx_kl            | 0.045404587 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.000233    |
|    loss                 | 0.114       |
|    n_updates            | 1815        |
|    policy_gradient_loss | 0.000368    |
|    value_loss           | 0.242       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.166
SELFPLAY: new best model, bumping up generation to 29
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 364      |
|    time_elapsed    | 144892   |
|    total_timesteps | 22364160 |
---------------------------------
Eval num_timesteps=22364524, episode_reward=-0.03 +/- 0.98
Episode length: 29.93 +/- 1.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 22364524    |
| train/                  |             |
|    approx_kl            | 0.045995213 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000233    |
|    loss                 | 0.0463      |
|    n_updates            | 1820        |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 365      |
|    time_elapsed    | 145371   |
|    total_timesteps | 22425600 |
---------------------------------
Eval num_timesteps=22425965, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.006      |
| time/                   |            |
|    total_timesteps      | 22425965   |
| train/                  |            |
|    approx_kl            | 0.04609533 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.806     |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.000233   |
|    loss                 | 0.111      |
|    n_updates            | 1825       |
|    policy_gradient_loss | 0.00123    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 366      |
|    time_elapsed    | 145866   |
|    total_timesteps | 22487040 |
---------------------------------
Eval num_timesteps=22487406, episode_reward=-0.01 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 22487406    |
| train/                  |             |
|    approx_kl            | 0.046561185 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000233    |
|    loss                 | 0.0816      |
|    n_updates            | 1830        |
|    policy_gradient_loss | 0.00134     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 367      |
|    time_elapsed    | 146351   |
|    total_timesteps | 22548480 |
---------------------------------
Eval num_timesteps=22548847, episode_reward=-0.00 +/- 0.99
Episode length: 29.84 +/- 1.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 22548847    |
| train/                  |             |
|    approx_kl            | 0.045318965 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000232    |
|    loss                 | 0.108       |
|    n_updates            | 1835        |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 153      |
|    iterations      | 368      |
|    time_elapsed    | 146855   |
|    total_timesteps | 22609920 |
---------------------------------
Eval num_timesteps=22610288, episode_reward=0.02 +/- 0.99
Episode length: 29.94 +/- 1.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 22610288   |
| train/                  |            |
|    approx_kl            | 0.04860954 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.000232   |
|    loss                 | 0.107      |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.000158  |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 369      |
|    time_elapsed    | 147315   |
|    total_timesteps | 22671360 |
---------------------------------
Eval num_timesteps=22671729, episode_reward=0.06 +/- 0.99
Episode length: 29.87 +/- 1.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 22671729    |
| train/                  |             |
|    approx_kl            | 0.043790955 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.787      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000232    |
|    loss                 | 0.141       |
|    n_updates            | 1845        |
|    policy_gradient_loss | 0.000462    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 153      |
|    iterations      | 370      |
|    time_elapsed    | 147844   |
|    total_timesteps | 22732800 |
---------------------------------
Eval num_timesteps=22733170, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.066      |
| time/                   |            |
|    total_timesteps      | 22733170   |
| train/                  |            |
|    approx_kl            | 0.04480883 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.781     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.000232   |
|    loss                 | 0.0471     |
|    n_updates            | 1850       |
|    policy_gradient_loss | 0.000179   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 371      |
|    time_elapsed    | 148226   |
|    total_timesteps | 22794240 |
---------------------------------
Eval num_timesteps=22794611, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 22794611    |
| train/                  |             |
|    approx_kl            | 0.044888306 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000232    |
|    loss                 | 0.104       |
|    n_updates            | 1855        |
|    policy_gradient_loss | -8.94e-05   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 372      |
|    time_elapsed    | 148574   |
|    total_timesteps | 22855680 |
---------------------------------
Eval num_timesteps=22856052, episode_reward=0.06 +/- 0.99
Episode length: 29.82 +/- 1.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 22856052    |
| train/                  |             |
|    approx_kl            | 0.044504274 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000231    |
|    loss                 | 0.157       |
|    n_updates            | 1860        |
|    policy_gradient_loss | 0.000712    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 373      |
|    time_elapsed    | 148923   |
|    total_timesteps | 22917120 |
---------------------------------
Eval num_timesteps=22917493, episode_reward=0.00 +/- 0.99
Episode length: 29.88 +/- 1.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 22917493   |
| train/                  |            |
|    approx_kl            | 0.04451551 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.776     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000231   |
|    loss                 | 0.0931     |
|    n_updates            | 1865       |
|    policy_gradient_loss | -0.000393  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 374      |
|    time_elapsed    | 149273   |
|    total_timesteps | 22978560 |
---------------------------------
Eval num_timesteps=22978934, episode_reward=-0.00 +/- 0.98
Episode length: 29.84 +/- 1.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 22978934    |
| train/                  |             |
|    approx_kl            | 0.042202514 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000231    |
|    loss                 | 0.0691      |
|    n_updates            | 1870        |
|    policy_gradient_loss | -3.99e-05   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 153      |
|    iterations      | 375      |
|    time_elapsed    | 149623   |
|    total_timesteps | 23040000 |
---------------------------------
Eval num_timesteps=23040375, episode_reward=0.10 +/- 0.98
Episode length: 29.86 +/- 1.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.098      |
| time/                   |            |
|    total_timesteps      | 23040375   |
| train/                  |            |
|    approx_kl            | 0.04435331 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000231   |
|    loss                 | 0.108      |
|    n_updates            | 1875       |
|    policy_gradient_loss | 0.000403   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 376      |
|    time_elapsed    | 149971   |
|    total_timesteps | 23101440 |
---------------------------------
Eval num_timesteps=23101816, episode_reward=0.05 +/- 0.99
Episode length: 29.89 +/- 1.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 23101816    |
| train/                  |             |
|    approx_kl            | 0.045592487 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000231    |
|    loss                 | 0.0795      |
|    n_updates            | 1880        |
|    policy_gradient_loss | 0.00096     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 377      |
|    time_elapsed    | 150319   |
|    total_timesteps | 23162880 |
---------------------------------
Eval num_timesteps=23163257, episode_reward=0.04 +/- 0.98
Episode length: 29.91 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 23163257    |
| train/                  |             |
|    approx_kl            | 0.045128416 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000231    |
|    loss                 | 0.0745      |
|    n_updates            | 1885        |
|    policy_gradient_loss | 0.000551    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 378      |
|    time_elapsed    | 150667   |
|    total_timesteps | 23224320 |
---------------------------------
Eval num_timesteps=23224698, episode_reward=0.10 +/- 0.98
Episode length: 29.94 +/- 1.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 23224698   |
| train/                  |            |
|    approx_kl            | 0.04383942 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.761     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.00023    |
|    loss                 | 0.121      |
|    n_updates            | 1890       |
|    policy_gradient_loss | 7.34e-05   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 379      |
|    time_elapsed    | 151016   |
|    total_timesteps | 23285760 |
---------------------------------
Eval num_timesteps=23286139, episode_reward=0.08 +/- 0.99
Episode length: 29.85 +/- 1.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 23286139    |
| train/                  |             |
|    approx_kl            | 0.043371923 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.00023     |
|    loss                 | 0.16        |
|    n_updates            | 1895        |
|    policy_gradient_loss | 0.0004      |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 380      |
|    time_elapsed    | 151365   |
|    total_timesteps | 23347200 |
---------------------------------
Eval num_timesteps=23347580, episode_reward=0.06 +/- 0.99
Episode length: 29.92 +/- 1.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.056     |
| time/                   |           |
|    total_timesteps      | 23347580  |
| train/                  |           |
|    approx_kl            | 0.0436092 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.752    |
|    explained_variance   | 0.361     |
|    learning_rate        | 0.00023   |
|    loss                 | 0.142     |
|    n_updates            | 1900      |
|    policy_gradient_loss | 0.000196  |
|    value_loss           | 0.233     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 154      |
|    iterations      | 381      |
|    time_elapsed    | 151713   |
|    total_timesteps | 23408640 |
---------------------------------
Eval num_timesteps=23409021, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 1.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.104     |
| time/                   |           |
|    total_timesteps      | 23409021  |
| train/                  |           |
|    approx_kl            | 0.0436467 |
|    clip_fraction        | 0.25      |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.748    |
|    explained_variance   | 0.342     |
|    learning_rate        | 0.00023   |
|    loss                 | 0.0577    |
|    n_updates            | 1905      |
|    policy_gradient_loss | 0.00188   |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 382      |
|    time_elapsed    | 152060   |
|    total_timesteps | 23470080 |
---------------------------------
Eval num_timesteps=23470462, episode_reward=0.02 +/- 0.98
Episode length: 29.93 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 23470462    |
| train/                  |             |
|    approx_kl            | 0.043808457 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.00023     |
|    loss                 | 0.1         |
|    n_updates            | 1910        |
|    policy_gradient_loss | 0.000227    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 154      |
|    iterations      | 383      |
|    time_elapsed    | 152408   |
|    total_timesteps | 23531520 |
---------------------------------
Eval num_timesteps=23531903, episode_reward=0.05 +/- 0.99
Episode length: 29.83 +/- 1.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.8       |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 23531903   |
| train/                  |            |
|    approx_kl            | 0.04182813 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000229   |
|    loss                 | 0.0294     |
|    n_updates            | 1915       |
|    policy_gradient_loss | 0.000642   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 384      |
|    time_elapsed    | 152757   |
|    total_timesteps | 23592960 |
---------------------------------
Eval num_timesteps=23593344, episode_reward=0.03 +/- 0.98
Episode length: 29.83 +/- 1.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 23593344    |
| train/                  |             |
|    approx_kl            | 0.042161204 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000229    |
|    loss                 | 0.0741      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.000584   |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 385      |
|    time_elapsed    | 153107   |
|    total_timesteps | 23654400 |
---------------------------------
Eval num_timesteps=23654785, episode_reward=0.10 +/- 0.98
Episode length: 29.92 +/- 1.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 23654785    |
| train/                  |             |
|    approx_kl            | 0.043841362 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000229    |
|    loss                 | 0.0892      |
|    n_updates            | 1925        |
|    policy_gradient_loss | 7.69e-05    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 386      |
|    time_elapsed    | 153456   |
|    total_timesteps | 23715840 |
---------------------------------
Eval num_timesteps=23716226, episode_reward=-0.00 +/- 0.99
Episode length: 29.86 +/- 1.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 23716226    |
| train/                  |             |
|    approx_kl            | 0.044283457 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000229    |
|    loss                 | 0.103       |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.000198   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 387      |
|    time_elapsed    | 153807   |
|    total_timesteps | 23777280 |
---------------------------------
Eval num_timesteps=23777667, episode_reward=0.05 +/- 0.98
Episode length: 29.80 +/- 2.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 23777667    |
| train/                  |             |
|    approx_kl            | 0.043104455 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000229    |
|    loss                 | 0.0561      |
|    n_updates            | 1935        |
|    policy_gradient_loss | -0.000645   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 388      |
|    time_elapsed    | 154194   |
|    total_timesteps | 23838720 |
---------------------------------
Eval num_timesteps=23839108, episode_reward=0.04 +/- 0.98
Episode length: 29.78 +/- 2.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 23839108    |
| train/                  |             |
|    approx_kl            | 0.042143177 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000228    |
|    loss                 | 0.114       |
|    n_updates            | 1940        |
|    policy_gradient_loss | 6.24e-05    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 389      |
|    time_elapsed    | 154599   |
|    total_timesteps | 23900160 |
---------------------------------
Eval num_timesteps=23900549, episode_reward=0.09 +/- 0.98
Episode length: 29.97 +/- 1.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.086      |
| time/                   |            |
|    total_timesteps      | 23900549   |
| train/                  |            |
|    approx_kl            | 0.04353835 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.000228   |
|    loss                 | 0.0365     |
|    n_updates            | 1945       |
|    policy_gradient_loss | 0.000875   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 390      |
|    time_elapsed    | 155011   |
|    total_timesteps | 23961600 |
---------------------------------
Eval num_timesteps=23961990, episode_reward=0.11 +/- 0.98
Episode length: 29.93 +/- 1.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 23961990    |
| train/                  |             |
|    approx_kl            | 0.044177763 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000228    |
|    loss                 | 0.0844      |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.000704   |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 154      |
|    iterations      | 391      |
|    time_elapsed    | 155589   |
|    total_timesteps | 24023040 |
---------------------------------
Eval num_timesteps=24023431, episode_reward=0.15 +/- 0.98
Episode length: 29.89 +/- 1.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.152       |
| time/                   |             |
|    total_timesteps      | 24023431    |
| train/                  |             |
|    approx_kl            | 0.044193514 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.755      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000228    |
|    loss                 | 0.0547      |
|    n_updates            | 1955        |
|    policy_gradient_loss | -2.83e-05   |
|    value_loss           | 0.243       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.152
SELFPLAY: new best model, bumping up generation to 30
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 392      |
|    time_elapsed    | 155989   |
|    total_timesteps | 24084480 |
---------------------------------
Eval num_timesteps=24084872, episode_reward=0.00 +/- 0.99
Episode length: 29.94 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 24084872    |
| train/                  |             |
|    approx_kl            | 0.044164214 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.784      |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000228    |
|    loss                 | 0.0983      |
|    n_updates            | 1960        |
|    policy_gradient_loss | 0.000125    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 393      |
|    time_elapsed    | 156390   |
|    total_timesteps | 24145920 |
---------------------------------
Eval num_timesteps=24146313, episode_reward=0.03 +/- 0.99
Episode length: 29.92 +/- 1.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 24146313    |
| train/                  |             |
|    approx_kl            | 0.043188486 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000228    |
|    loss                 | 0.145       |
|    n_updates            | 1965        |
|    policy_gradient_loss | -0.000279   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 394      |
|    time_elapsed    | 156857   |
|    total_timesteps | 24207360 |
---------------------------------
Eval num_timesteps=24207754, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 24207754    |
| train/                  |             |
|    approx_kl            | 0.049967725 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0656      |
|    n_updates            | 1970        |
|    policy_gradient_loss | 0.000174    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 395      |
|    time_elapsed    | 157381   |
|    total_timesteps | 24268800 |
---------------------------------
Eval num_timesteps=24269195, episode_reward=-0.03 +/- 0.99
Episode length: 29.95 +/- 0.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.028     |
| time/                   |            |
|    total_timesteps      | 24269195   |
| train/                  |            |
|    approx_kl            | 0.04653578 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.777     |
|    explained_variance   | 0.339      |
|    learning_rate        | 0.000227   |
|    loss                 | 0.162      |
|    n_updates            | 1975       |
|    policy_gradient_loss | 0.000782   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 396      |
|    time_elapsed    | 157783   |
|    total_timesteps | 24330240 |
---------------------------------
Eval num_timesteps=24330636, episode_reward=0.05 +/- 0.99
Episode length: 29.94 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 24330636    |
| train/                  |             |
|    approx_kl            | 0.044834375 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.782      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.133       |
|    n_updates            | 1980        |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 397      |
|    time_elapsed    | 158183   |
|    total_timesteps | 24391680 |
---------------------------------
Eval num_timesteps=24392077, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 24392077    |
| train/                  |             |
|    approx_kl            | 0.044025503 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0877      |
|    n_updates            | 1985        |
|    policy_gradient_loss | 0.00144     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 398      |
|    time_elapsed    | 158710   |
|    total_timesteps | 24453120 |
---------------------------------
Eval num_timesteps=24453518, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 1.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 24453518    |
| train/                  |             |
|    approx_kl            | 0.043846574 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.13        |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.000441   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 399      |
|    time_elapsed    | 159178   |
|    total_timesteps | 24514560 |
---------------------------------
Eval num_timesteps=24514959, episode_reward=0.02 +/- 0.99
Episode length: 29.93 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 24514959    |
| train/                  |             |
|    approx_kl            | 0.044078197 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000226    |
|    loss                 | 0.0859      |
|    n_updates            | 1995        |
|    policy_gradient_loss | 0.00171     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 400      |
|    time_elapsed    | 159578   |
|    total_timesteps | 24576000 |
---------------------------------
Eval num_timesteps=24576400, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.022      |
| time/                   |            |
|    total_timesteps      | 24576400   |
| train/                  |            |
|    approx_kl            | 0.04404931 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.781     |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.000226   |
|    loss                 | 0.0589     |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.000408  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 401      |
|    time_elapsed    | 160055   |
|    total_timesteps | 24637440 |
---------------------------------
Eval num_timesteps=24637841, episode_reward=0.12 +/- 0.98
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.124       |
| time/                   |             |
|    total_timesteps      | 24637841    |
| train/                  |             |
|    approx_kl            | 0.044121202 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000226    |
|    loss                 | 0.0904      |
|    n_updates            | 2005        |
|    policy_gradient_loss | 0.00175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 402      |
|    time_elapsed    | 160498   |
|    total_timesteps | 24698880 |
---------------------------------
Eval num_timesteps=24699282, episode_reward=0.05 +/- 0.98
Episode length: 29.88 +/- 1.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 24699282    |
| train/                  |             |
|    approx_kl            | 0.045033257 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.782      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000226    |
|    loss                 | 0.0516      |
|    n_updates            | 2010        |
|    policy_gradient_loss | 0.000213    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 403      |
|    time_elapsed    | 160972   |
|    total_timesteps | 24760320 |
---------------------------------
Eval num_timesteps=24760723, episode_reward=0.02 +/- 0.99
Episode length: 29.74 +/- 2.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 24760723    |
| train/                  |             |
|    approx_kl            | 0.044621713 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000226    |
|    loss                 | 0.102       |
|    n_updates            | 2015        |
|    policy_gradient_loss | 0.000519    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 404      |
|    time_elapsed    | 161384   |
|    total_timesteps | 24821760 |
---------------------------------
Eval num_timesteps=24822164, episode_reward=0.03 +/- 0.98
Episode length: 29.85 +/- 1.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 24822164    |
| train/                  |             |
|    approx_kl            | 0.047393247 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000226    |
|    loss                 | 0.0592      |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.000188   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 405      |
|    time_elapsed    | 161884   |
|    total_timesteps | 24883200 |
---------------------------------
Eval num_timesteps=24883605, episode_reward=0.05 +/- 0.99
Episode length: 29.88 +/- 1.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 24883605    |
| train/                  |             |
|    approx_kl            | 0.045540076 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.786      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000225    |
|    loss                 | 0.109       |
|    n_updates            | 2025        |
|    policy_gradient_loss | -0.000131   |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 406      |
|    time_elapsed    | 162297   |
|    total_timesteps | 24944640 |
---------------------------------
Eval num_timesteps=24945046, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 24945046    |
| train/                  |             |
|    approx_kl            | 0.047218457 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.786      |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000225    |
|    loss                 | 0.112       |
|    n_updates            | 2030        |
|    policy_gradient_loss | 0.000833    |
|    value_loss           | 0.248       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 407      |
|    time_elapsed    | 162767   |
|    total_timesteps | 25006080 |
---------------------------------
Eval num_timesteps=25006487, episode_reward=0.03 +/- 0.99
Episode length: 29.92 +/- 1.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.032      |
| time/                   |            |
|    total_timesteps      | 25006487   |
| train/                  |            |
|    approx_kl            | 0.04544505 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.792     |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.000225   |
|    loss                 | 0.0316     |
|    n_updates            | 2035       |
|    policy_gradient_loss | 0.00136    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.19    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 408      |
|    time_elapsed    | 163261   |
|    total_timesteps | 25067520 |
---------------------------------
Eval num_timesteps=25067928, episode_reward=0.09 +/- 0.98
Episode length: 29.92 +/- 1.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.094      |
| time/                   |            |
|    total_timesteps      | 25067928   |
| train/                  |            |
|    approx_kl            | 0.04742051 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.786     |
|    explained_variance   | 0.318      |
|    learning_rate        | 0.000225   |
|    loss                 | 0.117      |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.000244  |
|    value_loss           | 0.248      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 409      |
|    time_elapsed    | 163683   |
|    total_timesteps | 25128960 |
---------------------------------
Eval num_timesteps=25129369, episode_reward=0.08 +/- 0.98
Episode length: 29.96 +/- 1.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.084      |
| time/                   |            |
|    total_timesteps      | 25129369   |
| train/                  |            |
|    approx_kl            | 0.04629491 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.788     |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.000225   |
|    loss                 | 0.027      |
|    n_updates            | 2045       |
|    policy_gradient_loss | 0.00122    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 410      |
|    time_elapsed    | 164119   |
|    total_timesteps | 25190400 |
---------------------------------
Eval num_timesteps=25190810, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 25190810    |
| train/                  |             |
|    approx_kl            | 0.046153557 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000224    |
|    loss                 | 0.103       |
|    n_updates            | 2050        |
|    policy_gradient_loss | 0.000152    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 411      |
|    time_elapsed    | 164613   |
|    total_timesteps | 25251840 |
---------------------------------
Eval num_timesteps=25252251, episode_reward=0.01 +/- 0.99
Episode length: 29.94 +/- 1.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.008     |
| time/                   |           |
|    total_timesteps      | 25252251  |
| train/                  |           |
|    approx_kl            | 0.0451285 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.789    |
|    explained_variance   | 0.333     |
|    learning_rate        | 0.000224  |
|    loss                 | 0.144     |
|    n_updates            | 2055      |
|    policy_gradient_loss | 0.00169   |
|    value_loss           | 0.24      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 412      |
|    time_elapsed    | 165083   |
|    total_timesteps | 25313280 |
---------------------------------
Eval num_timesteps=25313692, episode_reward=-0.03 +/- 0.99
Episode length: 29.86 +/- 1.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.028     |
| time/                   |            |
|    total_timesteps      | 25313692   |
| train/                  |            |
|    approx_kl            | 0.04506985 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.801     |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.000224   |
|    loss                 | 0.0784     |
|    n_updates            | 2060       |
|    policy_gradient_loss | 0.000796   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 413      |
|    time_elapsed    | 165482   |
|    total_timesteps | 25374720 |
---------------------------------
Eval num_timesteps=25375133, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 1.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 25375133    |
| train/                  |             |
|    approx_kl            | 0.045744173 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.799      |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.000224    |
|    loss                 | 0.161       |
|    n_updates            | 2065        |
|    policy_gradient_loss | 0.000376    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 414      |
|    time_elapsed    | 165941   |
|    total_timesteps | 25436160 |
---------------------------------
Eval num_timesteps=25436574, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 25436574    |
| train/                  |             |
|    approx_kl            | 0.044736207 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.805      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000224    |
|    loss                 | 0.13        |
|    n_updates            | 2070        |
|    policy_gradient_loss | 0.000683    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 415      |
|    time_elapsed    | 166464   |
|    total_timesteps | 25497600 |
---------------------------------
Eval num_timesteps=25498015, episode_reward=-0.01 +/- 0.99
Episode length: 29.94 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 25498015    |
| train/                  |             |
|    approx_kl            | 0.044540774 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.805      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000224    |
|    loss                 | 0.135       |
|    n_updates            | 2075        |
|    policy_gradient_loss | -0.000419   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 416      |
|    time_elapsed    | 166879   |
|    total_timesteps | 25559040 |
---------------------------------
Eval num_timesteps=25559456, episode_reward=0.04 +/- 0.99
Episode length: 29.90 +/- 1.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 25559456    |
| train/                  |             |
|    approx_kl            | 0.045323305 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000223    |
|    loss                 | 0.135       |
|    n_updates            | 2080        |
|    policy_gradient_loss | 5.23e-05    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 417      |
|    time_elapsed    | 167281   |
|    total_timesteps | 25620480 |
---------------------------------
Eval num_timesteps=25620897, episode_reward=0.08 +/- 0.98
Episode length: 29.96 +/- 1.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 25620897   |
| train/                  |            |
|    approx_kl            | 0.04537727 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.331      |
|    learning_rate        | 0.000223   |
|    loss                 | 0.115      |
|    n_updates            | 2085       |
|    policy_gradient_loss | 0.000949   |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 418      |
|    time_elapsed    | 167792   |
|    total_timesteps | 25681920 |
---------------------------------
Eval num_timesteps=25682338, episode_reward=0.01 +/- 0.98
Episode length: 29.93 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 25682338    |
| train/                  |             |
|    approx_kl            | 0.046212826 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000223    |
|    loss                 | 0.107       |
|    n_updates            | 2090        |
|    policy_gradient_loss | 0.000612    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 419      |
|    time_elapsed    | 168275   |
|    total_timesteps | 25743360 |
---------------------------------
Eval num_timesteps=25743779, episode_reward=0.06 +/- 0.99
Episode length: 29.99 +/- 0.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.058      |
| time/                   |            |
|    total_timesteps      | 25743779   |
| train/                  |            |
|    approx_kl            | 0.04540257 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.789     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000223   |
|    loss                 | 0.138      |
|    n_updates            | 2095       |
|    policy_gradient_loss | 0.00114    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 420      |
|    time_elapsed    | 168679   |
|    total_timesteps | 25804800 |
---------------------------------
Eval num_timesteps=25805220, episode_reward=0.08 +/- 0.99
Episode length: 29.88 +/- 1.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 25805220    |
| train/                  |             |
|    approx_kl            | 0.044964664 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.781      |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000223    |
|    loss                 | 0.119       |
|    n_updates            | 2100        |
|    policy_gradient_loss | 0.000182    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 421      |
|    time_elapsed    | 169078   |
|    total_timesteps | 25866240 |
---------------------------------
Eval num_timesteps=25866661, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 25866661    |
| train/                  |             |
|    approx_kl            | 0.044264387 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000222    |
|    loss                 | 0.0817      |
|    n_updates            | 2105        |
|    policy_gradient_loss | 0.00124     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 422      |
|    time_elapsed    | 169655   |
|    total_timesteps | 25927680 |
---------------------------------
Eval num_timesteps=25928102, episode_reward=0.05 +/- 0.98
Episode length: 29.96 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 25928102    |
| train/                  |             |
|    approx_kl            | 0.044748295 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.784      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000222    |
|    loss                 | 0.0667      |
|    n_updates            | 2110        |
|    policy_gradient_loss | 0.000826    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 423      |
|    time_elapsed    | 170072   |
|    total_timesteps | 25989120 |
---------------------------------
Eval num_timesteps=25989543, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 25989543    |
| train/                  |             |
|    approx_kl            | 0.044200525 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.000222    |
|    loss                 | 0.156       |
|    n_updates            | 2115        |
|    policy_gradient_loss | 0.000749    |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 424      |
|    time_elapsed    | 170478   |
|    total_timesteps | 26050560 |
---------------------------------
Eval num_timesteps=26050984, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 1.10
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.074    |
| time/                   |          |
|    total_timesteps      | 26050984 |
| train/                  |          |
|    approx_kl            | 0.044734 |
|    clip_fraction        | 0.259    |
|    clip_range           | 0.17     |
|    entropy_loss         | -0.795   |
|    explained_variance   | 0.327    |
|    learning_rate        | 0.000222 |
|    loss                 | 0.142    |
|    n_updates            | 2120     |
|    policy_gradient_loss | 0.000105 |
|    value_loss           | 0.242    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 425      |
|    time_elapsed    | 170922   |
|    total_timesteps | 26112000 |
---------------------------------
Eval num_timesteps=26112425, episode_reward=-0.02 +/- 0.98
Episode length: 29.96 +/- 0.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 26112425    |
| train/                  |             |
|    approx_kl            | 0.041999444 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.000222    |
|    loss                 | 0.0886      |
|    n_updates            | 2125        |
|    policy_gradient_loss | -3.39e-05   |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 426      |
|    time_elapsed    | 171470   |
|    total_timesteps | 26173440 |
---------------------------------
Eval num_timesteps=26173866, episode_reward=0.05 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 26173866    |
| train/                  |             |
|    approx_kl            | 0.044832706 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.801      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000221    |
|    loss                 | 0.0878      |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.000452   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 427      |
|    time_elapsed    | 171870   |
|    total_timesteps | 26234880 |
---------------------------------
Eval num_timesteps=26235307, episode_reward=0.06 +/- 0.99
Episode length: 29.89 +/- 1.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 26235307    |
| train/                  |             |
|    approx_kl            | 0.050151777 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.000221    |
|    loss                 | 0.129       |
|    n_updates            | 2135        |
|    policy_gradient_loss | 6.86e-05    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 428      |
|    time_elapsed    | 172273   |
|    total_timesteps | 26296320 |
---------------------------------
Eval num_timesteps=26296748, episode_reward=0.11 +/- 0.98
Episode length: 29.90 +/- 1.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 26296748    |
| train/                  |             |
|    approx_kl            | 0.044973135 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.797      |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.000221    |
|    loss                 | 0.0874      |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.000201   |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 429      |
|    time_elapsed    | 172798   |
|    total_timesteps | 26357760 |
---------------------------------
Eval num_timesteps=26358189, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 26358189    |
| train/                  |             |
|    approx_kl            | 0.043396927 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.804      |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000221    |
|    loss                 | 0.0974      |
|    n_updates            | 2145        |
|    policy_gradient_loss | -0.000104   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 430      |
|    time_elapsed    | 173263   |
|    total_timesteps | 26419200 |
---------------------------------
Eval num_timesteps=26419630, episode_reward=0.13 +/- 0.98
Episode length: 29.94 +/- 1.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 26419630   |
| train/                  |            |
|    approx_kl            | 0.04347179 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.801     |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.000221   |
|    loss                 | 0.0751     |
|    n_updates            | 2150       |
|    policy_gradient_loss | 0.000653   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 431      |
|    time_elapsed    | 173665   |
|    total_timesteps | 26480640 |
---------------------------------
Eval num_timesteps=26481071, episode_reward=0.07 +/- 0.98
Episode length: 29.93 +/- 1.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 26481071   |
| train/                  |            |
|    approx_kl            | 0.04328131 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.806     |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000221   |
|    loss                 | 0.071      |
|    n_updates            | 2155       |
|    policy_gradient_loss | 0.000156   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 432      |
|    time_elapsed    | 174084   |
|    total_timesteps | 26542080 |
---------------------------------
Eval num_timesteps=26542512, episode_reward=0.06 +/- 0.98
Episode length: 29.87 +/- 1.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 26542512   |
| train/                  |            |
|    approx_kl            | 0.04364484 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.798     |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.00022    |
|    loss                 | 0.0927     |
|    n_updates            | 2160       |
|    policy_gradient_loss | 6.77e-05   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 433      |
|    time_elapsed    | 174606   |
|    total_timesteps | 26603520 |
---------------------------------
Eval num_timesteps=26603953, episode_reward=0.13 +/- 0.98
Episode length: 29.98 +/- 1.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.132      |
| time/                   |            |
|    total_timesteps      | 26603953   |
| train/                  |            |
|    approx_kl            | 0.04495546 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.806     |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.00022    |
|    loss                 | 0.08       |
|    n_updates            | 2165       |
|    policy_gradient_loss | -0.000646  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 434      |
|    time_elapsed    | 175061   |
|    total_timesteps | 26664960 |
---------------------------------
Eval num_timesteps=26665394, episode_reward=0.10 +/- 0.98
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 26665394    |
| train/                  |             |
|    approx_kl            | 0.045851808 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.803      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.00022     |
|    loss                 | 0.0468      |
|    n_updates            | 2170        |
|    policy_gradient_loss | 0.000118    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 435      |
|    time_elapsed    | 175460   |
|    total_timesteps | 26726400 |
---------------------------------
Eval num_timesteps=26726835, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 26726835   |
| train/                  |            |
|    approx_kl            | 0.04571933 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.00022    |
|    loss                 | 0.0482     |
|    n_updates            | 2175       |
|    policy_gradient_loss | 0.00028    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 436      |
|    time_elapsed    | 175926   |
|    total_timesteps | 26787840 |
---------------------------------
Eval num_timesteps=26788276, episode_reward=0.12 +/- 0.98
Episode length: 30.01 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 26788276    |
| train/                  |             |
|    approx_kl            | 0.044921007 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.805      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.00022     |
|    loss                 | 0.1         |
|    n_updates            | 2180        |
|    policy_gradient_loss | 0.000161    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 437      |
|    time_elapsed    | 176425   |
|    total_timesteps | 26849280 |
---------------------------------
Eval num_timesteps=26849717, episode_reward=0.08 +/- 0.98
Episode length: 29.95 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 26849717   |
| train/                  |            |
|    approx_kl            | 0.04485425 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.798     |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.000219   |
|    loss                 | 0.125      |
|    n_updates            | 2185       |
|    policy_gradient_loss | -0.000191  |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 438      |
|    time_elapsed    | 176789   |
|    total_timesteps | 26910720 |
---------------------------------
Eval num_timesteps=26911158, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 26911158    |
| train/                  |             |
|    approx_kl            | 0.047336187 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.797      |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000219    |
|    loss                 | 0.1         |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.000436   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 439      |
|    time_elapsed    | 177138   |
|    total_timesteps | 26972160 |
---------------------------------
Eval num_timesteps=26972599, episode_reward=0.01 +/- 0.98
Episode length: 29.93 +/- 1.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 26972599    |
| train/                  |             |
|    approx_kl            | 0.045016613 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.795      |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.000219    |
|    loss                 | 0.102       |
|    n_updates            | 2195        |
|    policy_gradient_loss | -0.000501   |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 440      |
|    time_elapsed    | 177487   |
|    total_timesteps | 27033600 |
---------------------------------
Eval num_timesteps=27034040, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 27034040    |
| train/                  |             |
|    approx_kl            | 0.045257296 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.794      |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.000219    |
|    loss                 | 0.0907      |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.000426   |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 152      |
|    iterations      | 441      |
|    time_elapsed    | 177836   |
|    total_timesteps | 27095040 |
---------------------------------
Eval num_timesteps=27095481, episode_reward=-0.02 +/- 0.99
Episode length: 29.87 +/- 1.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 27095481   |
| train/                  |            |
|    approx_kl            | 0.04581565 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.339      |
|    learning_rate        | 0.000219   |
|    loss                 | 0.0964     |
|    n_updates            | 2205       |
|    policy_gradient_loss | 0.00145    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 442      |
|    time_elapsed    | 178185   |
|    total_timesteps | 27156480 |
---------------------------------
Eval num_timesteps=27156922, episode_reward=0.05 +/- 0.99
Episode length: 29.93 +/- 1.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 27156922   |
| train/                  |            |
|    approx_kl            | 0.04464404 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.798     |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.000219   |
|    loss                 | 0.121      |
|    n_updates            | 2210       |
|    policy_gradient_loss | 0.00112    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 443      |
|    time_elapsed    | 178534   |
|    total_timesteps | 27217920 |
---------------------------------
Eval num_timesteps=27218363, episode_reward=0.11 +/- 0.97
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.114      |
| time/                   |            |
|    total_timesteps      | 27218363   |
| train/                  |            |
|    approx_kl            | 0.04693311 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.8       |
|    explained_variance   | 0.324      |
|    learning_rate        | 0.000218   |
|    loss                 | 0.0766     |
|    n_updates            | 2215       |
|    policy_gradient_loss | 0.00117    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 444      |
|    time_elapsed    | 178884   |
|    total_timesteps | 27279360 |
---------------------------------
Eval num_timesteps=27279804, episode_reward=0.12 +/- 0.97
Episode length: 29.99 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.116      |
| time/                   |            |
|    total_timesteps      | 27279804   |
| train/                  |            |
|    approx_kl            | 0.04736946 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.792     |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.000218   |
|    loss                 | -0.00931   |
|    n_updates            | 2220       |
|    policy_gradient_loss | 0.00146    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 445      |
|    time_elapsed    | 179235   |
|    total_timesteps | 27340800 |
---------------------------------
Eval num_timesteps=27341245, episode_reward=0.12 +/- 0.98
Episode length: 29.94 +/- 1.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 27341245    |
| train/                  |             |
|    approx_kl            | 0.045420848 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.789      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000218    |
|    loss                 | 0.136       |
|    n_updates            | 2225        |
|    policy_gradient_loss | 0.000348    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 446      |
|    time_elapsed    | 179586   |
|    total_timesteps | 27402240 |
---------------------------------
Eval num_timesteps=27402686, episode_reward=0.15 +/- 0.97
Episode length: 29.99 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 27402686    |
| train/                  |             |
|    approx_kl            | 0.045950238 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000218    |
|    loss                 | 0.134       |
|    n_updates            | 2230        |
|    policy_gradient_loss | 0.000551    |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.15
SELFPLAY: new best model, bumping up generation to 31
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 447      |
|    time_elapsed    | 179936   |
|    total_timesteps | 27463680 |
---------------------------------
Eval num_timesteps=27464127, episode_reward=0.05 +/- 0.98
Episode length: 29.91 +/- 1.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 27464127    |
| train/                  |             |
|    approx_kl            | 0.049753595 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000218    |
|    loss                 | 0.0231      |
|    n_updates            | 2235        |
|    policy_gradient_loss | 0.00136     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 448      |
|    time_elapsed    | 180285   |
|    total_timesteps | 27525120 |
---------------------------------
Eval num_timesteps=27525568, episode_reward=0.04 +/- 0.99
Episode length: 29.96 +/- 1.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 27525568    |
| train/                  |             |
|    approx_kl            | 0.051496617 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.833      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000217    |
|    loss                 | 0.0642      |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.000504   |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 449      |
|    time_elapsed    | 180634   |
|    total_timesteps | 27586560 |
---------------------------------
Eval num_timesteps=27587009, episode_reward=-0.05 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.046      |
| time/                   |             |
|    total_timesteps      | 27587009    |
| train/                  |             |
|    approx_kl            | 0.047847167 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.831      |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.000217    |
|    loss                 | 0.0686      |
|    n_updates            | 2245        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 450      |
|    time_elapsed    | 180984   |
|    total_timesteps | 27648000 |
---------------------------------
Eval num_timesteps=27648450, episode_reward=0.02 +/- 0.98
Episode length: 29.92 +/- 1.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 27648450   |
| train/                  |            |
|    approx_kl            | 0.04864345 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.835     |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.000217   |
|    loss                 | 0.109      |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.000422  |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 152      |
|    iterations      | 451      |
|    time_elapsed    | 181334   |
|    total_timesteps | 27709440 |
---------------------------------
Eval num_timesteps=27709891, episode_reward=0.00 +/- 0.98
Episode length: 29.89 +/- 1.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 27709891    |
| train/                  |             |
|    approx_kl            | 0.047969397 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000217    |
|    loss                 | 0.0845      |
|    n_updates            | 2255        |
|    policy_gradient_loss | 0.000957    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 452      |
|    time_elapsed    | 181683   |
|    total_timesteps | 27770880 |
---------------------------------
Eval num_timesteps=27771332, episode_reward=0.05 +/- 0.99
Episode length: 29.93 +/- 1.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 27771332    |
| train/                  |             |
|    approx_kl            | 0.047986157 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.836      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000217    |
|    loss                 | 0.342       |
|    n_updates            | 2260        |
|    policy_gradient_loss | 0.000864    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 453      |
|    time_elapsed    | 182031   |
|    total_timesteps | 27832320 |
---------------------------------
Eval num_timesteps=27832773, episode_reward=0.03 +/- 0.98
Episode length: 29.87 +/- 1.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 27832773   |
| train/                  |            |
|    approx_kl            | 0.05179663 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.842     |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.000217   |
|    loss                 | 0.1        |
|    n_updates            | 2265       |
|    policy_gradient_loss | 0.000138   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 454      |
|    time_elapsed    | 182381   |
|    total_timesteps | 27893760 |
---------------------------------
Eval num_timesteps=27894214, episode_reward=-0.06 +/- 0.99
Episode length: 29.96 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.06       |
| time/                   |             |
|    total_timesteps      | 27894214    |
| train/                  |             |
|    approx_kl            | 0.047755852 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000216    |
|    loss                 | 0.113       |
|    n_updates            | 2270        |
|    policy_gradient_loss | 0.00185     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 152      |
|    iterations      | 455      |
|    time_elapsed    | 182732   |
|    total_timesteps | 27955200 |
---------------------------------
Eval num_timesteps=27955655, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 27955655   |
| train/                  |            |
|    approx_kl            | 0.04796435 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.828     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.000216   |
|    loss                 | 0.173      |
|    n_updates            | 2275       |
|    policy_gradient_loss | 0.00149    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 456      |
|    time_elapsed    | 183083   |
|    total_timesteps | 28016640 |
---------------------------------
Eval num_timesteps=28017096, episode_reward=0.10 +/- 0.99
Episode length: 29.91 +/- 1.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.096      |
| time/                   |            |
|    total_timesteps      | 28017096   |
| train/                  |            |
|    approx_kl            | 0.04599229 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.817     |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.000216   |
|    loss                 | 0.117      |
|    n_updates            | 2280       |
|    policy_gradient_loss | 0.000916   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 457      |
|    time_elapsed    | 183433   |
|    total_timesteps | 28078080 |
---------------------------------
Eval num_timesteps=28078537, episode_reward=0.09 +/- 0.98
Episode length: 29.87 +/- 1.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 28078537    |
| train/                  |             |
|    approx_kl            | 0.046766996 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000216    |
|    loss                 | 0.0588      |
|    n_updates            | 2285        |
|    policy_gradient_loss | 0.000366    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 153      |
|    iterations      | 458      |
|    time_elapsed    | 183782   |
|    total_timesteps | 28139520 |
---------------------------------
Eval num_timesteps=28139978, episode_reward=0.05 +/- 0.99
Episode length: 29.93 +/- 1.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 28139978   |
| train/                  |            |
|    approx_kl            | 0.04574939 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.804     |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.000216   |
|    loss                 | 0.0625     |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.000914  |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 459      |
|    time_elapsed    | 184132   |
|    total_timesteps | 28200960 |
---------------------------------
Eval num_timesteps=28201419, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 28201419   |
| train/                  |            |
|    approx_kl            | 0.04567964 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.805     |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.000215   |
|    loss                 | 0.0533     |
|    n_updates            | 2295       |
|    policy_gradient_loss | 0.000523   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 153      |
|    iterations      | 460      |
|    time_elapsed    | 184481   |
|    total_timesteps | 28262400 |
---------------------------------
Eval num_timesteps=28262860, episode_reward=0.13 +/- 0.98
Episode length: 29.91 +/- 1.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 28262860   |
| train/                  |            |
|    approx_kl            | 0.04685848 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.801     |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.000215   |
|    loss                 | 0.147      |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.00109   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 461      |
|    time_elapsed    | 184831   |
|    total_timesteps | 28323840 |
---------------------------------
Eval num_timesteps=28324301, episode_reward=0.08 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 28324301    |
| train/                  |             |
|    approx_kl            | 0.045571066 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.79       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000215    |
|    loss                 | 0.06        |
|    n_updates            | 2305        |
|    policy_gradient_loss | -0.00056    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 462      |
|    time_elapsed    | 185181   |
|    total_timesteps | 28385280 |
---------------------------------
Eval num_timesteps=28385742, episode_reward=0.15 +/- 0.97
Episode length: 29.95 +/- 1.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.148       |
| time/                   |             |
|    total_timesteps      | 28385742    |
| train/                  |             |
|    approx_kl            | 0.043984286 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.789      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000215    |
|    loss                 | 0.167       |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.000363   |
|    value_loss           | 0.24        |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 32
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 463      |
|    time_elapsed    | 185529   |
|    total_timesteps | 28446720 |
---------------------------------
Eval num_timesteps=28447183, episode_reward=0.09 +/- 0.98
Episode length: 29.94 +/- 1.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.092      |
| time/                   |            |
|    total_timesteps      | 28447183   |
| train/                  |            |
|    approx_kl            | 0.04903647 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.85      |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000215   |
|    loss                 | 0.0551     |
|    n_updates            | 2315       |
|    policy_gradient_loss | 0.000936   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 464      |
|    time_elapsed    | 185879   |
|    total_timesteps | 28508160 |
---------------------------------
Eval num_timesteps=28508624, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 28508624    |
| train/                  |             |
|    approx_kl            | 0.044867735 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000214    |
|    loss                 | 0.198       |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.000905   |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 465      |
|    time_elapsed    | 186228   |
|    total_timesteps | 28569600 |
---------------------------------
Eval num_timesteps=28570065, episode_reward=0.01 +/- 0.98
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.008      |
| time/                   |            |
|    total_timesteps      | 28570065   |
| train/                  |            |
|    approx_kl            | 0.04682625 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.84      |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.000214   |
|    loss                 | 0.113      |
|    n_updates            | 2325       |
|    policy_gradient_loss | 0.000795   |
|    value_loss           | 0.247      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 466      |
|    time_elapsed    | 186580   |
|    total_timesteps | 28631040 |
---------------------------------
Eval num_timesteps=28631506, episode_reward=0.07 +/- 0.98
Episode length: 29.96 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 28631506   |
| train/                  |            |
|    approx_kl            | 0.04684552 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.84      |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.000214   |
|    loss                 | 0.184      |
|    n_updates            | 2330       |
|    policy_gradient_loss | -0.000694  |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 467      |
|    time_elapsed    | 186931   |
|    total_timesteps | 28692480 |
---------------------------------
Eval num_timesteps=28692947, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 28692947    |
| train/                  |             |
|    approx_kl            | 0.046440784 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.838      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000214    |
|    loss                 | 0.0971      |
|    n_updates            | 2335        |
|    policy_gradient_loss | -0.000203   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 153      |
|    iterations      | 468      |
|    time_elapsed    | 187281   |
|    total_timesteps | 28753920 |
---------------------------------
Eval num_timesteps=28754388, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 28754388   |
| train/                  |            |
|    approx_kl            | 0.04582224 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.842     |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.000214   |
|    loss                 | 0.109      |
|    n_updates            | 2340       |
|    policy_gradient_loss | 0.000293   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 469      |
|    time_elapsed    | 187631   |
|    total_timesteps | 28815360 |
---------------------------------
Eval num_timesteps=28815829, episode_reward=-0.01 +/- 0.98
Episode length: 29.96 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 28815829    |
| train/                  |             |
|    approx_kl            | 0.047014076 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000214    |
|    loss                 | 0.0516      |
|    n_updates            | 2345        |
|    policy_gradient_loss | 0.00136     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 470      |
|    time_elapsed    | 187981   |
|    total_timesteps | 28876800 |
---------------------------------
Eval num_timesteps=28877270, episode_reward=-0.05 +/- 0.99
Episode length: 29.88 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.054      |
| time/                   |             |
|    total_timesteps      | 28877270    |
| train/                  |             |
|    approx_kl            | 0.045056183 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000213    |
|    loss                 | 0.0538      |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.000482   |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 471      |
|    time_elapsed    | 188330   |
|    total_timesteps | 28938240 |
---------------------------------
Eval num_timesteps=28938711, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.136      |
| time/                   |            |
|    total_timesteps      | 28938711   |
| train/                  |            |
|    approx_kl            | 0.04581256 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.813     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000213   |
|    loss                 | 0.0764     |
|    n_updates            | 2355       |
|    policy_gradient_loss | -0.000705  |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 472      |
|    time_elapsed    | 188681   |
|    total_timesteps | 28999680 |
---------------------------------
Eval num_timesteps=29000152, episode_reward=0.03 +/- 0.98
Episode length: 29.95 +/- 0.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 29000152    |
| train/                  |             |
|    approx_kl            | 0.046515577 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.806      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000213    |
|    loss                 | 0.0749      |
|    n_updates            | 2360        |
|    policy_gradient_loss | 0.00229     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 473      |
|    time_elapsed    | 189030   |
|    total_timesteps | 29061120 |
---------------------------------
Eval num_timesteps=29061593, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 29061593    |
| train/                  |             |
|    approx_kl            | 0.043916065 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000213    |
|    loss                 | 0.102       |
|    n_updates            | 2365        |
|    policy_gradient_loss | -0.000805   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 474      |
|    time_elapsed    | 189378   |
|    total_timesteps | 29122560 |
---------------------------------
Eval num_timesteps=29123034, episode_reward=-0.01 +/- 0.99
Episode length: 29.93 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.006     |
| time/                   |            |
|    total_timesteps      | 29123034   |
| train/                  |            |
|    approx_kl            | 0.04655866 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.802     |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.000213   |
|    loss                 | 0.0758     |
|    n_updates            | 2370       |
|    policy_gradient_loss | 0.000192   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 475      |
|    time_elapsed    | 189728   |
|    total_timesteps | 29184000 |
---------------------------------
Eval num_timesteps=29184475, episode_reward=0.08 +/- 0.98
Episode length: 29.93 +/- 1.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.084      |
| time/                   |            |
|    total_timesteps      | 29184475   |
| train/                  |            |
|    approx_kl            | 0.04521268 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.804     |
|    explained_variance   | 0.328      |
|    learning_rate        | 0.000212   |
|    loss                 | 0.0857     |
|    n_updates            | 2375       |
|    policy_gradient_loss | -0.00107   |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 476      |
|    time_elapsed    | 190079   |
|    total_timesteps | 29245440 |
---------------------------------
Eval num_timesteps=29245916, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.078      |
| time/                   |            |
|    total_timesteps      | 29245916   |
| train/                  |            |
|    approx_kl            | 0.04470372 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.798     |
|    explained_variance   | 0.32       |
|    learning_rate        | 0.000212   |
|    loss                 | 0.0728     |
|    n_updates            | 2380       |
|    policy_gradient_loss | -4.12e-05  |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 477      |
|    time_elapsed    | 190431   |
|    total_timesteps | 29306880 |
---------------------------------
Eval num_timesteps=29307357, episode_reward=-0.01 +/- 0.99
Episode length: 29.92 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 29307357    |
| train/                  |             |
|    approx_kl            | 0.046775322 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.801      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000212    |
|    loss                 | 0.14        |
|    n_updates            | 2385        |
|    policy_gradient_loss | 0.000212    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 478      |
|    time_elapsed    | 190782   |
|    total_timesteps | 29368320 |
---------------------------------
Eval num_timesteps=29368798, episode_reward=0.06 +/- 0.99
Episode length: 29.95 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.058     |
| time/                   |           |
|    total_timesteps      | 29368798  |
| train/                  |           |
|    approx_kl            | 0.0448262 |
|    clip_fraction        | 0.258     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.789    |
|    explained_variance   | 0.344     |
|    learning_rate        | 0.000212  |
|    loss                 | 0.0338    |
|    n_updates            | 2390      |
|    policy_gradient_loss | -0.000283 |
|    value_loss           | 0.236     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 153      |
|    iterations      | 479      |
|    time_elapsed    | 191132   |
|    total_timesteps | 29429760 |
---------------------------------
Eval num_timesteps=29430239, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 29430239    |
| train/                  |             |
|    approx_kl            | 0.043730605 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.802      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000212    |
|    loss                 | 0.0883      |
|    n_updates            | 2395        |
|    policy_gradient_loss | 3.07e-05    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 480      |
|    time_elapsed    | 191481   |
|    total_timesteps | 29491200 |
---------------------------------
Eval num_timesteps=29491680, episode_reward=-0.01 +/- 0.99
Episode length: 29.91 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 29491680    |
| train/                  |             |
|    approx_kl            | 0.047321152 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.802      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000212    |
|    loss                 | 0.0965      |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.000102   |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 481      |
|    time_elapsed    | 191830   |
|    total_timesteps | 29552640 |
---------------------------------
Eval num_timesteps=29553121, episode_reward=0.07 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 29553121    |
| train/                  |             |
|    approx_kl            | 0.045032494 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000211    |
|    loss                 | 0.0257      |
|    n_updates            | 2405        |
|    policy_gradient_loss | 0.000131    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 482      |
|    time_elapsed    | 192180   |
|    total_timesteps | 29614080 |
---------------------------------
Eval num_timesteps=29614562, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.042     |
| time/                   |           |
|    total_timesteps      | 29614562  |
| train/                  |           |
|    approx_kl            | 0.0461193 |
|    clip_fraction        | 0.259     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.796    |
|    explained_variance   | 0.323     |
|    learning_rate        | 0.000211  |
|    loss                 | 0.149     |
|    n_updates            | 2410      |
|    policy_gradient_loss | 0.000232  |
|    value_loss           | 0.241     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 483      |
|    time_elapsed    | 192530   |
|    total_timesteps | 29675520 |
---------------------------------
Eval num_timesteps=29676003, episode_reward=0.07 +/- 0.98
Episode length: 29.87 +/- 1.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 29676003    |
| train/                  |             |
|    approx_kl            | 0.042374115 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000211    |
|    loss                 | 0.164       |
|    n_updates            | 2415        |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 484      |
|    time_elapsed    | 192878   |
|    total_timesteps | 29736960 |
---------------------------------
Eval num_timesteps=29737444, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 29737444   |
| train/                  |            |
|    approx_kl            | 0.04475059 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.787     |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.000211   |
|    loss                 | 0.0968     |
|    n_updates            | 2420       |
|    policy_gradient_loss | 0.000611   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 485      |
|    time_elapsed    | 193227   |
|    total_timesteps | 29798400 |
---------------------------------
Eval num_timesteps=29798885, episode_reward=0.03 +/- 0.98
Episode length: 29.91 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 29798885   |
| train/                  |            |
|    approx_kl            | 0.04580429 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.778     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000211   |
|    loss                 | 0.105      |
|    n_updates            | 2425       |
|    policy_gradient_loss | -0.000352  |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 486      |
|    time_elapsed    | 193577   |
|    total_timesteps | 29859840 |
---------------------------------
Eval num_timesteps=29860326, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 29860326    |
| train/                  |             |
|    approx_kl            | 0.040328186 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.00021     |
|    loss                 | 0.113       |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 487      |
|    time_elapsed    | 193928   |
|    total_timesteps | 29921280 |
---------------------------------
Eval num_timesteps=29921767, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 29921767    |
| train/                  |             |
|    approx_kl            | 0.041962314 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.00021     |
|    loss                 | 0.134       |
|    n_updates            | 2435        |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 488      |
|    time_elapsed    | 194280   |
|    total_timesteps | 29982720 |
---------------------------------
Eval num_timesteps=29983208, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 29983208    |
| train/                  |             |
|    approx_kl            | 0.043479603 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.00021     |
|    loss                 | 0.039       |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.000901   |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 489      |
|    time_elapsed    | 194631   |
|    total_timesteps | 30044160 |
---------------------------------
Eval num_timesteps=30044649, episode_reward=0.10 +/- 0.98
Episode length: 29.95 +/- 0.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.096      |
| time/                   |            |
|    total_timesteps      | 30044649   |
| train/                  |            |
|    approx_kl            | 0.04456506 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.777     |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.00021    |
|    loss                 | 0.085      |
|    n_updates            | 2445       |
|    policy_gradient_loss | -0.0008    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 490      |
|    time_elapsed    | 194980   |
|    total_timesteps | 30105600 |
---------------------------------
Eval num_timesteps=30106090, episode_reward=0.11 +/- 0.98
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 30106090    |
| train/                  |             |
|    approx_kl            | 0.045433767 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.00021     |
|    loss                 | 0.172       |
|    n_updates            | 2450        |
|    policy_gradient_loss | 7e-05       |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 154      |
|    iterations      | 491      |
|    time_elapsed    | 195330   |
|    total_timesteps | 30167040 |
---------------------------------
Eval num_timesteps=30167531, episode_reward=0.10 +/- 0.98
Episode length: 29.95 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.096      |
| time/                   |            |
|    total_timesteps      | 30167531   |
| train/                  |            |
|    approx_kl            | 0.04311587 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000209   |
|    loss                 | 0.155      |
|    n_updates            | 2455       |
|    policy_gradient_loss | -0.000951  |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 492      |
|    time_elapsed    | 195680   |
|    total_timesteps | 30228480 |
---------------------------------
Eval num_timesteps=30228972, episode_reward=0.14 +/- 0.98
Episode length: 29.90 +/- 1.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 30228972    |
| train/                  |             |
|    approx_kl            | 0.041390516 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000209    |
|    loss                 | 0.0745      |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 493      |
|    time_elapsed    | 196030   |
|    total_timesteps | 30289920 |
---------------------------------
Eval num_timesteps=30290413, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 30290413    |
| train/                  |             |
|    approx_kl            | 0.044410467 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000209    |
|    loss                 | 0.111       |
|    n_updates            | 2465        |
|    policy_gradient_loss | 0.000209    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 494      |
|    time_elapsed    | 196379   |
|    total_timesteps | 30351360 |
---------------------------------
Eval num_timesteps=30351854, episode_reward=0.03 +/- 0.99
Episode length: 29.92 +/- 0.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 30351854    |
| train/                  |             |
|    approx_kl            | 0.042808995 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000209    |
|    loss                 | 0.0984      |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00017    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 495      |
|    time_elapsed    | 196728   |
|    total_timesteps | 30412800 |
---------------------------------
Eval num_timesteps=30413295, episode_reward=0.09 +/- 0.99
Episode length: 29.92 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 30413295    |
| train/                  |             |
|    approx_kl            | 0.043188795 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000209    |
|    loss                 | 0.113       |
|    n_updates            | 2475        |
|    policy_gradient_loss | -0.00178    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 496      |
|    time_elapsed    | 197077   |
|    total_timesteps | 30474240 |
---------------------------------
Eval num_timesteps=30474736, episode_reward=0.08 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 30474736    |
| train/                  |             |
|    approx_kl            | 0.047718175 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000209    |
|    loss                 | 0.162       |
|    n_updates            | 2480        |
|    policy_gradient_loss | 0.000653    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 497      |
|    time_elapsed    | 197427   |
|    total_timesteps | 30535680 |
---------------------------------
Eval num_timesteps=30536177, episode_reward=0.04 +/- 0.99
Episode length: 29.91 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 30536177    |
| train/                  |             |
|    approx_kl            | 0.042960875 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000208    |
|    loss                 | 0.0958      |
|    n_updates            | 2485        |
|    policy_gradient_loss | -0.000795   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 498      |
|    time_elapsed    | 197779   |
|    total_timesteps | 30597120 |
---------------------------------
Eval num_timesteps=30597618, episode_reward=0.11 +/- 0.99
Episode length: 29.98 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total_timesteps      | 30597618   |
| train/                  |            |
|    approx_kl            | 0.04276045 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.000208   |
|    loss                 | 0.0446     |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.000182  |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 154      |
|    iterations      | 499      |
|    time_elapsed    | 198129   |
|    total_timesteps | 30658560 |
---------------------------------
Eval num_timesteps=30659059, episode_reward=0.17 +/- 0.97
Episode length: 29.93 +/- 1.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.17        |
| time/                   |             |
|    total_timesteps      | 30659059    |
| train/                  |             |
|    approx_kl            | 0.047134172 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000208    |
|    loss                 | 0.095       |
|    n_updates            | 2495        |
|    policy_gradient_loss | -0.000304   |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.17
SELFPLAY: new best model, bumping up generation to 33
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 500      |
|    time_elapsed    | 198478   |
|    total_timesteps | 30720000 |
---------------------------------
Eval num_timesteps=30720500, episode_reward=-0.00 +/- 0.99
Episode length: 29.94 +/- 0.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 30720500    |
| train/                  |             |
|    approx_kl            | 0.047801126 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.377       |
|    learning_rate        | 0.000208    |
|    loss                 | 0.0995      |
|    n_updates            | 2500        |
|    policy_gradient_loss | 0.000901    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 501      |
|    time_elapsed    | 198828   |
|    total_timesteps | 30781440 |
---------------------------------
Eval num_timesteps=30781941, episode_reward=0.00 +/- 0.99
Episode length: 29.91 +/- 0.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 30781941    |
| train/                  |             |
|    approx_kl            | 0.046943773 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000208    |
|    loss                 | 0.0487      |
|    n_updates            | 2505        |
|    policy_gradient_loss | 0.000492    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 502      |
|    time_elapsed    | 199177   |
|    total_timesteps | 30842880 |
---------------------------------
Eval num_timesteps=30843382, episode_reward=0.12 +/- 0.98
Episode length: 29.97 +/- 1.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.116     |
| time/                   |           |
|    total_timesteps      | 30843382  |
| train/                  |           |
|    approx_kl            | 0.0443901 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.831    |
|    explained_variance   | 0.364     |
|    learning_rate        | 0.000207  |
|    loss                 | 0.102     |
|    n_updates            | 2510      |
|    policy_gradient_loss | -0.000133 |
|    value_loss           | 0.235     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 503      |
|    time_elapsed    | 199527   |
|    total_timesteps | 30904320 |
---------------------------------
Eval num_timesteps=30904823, episode_reward=0.07 +/- 0.98
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 30904823    |
| train/                  |             |
|    approx_kl            | 0.047371805 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.000207    |
|    loss                 | 0.143       |
|    n_updates            | 2515        |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 504      |
|    time_elapsed    | 199876   |
|    total_timesteps | 30965760 |
---------------------------------
Eval num_timesteps=30966264, episode_reward=0.16 +/- 0.97
Episode length: 29.91 +/- 1.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.156       |
| time/                   |             |
|    total_timesteps      | 30966264    |
| train/                  |             |
|    approx_kl            | 0.045325134 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000207    |
|    loss                 | 0.0954      |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.000197   |
|    value_loss           | 0.24        |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.156
SELFPLAY: new best model, bumping up generation to 34
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 505      |
|    time_elapsed    | 200225   |
|    total_timesteps | 31027200 |
---------------------------------
Eval num_timesteps=31027705, episode_reward=-0.10 +/- 0.98
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.098     |
| time/                   |            |
|    total_timesteps      | 31027705   |
| train/                  |            |
|    approx_kl            | 0.04600245 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.838     |
|    explained_variance   | 0.355      |
|    learning_rate        | 0.000207   |
|    loss                 | 0.0513     |
|    n_updates            | 2525       |
|    policy_gradient_loss | -0.000616  |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 506      |
|    time_elapsed    | 200574   |
|    total_timesteps | 31088640 |
---------------------------------
Eval num_timesteps=31089146, episode_reward=0.01 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 31089146    |
| train/                  |             |
|    approx_kl            | 0.046684887 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.836      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000207    |
|    loss                 | 0.0495      |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.000613   |
|    value_loss           | 0.247       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 155      |
|    iterations      | 507      |
|    time_elapsed    | 200923   |
|    total_timesteps | 31150080 |
---------------------------------
Eval num_timesteps=31150587, episode_reward=0.07 +/- 0.97
Episode length: 29.91 +/- 1.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 31150587    |
| train/                  |             |
|    approx_kl            | 0.044644635 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.836      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000207    |
|    loss                 | 0.0665      |
|    n_updates            | 2535        |
|    policy_gradient_loss | 0.000647    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 155      |
|    iterations      | 508      |
|    time_elapsed    | 201273   |
|    total_timesteps | 31211520 |
---------------------------------
Eval num_timesteps=31212028, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 31212028    |
| train/                  |             |
|    approx_kl            | 0.044754453 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000206    |
|    loss                 | 0.0858      |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 509      |
|    time_elapsed    | 201625   |
|    total_timesteps | 31272960 |
---------------------------------
Eval num_timesteps=31273469, episode_reward=0.04 +/- 0.98
Episode length: 29.95 +/- 1.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 31273469    |
| train/                  |             |
|    approx_kl            | 0.047478843 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.833      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000206    |
|    loss                 | 0.0592      |
|    n_updates            | 2545        |
|    policy_gradient_loss | -0.000792   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 510      |
|    time_elapsed    | 201975   |
|    total_timesteps | 31334400 |
---------------------------------
Eval num_timesteps=31334910, episode_reward=0.02 +/- 0.99
Episode length: 29.93 +/- 1.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.016     |
| time/                   |           |
|    total_timesteps      | 31334910  |
| train/                  |           |
|    approx_kl            | 0.0460704 |
|    clip_fraction        | 0.265     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.825    |
|    explained_variance   | 0.349     |
|    learning_rate        | 0.000206  |
|    loss                 | 0.0986    |
|    n_updates            | 2550      |
|    policy_gradient_loss | -0.000496 |
|    value_loss           | 0.237     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 511      |
|    time_elapsed    | 202324   |
|    total_timesteps | 31395840 |
---------------------------------
Eval num_timesteps=31396351, episode_reward=0.06 +/- 0.99
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 31396351    |
| train/                  |             |
|    approx_kl            | 0.045362122 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.000206    |
|    loss                 | 0.0971      |
|    n_updates            | 2555        |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 512      |
|    time_elapsed    | 202674   |
|    total_timesteps | 31457280 |
---------------------------------
Eval num_timesteps=31457792, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 31457792    |
| train/                  |             |
|    approx_kl            | 0.044656817 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000206    |
|    loss                 | 0.0467      |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.000803   |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 155      |
|    iterations      | 513      |
|    time_elapsed    | 203024   |
|    total_timesteps | 31518720 |
---------------------------------
Eval num_timesteps=31519233, episode_reward=0.03 +/- 0.99
Episode length: 29.89 +/- 1.46
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 29.9     |
|    mean_reward          | 0.026    |
| time/                   |          |
|    total_timesteps      | 31519233 |
| train/                  |          |
|    approx_kl            | 0.045436 |
|    clip_fraction        | 0.26     |
|    clip_range           | 0.17     |
|    entropy_loss         | -0.817   |
|    explained_variance   | 0.338    |
|    learning_rate        | 0.000205 |
|    loss                 | 0.129    |
|    n_updates            | 2565     |
|    policy_gradient_loss | -0.00165 |
|    value_loss           | 0.242    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 514      |
|    time_elapsed    | 203374   |
|    total_timesteps | 31580160 |
---------------------------------
Eval num_timesteps=31580674, episode_reward=0.04 +/- 0.98
Episode length: 29.86 +/- 1.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 31580674   |
| train/                  |            |
|    approx_kl            | 0.04803262 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.817     |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.000205   |
|    loss                 | 0.0348     |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.000162  |
|    value_loss           | 0.232      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 155      |
|    iterations      | 515      |
|    time_elapsed    | 203723   |
|    total_timesteps | 31641600 |
---------------------------------
Eval num_timesteps=31642115, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.022      |
| time/                   |            |
|    total_timesteps      | 31642115   |
| train/                  |            |
|    approx_kl            | 0.04305298 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.806     |
|    explained_variance   | 0.355      |
|    learning_rate        | 0.000205   |
|    loss                 | 0.128      |
|    n_updates            | 2575       |
|    policy_gradient_loss | -0.00133   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 516      |
|    time_elapsed    | 204072   |
|    total_timesteps | 31703040 |
---------------------------------
Eval num_timesteps=31703556, episode_reward=-0.05 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.048     |
| time/                   |            |
|    total_timesteps      | 31703556   |
| train/                  |            |
|    approx_kl            | 0.04525421 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000205   |
|    loss                 | 0.0547     |
|    n_updates            | 2580       |
|    policy_gradient_loss | -5.46e-05  |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 155      |
|    iterations      | 517      |
|    time_elapsed    | 204421   |
|    total_timesteps | 31764480 |
---------------------------------
Eval num_timesteps=31764997, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 31764997    |
| train/                  |             |
|    approx_kl            | 0.043954257 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.804      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000205    |
|    loss                 | 0.0516      |
|    n_updates            | 2585        |
|    policy_gradient_loss | -0.000986   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 518      |
|    time_elapsed    | 204770   |
|    total_timesteps | 31825920 |
---------------------------------
Eval num_timesteps=31826438, episode_reward=0.04 +/- 0.98
Episode length: 29.91 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 31826438    |
| train/                  |             |
|    approx_kl            | 0.044130262 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.804      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000205    |
|    loss                 | 0.0548      |
|    n_updates            | 2590        |
|    policy_gradient_loss | -3.97e-05   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 519      |
|    time_elapsed    | 205121   |
|    total_timesteps | 31887360 |
---------------------------------
Eval num_timesteps=31887879, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 31887879    |
| train/                  |             |
|    approx_kl            | 0.043515213 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.797      |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.000204    |
|    loss                 | 0.0838      |
|    n_updates            | 2595        |
|    policy_gradient_loss | -0.00126    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 520      |
|    time_elapsed    | 205472   |
|    total_timesteps | 31948800 |
---------------------------------
Eval num_timesteps=31949320, episode_reward=0.02 +/- 0.99
Episode length: 29.87 +/- 1.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 31949320    |
| train/                  |             |
|    approx_kl            | 0.045801688 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.791      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000204    |
|    loss                 | 0.121       |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.00072    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 155      |
|    iterations      | 521      |
|    time_elapsed    | 205822   |
|    total_timesteps | 32010240 |
---------------------------------
Eval num_timesteps=32010761, episode_reward=0.06 +/- 0.98
Episode length: 29.94 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 32010761    |
| train/                  |             |
|    approx_kl            | 0.043760795 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000204    |
|    loss                 | 0.057       |
|    n_updates            | 2605        |
|    policy_gradient_loss | -0.000528   |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 155      |
|    iterations      | 522      |
|    time_elapsed    | 206172   |
|    total_timesteps | 32071680 |
---------------------------------
Eval num_timesteps=32072202, episode_reward=0.11 +/- 0.98
Episode length: 29.91 +/- 1.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.106      |
| time/                   |            |
|    total_timesteps      | 32072202   |
| train/                  |            |
|    approx_kl            | 0.04168458 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.782     |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.000204   |
|    loss                 | 0.0309     |
|    n_updates            | 2610       |
|    policy_gradient_loss | -0.00181   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 523      |
|    time_elapsed    | 206522   |
|    total_timesteps | 32133120 |
---------------------------------
Eval num_timesteps=32133643, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 32133643    |
| train/                  |             |
|    approx_kl            | 0.042448703 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000204    |
|    loss                 | 0.145       |
|    n_updates            | 2615        |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 524      |
|    time_elapsed    | 206871   |
|    total_timesteps | 32194560 |
---------------------------------
Eval num_timesteps=32195084, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 32195084   |
| train/                  |            |
|    approx_kl            | 0.04562993 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.774     |
|    explained_variance   | 0.355      |
|    learning_rate        | 0.000203   |
|    loss                 | 0.118      |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.000711  |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 525      |
|    time_elapsed    | 207221   |
|    total_timesteps | 32256000 |
---------------------------------
Eval num_timesteps=32256525, episode_reward=0.05 +/- 0.99
Episode length: 29.79 +/- 2.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.8       |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 32256525   |
| train/                  |            |
|    approx_kl            | 0.04346119 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.356      |
|    learning_rate        | 0.000203   |
|    loss                 | 0.0479     |
|    n_updates            | 2625       |
|    policy_gradient_loss | 4.31e-06   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 155      |
|    iterations      | 526      |
|    time_elapsed    | 207570   |
|    total_timesteps | 32317440 |
---------------------------------
Eval num_timesteps=32317966, episode_reward=0.10 +/- 0.99
Episode length: 29.91 +/- 1.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.096      |
| time/                   |            |
|    total_timesteps      | 32317966   |
| train/                  |            |
|    approx_kl            | 0.04241961 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.768     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000203   |
|    loss                 | 0.152      |
|    n_updates            | 2630       |
|    policy_gradient_loss | -0.000275  |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 155      |
|    iterations      | 527      |
|    time_elapsed    | 207919   |
|    total_timesteps | 32378880 |
---------------------------------
Eval num_timesteps=32379407, episode_reward=0.05 +/- 0.99
Episode length: 29.84 +/- 1.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 32379407    |
| train/                  |             |
|    approx_kl            | 0.043237325 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000203    |
|    loss                 | 0.0903      |
|    n_updates            | 2635        |
|    policy_gradient_loss | -4.54e-05   |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 528      |
|    time_elapsed    | 208268   |
|    total_timesteps | 32440320 |
---------------------------------
Eval num_timesteps=32440848, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.12        |
| time/                   |             |
|    total_timesteps      | 32440848    |
| train/                  |             |
|    approx_kl            | 0.043492433 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000203    |
|    loss                 | 0.0414      |
|    n_updates            | 2640        |
|    policy_gradient_loss | -1.38e-05   |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 529      |
|    time_elapsed    | 208618   |
|    total_timesteps | 32501760 |
---------------------------------
Eval num_timesteps=32502289, episode_reward=0.04 +/- 0.98
Episode length: 29.94 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 32502289    |
| train/                  |             |
|    approx_kl            | 0.043948326 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000202    |
|    loss                 | 0.0646      |
|    n_updates            | 2645        |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 530      |
|    time_elapsed    | 208970   |
|    total_timesteps | 32563200 |
---------------------------------
Eval num_timesteps=32563730, episode_reward=0.01 +/- 0.99
Episode length: 29.90 +/- 0.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.008      |
| time/                   |            |
|    total_timesteps      | 32563730   |
| train/                  |            |
|    approx_kl            | 0.04087386 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.746     |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.000202   |
|    loss                 | 0.105      |
|    n_updates            | 2650       |
|    policy_gradient_loss | -0.000212  |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 155      |
|    iterations      | 531      |
|    time_elapsed    | 209320   |
|    total_timesteps | 32624640 |
---------------------------------
Eval num_timesteps=32625171, episode_reward=0.05 +/- 0.98
Episode length: 29.95 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 32625171    |
| train/                  |             |
|    approx_kl            | 0.042854615 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.000202    |
|    loss                 | 0.0833      |
|    n_updates            | 2655        |
|    policy_gradient_loss | -0.000816   |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 532      |
|    time_elapsed    | 209670   |
|    total_timesteps | 32686080 |
---------------------------------
Eval num_timesteps=32686612, episode_reward=0.11 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 32686612   |
| train/                  |            |
|    approx_kl            | 0.04069033 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.356      |
|    learning_rate        | 0.000202   |
|    loss                 | 0.0578     |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.000901  |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 533      |
|    time_elapsed    | 210020   |
|    total_timesteps | 32747520 |
---------------------------------
Eval num_timesteps=32748053, episode_reward=0.08 +/- 0.98
Episode length: 29.96 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 32748053    |
| train/                  |             |
|    approx_kl            | 0.042656478 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000202    |
|    loss                 | 0.0495      |
|    n_updates            | 2665        |
|    policy_gradient_loss | -0.000466   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 534      |
|    time_elapsed    | 210370   |
|    total_timesteps | 32808960 |
---------------------------------
Eval num_timesteps=32809494, episode_reward=0.11 +/- 0.98
Episode length: 29.97 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.108     |
| time/                   |           |
|    total_timesteps      | 32809494  |
| train/                  |           |
|    approx_kl            | 0.0416365 |
|    clip_fraction        | 0.242     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.748    |
|    explained_variance   | 0.347     |
|    learning_rate        | 0.000202  |
|    loss                 | 0.0688    |
|    n_updates            | 2670      |
|    policy_gradient_loss | -0.000177 |
|    value_loss           | 0.236     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 535      |
|    time_elapsed    | 210720   |
|    total_timesteps | 32870400 |
---------------------------------
Eval num_timesteps=32870935, episode_reward=0.11 +/- 0.98
Episode length: 29.91 +/- 1.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total_timesteps      | 32870935   |
| train/                  |            |
|    approx_kl            | 0.04500098 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.754     |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.000201   |
|    loss                 | 0.102      |
|    n_updates            | 2675       |
|    policy_gradient_loss | -0.0005    |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 536      |
|    time_elapsed    | 211069   |
|    total_timesteps | 32931840 |
---------------------------------
Eval num_timesteps=32932376, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.102      |
| time/                   |            |
|    total_timesteps      | 32932376   |
| train/                  |            |
|    approx_kl            | 0.04271953 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.371      |
|    learning_rate        | 0.000201   |
|    loss                 | 0.0786     |
|    n_updates            | 2680       |
|    policy_gradient_loss | 0.000215   |
|    value_loss           | 0.232      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 537      |
|    time_elapsed    | 211418   |
|    total_timesteps | 32993280 |
---------------------------------
Eval num_timesteps=32993817, episode_reward=0.06 +/- 0.99
Episode length: 29.90 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 32993817    |
| train/                  |             |
|    approx_kl            | 0.043160178 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000201    |
|    loss                 | 0.0884      |
|    n_updates            | 2685        |
|    policy_gradient_loss | -0.000618   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 538      |
|    time_elapsed    | 211767   |
|    total_timesteps | 33054720 |
---------------------------------
Eval num_timesteps=33055258, episode_reward=0.07 +/- 0.99
Episode length: 29.92 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 33055258    |
| train/                  |             |
|    approx_kl            | 0.042563796 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000201    |
|    loss                 | 0.0722      |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 539      |
|    time_elapsed    | 212116   |
|    total_timesteps | 33116160 |
---------------------------------
Eval num_timesteps=33116699, episode_reward=0.15 +/- 0.97
Episode length: 29.90 +/- 1.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.154       |
| time/                   |             |
|    total_timesteps      | 33116699    |
| train/                  |             |
|    approx_kl            | 0.043643888 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000201    |
|    loss                 | 0.0674      |
|    n_updates            | 2695        |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.154
SELFPLAY: new best model, bumping up generation to 35
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 540      |
|    time_elapsed    | 212467   |
|    total_timesteps | 33177600 |
---------------------------------
Eval num_timesteps=33178140, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 33178140    |
| train/                  |             |
|    approx_kl            | 0.041791324 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.0929      |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 541      |
|    time_elapsed    | 212818   |
|    total_timesteps | 33239040 |
---------------------------------
Eval num_timesteps=33239581, episode_reward=-0.04 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.042      |
| time/                   |             |
|    total_timesteps      | 33239581    |
| train/                  |             |
|    approx_kl            | 0.044753153 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.755      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.12        |
|    n_updates            | 2705        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 542      |
|    time_elapsed    | 213168   |
|    total_timesteps | 33300480 |
---------------------------------
Eval num_timesteps=33301022, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 33301022    |
| train/                  |             |
|    approx_kl            | 0.043485105 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.0002      |
|    loss                 | 0.111       |
|    n_updates            | 2710        |
|    policy_gradient_loss | 0.000332    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 543      |
|    time_elapsed    | 213518   |
|    total_timesteps | 33361920 |
---------------------------------
Eval num_timesteps=33362463, episode_reward=0.00 +/- 0.99
Episode length: 29.95 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 33362463    |
| train/                  |             |
|    approx_kl            | 0.042282637 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.126       |
|    n_updates            | 2715        |
|    policy_gradient_loss | -0.000533   |
|    value_loss           | 0.247       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 544      |
|    time_elapsed    | 213867   |
|    total_timesteps | 33423360 |
---------------------------------
Eval num_timesteps=33423904, episode_reward=0.04 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 33423904    |
| train/                  |             |
|    approx_kl            | 0.043100752 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.0951      |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.000791   |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 545      |
|    time_elapsed    | 214217   |
|    total_timesteps | 33484800 |
---------------------------------
Eval num_timesteps=33485345, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 33485345    |
| train/                  |             |
|    approx_kl            | 0.042600345 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.755      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.137       |
|    n_updates            | 2725        |
|    policy_gradient_loss | -0.000105   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 546      |
|    time_elapsed    | 214567   |
|    total_timesteps | 33546240 |
---------------------------------
Eval num_timesteps=33546786, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 33546786   |
| train/                  |            |
|    approx_kl            | 0.04270184 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000199   |
|    loss                 | 0.171      |
|    n_updates            | 2730       |
|    policy_gradient_loss | -0.0011    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 547      |
|    time_elapsed    | 214916   |
|    total_timesteps | 33607680 |
---------------------------------
Eval num_timesteps=33608227, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 33608227    |
| train/                  |             |
|    approx_kl            | 0.044051085 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000199    |
|    loss                 | 0.0151      |
|    n_updates            | 2735        |
|    policy_gradient_loss | -0.000975   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 548      |
|    time_elapsed    | 215265   |
|    total_timesteps | 33669120 |
---------------------------------
Eval num_timesteps=33669668, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 33669668    |
| train/                  |             |
|    approx_kl            | 0.043018065 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000199    |
|    loss                 | 0.108       |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.000188   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 549      |
|    time_elapsed    | 215614   |
|    total_timesteps | 33730560 |
---------------------------------
Eval num_timesteps=33731109, episode_reward=-0.07 +/- 0.98
Episode length: 29.96 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.066      |
| time/                   |             |
|    total_timesteps      | 33731109    |
| train/                  |             |
|    approx_kl            | 0.041635204 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.000199    |
|    loss                 | 0.0579      |
|    n_updates            | 2745        |
|    policy_gradient_loss | -0.000517   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 550      |
|    time_elapsed    | 215965   |
|    total_timesteps | 33792000 |
---------------------------------
Eval num_timesteps=33792550, episode_reward=-0.00 +/- 0.98
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.004     |
| time/                   |            |
|    total_timesteps      | 33792550   |
| train/                  |            |
|    approx_kl            | 0.04182702 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.000199   |
|    loss                 | 0.0423     |
|    n_updates            | 2750       |
|    policy_gradient_loss | -0.000797  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 551      |
|    time_elapsed    | 216315   |
|    total_timesteps | 33853440 |
---------------------------------
Eval num_timesteps=33853991, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.006     |
| time/                   |            |
|    total_timesteps      | 33853991   |
| train/                  |            |
|    approx_kl            | 0.04125291 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.751     |
|    explained_variance   | 0.343      |
|    learning_rate        | 0.000198   |
|    loss                 | 0.0859     |
|    n_updates            | 2755       |
|    policy_gradient_loss | 0.000979   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 552      |
|    time_elapsed    | 216665   |
|    total_timesteps | 33914880 |
---------------------------------
Eval num_timesteps=33915432, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 33915432   |
| train/                  |            |
|    approx_kl            | 0.04476095 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.000198   |
|    loss                 | 0.0854     |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.00115   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 553      |
|    time_elapsed    | 217015   |
|    total_timesteps | 33976320 |
---------------------------------
Eval num_timesteps=33976873, episode_reward=0.01 +/- 0.98
Episode length: 29.99 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 33976873    |
| train/                  |             |
|    approx_kl            | 0.043494392 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000198    |
|    loss                 | 0.0824      |
|    n_updates            | 2765        |
|    policy_gradient_loss | -0.000566   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 554      |
|    time_elapsed    | 217364   |
|    total_timesteps | 34037760 |
---------------------------------
Eval num_timesteps=34038314, episode_reward=0.00 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 34038314    |
| train/                  |             |
|    approx_kl            | 0.043608017 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.751      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000198    |
|    loss                 | 0.0914      |
|    n_updates            | 2770        |
|    policy_gradient_loss | 0.000751    |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 555      |
|    time_elapsed    | 217714   |
|    total_timesteps | 34099200 |
---------------------------------
Eval num_timesteps=34099755, episode_reward=0.13 +/- 0.98
Episode length: 29.97 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.128      |
| time/                   |            |
|    total_timesteps      | 34099755   |
| train/                  |            |
|    approx_kl            | 0.04426219 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.749     |
|    explained_variance   | 0.354      |
|    learning_rate        | 0.000198   |
|    loss                 | 0.0639     |
|    n_updates            | 2775       |
|    policy_gradient_loss | -0.000799  |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 556      |
|    time_elapsed    | 218063   |
|    total_timesteps | 34160640 |
---------------------------------
Eval num_timesteps=34161196, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 34161196   |
| train/                  |            |
|    approx_kl            | 0.04360598 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.746     |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.000198   |
|    loss                 | 0.145      |
|    n_updates            | 2780       |
|    policy_gradient_loss | -1.31e-05  |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 557      |
|    time_elapsed    | 218412   |
|    total_timesteps | 34222080 |
---------------------------------
Eval num_timesteps=34222637, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 34222637   |
| train/                  |            |
|    approx_kl            | 0.04131391 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.000197   |
|    loss                 | 0.146      |
|    n_updates            | 2785       |
|    policy_gradient_loss | -4.76e-05  |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 558      |
|    time_elapsed    | 218761   |
|    total_timesteps | 34283520 |
---------------------------------
Eval num_timesteps=34284078, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 34284078   |
| train/                  |            |
|    approx_kl            | 0.04215129 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000197   |
|    loss                 | 0.0458     |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.000479  |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 559      |
|    time_elapsed    | 219110   |
|    total_timesteps | 34344960 |
---------------------------------
Eval num_timesteps=34345519, episode_reward=0.06 +/- 0.99
Episode length: 29.99 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.064     |
| time/                   |           |
|    total_timesteps      | 34345519  |
| train/                  |           |
|    approx_kl            | 0.0421999 |
|    clip_fraction        | 0.238     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.732    |
|    explained_variance   | 0.344     |
|    learning_rate        | 0.000197  |
|    loss                 | 0.12      |
|    n_updates            | 2795      |
|    policy_gradient_loss | -0.00118  |
|    value_loss           | 0.241     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 560      |
|    time_elapsed    | 219459   |
|    total_timesteps | 34406400 |
---------------------------------
Eval num_timesteps=34406960, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 34406960    |
| train/                  |             |
|    approx_kl            | 0.044443227 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.73       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000197    |
|    loss                 | 0.0408      |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 561      |
|    time_elapsed    | 219809   |
|    total_timesteps | 34467840 |
---------------------------------
Eval num_timesteps=34468401, episode_reward=0.13 +/- 0.97
Episode length: 30.01 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.132      |
| time/                   |            |
|    total_timesteps      | 34468401   |
| train/                  |            |
|    approx_kl            | 0.04216523 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.731     |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.000197   |
|    loss                 | 0.0503     |
|    n_updates            | 2805       |
|    policy_gradient_loss | -0.000158  |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 562      |
|    time_elapsed    | 220161   |
|    total_timesteps | 34529280 |
---------------------------------
Eval num_timesteps=34529842, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.116      |
| time/                   |            |
|    total_timesteps      | 34529842   |
| train/                  |            |
|    approx_kl            | 0.04300767 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000196   |
|    loss                 | 0.163      |
|    n_updates            | 2810       |
|    policy_gradient_loss | -0.00158   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 563      |
|    time_elapsed    | 220511   |
|    total_timesteps | 34590720 |
---------------------------------
Eval num_timesteps=34591283, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 34591283    |
| train/                  |             |
|    approx_kl            | 0.041235745 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.000196    |
|    loss                 | 0.0852      |
|    n_updates            | 2815        |
|    policy_gradient_loss | -0.000237   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 564      |
|    time_elapsed    | 220861   |
|    total_timesteps | 34652160 |
---------------------------------
Eval num_timesteps=34652724, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 34652724   |
| train/                  |            |
|    approx_kl            | 0.04222574 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.732     |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.000196   |
|    loss                 | 0.118      |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.000677  |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 565      |
|    time_elapsed    | 221211   |
|    total_timesteps | 34713600 |
---------------------------------
Eval num_timesteps=34714165, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.068     |
| time/                   |           |
|    total_timesteps      | 34714165  |
| train/                  |           |
|    approx_kl            | 0.0421294 |
|    clip_fraction        | 0.238     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.734    |
|    explained_variance   | 0.344     |
|    learning_rate        | 0.000196  |
|    loss                 | 0.0463    |
|    n_updates            | 2825      |
|    policy_gradient_loss | -0.000677 |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 566      |
|    time_elapsed    | 221560   |
|    total_timesteps | 34775040 |
---------------------------------
Eval num_timesteps=34775606, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 34775606    |
| train/                  |             |
|    approx_kl            | 0.041747216 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000196    |
|    loss                 | 0.193       |
|    n_updates            | 2830        |
|    policy_gradient_loss | 0.00133     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 567      |
|    time_elapsed    | 221910   |
|    total_timesteps | 34836480 |
---------------------------------
Eval num_timesteps=34837047, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 1.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.062      |
| time/                   |            |
|    total_timesteps      | 34837047   |
| train/                  |            |
|    approx_kl            | 0.04320778 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.356      |
|    learning_rate        | 0.000195   |
|    loss                 | 0.112      |
|    n_updates            | 2835       |
|    policy_gradient_loss | -0.00163   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 568      |
|    time_elapsed    | 222259   |
|    total_timesteps | 34897920 |
---------------------------------
Eval num_timesteps=34898488, episode_reward=0.02 +/- 0.98
Episode length: 29.90 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 34898488    |
| train/                  |             |
|    approx_kl            | 0.044004325 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000195    |
|    loss                 | 0.0575      |
|    n_updates            | 2840        |
|    policy_gradient_loss | 5e-05       |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 569      |
|    time_elapsed    | 222608   |
|    total_timesteps | 34959360 |
---------------------------------
Eval num_timesteps=34959929, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 34959929    |
| train/                  |             |
|    approx_kl            | 0.041590545 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000195    |
|    loss                 | 0.0983      |
|    n_updates            | 2845        |
|    policy_gradient_loss | -0.000581   |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.32     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 570      |
|    time_elapsed    | 222957   |
|    total_timesteps | 35020800 |
---------------------------------
Eval num_timesteps=35021370, episode_reward=0.04 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 35021370    |
| train/                  |             |
|    approx_kl            | 0.042853314 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000195    |
|    loss                 | 0.103       |
|    n_updates            | 2850        |
|    policy_gradient_loss | 0.000217    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 571      |
|    time_elapsed    | 223307   |
|    total_timesteps | 35082240 |
---------------------------------
Eval num_timesteps=35082811, episode_reward=0.06 +/- 0.99
Episode length: 29.97 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.058     |
| time/                   |           |
|    total_timesteps      | 35082811  |
| train/                  |           |
|    approx_kl            | 0.041637  |
|    clip_fraction        | 0.236     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.716    |
|    explained_variance   | 0.353     |
|    learning_rate        | 0.000195  |
|    loss                 | 0.113     |
|    n_updates            | 2855      |
|    policy_gradient_loss | -9.98e-05 |
|    value_loss           | 0.239     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 572      |
|    time_elapsed    | 223658   |
|    total_timesteps | 35143680 |
---------------------------------
Eval num_timesteps=35144252, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 35144252   |
| train/                  |            |
|    approx_kl            | 0.03992302 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.701     |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.000195   |
|    loss                 | 0.0645     |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.00112   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 157      |
|    iterations      | 573      |
|    time_elapsed    | 224008   |
|    total_timesteps | 35205120 |
---------------------------------
Eval num_timesteps=35205693, episode_reward=0.02 +/- 0.99
Episode length: 29.95 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 35205693    |
| train/                  |             |
|    approx_kl            | 0.041099317 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000194    |
|    loss                 | 0.0799      |
|    n_updates            | 2865        |
|    policy_gradient_loss | 0.000679    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 574      |
|    time_elapsed    | 224358   |
|    total_timesteps | 35266560 |
---------------------------------
Eval num_timesteps=35267134, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 35267134    |
| train/                  |             |
|    approx_kl            | 0.041951723 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000194    |
|    loss                 | 0.0923      |
|    n_updates            | 2870        |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 575      |
|    time_elapsed    | 224708   |
|    total_timesteps | 35328000 |
---------------------------------
Eval num_timesteps=35328575, episode_reward=0.06 +/- 0.99
Episode length: 29.93 +/- 1.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 35328575   |
| train/                  |            |
|    approx_kl            | 0.04166565 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000194   |
|    loss                 | 0.0833     |
|    n_updates            | 2875       |
|    policy_gradient_loss | -0.000591  |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 576      |
|    time_elapsed    | 225058   |
|    total_timesteps | 35389440 |
---------------------------------
Eval num_timesteps=35390016, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 35390016    |
| train/                  |             |
|    approx_kl            | 0.043022905 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000194    |
|    loss                 | 0.0961      |
|    n_updates            | 2880        |
|    policy_gradient_loss | -5.74e-05   |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 577      |
|    time_elapsed    | 225407   |
|    total_timesteps | 35450880 |
---------------------------------
Eval num_timesteps=35451457, episode_reward=0.02 +/- 0.99
Episode length: 29.94 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.022      |
| time/                   |            |
|    total_timesteps      | 35451457   |
| train/                  |            |
|    approx_kl            | 0.04245917 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.698     |
|    explained_variance   | 0.368      |
|    learning_rate        | 0.000194   |
|    loss                 | 0.0669     |
|    n_updates            | 2885       |
|    policy_gradient_loss | -0.00102   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 578      |
|    time_elapsed    | 225756   |
|    total_timesteps | 35512320 |
---------------------------------
Eval num_timesteps=35512898, episode_reward=0.06 +/- 0.98
Episode length: 29.93 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.058      |
| time/                   |            |
|    total_timesteps      | 35512898   |
| train/                  |            |
|    approx_kl            | 0.04152896 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.698     |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.000193   |
|    loss                 | 0.112      |
|    n_updates            | 2890       |
|    policy_gradient_loss | -0.00082   |
|    value_loss           | 0.232      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 579      |
|    time_elapsed    | 226105   |
|    total_timesteps | 35573760 |
---------------------------------
Eval num_timesteps=35574339, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 35574339    |
| train/                  |             |
|    approx_kl            | 0.041333113 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000193    |
|    loss                 | 0.156       |
|    n_updates            | 2895        |
|    policy_gradient_loss | -0.000204   |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 580      |
|    time_elapsed    | 226454   |
|    total_timesteps | 35635200 |
---------------------------------
Eval num_timesteps=35635780, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.094      |
| time/                   |            |
|    total_timesteps      | 35635780   |
| train/                  |            |
|    approx_kl            | 0.04081041 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.701     |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.000193   |
|    loss                 | 0.148      |
|    n_updates            | 2900       |
|    policy_gradient_loss | 0.000144   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 581      |
|    time_elapsed    | 226803   |
|    total_timesteps | 35696640 |
---------------------------------
Eval num_timesteps=35697221, episode_reward=0.03 +/- 0.99
Episode length: 29.91 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.03       |
| time/                   |            |
|    total_timesteps      | 35697221   |
| train/                  |            |
|    approx_kl            | 0.03942303 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.695     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000193   |
|    loss                 | 0.115      |
|    n_updates            | 2905       |
|    policy_gradient_loss | -0.00058   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 582      |
|    time_elapsed    | 227153   |
|    total_timesteps | 35758080 |
---------------------------------
Eval num_timesteps=35758662, episode_reward=0.12 +/- 0.98
Episode length: 29.93 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 35758662   |
| train/                  |            |
|    approx_kl            | 0.04163186 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.695     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000193   |
|    loss                 | 0.12       |
|    n_updates            | 2910       |
|    policy_gradient_loss | -0.000523  |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 583      |
|    time_elapsed    | 227504   |
|    total_timesteps | 35819520 |
---------------------------------
Eval num_timesteps=35820103, episode_reward=0.06 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.062      |
| time/                   |            |
|    total_timesteps      | 35820103   |
| train/                  |            |
|    approx_kl            | 0.03830118 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.684     |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.000193   |
|    loss                 | 0.0957     |
|    n_updates            | 2915       |
|    policy_gradient_loss | -0.000951  |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 584      |
|    time_elapsed    | 227853   |
|    total_timesteps | 35880960 |
---------------------------------
Eval num_timesteps=35881544, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 35881544    |
| train/                  |             |
|    approx_kl            | 0.040238142 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.683      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000192    |
|    loss                 | 0.102       |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.000601   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 585      |
|    time_elapsed    | 228203   |
|    total_timesteps | 35942400 |
---------------------------------
Eval num_timesteps=35942985, episode_reward=0.16 +/- 0.97
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.16        |
| time/                   |             |
|    total_timesteps      | 35942985    |
| train/                  |             |
|    approx_kl            | 0.041397866 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000192    |
|    loss                 | 0.131       |
|    n_updates            | 2925        |
|    policy_gradient_loss | -0.00052    |
|    value_loss           | 0.24        |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.16
SELFPLAY: new best model, bumping up generation to 36
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 586      |
|    time_elapsed    | 228553   |
|    total_timesteps | 36003840 |
---------------------------------
Eval num_timesteps=36004426, episode_reward=0.03 +/- 0.99
Episode length: 29.92 +/- 1.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.028      |
| time/                   |            |
|    total_timesteps      | 36004426   |
| train/                  |            |
|    approx_kl            | 0.04357578 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000192   |
|    loss                 | 0.153      |
|    n_updates            | 2930       |
|    policy_gradient_loss | -8.91e-05  |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 587      |
|    time_elapsed    | 228903   |
|    total_timesteps | 36065280 |
---------------------------------
Eval num_timesteps=36065867, episode_reward=0.06 +/- 0.98
Episode length: 29.91 +/- 1.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 36065867   |
| train/                  |            |
|    approx_kl            | 0.04159371 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.722     |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.000192   |
|    loss                 | 0.148      |
|    n_updates            | 2935       |
|    policy_gradient_loss | -0.00107   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 588      |
|    time_elapsed    | 229252   |
|    total_timesteps | 36126720 |
---------------------------------
Eval num_timesteps=36127308, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.036      |
| time/                   |            |
|    total_timesteps      | 36127308   |
| train/                  |            |
|    approx_kl            | 0.04296402 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.716     |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.000192   |
|    loss                 | 0.118      |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.00086   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 589      |
|    time_elapsed    | 229602   |
|    total_timesteps | 36188160 |
---------------------------------
Eval num_timesteps=36188749, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 36188749    |
| train/                  |             |
|    approx_kl            | 0.043123476 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.0725      |
|    n_updates            | 2945        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 590      |
|    time_elapsed    | 229952   |
|    total_timesteps | 36249600 |
---------------------------------
Eval num_timesteps=36250190, episode_reward=0.01 +/- 0.98
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 36250190    |
| train/                  |             |
|    approx_kl            | 0.045034233 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000191    |
|    loss                 | 0.106       |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.000691   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 591      |
|    time_elapsed    | 230301   |
|    total_timesteps | 36311040 |
---------------------------------
Eval num_timesteps=36311631, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 36311631    |
| train/                  |             |
|    approx_kl            | 0.042325154 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.709      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.0851      |
|    n_updates            | 2955        |
|    policy_gradient_loss | -0.000517   |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 592      |
|    time_elapsed    | 230649   |
|    total_timesteps | 36372480 |
---------------------------------
Eval num_timesteps=36373072, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 1.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 36373072    |
| train/                  |             |
|    approx_kl            | 0.041977756 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000191    |
|    loss                 | 0.11        |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.000566   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 593      |
|    time_elapsed    | 231000   |
|    total_timesteps | 36433920 |
---------------------------------
Eval num_timesteps=36434513, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 36434513   |
| train/                  |            |
|    approx_kl            | 0.04309577 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.716     |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.000191   |
|    loss                 | 0.0732     |
|    n_updates            | 2965       |
|    policy_gradient_loss | -3.38e-06  |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 594      |
|    time_elapsed    | 231351   |
|    total_timesteps | 36495360 |
---------------------------------
Eval num_timesteps=36495954, episode_reward=-0.04 +/- 0.99
Episode length: 29.96 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 36495954    |
| train/                  |             |
|    approx_kl            | 0.041646264 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.0569      |
|    n_updates            | 2970        |
|    policy_gradient_loss | 0.000365    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 595      |
|    time_elapsed    | 231702   |
|    total_timesteps | 36556800 |
---------------------------------
Eval num_timesteps=36557395, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 36557395    |
| train/                  |             |
|    approx_kl            | 0.042002868 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.00019     |
|    loss                 | 0.0523      |
|    n_updates            | 2975        |
|    policy_gradient_loss | 0.000134    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 596      |
|    time_elapsed    | 232051   |
|    total_timesteps | 36618240 |
---------------------------------
Eval num_timesteps=36618836, episode_reward=0.02 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 36618836    |
| train/                  |             |
|    approx_kl            | 0.043684795 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.00019     |
|    loss                 | 0.0826      |
|    n_updates            | 2980        |
|    policy_gradient_loss | -2.55e-06   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 597      |
|    time_elapsed    | 232401   |
|    total_timesteps | 36679680 |
---------------------------------
Eval num_timesteps=36680277, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 36680277   |
| train/                  |            |
|    approx_kl            | 0.04367226 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.00019    |
|    loss                 | 0.175      |
|    n_updates            | 2985       |
|    policy_gradient_loss | -0.000768  |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 598      |
|    time_elapsed    | 232751   |
|    total_timesteps | 36741120 |
---------------------------------
Eval num_timesteps=36741718, episode_reward=0.12 +/- 0.98
Episode length: 29.97 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.116      |
| time/                   |            |
|    total_timesteps      | 36741718   |
| train/                  |            |
|    approx_kl            | 0.04110652 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.00019    |
|    loss                 | 0.132      |
|    n_updates            | 2990       |
|    policy_gradient_loss | -0.000885  |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 599      |
|    time_elapsed    | 233100   |
|    total_timesteps | 36802560 |
---------------------------------
Eval num_timesteps=36803159, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 36803159    |
| train/                  |             |
|    approx_kl            | 0.042648785 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.00019     |
|    loss                 | 0.116       |
|    n_updates            | 2995        |
|    policy_gradient_loss | -0.000633   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 600      |
|    time_elapsed    | 233449   |
|    total_timesteps | 36864000 |
---------------------------------
Eval num_timesteps=36864600, episode_reward=-0.01 +/- 0.99
Episode length: 29.94 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 36864600    |
| train/                  |             |
|    approx_kl            | 0.042975765 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000189    |
|    loss                 | 0.0619      |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 601      |
|    time_elapsed    | 233798   |
|    total_timesteps | 36925440 |
---------------------------------
Eval num_timesteps=36926041, episode_reward=0.02 +/- 0.99
Episode length: 29.92 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.022      |
| time/                   |            |
|    total_timesteps      | 36926041   |
| train/                  |            |
|    approx_kl            | 0.04178328 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.000189   |
|    loss                 | 0.0587     |
|    n_updates            | 3005       |
|    policy_gradient_loss | -0.000463  |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 602      |
|    time_elapsed    | 234147   |
|    total_timesteps | 36986880 |
---------------------------------
Eval num_timesteps=36987482, episode_reward=0.00 +/- 0.98
Episode length: 30.00 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 36987482    |
| train/                  |             |
|    approx_kl            | 0.041179154 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000189    |
|    loss                 | 0.104       |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.000184   |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 603      |
|    time_elapsed    | 234497   |
|    total_timesteps | 37048320 |
---------------------------------
Eval num_timesteps=37048923, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 37048923    |
| train/                  |             |
|    approx_kl            | 0.041471146 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000189    |
|    loss                 | 0.12        |
|    n_updates            | 3015        |
|    policy_gradient_loss | -0.000813   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 604      |
|    time_elapsed    | 234849   |
|    total_timesteps | 37109760 |
---------------------------------
Eval num_timesteps=37110364, episode_reward=0.05 +/- 0.99
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 37110364   |
| train/                  |            |
|    approx_kl            | 0.04121046 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.000189   |
|    loss                 | 0.0843     |
|    n_updates            | 3020       |
|    policy_gradient_loss | 6.19e-05   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 158      |
|    iterations      | 605      |
|    time_elapsed    | 235200   |
|    total_timesteps | 37171200 |
---------------------------------
Eval num_timesteps=37171805, episode_reward=0.03 +/- 0.99
Episode length: 29.91 +/- 1.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.032      |
| time/                   |            |
|    total_timesteps      | 37171805   |
| train/                  |            |
|    approx_kl            | 0.04220628 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.698     |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.000188   |
|    loss                 | 0.114      |
|    n_updates            | 3025       |
|    policy_gradient_loss | -0.000685  |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 606      |
|    time_elapsed    | 235550   |
|    total_timesteps | 37232640 |
---------------------------------
Eval num_timesteps=37233246, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 37233246    |
| train/                  |             |
|    approx_kl            | 0.040350284 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000188    |
|    loss                 | 0.11        |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 607      |
|    time_elapsed    | 235900   |
|    total_timesteps | 37294080 |
---------------------------------
Eval num_timesteps=37294687, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 37294687    |
| train/                  |             |
|    approx_kl            | 0.041482646 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000188    |
|    loss                 | 0.109       |
|    n_updates            | 3035        |
|    policy_gradient_loss | -0.000746   |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 608      |
|    time_elapsed    | 236250   |
|    total_timesteps | 37355520 |
---------------------------------
Eval num_timesteps=37356128, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.072      |
| time/                   |            |
|    total_timesteps      | 37356128   |
| train/                  |            |
|    approx_kl            | 0.04320648 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.696     |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.000188   |
|    loss                 | 0.0868     |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.00116   |
|    value_loss           | 0.246      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 609      |
|    time_elapsed    | 236600   |
|    total_timesteps | 37416960 |
---------------------------------
Eval num_timesteps=37417569, episode_reward=0.10 +/- 0.97
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 37417569    |
| train/                  |             |
|    approx_kl            | 0.042064633 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.695      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000188    |
|    loss                 | 0.0468      |
|    n_updates            | 3045        |
|    policy_gradient_loss | -4.24e-05   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 610      |
|    time_elapsed    | 236950   |
|    total_timesteps | 37478400 |
---------------------------------
Eval num_timesteps=37479010, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 37479010    |
| train/                  |             |
|    approx_kl            | 0.041027647 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000188    |
|    loss                 | 0.134       |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.000669   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 611      |
|    time_elapsed    | 237299   |
|    total_timesteps | 37539840 |
---------------------------------
Eval num_timesteps=37540451, episode_reward=0.06 +/- 0.98
Episode length: 29.97 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 37540451   |
| train/                  |            |
|    approx_kl            | 0.04103782 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.691     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000187   |
|    loss                 | 0.134      |
|    n_updates            | 3055       |
|    policy_gradient_loss | -0.000759  |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 612      |
|    time_elapsed    | 237648   |
|    total_timesteps | 37601280 |
---------------------------------
Eval num_timesteps=37601892, episode_reward=0.06 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 37601892    |
| train/                  |             |
|    approx_kl            | 0.042382106 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000187    |
|    loss                 | 0.0582      |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.00148    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 613      |
|    time_elapsed    | 237997   |
|    total_timesteps | 37662720 |
---------------------------------
Eval num_timesteps=37663333, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 37663333    |
| train/                  |             |
|    approx_kl            | 0.042219743 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000187    |
|    loss                 | 0.082       |
|    n_updates            | 3065        |
|    policy_gradient_loss | 0.000685    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 614      |
|    time_elapsed    | 238348   |
|    total_timesteps | 37724160 |
---------------------------------
Eval num_timesteps=37724774, episode_reward=0.04 +/- 0.98
Episode length: 29.95 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 37724774    |
| train/                  |             |
|    approx_kl            | 0.043244597 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000187    |
|    loss                 | 0.141       |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.000316   |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 615      |
|    time_elapsed    | 238700   |
|    total_timesteps | 37785600 |
---------------------------------
Eval num_timesteps=37786215, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 37786215    |
| train/                  |             |
|    approx_kl            | 0.041387256 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.678      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000187    |
|    loss                 | 0.0675      |
|    n_updates            | 3075        |
|    policy_gradient_loss | -0.000773   |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 616      |
|    time_elapsed    | 239050   |
|    total_timesteps | 37847040 |
---------------------------------
Eval num_timesteps=37847656, episode_reward=-0.01 +/- 0.99
Episode length: 29.95 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.008     |
| time/                   |            |
|    total_timesteps      | 37847656   |
| train/                  |            |
|    approx_kl            | 0.04244286 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.681     |
|    explained_variance   | 0.356      |
|    learning_rate        | 0.000186   |
|    loss                 | 0.111      |
|    n_updates            | 3080       |
|    policy_gradient_loss | 5.04e-05   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 617      |
|    time_elapsed    | 239399   |
|    total_timesteps | 37908480 |
---------------------------------
Eval num_timesteps=37909097, episode_reward=0.04 +/- 0.99
Episode length: 29.99 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 37909097    |
| train/                  |             |
|    approx_kl            | 0.041914783 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000186    |
|    loss                 | 0.131       |
|    n_updates            | 3085        |
|    policy_gradient_loss | -9.77e-05   |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 618      |
|    time_elapsed    | 239748   |
|    total_timesteps | 37969920 |
---------------------------------
Eval num_timesteps=37970538, episode_reward=0.06 +/- 0.99
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 37970538   |
| train/                  |            |
|    approx_kl            | 0.04351643 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.671     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000186   |
|    loss                 | 0.191      |
|    n_updates            | 3090       |
|    policy_gradient_loss | 0.000171   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 619      |
|    time_elapsed    | 240097   |
|    total_timesteps | 38031360 |
---------------------------------
Eval num_timesteps=38031979, episode_reward=0.03 +/- 0.99
Episode length: 29.91 +/- 1.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.032      |
| time/                   |            |
|    total_timesteps      | 38031979   |
| train/                  |            |
|    approx_kl            | 0.04124395 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.677     |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.000186   |
|    loss                 | 0.115      |
|    n_updates            | 3095       |
|    policy_gradient_loss | -0.00145   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 620      |
|    time_elapsed    | 240447   |
|    total_timesteps | 38092800 |
---------------------------------
Eval num_timesteps=38093420, episode_reward=0.04 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 38093420    |
| train/                  |             |
|    approx_kl            | 0.043364666 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000186    |
|    loss                 | 0.0776      |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.000453   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 621      |
|    time_elapsed    | 240796   |
|    total_timesteps | 38154240 |
---------------------------------
Eval num_timesteps=38154861, episode_reward=0.06 +/- 0.98
Episode length: 29.94 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 38154861   |
| train/                  |            |
|    approx_kl            | 0.04463582 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.676     |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.000186   |
|    loss                 | 0.155      |
|    n_updates            | 3105       |
|    policy_gradient_loss | -3.83e-05  |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 622      |
|    time_elapsed    | 241146   |
|    total_timesteps | 38215680 |
---------------------------------
Eval num_timesteps=38216302, episode_reward=0.10 +/- 0.99
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 38216302    |
| train/                  |             |
|    approx_kl            | 0.043416914 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000185    |
|    loss                 | 0.0569      |
|    n_updates            | 3110        |
|    policy_gradient_loss | -0.000871   |
|    value_loss           | 0.247       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 623      |
|    time_elapsed    | 241495   |
|    total_timesteps | 38277120 |
---------------------------------
Eval num_timesteps=38277743, episode_reward=0.04 +/- 0.97
Episode length: 29.97 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 38277743    |
| train/                  |             |
|    approx_kl            | 0.041800722 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000185    |
|    loss                 | 0.133       |
|    n_updates            | 3115        |
|    policy_gradient_loss | -0.000241   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 624      |
|    time_elapsed    | 241844   |
|    total_timesteps | 38338560 |
---------------------------------
Eval num_timesteps=38339184, episode_reward=-0.03 +/- 0.99
Episode length: 29.88 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 38339184    |
| train/                  |             |
|    approx_kl            | 0.041683156 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000185    |
|    loss                 | 0.0889      |
|    n_updates            | 3120        |
|    policy_gradient_loss | 0.000251    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 625      |
|    time_elapsed    | 242194   |
|    total_timesteps | 38400000 |
---------------------------------
Eval num_timesteps=38400625, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 38400625    |
| train/                  |             |
|    approx_kl            | 0.041075636 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.661      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000185    |
|    loss                 | 0.0191      |
|    n_updates            | 3125        |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 626      |
|    time_elapsed    | 242546   |
|    total_timesteps | 38461440 |
---------------------------------
Eval num_timesteps=38462066, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 38462066    |
| train/                  |             |
|    approx_kl            | 0.040664807 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000185    |
|    loss                 | 0.0569      |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 627      |
|    time_elapsed    | 242896   |
|    total_timesteps | 38522880 |
---------------------------------
Eval num_timesteps=38523507, episode_reward=0.13 +/- 0.98
Episode length: 29.96 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.134      |
| time/                   |            |
|    total_timesteps      | 38523507   |
| train/                  |            |
|    approx_kl            | 0.04237376 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.666     |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.000184   |
|    loss                 | 0.127      |
|    n_updates            | 3135       |
|    policy_gradient_loss | -0.000524  |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 628      |
|    time_elapsed    | 243245   |
|    total_timesteps | 38584320 |
---------------------------------
Eval num_timesteps=38584948, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.062      |
| time/                   |            |
|    total_timesteps      | 38584948   |
| train/                  |            |
|    approx_kl            | 0.04299725 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.674     |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.000184   |
|    loss                 | 0.118      |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.000836  |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 629      |
|    time_elapsed    | 243594   |
|    total_timesteps | 38645760 |
---------------------------------
Eval num_timesteps=38646389, episode_reward=0.08 +/- 0.99
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 38646389    |
| train/                  |             |
|    approx_kl            | 0.042780887 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000184    |
|    loss                 | 0.0957      |
|    n_updates            | 3145        |
|    policy_gradient_loss | 4.53e-05    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 630      |
|    time_elapsed    | 243943   |
|    total_timesteps | 38707200 |
---------------------------------
Eval num_timesteps=38707830, episode_reward=0.11 +/- 0.98
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.106      |
| time/                   |            |
|    total_timesteps      | 38707830   |
| train/                  |            |
|    approx_kl            | 0.04137927 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.659     |
|    explained_variance   | 0.339      |
|    learning_rate        | 0.000184   |
|    loss                 | 0.0787     |
|    n_updates            | 3150       |
|    policy_gradient_loss | -0.0015    |
|    value_loss           | 0.245      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 631      |
|    time_elapsed    | 244293   |
|    total_timesteps | 38768640 |
---------------------------------
Eval num_timesteps=38769271, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 38769271    |
| train/                  |             |
|    approx_kl            | 0.039873246 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000184    |
|    loss                 | 0.0758      |
|    n_updates            | 3155        |
|    policy_gradient_loss | 0.000221    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 632      |
|    time_elapsed    | 244642   |
|    total_timesteps | 38830080 |
---------------------------------
Eval num_timesteps=38830712, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 38830712    |
| train/                  |             |
|    approx_kl            | 0.040704645 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.000184    |
|    loss                 | 0.141       |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.000409   |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 633      |
|    time_elapsed    | 244991   |
|    total_timesteps | 38891520 |
---------------------------------
Eval num_timesteps=38892153, episode_reward=0.14 +/- 0.98
Episode length: 29.96 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 38892153    |
| train/                  |             |
|    approx_kl            | 0.043661084 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000183    |
|    loss                 | 0.0836      |
|    n_updates            | 3165        |
|    policy_gradient_loss | 1.6e-05     |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 37
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 634      |
|    time_elapsed    | 245340   |
|    total_timesteps | 38952960 |
---------------------------------
Eval num_timesteps=38953594, episode_reward=0.05 +/- 0.99
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 38953594    |
| train/                  |             |
|    approx_kl            | 0.039744258 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.672      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000183    |
|    loss                 | 0.136       |
|    n_updates            | 3170        |
|    policy_gradient_loss | -0.000791   |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 635      |
|    time_elapsed    | 245690   |
|    total_timesteps | 39014400 |
---------------------------------
Eval num_timesteps=39015035, episode_reward=-0.04 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.044      |
| time/                   |             |
|    total_timesteps      | 39015035    |
| train/                  |             |
|    approx_kl            | 0.043816026 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.68       |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000183    |
|    loss                 | 0.163       |
|    n_updates            | 3175        |
|    policy_gradient_loss | -3.63e-05   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 636      |
|    time_elapsed    | 246042   |
|    total_timesteps | 39075840 |
---------------------------------
Eval num_timesteps=39076476, episode_reward=-0.03 +/- 0.99
Episode length: 29.95 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.028     |
| time/                   |            |
|    total_timesteps      | 39076476   |
| train/                  |            |
|    approx_kl            | 0.04049421 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.687     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000183   |
|    loss                 | 0.0734     |
|    n_updates            | 3180       |
|    policy_gradient_loss | -9.6e-05   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 637      |
|    time_elapsed    | 246392   |
|    total_timesteps | 39137280 |
---------------------------------
Eval num_timesteps=39137917, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 39137917    |
| train/                  |             |
|    approx_kl            | 0.042683657 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.688      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000183    |
|    loss                 | 0.121       |
|    n_updates            | 3185        |
|    policy_gradient_loss | -0.00138    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 638      |
|    time_elapsed    | 246742   |
|    total_timesteps | 39198720 |
---------------------------------
Eval num_timesteps=39199358, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.074     |
| time/                   |           |
|    total_timesteps      | 39199358  |
| train/                  |           |
|    approx_kl            | 0.0408171 |
|    clip_fraction        | 0.229     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.688    |
|    explained_variance   | 0.349     |
|    learning_rate        | 0.000182  |
|    loss                 | 0.116     |
|    n_updates            | 3190      |
|    policy_gradient_loss | -0.00076  |
|    value_loss           | 0.241     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 158      |
|    iterations      | 639      |
|    time_elapsed    | 247092   |
|    total_timesteps | 39260160 |
---------------------------------
Eval num_timesteps=39260799, episode_reward=-0.01 +/- 0.99
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 39260799    |
| train/                  |             |
|    approx_kl            | 0.040142383 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.679      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000182    |
|    loss                 | 0.0946      |
|    n_updates            | 3195        |
|    policy_gradient_loss | -0.000594   |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 640      |
|    time_elapsed    | 247442   |
|    total_timesteps | 39321600 |
---------------------------------
Eval num_timesteps=39322240, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 39322240    |
| train/                  |             |
|    approx_kl            | 0.040565167 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.683      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000182    |
|    loss                 | 0.0943      |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.000793   |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 641      |
|    time_elapsed    | 247792   |
|    total_timesteps | 39383040 |
---------------------------------
Eval num_timesteps=39383681, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.006      |
| time/                   |            |
|    total_timesteps      | 39383681   |
| train/                  |            |
|    approx_kl            | 0.04073686 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.68      |
|    explained_variance   | 0.359      |
|    learning_rate        | 0.000182   |
|    loss                 | 0.158      |
|    n_updates            | 3205       |
|    policy_gradient_loss | -0.00154   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 642      |
|    time_elapsed    | 248141   |
|    total_timesteps | 39444480 |
---------------------------------
Eval num_timesteps=39445122, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 39445122    |
| train/                  |             |
|    approx_kl            | 0.042255405 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.677      |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.000182    |
|    loss                 | 0.139       |
|    n_updates            | 3210        |
|    policy_gradient_loss | -7e-05      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 643      |
|    time_elapsed    | 248490   |
|    total_timesteps | 39505920 |
---------------------------------
Eval num_timesteps=39506563, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.008      |
| time/                   |            |
|    total_timesteps      | 39506563   |
| train/                  |            |
|    approx_kl            | 0.04238979 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.675     |
|    explained_variance   | 0.354      |
|    learning_rate        | 0.000181   |
|    loss                 | 0.00338    |
|    n_updates            | 3215       |
|    policy_gradient_loss | -0.00162   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 644      |
|    time_elapsed    | 248839   |
|    total_timesteps | 39567360 |
---------------------------------
Eval num_timesteps=39568004, episode_reward=0.02 +/- 0.98
Episode length: 29.94 +/- 1.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 39568004   |
| train/                  |            |
|    approx_kl            | 0.04062215 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.666     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000181   |
|    loss                 | 0.106      |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.000258  |
|    value_loss           | 0.246      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 645      |
|    time_elapsed    | 249189   |
|    total_timesteps | 39628800 |
---------------------------------
Eval num_timesteps=39629445, episode_reward=0.05 +/- 0.99
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 39629445    |
| train/                  |             |
|    approx_kl            | 0.040137496 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000181    |
|    loss                 | 0.0948      |
|    n_updates            | 3225        |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 646      |
|    time_elapsed    | 249541   |
|    total_timesteps | 39690240 |
---------------------------------
Eval num_timesteps=39690886, episode_reward=0.06 +/- 0.99
Episode length: 30.03 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 39690886   |
| train/                  |            |
|    approx_kl            | 0.03843921 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.662     |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.000181   |
|    loss                 | 0.0724     |
|    n_updates            | 3230       |
|    policy_gradient_loss | -0.00163   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 647      |
|    time_elapsed    | 249891   |
|    total_timesteps | 39751680 |
---------------------------------
Eval num_timesteps=39752327, episode_reward=0.05 +/- 0.99
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 39752327    |
| train/                  |             |
|    approx_kl            | 0.041265972 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000181    |
|    loss                 | 0.136       |
|    n_updates            | 3235        |
|    policy_gradient_loss | 0.000465    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 648      |
|    time_elapsed    | 250240   |
|    total_timesteps | 39813120 |
---------------------------------
Eval num_timesteps=39813768, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 39813768   |
| train/                  |            |
|    approx_kl            | 0.04048885 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.668     |
|    explained_variance   | 0.339      |
|    learning_rate        | 0.000181   |
|    loss                 | 0.108      |
|    n_updates            | 3240       |
|    policy_gradient_loss | -0.00125   |
|    value_loss           | 0.244      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 649      |
|    time_elapsed    | 250590   |
|    total_timesteps | 39874560 |
---------------------------------
Eval num_timesteps=39875209, episode_reward=-0.02 +/- 0.98
Episode length: 29.93 +/- 0.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 39875209    |
| train/                  |             |
|    approx_kl            | 0.046931554 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.00018     |
|    loss                 | 0.14        |
|    n_updates            | 3245        |
|    policy_gradient_loss | -0.00194    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 650      |
|    time_elapsed    | 250939   |
|    total_timesteps | 39936000 |
---------------------------------
Eval num_timesteps=39936650, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 39936650   |
| train/                  |            |
|    approx_kl            | 0.04157393 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.662     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.00018    |
|    loss                 | 0.12       |
|    n_updates            | 3250       |
|    policy_gradient_loss | -0.00108   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 651      |
|    time_elapsed    | 251289   |
|    total_timesteps | 39997440 |
---------------------------------
Eval num_timesteps=39998091, episode_reward=0.02 +/- 0.98
Episode length: 29.88 +/- 1.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 39998091    |
| train/                  |             |
|    approx_kl            | 0.040550623 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.00018     |
|    loss                 | 0.0715      |
|    n_updates            | 3255        |
|    policy_gradient_loss | -0.000967   |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 652      |
|    time_elapsed    | 251639   |
|    total_timesteps | 40058880 |
---------------------------------
Eval num_timesteps=40059532, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 40059532    |
| train/                  |             |
|    approx_kl            | 0.041514628 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.00018     |
|    loss                 | 0.052       |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.000541   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 653      |
|    time_elapsed    | 251987   |
|    total_timesteps | 40120320 |
---------------------------------
Eval num_timesteps=40120973, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 40120973    |
| train/                  |             |
|    approx_kl            | 0.040562745 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.00018     |
|    loss                 | 0.0724      |
|    n_updates            | 3265        |
|    policy_gradient_loss | -0.00052    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 654      |
|    time_elapsed    | 252337   |
|    total_timesteps | 40181760 |
---------------------------------
Eval num_timesteps=40182414, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 40182414    |
| train/                  |             |
|    approx_kl            | 0.040605105 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000179    |
|    loss                 | 0.106       |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.000178   |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 655      |
|    time_elapsed    | 252687   |
|    total_timesteps | 40243200 |
---------------------------------
Eval num_timesteps=40243855, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 40243855    |
| train/                  |             |
|    approx_kl            | 0.041038748 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000179    |
|    loss                 | 0.0849      |
|    n_updates            | 3275        |
|    policy_gradient_loss | -0.000408   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 656      |
|    time_elapsed    | 253037   |
|    total_timesteps | 40304640 |
---------------------------------
Eval num_timesteps=40305296, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 40305296    |
| train/                  |             |
|    approx_kl            | 0.041849528 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.000179    |
|    loss                 | 0.136       |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.000465   |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 657      |
|    time_elapsed    | 253388   |
|    total_timesteps | 40366080 |
---------------------------------
Eval num_timesteps=40366737, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 40366737    |
| train/                  |             |
|    approx_kl            | 0.042055655 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.667      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000179    |
|    loss                 | 0.145       |
|    n_updates            | 3285        |
|    policy_gradient_loss | -0.000715   |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 658      |
|    time_elapsed    | 253738   |
|    total_timesteps | 40427520 |
---------------------------------
Eval num_timesteps=40428178, episode_reward=0.00 +/- 0.99
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 40428178    |
| train/                  |             |
|    approx_kl            | 0.040586527 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000179    |
|    loss                 | 0.13        |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 659      |
|    time_elapsed    | 254087   |
|    total_timesteps | 40488960 |
---------------------------------
Eval num_timesteps=40489619, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 40489619    |
| train/                  |             |
|    approx_kl            | 0.041747227 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000179    |
|    loss                 | 0.0548      |
|    n_updates            | 3295        |
|    policy_gradient_loss | -0.0014     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 660      |
|    time_elapsed    | 254437   |
|    total_timesteps | 40550400 |
---------------------------------
Eval num_timesteps=40551060, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 40551060    |
| train/                  |             |
|    approx_kl            | 0.041281246 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.000178    |
|    loss                 | 0.146       |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 661      |
|    time_elapsed    | 254787   |
|    total_timesteps | 40611840 |
---------------------------------
Eval num_timesteps=40612501, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 40612501    |
| train/                  |             |
|    approx_kl            | 0.041981895 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.652      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000178    |
|    loss                 | 0.162       |
|    n_updates            | 3305        |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 662      |
|    time_elapsed    | 255136   |
|    total_timesteps | 40673280 |
---------------------------------
Eval num_timesteps=40673942, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 40673942    |
| train/                  |             |
|    approx_kl            | 0.042849343 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000178    |
|    loss                 | 0.0741      |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 663      |
|    time_elapsed    | 255485   |
|    total_timesteps | 40734720 |
---------------------------------
Eval num_timesteps=40735383, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.14        |
| time/                   |             |
|    total_timesteps      | 40735383    |
| train/                  |             |
|    approx_kl            | 0.041093074 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000178    |
|    loss                 | 0.0528      |
|    n_updates            | 3315        |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 664      |
|    time_elapsed    | 255834   |
|    total_timesteps | 40796160 |
---------------------------------
Eval num_timesteps=40796824, episode_reward=0.05 +/- 0.99
Episode length: 29.92 +/- 0.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 40796824    |
| train/                  |             |
|    approx_kl            | 0.041264597 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000178    |
|    loss                 | 0.0599      |
|    n_updates            | 3320        |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 665      |
|    time_elapsed    | 256183   |
|    total_timesteps | 40857600 |
---------------------------------
Eval num_timesteps=40858265, episode_reward=0.09 +/- 0.99
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.092      |
| time/                   |            |
|    total_timesteps      | 40858265   |
| train/                  |            |
|    approx_kl            | 0.04096358 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.335      |
|    learning_rate        | 0.000177   |
|    loss                 | 0.124      |
|    n_updates            | 3325       |
|    policy_gradient_loss | -0.00153   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 666      |
|    time_elapsed    | 256534   |
|    total_timesteps | 40919040 |
---------------------------------
Eval num_timesteps=40919706, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 40919706   |
| train/                  |            |
|    approx_kl            | 0.04337059 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.655     |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.000177   |
|    loss                 | 0.223      |
|    n_updates            | 3330       |
|    policy_gradient_loss | 0.000685   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 667      |
|    time_elapsed    | 256885   |
|    total_timesteps | 40980480 |
---------------------------------
Eval num_timesteps=40981147, episode_reward=0.16 +/- 0.97
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.16        |
| time/                   |             |
|    total_timesteps      | 40981147    |
| train/                  |             |
|    approx_kl            | 0.038757917 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.000177    |
|    loss                 | 0.0923      |
|    n_updates            | 3335        |
|    policy_gradient_loss | -0.000854   |
|    value_loss           | 0.24        |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.16
SELFPLAY: new best model, bumping up generation to 38
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 668      |
|    time_elapsed    | 257236   |
|    total_timesteps | 41041920 |
---------------------------------
Eval num_timesteps=41042588, episode_reward=0.04 +/- 0.99
Episode length: 29.90 +/- 1.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 41042588    |
| train/                  |             |
|    approx_kl            | 0.042600457 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000177    |
|    loss                 | 0.0775      |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 669      |
|    time_elapsed    | 257585   |
|    total_timesteps | 41103360 |
---------------------------------
Eval num_timesteps=41104029, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 41104029   |
| train/                  |            |
|    approx_kl            | 0.04257738 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.000177   |
|    loss                 | 0.187      |
|    n_updates            | 3345       |
|    policy_gradient_loss | -0.00156   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 670      |
|    time_elapsed    | 257935   |
|    total_timesteps | 41164800 |
---------------------------------
Eval num_timesteps=41165470, episode_reward=0.06 +/- 0.97
Episode length: 29.95 +/- 1.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.062      |
| time/                   |            |
|    total_timesteps      | 41165470   |
| train/                  |            |
|    approx_kl            | 0.03974398 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.000177   |
|    loss                 | 0.102      |
|    n_updates            | 3350       |
|    policy_gradient_loss | -0.00164   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 671      |
|    time_elapsed    | 258284   |
|    total_timesteps | 41226240 |
---------------------------------
Eval num_timesteps=41226911, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 41226911    |
| train/                  |             |
|    approx_kl            | 0.039984744 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000176    |
|    loss                 | 0.0853      |
|    n_updates            | 3355        |
|    policy_gradient_loss | -0.00192    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 672      |
|    time_elapsed    | 258634   |
|    total_timesteps | 41287680 |
---------------------------------
Eval num_timesteps=41288352, episode_reward=0.06 +/- 0.98
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 41288352    |
| train/                  |             |
|    approx_kl            | 0.042864703 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.377       |
|    learning_rate        | 0.000176    |
|    loss                 | 0.096       |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 673      |
|    time_elapsed    | 258983   |
|    total_timesteps | 41349120 |
---------------------------------
Eval num_timesteps=41349793, episode_reward=-0.03 +/- 0.99
Episode length: 29.94 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 41349793    |
| train/                  |             |
|    approx_kl            | 0.044593383 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.000176    |
|    loss                 | 0.135       |
|    n_updates            | 3365        |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 674      |
|    time_elapsed    | 259332   |
|    total_timesteps | 41410560 |
---------------------------------
Eval num_timesteps=41411234, episode_reward=0.03 +/- 0.98
Episode length: 29.93 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.028      |
| time/                   |            |
|    total_timesteps      | 41411234   |
| train/                  |            |
|    approx_kl            | 0.04181653 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.699     |
|    explained_variance   | 0.369      |
|    learning_rate        | 0.000176   |
|    loss                 | 0.0875     |
|    n_updates            | 3370       |
|    policy_gradient_loss | -0.00285   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 675      |
|    time_elapsed    | 259682   |
|    total_timesteps | 41472000 |
---------------------------------
Eval num_timesteps=41472675, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 41472675    |
| train/                  |             |
|    approx_kl            | 0.043583013 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.000176    |
|    loss                 | 0.106       |
|    n_updates            | 3375        |
|    policy_gradient_loss | -0.0022     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 676      |
|    time_elapsed    | 260031   |
|    total_timesteps | 41533440 |
---------------------------------
Eval num_timesteps=41534116, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 41534116    |
| train/                  |             |
|    approx_kl            | 0.040571142 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000175    |
|    loss                 | 0.0557      |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00103    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 677      |
|    time_elapsed    | 260382   |
|    total_timesteps | 41594880 |
---------------------------------
Eval num_timesteps=41595557, episode_reward=0.03 +/- 0.98
Episode length: 29.93 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 41595557    |
| train/                  |             |
|    approx_kl            | 0.041252844 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.68       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.000175    |
|    loss                 | 0.0536      |
|    n_updates            | 3385        |
|    policy_gradient_loss | -0.000994   |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 678      |
|    time_elapsed    | 260733   |
|    total_timesteps | 41656320 |
---------------------------------
Eval num_timesteps=41656998, episode_reward=0.05 +/- 0.99
Episode length: 29.94 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 41656998    |
| train/                  |             |
|    approx_kl            | 0.040936906 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.686      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.000175    |
|    loss                 | 0.0635      |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 679      |
|    time_elapsed    | 261083   |
|    total_timesteps | 41717760 |
---------------------------------
Eval num_timesteps=41718439, episode_reward=0.08 +/- 0.98
Episode length: 29.90 +/- 1.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 41718439    |
| train/                  |             |
|    approx_kl            | 0.041938342 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000175    |
|    loss                 | 0.0493      |
|    n_updates            | 3395        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 680      |
|    time_elapsed    | 261432   |
|    total_timesteps | 41779200 |
---------------------------------
Eval num_timesteps=41779880, episode_reward=0.05 +/- 0.99
Episode length: 29.93 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 41779880    |
| train/                  |             |
|    approx_kl            | 0.042003457 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.000175    |
|    loss                 | 0.113       |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 681      |
|    time_elapsed    | 261781   |
|    total_timesteps | 41840640 |
---------------------------------
Eval num_timesteps=41841321, episode_reward=0.04 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.044      |
| time/                   |            |
|    total_timesteps      | 41841321   |
| train/                  |            |
|    approx_kl            | 0.04370284 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.687     |
|    explained_variance   | 0.378      |
|    learning_rate        | 0.000174   |
|    loss                 | 0.0131     |
|    n_updates            | 3405       |
|    policy_gradient_loss | -0.000129  |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 682      |
|    time_elapsed    | 262131   |
|    total_timesteps | 41902080 |
---------------------------------
Eval num_timesteps=41902762, episode_reward=0.00 +/- 0.99
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 41902762    |
| train/                  |             |
|    approx_kl            | 0.043912873 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.000174    |
|    loss                 | 0.0801      |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 683      |
|    time_elapsed    | 262481   |
|    total_timesteps | 41963520 |
---------------------------------
Eval num_timesteps=41964203, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 41964203    |
| train/                  |             |
|    approx_kl            | 0.043782976 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.674      |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.000174    |
|    loss                 | 0.483       |
|    n_updates            | 3415        |
|    policy_gradient_loss | -0.000177   |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 684      |
|    time_elapsed    | 262830   |
|    total_timesteps | 42024960 |
---------------------------------
Eval num_timesteps=42025644, episode_reward=-0.02 +/- 0.98
Episode length: 29.87 +/- 1.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 42025644    |
| train/                  |             |
|    approx_kl            | 0.040096726 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.672      |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.000174    |
|    loss                 | 0.054       |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.000883   |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 685      |
|    time_elapsed    | 263178   |
|    total_timesteps | 42086400 |
---------------------------------
Eval num_timesteps=42087085, episode_reward=0.06 +/- 0.98
Episode length: 29.94 +/- 1.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.064     |
| time/                   |           |
|    total_timesteps      | 42087085  |
| train/                  |           |
|    approx_kl            | 0.0431258 |
|    clip_fraction        | 0.228     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.668    |
|    explained_variance   | 0.378     |
|    learning_rate        | 0.000174  |
|    loss                 | 0.147     |
|    n_updates            | 3425      |
|    policy_gradient_loss | -0.00214  |
|    value_loss           | 0.234     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 686      |
|    time_elapsed    | 263527   |
|    total_timesteps | 42147840 |
---------------------------------
Eval num_timesteps=42148526, episode_reward=0.08 +/- 0.97
Episode length: 30.01 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.082     |
| time/                   |           |
|    total_timesteps      | 42148526  |
| train/                  |           |
|    approx_kl            | 0.0441283 |
|    clip_fraction        | 0.231     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.669    |
|    explained_variance   | 0.372     |
|    learning_rate        | 0.000174  |
|    loss                 | 0.0834    |
|    n_updates            | 3430      |
|    policy_gradient_loss | -0.00102  |
|    value_loss           | 0.236     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 687      |
|    time_elapsed    | 263877   |
|    total_timesteps | 42209280 |
---------------------------------
Eval num_timesteps=42209967, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.122      |
| time/                   |            |
|    total_timesteps      | 42209967   |
| train/                  |            |
|    approx_kl            | 0.04145396 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.665     |
|    explained_variance   | 0.377      |
|    learning_rate        | 0.000173   |
|    loss                 | 0.101      |
|    n_updates            | 3435       |
|    policy_gradient_loss | -0.000638  |
|    value_loss           | 0.232      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 688      |
|    time_elapsed    | 264229   |
|    total_timesteps | 42270720 |
---------------------------------
Eval num_timesteps=42271408, episode_reward=0.03 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.032      |
| time/                   |            |
|    total_timesteps      | 42271408   |
| train/                  |            |
|    approx_kl            | 0.04106059 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.658     |
|    explained_variance   | 0.377      |
|    learning_rate        | 0.000173   |
|    loss                 | 0.0808     |
|    n_updates            | 3440       |
|    policy_gradient_loss | -0.00124   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 689      |
|    time_elapsed    | 264578   |
|    total_timesteps | 42332160 |
---------------------------------
Eval num_timesteps=42332849, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 42332849    |
| train/                  |             |
|    approx_kl            | 0.043464374 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.37        |
|    learning_rate        | 0.000173    |
|    loss                 | 0.0873      |
|    n_updates            | 3445        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 690      |
|    time_elapsed    | 264928   |
|    total_timesteps | 42393600 |
---------------------------------
Eval num_timesteps=42394290, episode_reward=0.06 +/- 0.98
Episode length: 30.01 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 42394290    |
| train/                  |             |
|    approx_kl            | 0.041412454 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000173    |
|    loss                 | 0.0896      |
|    n_updates            | 3450        |
|    policy_gradient_loss | -0.00115    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 691      |
|    time_elapsed    | 265277   |
|    total_timesteps | 42455040 |
---------------------------------
Eval num_timesteps=42455731, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 42455731   |
| train/                  |            |
|    approx_kl            | 0.04117977 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.655     |
|    explained_variance   | 0.375      |
|    learning_rate        | 0.000173   |
|    loss                 | 0.153      |
|    n_updates            | 3455       |
|    policy_gradient_loss | -0.000437  |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 692      |
|    time_elapsed    | 265627   |
|    total_timesteps | 42516480 |
---------------------------------
Eval num_timesteps=42517172, episode_reward=0.02 +/- 0.98
Episode length: 29.89 +/- 1.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.022     |
| time/                   |           |
|    total_timesteps      | 42517172  |
| train/                  |           |
|    approx_kl            | 0.0422611 |
|    clip_fraction        | 0.223     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.646    |
|    explained_variance   | 0.368     |
|    learning_rate        | 0.000172  |
|    loss                 | 0.0588    |
|    n_updates            | 3460      |
|    policy_gradient_loss | -0.000469 |
|    value_loss           | 0.239     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 693      |
|    time_elapsed    | 265976   |
|    total_timesteps | 42577920 |
---------------------------------
Eval num_timesteps=42578613, episode_reward=0.07 +/- 0.98
Episode length: 29.90 +/- 1.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 42578613    |
| train/                  |             |
|    approx_kl            | 0.042379405 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.000172    |
|    loss                 | 0.0348      |
|    n_updates            | 3465        |
|    policy_gradient_loss | -0.000762   |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 694      |
|    time_elapsed    | 266325   |
|    total_timesteps | 42639360 |
---------------------------------
Eval num_timesteps=42640054, episode_reward=0.10 +/- 0.98
Episode length: 29.90 +/- 1.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.098      |
| time/                   |            |
|    total_timesteps      | 42640054   |
| train/                  |            |
|    approx_kl            | 0.04078598 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.643     |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.000172   |
|    loss                 | 0.0509     |
|    n_updates            | 3470       |
|    policy_gradient_loss | -0.0012    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 695      |
|    time_elapsed    | 266674   |
|    total_timesteps | 42700800 |
---------------------------------
Eval num_timesteps=42701495, episode_reward=0.14 +/- 0.97
Episode length: 29.89 +/- 1.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 42701495    |
| train/                  |             |
|    approx_kl            | 0.040085975 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000172    |
|    loss                 | 0.124       |
|    n_updates            | 3475        |
|    policy_gradient_loss | -0.00121    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 696      |
|    time_elapsed    | 267023   |
|    total_timesteps | 42762240 |
---------------------------------
Eval num_timesteps=42762936, episode_reward=0.16 +/- 0.97
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.158      |
| time/                   |            |
|    total_timesteps      | 42762936   |
| train/                  |            |
|    approx_kl            | 0.03992248 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.651     |
|    explained_variance   | 0.371      |
|    learning_rate        | 0.000172   |
|    loss                 | 0.122      |
|    n_updates            | 3480       |
|    policy_gradient_loss | -0.0011    |
|    value_loss           | 0.233      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.158
SELFPLAY: new best model, bumping up generation to 39
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.19    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 697      |
|    time_elapsed    | 267372   |
|    total_timesteps | 42823680 |
---------------------------------
Eval num_timesteps=42824377, episode_reward=0.04 +/- 0.99
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 42824377   |
| train/                  |            |
|    approx_kl            | 0.04563408 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.7       |
|    explained_variance   | 0.381      |
|    learning_rate        | 0.000172   |
|    loss                 | 0.0571     |
|    n_updates            | 3485       |
|    policy_gradient_loss | -0.00177   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 698      |
|    time_elapsed    | 267723   |
|    total_timesteps | 42885120 |
---------------------------------
Eval num_timesteps=42885818, episode_reward=0.06 +/- 0.98
Episode length: 29.94 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 42885818    |
| train/                  |             |
|    approx_kl            | 0.041900177 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.000171    |
|    loss                 | 0.0623      |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.000878   |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 699      |
|    time_elapsed    | 268074   |
|    total_timesteps | 42946560 |
---------------------------------
Eval num_timesteps=42947259, episode_reward=0.06 +/- 0.99
Episode length: 29.95 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 42947259    |
| train/                  |             |
|    approx_kl            | 0.043475784 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.000171    |
|    loss                 | 0.142       |
|    n_updates            | 3495        |
|    policy_gradient_loss | -0.000838   |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 700      |
|    time_elapsed    | 268424   |
|    total_timesteps | 43008000 |
---------------------------------
Eval num_timesteps=43008700, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 43008700    |
| train/                  |             |
|    approx_kl            | 0.042952914 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000171    |
|    loss                 | 0.15        |
|    n_updates            | 3500        |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 701      |
|    time_elapsed    | 268774   |
|    total_timesteps | 43069440 |
---------------------------------
Eval num_timesteps=43070141, episode_reward=-0.01 +/- 0.98
Episode length: 29.93 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 43070141    |
| train/                  |             |
|    approx_kl            | 0.040951524 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.000171    |
|    loss                 | 0.0349      |
|    n_updates            | 3505        |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 702      |
|    time_elapsed    | 269123   |
|    total_timesteps | 43130880 |
---------------------------------
Eval num_timesteps=43131582, episode_reward=-0.02 +/- 0.98
Episode length: 29.95 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.024      |
| time/                   |             |
|    total_timesteps      | 43131582    |
| train/                  |             |
|    approx_kl            | 0.043171607 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.000171    |
|    loss                 | 0.0562      |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 703      |
|    time_elapsed    | 269473   |
|    total_timesteps | 43192320 |
---------------------------------
Eval num_timesteps=43193023, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 43193023    |
| train/                  |             |
|    approx_kl            | 0.041571144 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.00017     |
|    loss                 | 0.0694      |
|    n_updates            | 3515        |
|    policy_gradient_loss | -0.000994   |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 704      |
|    time_elapsed    | 269823   |
|    total_timesteps | 43253760 |
---------------------------------
Eval num_timesteps=43254464, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 43254464    |
| train/                  |             |
|    approx_kl            | 0.039997958 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.68       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.00017     |
|    loss                 | 0.0905      |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 705      |
|    time_elapsed    | 270172   |
|    total_timesteps | 43315200 |
---------------------------------
Eval num_timesteps=43315905, episode_reward=0.05 +/- 0.98
Episode length: 29.92 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 43315905   |
| train/                  |            |
|    approx_kl            | 0.04163781 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.683     |
|    explained_variance   | 0.369      |
|    learning_rate        | 0.00017    |
|    loss                 | 0.0517     |
|    n_updates            | 3525       |
|    policy_gradient_loss | -0.00135   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 706      |
|    time_elapsed    | 270521   |
|    total_timesteps | 43376640 |
---------------------------------
Eval num_timesteps=43377346, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 43377346    |
| train/                  |             |
|    approx_kl            | 0.042679667 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.678      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.00017     |
|    loss                 | 0.0933      |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 707      |
|    time_elapsed    | 270870   |
|    total_timesteps | 43438080 |
---------------------------------
Eval num_timesteps=43438787, episode_reward=0.07 +/- 0.99
Episode length: 29.97 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 43438787   |
| train/                  |            |
|    approx_kl            | 0.04097968 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.676     |
|    explained_variance   | 0.381      |
|    learning_rate        | 0.00017    |
|    loss                 | 0.104      |
|    n_updates            | 3535       |
|    policy_gradient_loss | -0.00173   |
|    value_loss           | 0.232      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 708      |
|    time_elapsed    | 271220   |
|    total_timesteps | 43499520 |
---------------------------------
Eval num_timesteps=43500228, episode_reward=0.10 +/- 0.99
Episode length: 29.92 +/- 1.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 43500228    |
| train/                  |             |
|    approx_kl            | 0.040264364 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.00017     |
|    loss                 | 0.123       |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.000694   |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 709      |
|    time_elapsed    | 271570   |
|    total_timesteps | 43560960 |
---------------------------------
Eval num_timesteps=43561669, episode_reward=0.02 +/- 0.98
Episode length: 29.91 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.02      |
| time/                   |           |
|    total_timesteps      | 43561669  |
| train/                  |           |
|    approx_kl            | 0.0388119 |
|    clip_fraction        | 0.223     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.679    |
|    explained_variance   | 0.351     |
|    learning_rate        | 0.000169  |
|    loss                 | 0.132     |
|    n_updates            | 3545      |
|    policy_gradient_loss | -0.000784 |
|    value_loss           | 0.24      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 710      |
|    time_elapsed    | 271921   |
|    total_timesteps | 43622400 |
---------------------------------
Eval num_timesteps=43623110, episode_reward=0.05 +/- 0.98
Episode length: 29.86 +/- 1.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 43623110   |
| train/                  |            |
|    approx_kl            | 0.04084986 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.68      |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.000169   |
|    loss                 | 0.119      |
|    n_updates            | 3550       |
|    policy_gradient_loss | -0.00197   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 711      |
|    time_elapsed    | 272270   |
|    total_timesteps | 43683840 |
---------------------------------
Eval num_timesteps=43684551, episode_reward=-0.02 +/- 0.99
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 43684551    |
| train/                  |             |
|    approx_kl            | 0.041759048 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.677      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.000169    |
|    loss                 | 0.0898      |
|    n_updates            | 3555        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 712      |
|    time_elapsed    | 272619   |
|    total_timesteps | 43745280 |
---------------------------------
Eval num_timesteps=43745992, episode_reward=0.17 +/- 0.97
Episode length: 29.90 +/- 1.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.17        |
| time/                   |             |
|    total_timesteps      | 43745992    |
| train/                  |             |
|    approx_kl            | 0.039366644 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.667      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.000169    |
|    loss                 | 0.131       |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.17
SELFPLAY: new best model, bumping up generation to 40
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 713      |
|    time_elapsed    | 272968   |
|    total_timesteps | 43806720 |
---------------------------------
Eval num_timesteps=43807433, episode_reward=0.00 +/- 0.99
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.004      |
| time/                   |            |
|    total_timesteps      | 43807433   |
| train/                  |            |
|    approx_kl            | 0.04279185 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.722     |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.000169   |
|    loss                 | 0.057      |
|    n_updates            | 3565       |
|    policy_gradient_loss | -0.002     |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 714      |
|    time_elapsed    | 273318   |
|    total_timesteps | 43868160 |
---------------------------------
Eval num_timesteps=43868874, episode_reward=-0.04 +/- 0.99
Episode length: 29.94 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.038     |
| time/                   |            |
|    total_timesteps      | 43868874   |
| train/                  |            |
|    approx_kl            | 0.04120241 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.718     |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.000168   |
|    loss                 | 0.0296     |
|    n_updates            | 3570       |
|    policy_gradient_loss | -0.00195   |
|    value_loss           | 0.246      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 715      |
|    time_elapsed    | 273668   |
|    total_timesteps | 43929600 |
---------------------------------
Eval num_timesteps=43930315, episode_reward=-0.05 +/- 0.98
Episode length: 29.91 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.054      |
| time/                   |             |
|    total_timesteps      | 43930315    |
| train/                  |             |
|    approx_kl            | 0.040062733 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.362       |
|    learning_rate        | 0.000168    |
|    loss                 | 0.05        |
|    n_updates            | 3575        |
|    policy_gradient_loss | -0.00237    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 716      |
|    time_elapsed    | 274017   |
|    total_timesteps | 43991040 |
---------------------------------
Eval num_timesteps=43991756, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 43991756    |
| train/                  |             |
|    approx_kl            | 0.042476576 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000168    |
|    loss                 | 0.0802      |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 0.246       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 717      |
|    time_elapsed    | 274366   |
|    total_timesteps | 44052480 |
---------------------------------
Eval num_timesteps=44053197, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 1.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.074      |
| time/                   |            |
|    total_timesteps      | 44053197   |
| train/                  |            |
|    approx_kl            | 0.04065114 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000168   |
|    loss                 | 0.159      |
|    n_updates            | 3585       |
|    policy_gradient_loss | -0.00185   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 718      |
|    time_elapsed    | 274715   |
|    total_timesteps | 44113920 |
---------------------------------
Eval num_timesteps=44114638, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 44114638    |
| train/                  |             |
|    approx_kl            | 0.041589532 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000168    |
|    loss                 | 0.0686      |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00214    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 719      |
|    time_elapsed    | 275066   |
|    total_timesteps | 44175360 |
---------------------------------
Eval num_timesteps=44176079, episode_reward=0.04 +/- 0.99
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 44176079    |
| train/                  |             |
|    approx_kl            | 0.040944364 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.000167    |
|    loss                 | 0.1         |
|    n_updates            | 3595        |
|    policy_gradient_loss | -0.00163    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 720      |
|    time_elapsed    | 275418   |
|    total_timesteps | 44236800 |
---------------------------------
Eval num_timesteps=44237520, episode_reward=0.06 +/- 0.99
Episode length: 30.01 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.06      |
| time/                   |           |
|    total_timesteps      | 44237520  |
| train/                  |           |
|    approx_kl            | 0.0421812 |
|    clip_fraction        | 0.234     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.718    |
|    explained_variance   | 0.36      |
|    learning_rate        | 0.000167  |
|    loss                 | 0.0556    |
|    n_updates            | 3600      |
|    policy_gradient_loss | -0.00132  |
|    value_loss           | 0.239     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 721      |
|    time_elapsed    | 275768   |
|    total_timesteps | 44298240 |
---------------------------------
Eval num_timesteps=44298961, episode_reward=-0.04 +/- 0.98
Episode length: 29.93 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.038      |
| time/                   |             |
|    total_timesteps      | 44298961    |
| train/                  |             |
|    approx_kl            | 0.041593313 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000167    |
|    loss                 | 0.108       |
|    n_updates            | 3605        |
|    policy_gradient_loss | -0.001      |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 160      |
|    iterations      | 722      |
|    time_elapsed    | 276117   |
|    total_timesteps | 44359680 |
---------------------------------
Eval num_timesteps=44360402, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 44360402    |
| train/                  |             |
|    approx_kl            | 0.042535253 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000167    |
|    loss                 | 0.118       |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 723      |
|    time_elapsed    | 276467   |
|    total_timesteps | 44421120 |
---------------------------------
Eval num_timesteps=44421843, episode_reward=0.02 +/- 0.98
Episode length: 29.94 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 44421843    |
| train/                  |             |
|    approx_kl            | 0.043438975 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000167    |
|    loss                 | 0.0485      |
|    n_updates            | 3615        |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 724      |
|    time_elapsed    | 276816   |
|    total_timesteps | 44482560 |
---------------------------------
Eval num_timesteps=44483284, episode_reward=0.01 +/- 0.99
Episode length: 29.95 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 44483284    |
| train/                  |             |
|    approx_kl            | 0.042603698 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000167    |
|    loss                 | 0.0706      |
|    n_updates            | 3620        |
|    policy_gradient_loss | -0.00237    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 725      |
|    time_elapsed    | 277165   |
|    total_timesteps | 44544000 |
---------------------------------
Eval num_timesteps=44544725, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.124       |
| time/                   |             |
|    total_timesteps      | 44544725    |
| train/                  |             |
|    approx_kl            | 0.040096737 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000166    |
|    loss                 | 0.111       |
|    n_updates            | 3625        |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 726      |
|    time_elapsed    | 277515   |
|    total_timesteps | 44605440 |
---------------------------------
Eval num_timesteps=44606166, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 44606166    |
| train/                  |             |
|    approx_kl            | 0.039223075 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000166    |
|    loss                 | 0.11        |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.00299    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 727      |
|    time_elapsed    | 277864   |
|    total_timesteps | 44666880 |
---------------------------------
Eval num_timesteps=44667607, episode_reward=-0.01 +/- 0.99
Episode length: 29.92 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 44667607    |
| train/                  |             |
|    approx_kl            | 0.041531537 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000166    |
|    loss                 | 0.146       |
|    n_updates            | 3635        |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 728      |
|    time_elapsed    | 278213   |
|    total_timesteps | 44728320 |
---------------------------------
Eval num_timesteps=44729048, episode_reward=0.09 +/- 0.98
Episode length: 29.94 +/- 1.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.09      |
| time/                   |           |
|    total_timesteps      | 44729048  |
| train/                  |           |
|    approx_kl            | 0.0409117 |
|    clip_fraction        | 0.227     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.714    |
|    explained_variance   | 0.368     |
|    learning_rate        | 0.000166  |
|    loss                 | 0.108     |
|    n_updates            | 3640      |
|    policy_gradient_loss | -0.00285  |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 729      |
|    time_elapsed    | 278562   |
|    total_timesteps | 44789760 |
---------------------------------
Eval num_timesteps=44790489, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 1.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 44790489    |
| train/                  |             |
|    approx_kl            | 0.041185953 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.000166    |
|    loss                 | 0.186       |
|    n_updates            | 3645        |
|    policy_gradient_loss | -0.00286    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 730      |
|    time_elapsed    | 278911   |
|    total_timesteps | 44851200 |
---------------------------------
Eval num_timesteps=44851930, episode_reward=-0.07 +/- 0.98
Episode length: 29.94 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.066      |
| time/                   |             |
|    total_timesteps      | 44851930    |
| train/                  |             |
|    approx_kl            | 0.040367313 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.000165    |
|    loss                 | 0.144       |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 731      |
|    time_elapsed    | 279263   |
|    total_timesteps | 44912640 |
---------------------------------
Eval num_timesteps=44913371, episode_reward=0.05 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 44913371    |
| train/                  |             |
|    approx_kl            | 0.041845933 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000165    |
|    loss                 | 0.0528      |
|    n_updates            | 3655        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 732      |
|    time_elapsed    | 279612   |
|    total_timesteps | 44974080 |
---------------------------------
Eval num_timesteps=44974812, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 44974812    |
| train/                  |             |
|    approx_kl            | 0.041342862 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000165    |
|    loss                 | 0.102       |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 733      |
|    time_elapsed    | 279962   |
|    total_timesteps | 45035520 |
---------------------------------
Eval num_timesteps=45036253, episode_reward=-0.01 +/- 0.98
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 45036253    |
| train/                  |             |
|    approx_kl            | 0.039805647 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000165    |
|    loss                 | 0.189       |
|    n_updates            | 3665        |
|    policy_gradient_loss | -0.00244    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 734      |
|    time_elapsed    | 280311   |
|    total_timesteps | 45096960 |
---------------------------------
Eval num_timesteps=45097694, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 45097694    |
| train/                  |             |
|    approx_kl            | 0.040103793 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000165    |
|    loss                 | 0.117       |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00199    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 735      |
|    time_elapsed    | 280661   |
|    total_timesteps | 45158400 |
---------------------------------
Eval num_timesteps=45159135, episode_reward=0.03 +/- 0.98
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 45159135    |
| train/                  |             |
|    approx_kl            | 0.042192064 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000165    |
|    loss                 | 0.0819      |
|    n_updates            | 3675        |
|    policy_gradient_loss | -0.0013     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 736      |
|    time_elapsed    | 281010   |
|    total_timesteps | 45219840 |
---------------------------------
Eval num_timesteps=45220576, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.086     |
| time/                   |           |
|    total_timesteps      | 45220576  |
| train/                  |           |
|    approx_kl            | 0.0394868 |
|    clip_fraction        | 0.226     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.721    |
|    explained_variance   | 0.352     |
|    learning_rate        | 0.000164  |
|    loss                 | 0.161     |
|    n_updates            | 3680      |
|    policy_gradient_loss | -0.00156  |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 737      |
|    time_elapsed    | 281360   |
|    total_timesteps | 45281280 |
---------------------------------
Eval num_timesteps=45282017, episode_reward=0.05 +/- 0.98
Episode length: 29.94 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 45282017    |
| train/                  |             |
|    approx_kl            | 0.041162506 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.059       |
|    n_updates            | 3685        |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 738      |
|    time_elapsed    | 281708   |
|    total_timesteps | 45342720 |
---------------------------------
Eval num_timesteps=45343458, episode_reward=0.05 +/- 0.99
Episode length: 29.98 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 45343458    |
| train/                  |             |
|    approx_kl            | 0.040660698 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.107       |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 739      |
|    time_elapsed    | 282057   |
|    total_timesteps | 45404160 |
---------------------------------
Eval num_timesteps=45404899, episode_reward=0.02 +/- 0.99
Episode length: 29.95 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 45404899    |
| train/                  |             |
|    approx_kl            | 0.041369755 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0632      |
|    n_updates            | 3695        |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 740      |
|    time_elapsed    | 282406   |
|    total_timesteps | 45465600 |
---------------------------------
Eval num_timesteps=45466340, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 45466340    |
| train/                  |             |
|    approx_kl            | 0.042605344 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0771      |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 741      |
|    time_elapsed    | 282757   |
|    total_timesteps | 45527040 |
---------------------------------
Eval num_timesteps=45527781, episode_reward=0.09 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 45527781    |
| train/                  |             |
|    approx_kl            | 0.043008365 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.377       |
|    learning_rate        | 0.000163    |
|    loss                 | 0.169       |
|    n_updates            | 3705        |
|    policy_gradient_loss | -0.00212    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 742      |
|    time_elapsed    | 283107   |
|    total_timesteps | 45588480 |
---------------------------------
Eval num_timesteps=45589222, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.106     |
| time/                   |           |
|    total_timesteps      | 45589222  |
| train/                  |           |
|    approx_kl            | 0.0396801 |
|    clip_fraction        | 0.227     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.706    |
|    explained_variance   | 0.358     |
|    learning_rate        | 0.000163  |
|    loss                 | 0.0903    |
|    n_updates            | 3710      |
|    policy_gradient_loss | -0.000992 |
|    value_loss           | 0.236     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 743      |
|    time_elapsed    | 283457   |
|    total_timesteps | 45649920 |
---------------------------------
Eval num_timesteps=45650663, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 45650663   |
| train/                  |            |
|    approx_kl            | 0.04051296 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000163   |
|    loss                 | 0.112      |
|    n_updates            | 3715       |
|    policy_gradient_loss | -0.000959  |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 744      |
|    time_elapsed    | 283806   |
|    total_timesteps | 45711360 |
---------------------------------
Eval num_timesteps=45712104, episode_reward=0.05 +/- 0.99
Episode length: 29.96 +/- 0.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 45712104   |
| train/                  |            |
|    approx_kl            | 0.04281027 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.7       |
|    explained_variance   | 0.331      |
|    learning_rate        | 0.000163   |
|    loss                 | 0.2        |
|    n_updates            | 3720       |
|    policy_gradient_loss | -0.00246   |
|    value_loss           | 0.244      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 745      |
|    time_elapsed    | 284155   |
|    total_timesteps | 45772800 |
---------------------------------
Eval num_timesteps=45773545, episode_reward=0.12 +/- 0.98
Episode length: 30.03 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 45773545    |
| train/                  |             |
|    approx_kl            | 0.039416965 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000163    |
|    loss                 | 0.185       |
|    n_updates            | 3725        |
|    policy_gradient_loss | -0.000787   |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 746      |
|    time_elapsed    | 284505   |
|    total_timesteps | 45834240 |
---------------------------------
Eval num_timesteps=45834986, episode_reward=0.09 +/- 0.99
Episode length: 29.97 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 45834986    |
| train/                  |             |
|    approx_kl            | 0.039784525 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000162    |
|    loss                 | 0.104       |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 747      |
|    time_elapsed    | 284854   |
|    total_timesteps | 45895680 |
---------------------------------
Eval num_timesteps=45896427, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 45896427    |
| train/                  |             |
|    approx_kl            | 0.039798405 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000162    |
|    loss                 | 0.0844      |
|    n_updates            | 3735        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 748      |
|    time_elapsed    | 285203   |
|    total_timesteps | 45957120 |
---------------------------------
Eval num_timesteps=45957868, episode_reward=0.13 +/- 0.98
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 45957868    |
| train/                  |             |
|    approx_kl            | 0.039729755 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.709      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000162    |
|    loss                 | 0.136       |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.00233    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 749      |
|    time_elapsed    | 285552   |
|    total_timesteps | 46018560 |
---------------------------------
Eval num_timesteps=46019309, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 46019309    |
| train/                  |             |
|    approx_kl            | 0.040147375 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000162    |
|    loss                 | 0.0867      |
|    n_updates            | 3745        |
|    policy_gradient_loss | -0.00255    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 750      |
|    time_elapsed    | 285900   |
|    total_timesteps | 46080000 |
---------------------------------
Eval num_timesteps=46080750, episode_reward=0.15 +/- 0.98
Episode length: 30.04 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.152       |
| time/                   |             |
|    total_timesteps      | 46080750    |
| train/                  |             |
|    approx_kl            | 0.040680483 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000162    |
|    loss                 | 0.0915      |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 0.243       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.152
SELFPLAY: new best model, bumping up generation to 41
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 751      |
|    time_elapsed    | 286251   |
|    total_timesteps | 46141440 |
---------------------------------
Eval num_timesteps=46142191, episode_reward=0.04 +/- 0.99
Episode length: 29.96 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 46142191    |
| train/                  |             |
|    approx_kl            | 0.042201176 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000162    |
|    loss                 | 0.0544      |
|    n_updates            | 3755        |
|    policy_gradient_loss | -0.00199    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 752      |
|    time_elapsed    | 286602   |
|    total_timesteps | 46202880 |
---------------------------------
Eval num_timesteps=46203632, episode_reward=0.05 +/- 0.99
Episode length: 29.96 +/- 0.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 46203632   |
| train/                  |            |
|    approx_kl            | 0.04019401 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.000161   |
|    loss                 | 0.0717     |
|    n_updates            | 3760       |
|    policy_gradient_loss | -0.00258   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 753      |
|    time_elapsed    | 286952   |
|    total_timesteps | 46264320 |
---------------------------------
Eval num_timesteps=46265073, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 46265073    |
| train/                  |             |
|    approx_kl            | 0.042001303 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.000161    |
|    loss                 | 0.0668      |
|    n_updates            | 3765        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 754      |
|    time_elapsed    | 287302   |
|    total_timesteps | 46325760 |
---------------------------------
Eval num_timesteps=46326514, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 1.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.008      |
| time/                   |            |
|    total_timesteps      | 46326514   |
| train/                  |            |
|    approx_kl            | 0.04181326 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000161   |
|    loss                 | 0.0978     |
|    n_updates            | 3770       |
|    policy_gradient_loss | -0.00344   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 755      |
|    time_elapsed    | 287651   |
|    total_timesteps | 46387200 |
---------------------------------
Eval num_timesteps=46387955, episode_reward=-0.02 +/- 0.99
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 46387955    |
| train/                  |             |
|    approx_kl            | 0.040302105 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000161    |
|    loss                 | 0.169       |
|    n_updates            | 3775        |
|    policy_gradient_loss | -0.0024     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.22    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 756      |
|    time_elapsed    | 288000   |
|    total_timesteps | 46448640 |
---------------------------------
Eval num_timesteps=46449396, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 46449396    |
| train/                  |             |
|    approx_kl            | 0.040345397 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000161    |
|    loss                 | 0.061       |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00282    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 757      |
|    time_elapsed    | 288350   |
|    total_timesteps | 46510080 |
---------------------------------
Eval num_timesteps=46510837, episode_reward=0.05 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 46510837    |
| train/                  |             |
|    approx_kl            | 0.040724356 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.00016     |
|    loss                 | 0.111       |
|    n_updates            | 3785        |
|    policy_gradient_loss | -0.00175    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 758      |
|    time_elapsed    | 288699   |
|    total_timesteps | 46571520 |
---------------------------------
Eval num_timesteps=46572278, episode_reward=0.04 +/- 0.98
Episode length: 29.91 +/- 1.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 46572278   |
| train/                  |            |
|    approx_kl            | 0.04173111 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.763     |
|    explained_variance   | 0.354      |
|    learning_rate        | 0.00016    |
|    loss                 | 0.0758     |
|    n_updates            | 3790       |
|    policy_gradient_loss | -0.00235   |
|    value_loss           | 0.245      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 759      |
|    time_elapsed    | 289048   |
|    total_timesteps | 46632960 |
---------------------------------
Eval num_timesteps=46633719, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 46633719    |
| train/                  |             |
|    approx_kl            | 0.041961823 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.00016     |
|    loss                 | 0.0799      |
|    n_updates            | 3795        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 760      |
|    time_elapsed    | 289396   |
|    total_timesteps | 46694400 |
---------------------------------
Eval num_timesteps=46695160, episode_reward=0.06 +/- 0.99
Episode length: 29.91 +/- 1.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 46695160    |
| train/                  |             |
|    approx_kl            | 0.041462682 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.00016     |
|    loss                 | 0.0609      |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.00353    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 761      |
|    time_elapsed    | 289745   |
|    total_timesteps | 46755840 |
---------------------------------
Eval num_timesteps=46756601, episode_reward=0.03 +/- 0.99
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 46756601    |
| train/                  |             |
|    approx_kl            | 0.040891632 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.00016     |
|    loss                 | 0.116       |
|    n_updates            | 3805        |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 762      |
|    time_elapsed    | 290096   |
|    total_timesteps | 46817280 |
---------------------------------
Eval num_timesteps=46818042, episode_reward=0.03 +/- 0.98
Episode length: 30.02 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 46818042    |
| train/                  |             |
|    approx_kl            | 0.042093236 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.00016     |
|    loss                 | 0.0945      |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.00247    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 763      |
|    time_elapsed    | 290447   |
|    total_timesteps | 46878720 |
---------------------------------
Eval num_timesteps=46879483, episode_reward=0.07 +/- 0.98
Episode length: 29.93 +/- 0.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 46879483    |
| train/                  |             |
|    approx_kl            | 0.041742064 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000159    |
|    loss                 | 0.135       |
|    n_updates            | 3815        |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 764      |
|    time_elapsed    | 290797   |
|    total_timesteps | 46940160 |
---------------------------------
Eval num_timesteps=46940924, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 46940924    |
| train/                  |             |
|    approx_kl            | 0.038486376 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000159    |
|    loss                 | 0.099       |
|    n_updates            | 3820        |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 765      |
|    time_elapsed    | 291146   |
|    total_timesteps | 47001600 |
---------------------------------
Eval num_timesteps=47002365, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.014     |
| time/                   |           |
|    total_timesteps      | 47002365  |
| train/                  |           |
|    approx_kl            | 0.0402952 |
|    clip_fraction        | 0.234     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.756    |
|    explained_variance   | 0.347     |
|    learning_rate        | 0.000159  |
|    loss                 | 0.0501    |
|    n_updates            | 3825      |
|    policy_gradient_loss | -0.00228  |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 766      |
|    time_elapsed    | 291496   |
|    total_timesteps | 47063040 |
---------------------------------
Eval num_timesteps=47063806, episode_reward=-0.05 +/- 0.99
Episode length: 29.94 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.046      |
| time/                   |             |
|    total_timesteps      | 47063806    |
| train/                  |             |
|    approx_kl            | 0.040838882 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000159    |
|    loss                 | 0.0847      |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.00341    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 767      |
|    time_elapsed    | 291845   |
|    total_timesteps | 47124480 |
---------------------------------
Eval num_timesteps=47125247, episode_reward=0.13 +/- 0.97
Episode length: 29.96 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.132      |
| time/                   |            |
|    total_timesteps      | 47125247   |
| train/                  |            |
|    approx_kl            | 0.03979202 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.761     |
|    explained_variance   | 0.356      |
|    learning_rate        | 0.000159   |
|    loss                 | 0.0569     |
|    n_updates            | 3835       |
|    policy_gradient_loss | -0.00216   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 768      |
|    time_elapsed    | 292196   |
|    total_timesteps | 47185920 |
---------------------------------
Eval num_timesteps=47186688, episode_reward=0.02 +/- 0.99
Episode length: 29.93 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 47186688    |
| train/                  |             |
|    approx_kl            | 0.040913057 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.000158    |
|    loss                 | 0.0392      |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 769      |
|    time_elapsed    | 292545   |
|    total_timesteps | 47247360 |
---------------------------------
Eval num_timesteps=47248129, episode_reward=-0.00 +/- 0.99
Episode length: 29.95 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 47248129   |
| train/                  |            |
|    approx_kl            | 0.03921988 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.000158   |
|    loss                 | 0.0499     |
|    n_updates            | 3845       |
|    policy_gradient_loss | -0.00314   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 770      |
|    time_elapsed    | 292894   |
|    total_timesteps | 47308800 |
---------------------------------
Eval num_timesteps=47309570, episode_reward=0.04 +/- 0.98
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 47309570    |
| train/                  |             |
|    approx_kl            | 0.040829163 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000158    |
|    loss                 | 0.0993      |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 771      |
|    time_elapsed    | 293243   |
|    total_timesteps | 47370240 |
---------------------------------
Eval num_timesteps=47371011, episode_reward=0.12 +/- 0.98
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.122      |
| time/                   |            |
|    total_timesteps      | 47371011   |
| train/                  |            |
|    approx_kl            | 0.04007203 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.000158   |
|    loss                 | 0.0534     |
|    n_updates            | 3855       |
|    policy_gradient_loss | -0.00313   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 772      |
|    time_elapsed    | 293593   |
|    total_timesteps | 47431680 |
---------------------------------
Eval num_timesteps=47432452, episode_reward=0.04 +/- 0.99
Episode length: 29.94 +/- 0.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 47432452   |
| train/                  |            |
|    approx_kl            | 0.04108011 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.37       |
|    learning_rate        | 0.000158   |
|    loss                 | 0.0904     |
|    n_updates            | 3860       |
|    policy_gradient_loss | -0.00314   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 773      |
|    time_elapsed    | 293944   |
|    total_timesteps | 47493120 |
---------------------------------
Eval num_timesteps=47493893, episode_reward=0.05 +/- 0.98
Episode length: 29.95 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 47493893    |
| train/                  |             |
|    approx_kl            | 0.038507447 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000158    |
|    loss                 | 0.102       |
|    n_updates            | 3865        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 774      |
|    time_elapsed    | 294295   |
|    total_timesteps | 47554560 |
---------------------------------
Eval num_timesteps=47555334, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 47555334    |
| train/                  |             |
|    approx_kl            | 0.040246356 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000157    |
|    loss                 | 0.101       |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00212    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 775      |
|    time_elapsed    | 294644   |
|    total_timesteps | 47616000 |
---------------------------------
Eval num_timesteps=47616775, episode_reward=0.01 +/- 0.99
Episode length: 29.92 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 47616775    |
| train/                  |             |
|    approx_kl            | 0.039899763 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.742      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000157    |
|    loss                 | 0.0891      |
|    n_updates            | 3875        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 776      |
|    time_elapsed    | 294994   |
|    total_timesteps | 47677440 |
---------------------------------
Eval num_timesteps=47678216, episode_reward=0.20 +/- 0.97
Episode length: 29.96 +/- 1.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.202      |
| time/                   |            |
|    total_timesteps      | 47678216   |
| train/                  |            |
|    approx_kl            | 0.03946317 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.000157   |
|    loss                 | 0.105      |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.0038    |
|    value_loss           | 0.236      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.202
SELFPLAY: new best model, bumping up generation to 42
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 777      |
|    time_elapsed    | 295343   |
|    total_timesteps | 47738880 |
---------------------------------
Eval num_timesteps=47739657, episode_reward=0.05 +/- 0.99
Episode length: 29.93 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 47739657    |
| train/                  |             |
|    approx_kl            | 0.043183472 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.000157    |
|    loss                 | 0.0728      |
|    n_updates            | 3885        |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 778      |
|    time_elapsed    | 295693   |
|    total_timesteps | 47800320 |
---------------------------------
Eval num_timesteps=47801098, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 47801098    |
| train/                  |             |
|    approx_kl            | 0.041793417 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.781      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000157    |
|    loss                 | 0.138       |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 779      |
|    time_elapsed    | 296042   |
|    total_timesteps | 47861760 |
---------------------------------
Eval num_timesteps=47862539, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 47862539    |
| train/                  |             |
|    approx_kl            | 0.039605904 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.787      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000156    |
|    loss                 | 0.0795      |
|    n_updates            | 3895        |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 780      |
|    time_elapsed    | 296391   |
|    total_timesteps | 47923200 |
---------------------------------
Eval num_timesteps=47923980, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 47923980    |
| train/                  |             |
|    approx_kl            | 0.041426763 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.000156    |
|    loss                 | 0.0761      |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 781      |
|    time_elapsed    | 296740   |
|    total_timesteps | 47984640 |
---------------------------------
Eval num_timesteps=47985421, episode_reward=0.01 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 47985421    |
| train/                  |             |
|    approx_kl            | 0.042990915 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.000156    |
|    loss                 | 0.133       |
|    n_updates            | 3905        |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 782      |
|    time_elapsed    | 297089   |
|    total_timesteps | 48046080 |
---------------------------------
Eval num_timesteps=48046862, episode_reward=-0.01 +/- 0.98
Episode length: 29.96 +/- 1.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 48046862    |
| train/                  |             |
|    approx_kl            | 0.041501388 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000156    |
|    loss                 | 0.106       |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 783      |
|    time_elapsed    | 297439   |
|    total_timesteps | 48107520 |
---------------------------------
Eval num_timesteps=48108303, episode_reward=-0.02 +/- 0.99
Episode length: 29.94 +/- 1.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 48108303    |
| train/                  |             |
|    approx_kl            | 0.041822422 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000156    |
|    loss                 | 0.118       |
|    n_updates            | 3915        |
|    policy_gradient_loss | -0.00299    |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 784      |
|    time_elapsed    | 297790   |
|    total_timesteps | 48168960 |
---------------------------------
Eval num_timesteps=48169744, episode_reward=0.04 +/- 0.99
Episode length: 29.96 +/- 0.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 48169744   |
| train/                  |            |
|    approx_kl            | 0.03876933 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.782     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000155   |
|    loss                 | 0.175      |
|    n_updates            | 3920       |
|    policy_gradient_loss | -0.00234   |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 785      |
|    time_elapsed    | 298141   |
|    total_timesteps | 48230400 |
---------------------------------
Eval num_timesteps=48231185, episode_reward=0.00 +/- 0.99
Episode length: 29.96 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 48231185    |
| train/                  |             |
|    approx_kl            | 0.039180677 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000155    |
|    loss                 | 0.159       |
|    n_updates            | 3925        |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 786      |
|    time_elapsed    | 298490   |
|    total_timesteps | 48291840 |
---------------------------------
Eval num_timesteps=48292626, episode_reward=0.02 +/- 0.98
Episode length: 29.90 +/- 1.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.022      |
| time/                   |            |
|    total_timesteps      | 48292626   |
| train/                  |            |
|    approx_kl            | 0.04061588 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.776     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000155   |
|    loss                 | 0.121      |
|    n_updates            | 3930       |
|    policy_gradient_loss | -0.00286   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 787      |
|    time_elapsed    | 298840   |
|    total_timesteps | 48353280 |
---------------------------------
Eval num_timesteps=48354067, episode_reward=-0.02 +/- 0.99
Episode length: 29.90 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.024     |
| time/                   |            |
|    total_timesteps      | 48354067   |
| train/                  |            |
|    approx_kl            | 0.03939205 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.766     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000155   |
|    loss                 | 0.0153     |
|    n_updates            | 3935       |
|    policy_gradient_loss | -0.00248   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 788      |
|    time_elapsed    | 299189   |
|    total_timesteps | 48414720 |
---------------------------------
Eval num_timesteps=48415508, episode_reward=0.00 +/- 0.99
Episode length: 29.97 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.004      |
| time/                   |            |
|    total_timesteps      | 48415508   |
| train/                  |            |
|    approx_kl            | 0.03998358 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.000155   |
|    loss                 | 0.0935     |
|    n_updates            | 3940       |
|    policy_gradient_loss | -0.00363   |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 789      |
|    time_elapsed    | 299539   |
|    total_timesteps | 48476160 |
---------------------------------
Eval num_timesteps=48476949, episode_reward=-0.01 +/- 0.98
Episode length: 29.91 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.012     |
| time/                   |            |
|    total_timesteps      | 48476949   |
| train/                  |            |
|    approx_kl            | 0.04017415 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.765     |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.000155   |
|    loss                 | 0.0719     |
|    n_updates            | 3945       |
|    policy_gradient_loss | -0.00273   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 790      |
|    time_elapsed    | 299889   |
|    total_timesteps | 48537600 |
---------------------------------
Eval num_timesteps=48538390, episode_reward=0.06 +/- 0.98
Episode length: 29.90 +/- 1.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 48538390   |
| train/                  |            |
|    approx_kl            | 0.03998296 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.354      |
|    learning_rate        | 0.000154   |
|    loss                 | 0.13       |
|    n_updates            | 3950       |
|    policy_gradient_loss | -0.00255   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 791      |
|    time_elapsed    | 300237   |
|    total_timesteps | 48599040 |
---------------------------------
Eval num_timesteps=48599831, episode_reward=-0.01 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 48599831    |
| train/                  |             |
|    approx_kl            | 0.040598717 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.000154    |
|    loss                 | 0.0938      |
|    n_updates            | 3955        |
|    policy_gradient_loss | -0.00354    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 792      |
|    time_elapsed    | 300586   |
|    total_timesteps | 48660480 |
---------------------------------
Eval num_timesteps=48661272, episode_reward=0.05 +/- 0.99
Episode length: 29.95 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 48661272    |
| train/                  |             |
|    approx_kl            | 0.039964158 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.755      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000154    |
|    loss                 | 0.169       |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 793      |
|    time_elapsed    | 300935   |
|    total_timesteps | 48721920 |
---------------------------------
Eval num_timesteps=48722713, episode_reward=0.02 +/- 0.99
Episode length: 29.91 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 48722713    |
| train/                  |             |
|    approx_kl            | 0.039841183 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000154    |
|    loss                 | 0.133       |
|    n_updates            | 3965        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.22    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 794      |
|    time_elapsed    | 301286   |
|    total_timesteps | 48783360 |
---------------------------------
Eval num_timesteps=48784154, episode_reward=0.04 +/- 0.98
Episode length: 29.95 +/- 1.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.044      |
| time/                   |            |
|    total_timesteps      | 48784154   |
| train/                  |            |
|    approx_kl            | 0.04062232 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.749     |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.000154   |
|    loss                 | 0.0824     |
|    n_updates            | 3970       |
|    policy_gradient_loss | -0.0028    |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 795      |
|    time_elapsed    | 301638   |
|    total_timesteps | 48844800 |
---------------------------------
Eval num_timesteps=48845595, episode_reward=0.08 +/- 0.98
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 48845595    |
| train/                  |             |
|    approx_kl            | 0.039078716 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.753      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000153    |
|    loss                 | 0.0472      |
|    n_updates            | 3975        |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 796      |
|    time_elapsed    | 301987   |
|    total_timesteps | 48906240 |
---------------------------------
Eval num_timesteps=48907036, episode_reward=0.07 +/- 0.98
Episode length: 29.95 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 48907036   |
| train/                  |            |
|    approx_kl            | 0.03951628 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.747     |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.000153   |
|    loss                 | 0.0883     |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.00262   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 797      |
|    time_elapsed    | 302336   |
|    total_timesteps | 48967680 |
---------------------------------
Eval num_timesteps=48968477, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 48968477    |
| train/                  |             |
|    approx_kl            | 0.038578622 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000153    |
|    loss                 | 0.0479      |
|    n_updates            | 3985        |
|    policy_gradient_loss | -0.00337    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 798      |
|    time_elapsed    | 302686   |
|    total_timesteps | 49029120 |
---------------------------------
Eval num_timesteps=49029918, episode_reward=0.02 +/- 0.99
Episode length: 29.92 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 49029918    |
| train/                  |             |
|    approx_kl            | 0.037458085 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.737      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000153    |
|    loss                 | 0.0799      |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 799      |
|    time_elapsed    | 303035   |
|    total_timesteps | 49090560 |
---------------------------------
Eval num_timesteps=49091359, episode_reward=-0.01 +/- 0.99
Episode length: 29.87 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 49091359    |
| train/                  |             |
|    approx_kl            | 0.039797783 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.000153    |
|    loss                 | 0.122       |
|    n_updates            | 3995        |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 800      |
|    time_elapsed    | 303385   |
|    total_timesteps | 49152000 |
---------------------------------
Eval num_timesteps=49152800, episode_reward=0.04 +/- 0.99
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 49152800    |
| train/                  |             |
|    approx_kl            | 0.039452143 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.735      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000153    |
|    loss                 | 0.0545      |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00354    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 801      |
|    time_elapsed    | 303734   |
|    total_timesteps | 49213440 |
---------------------------------
Eval num_timesteps=49214241, episode_reward=0.07 +/- 0.97
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.072      |
| time/                   |            |
|    total_timesteps      | 49214241   |
| train/                  |            |
|    approx_kl            | 0.03759215 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.734     |
|    explained_variance   | 0.344      |
|    learning_rate        | 0.000152   |
|    loss                 | 0.181      |
|    n_updates            | 4005       |
|    policy_gradient_loss | -0.00341   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 802      |
|    time_elapsed    | 304082   |
|    total_timesteps | 49274880 |
---------------------------------
Eval num_timesteps=49275682, episode_reward=0.09 +/- 0.99
Episode length: 29.88 +/- 1.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 49275682    |
| train/                  |             |
|    approx_kl            | 0.038713194 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000152    |
|    loss                 | 0.093       |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 803      |
|    time_elapsed    | 304431   |
|    total_timesteps | 49336320 |
---------------------------------
Eval num_timesteps=49337123, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 49337123    |
| train/                  |             |
|    approx_kl            | 0.039238665 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000152    |
|    loss                 | 0.1         |
|    n_updates            | 4015        |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 804      |
|    time_elapsed    | 304781   |
|    total_timesteps | 49397760 |
---------------------------------
Eval num_timesteps=49398564, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.036      |
| time/                   |            |
|    total_timesteps      | 49398564   |
| train/                  |            |
|    approx_kl            | 0.03866841 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.000152   |
|    loss                 | 0.0578     |
|    n_updates            | 4020       |
|    policy_gradient_loss | -0.00431   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 805      |
|    time_elapsed    | 305132   |
|    total_timesteps | 49459200 |
---------------------------------
Eval num_timesteps=49460005, episode_reward=0.12 +/- 0.98
Episode length: 29.93 +/- 1.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 49460005    |
| train/                  |             |
|    approx_kl            | 0.039772853 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000152    |
|    loss                 | 0.15        |
|    n_updates            | 4025        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 806      |
|    time_elapsed    | 305483   |
|    total_timesteps | 49520640 |
---------------------------------
Eval num_timesteps=49521446, episode_reward=0.05 +/- 0.99
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 49521446    |
| train/                  |             |
|    approx_kl            | 0.039087657 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.73       |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000151    |
|    loss                 | 0.0622      |
|    n_updates            | 4030        |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 807      |
|    time_elapsed    | 305833   |
|    total_timesteps | 49582080 |
---------------------------------
Eval num_timesteps=49582887, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.114      |
| time/                   |            |
|    total_timesteps      | 49582887   |
| train/                  |            |
|    approx_kl            | 0.03904078 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.000151   |
|    loss                 | 0.096      |
|    n_updates            | 4035       |
|    policy_gradient_loss | -0.00338   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 808      |
|    time_elapsed    | 306182   |
|    total_timesteps | 49643520 |
---------------------------------
Eval num_timesteps=49644328, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 49644328    |
| train/                  |             |
|    approx_kl            | 0.037731104 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.000151    |
|    loss                 | 0.13        |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 809      |
|    time_elapsed    | 306532   |
|    total_timesteps | 49704960 |
---------------------------------
Eval num_timesteps=49705769, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 49705769    |
| train/                  |             |
|    approx_kl            | 0.037779406 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000151    |
|    loss                 | 0.0574      |
|    n_updates            | 4045        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 810      |
|    time_elapsed    | 306881   |
|    total_timesteps | 49766400 |
---------------------------------
Eval num_timesteps=49767210, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total_timesteps      | 49767210   |
| train/                  |            |
|    approx_kl            | 0.03813628 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.000151   |
|    loss                 | 0.0666     |
|    n_updates            | 4050       |
|    policy_gradient_loss | -0.00341   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 811      |
|    time_elapsed    | 307230   |
|    total_timesteps | 49827840 |
---------------------------------
Eval num_timesteps=49828651, episode_reward=-0.05 +/- 0.99
Episode length: 29.95 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.054      |
| time/                   |             |
|    total_timesteps      | 49828651    |
| train/                  |             |
|    approx_kl            | 0.037685595 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000151    |
|    loss                 | 0.16        |
|    n_updates            | 4055        |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 812      |
|    time_elapsed    | 307579   |
|    total_timesteps | 49889280 |
---------------------------------
Eval num_timesteps=49890092, episode_reward=0.03 +/- 0.98
Episode length: 29.94 +/- 0.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 49890092   |
| train/                  |            |
|    approx_kl            | 0.03965736 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.00015    |
|    loss                 | 0.128      |
|    n_updates            | 4060       |
|    policy_gradient_loss | -0.00364   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 813      |
|    time_elapsed    | 307928   |
|    total_timesteps | 49950720 |
---------------------------------
Eval num_timesteps=49951533, episode_reward=0.10 +/- 0.98
Episode length: 29.87 +/- 1.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 49951533    |
| train/                  |             |
|    approx_kl            | 0.038055483 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.00015     |
|    loss                 | 0.121       |
|    n_updates            | 4065        |
|    policy_gradient_loss | -0.0035     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 814      |
|    time_elapsed    | 308277   |
|    total_timesteps | 50012160 |
---------------------------------
Eval num_timesteps=50012974, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 50012974    |
| train/                  |             |
|    approx_kl            | 0.037411388 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.00015     |
|    loss                 | 0.0632      |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.00286    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 815      |
|    time_elapsed    | 308628   |
|    total_timesteps | 50073600 |
---------------------------------
Eval num_timesteps=50074415, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.124      |
| time/                   |            |
|    total_timesteps      | 50074415   |
| train/                  |            |
|    approx_kl            | 0.03783746 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.716     |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.00015    |
|    loss                 | 0.0843     |
|    n_updates            | 4075       |
|    policy_gradient_loss | -0.00428   |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 816      |
|    time_elapsed    | 308979   |
|    total_timesteps | 50135040 |
---------------------------------
Eval num_timesteps=50135856, episode_reward=0.02 +/- 0.98
Episode length: 29.90 +/- 1.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 50135856    |
| train/                  |             |
|    approx_kl            | 0.038537774 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.00015     |
|    loss                 | 0.0634      |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 817      |
|    time_elapsed    | 309329   |
|    total_timesteps | 50196480 |
---------------------------------
Eval num_timesteps=50197297, episode_reward=-0.02 +/- 0.99
Episode length: 29.90 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 50197297    |
| train/                  |             |
|    approx_kl            | 0.037932016 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000149    |
|    loss                 | 0.0646      |
|    n_updates            | 4085        |
|    policy_gradient_loss | -0.00274    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 818      |
|    time_elapsed    | 309680   |
|    total_timesteps | 50257920 |
---------------------------------
Eval num_timesteps=50258738, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 50258738    |
| train/                  |             |
|    approx_kl            | 0.038263343 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000149    |
|    loss                 | 0.098       |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 819      |
|    time_elapsed    | 310030   |
|    total_timesteps | 50319360 |
---------------------------------
Eval num_timesteps=50320179, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.066      |
| time/                   |            |
|    total_timesteps      | 50320179   |
| train/                  |            |
|    approx_kl            | 0.03804073 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.709     |
|    explained_variance   | 0.354      |
|    learning_rate        | 0.000149   |
|    loss                 | 0.116      |
|    n_updates            | 4095       |
|    policy_gradient_loss | -0.00304   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 820      |
|    time_elapsed    | 310379   |
|    total_timesteps | 50380800 |
---------------------------------
Eval num_timesteps=50381620, episode_reward=0.12 +/- 0.97
Episode length: 29.92 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 50381620    |
| train/                  |             |
|    approx_kl            | 0.037840176 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000149    |
|    loss                 | 0.111       |
|    n_updates            | 4100        |
|    policy_gradient_loss | -0.00342    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 821      |
|    time_elapsed    | 310729   |
|    total_timesteps | 50442240 |
---------------------------------
Eval num_timesteps=50443061, episode_reward=0.12 +/- 0.98
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 50443061    |
| train/                  |             |
|    approx_kl            | 0.039669696 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000149    |
|    loss                 | 0.0683      |
|    n_updates            | 4105        |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 162      |
|    iterations      | 822      |
|    time_elapsed    | 311079   |
|    total_timesteps | 50503680 |
---------------------------------
Eval num_timesteps=50504502, episode_reward=0.04 +/- 0.99
Episode length: 29.94 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 50504502    |
| train/                  |             |
|    approx_kl            | 0.040178593 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0756      |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 823      |
|    time_elapsed    | 311428   |
|    total_timesteps | 50565120 |
---------------------------------
Eval num_timesteps=50565943, episode_reward=0.07 +/- 0.99
Episode length: 30.00 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 50565943    |
| train/                  |             |
|    approx_kl            | 0.037952837 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0797      |
|    n_updates            | 4115        |
|    policy_gradient_loss | -0.00372    |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 824      |
|    time_elapsed    | 311777   |
|    total_timesteps | 50626560 |
---------------------------------
Eval num_timesteps=50627384, episode_reward=0.02 +/- 0.99
Episode length: 29.94 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 50627384   |
| train/                  |            |
|    approx_kl            | 0.03763372 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000148   |
|    loss                 | 0.0869     |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.00432   |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 825      |
|    time_elapsed    | 312127   |
|    total_timesteps | 50688000 |
---------------------------------
Eval num_timesteps=50688825, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 50688825   |
| train/                  |            |
|    approx_kl            | 0.03632737 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.000148   |
|    loss                 | 0.0503     |
|    n_updates            | 4125       |
|    policy_gradient_loss | -0.0035    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 826      |
|    time_elapsed    | 312479   |
|    total_timesteps | 50749440 |
---------------------------------
Eval num_timesteps=50750266, episode_reward=0.07 +/- 0.98
Episode length: 29.91 +/- 1.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 50750266    |
| train/                  |             |
|    approx_kl            | 0.038731564 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.106       |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 827      |
|    time_elapsed    | 312829   |
|    total_timesteps | 50810880 |
---------------------------------
Eval num_timesteps=50811707, episode_reward=0.17 +/- 0.97
Episode length: 29.96 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.17        |
| time/                   |             |
|    total_timesteps      | 50811707    |
| train/                  |             |
|    approx_kl            | 0.037752844 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.082       |
|    n_updates            | 4135        |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 0.242       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.17
SELFPLAY: new best model, bumping up generation to 43
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 828      |
|    time_elapsed    | 313179   |
|    total_timesteps | 50872320 |
---------------------------------
Eval num_timesteps=50873148, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.028      |
| time/                   |            |
|    total_timesteps      | 50873148   |
| train/                  |            |
|    approx_kl            | 0.04008705 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.743     |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.000147   |
|    loss                 | 0.0466     |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.00291   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 829      |
|    time_elapsed    | 313529   |
|    total_timesteps | 50933760 |
---------------------------------
Eval num_timesteps=50934589, episode_reward=0.03 +/- 0.99
Episode length: 29.95 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 50934589    |
| train/                  |             |
|    approx_kl            | 0.037917387 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.741      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000147    |
|    loss                 | 0.119       |
|    n_updates            | 4145        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 830      |
|    time_elapsed    | 313878   |
|    total_timesteps | 50995200 |
---------------------------------
Eval num_timesteps=50996030, episode_reward=0.04 +/- 0.98
Episode length: 29.91 +/- 1.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.042      |
| time/                   |            |
|    total_timesteps      | 50996030   |
| train/                  |            |
|    approx_kl            | 0.03859272 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.739     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000147   |
|    loss                 | 0.0456     |
|    n_updates            | 4150       |
|    policy_gradient_loss | -0.00335   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 831      |
|    time_elapsed    | 314228   |
|    total_timesteps | 51056640 |
---------------------------------
Eval num_timesteps=51057471, episode_reward=0.05 +/- 0.98
Episode length: 29.93 +/- 1.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 51057471   |
| train/                  |            |
|    approx_kl            | 0.03692092 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.742     |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.000147   |
|    loss                 | 0.0911     |
|    n_updates            | 4155       |
|    policy_gradient_loss | -0.00309   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 832      |
|    time_elapsed    | 314577   |
|    total_timesteps | 51118080 |
---------------------------------
Eval num_timesteps=51118912, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 51118912   |
| train/                  |            |
|    approx_kl            | 0.03967709 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.742     |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.000147   |
|    loss                 | 0.0909     |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.00279   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 833      |
|    time_elapsed    | 314925   |
|    total_timesteps | 51179520 |
---------------------------------
Eval num_timesteps=51180353, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 51180353    |
| train/                  |             |
|    approx_kl            | 0.037221424 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.12        |
|    n_updates            | 4165        |
|    policy_gradient_loss | -0.00399    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 834      |
|    time_elapsed    | 315274   |
|    total_timesteps | 51240960 |
---------------------------------
Eval num_timesteps=51241794, episode_reward=-0.04 +/- 0.98
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 51241794    |
| train/                  |             |
|    approx_kl            | 0.038554564 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.742      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0587      |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00375    |
|    value_loss           | 0.246       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 835      |
|    time_elapsed    | 315623   |
|    total_timesteps | 51302400 |
---------------------------------
Eval num_timesteps=51303235, episode_reward=0.00 +/- 0.99
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 51303235    |
| train/                  |             |
|    approx_kl            | 0.038057275 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0999      |
|    n_updates            | 4175        |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 836      |
|    time_elapsed    | 315974   |
|    total_timesteps | 51363840 |
---------------------------------
Eval num_timesteps=51364676, episode_reward=0.02 +/- 0.99
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 51364676    |
| train/                  |             |
|    approx_kl            | 0.038120557 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0736      |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 837      |
|    time_elapsed    | 316326   |
|    total_timesteps | 51425280 |
---------------------------------
Eval num_timesteps=51426117, episode_reward=0.01 +/- 0.99
Episode length: 29.87 +/- 1.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.008     |
| time/                   |           |
|    total_timesteps      | 51426117  |
| train/                  |           |
|    approx_kl            | 0.0383415 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.742    |
|    explained_variance   | 0.34      |
|    learning_rate        | 0.000146  |
|    loss                 | 0.119     |
|    n_updates            | 4185      |
|    policy_gradient_loss | -0.00279  |
|    value_loss           | 0.242     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 838      |
|    time_elapsed    | 316676   |
|    total_timesteps | 51486720 |
---------------------------------
Eval num_timesteps=51487558, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 51487558    |
| train/                  |             |
|    approx_kl            | 0.039839517 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.111       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 839      |
|    time_elapsed    | 317026   |
|    total_timesteps | 51548160 |
---------------------------------
Eval num_timesteps=51548999, episode_reward=-0.01 +/- 0.97
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 51548999    |
| train/                  |             |
|    approx_kl            | 0.039054092 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.731      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000145    |
|    loss                 | 0.13        |
|    n_updates            | 4195        |
|    policy_gradient_loss | -0.00282    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 840      |
|    time_elapsed    | 317376   |
|    total_timesteps | 51609600 |
---------------------------------
Eval num_timesteps=51610440, episode_reward=-0.04 +/- 0.99
Episode length: 29.94 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.042      |
| time/                   |             |
|    total_timesteps      | 51610440    |
| train/                  |             |
|    approx_kl            | 0.038714156 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000145    |
|    loss                 | 0.105       |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 841      |
|    time_elapsed    | 317725   |
|    total_timesteps | 51671040 |
---------------------------------
Eval num_timesteps=51671881, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 51671881    |
| train/                  |             |
|    approx_kl            | 0.039528035 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.731      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000145    |
|    loss                 | 0.117       |
|    n_updates            | 4205        |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 842      |
|    time_elapsed    | 318074   |
|    total_timesteps | 51732480 |
---------------------------------
Eval num_timesteps=51733322, episode_reward=0.05 +/- 0.99
Episode length: 30.03 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 51733322   |
| train/                  |            |
|    approx_kl            | 0.03750657 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.339      |
|    learning_rate        | 0.000145   |
|    loss                 | 0.143      |
|    n_updates            | 4210       |
|    policy_gradient_loss | -0.00404   |
|    value_loss           | 0.246      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 843      |
|    time_elapsed    | 318423   |
|    total_timesteps | 51793920 |
---------------------------------
Eval num_timesteps=51794763, episode_reward=0.09 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.086      |
| time/                   |            |
|    total_timesteps      | 51794763   |
| train/                  |            |
|    approx_kl            | 0.03758243 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.000145   |
|    loss                 | 0.135      |
|    n_updates            | 4215       |
|    policy_gradient_loss | -0.00375   |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 844      |
|    time_elapsed    | 318772   |
|    total_timesteps | 51855360 |
---------------------------------
Eval num_timesteps=51856204, episode_reward=-0.04 +/- 0.98
Episode length: 29.95 +/- 0.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.042      |
| time/                   |             |
|    total_timesteps      | 51856204    |
| train/                  |             |
|    approx_kl            | 0.038959328 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000144    |
|    loss                 | 0.035       |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 845      |
|    time_elapsed    | 319121   |
|    total_timesteps | 51916800 |
---------------------------------
Eval num_timesteps=51917645, episode_reward=-0.02 +/- 0.98
Episode length: 29.91 +/- 1.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.018     |
| time/                   |            |
|    total_timesteps      | 51917645   |
| train/                  |            |
|    approx_kl            | 0.03832686 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.729     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000144   |
|    loss                 | 0.067      |
|    n_updates            | 4225       |
|    policy_gradient_loss | -0.00422   |
|    value_loss           | 0.244      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 846      |
|    time_elapsed    | 319471   |
|    total_timesteps | 51978240 |
---------------------------------
Eval num_timesteps=51979086, episode_reward=0.01 +/- 0.99
Episode length: 29.94 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 51979086    |
| train/                  |             |
|    approx_kl            | 0.037791666 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000144    |
|    loss                 | 0.0886      |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.00358    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 847      |
|    time_elapsed    | 319823   |
|    total_timesteps | 52039680 |
---------------------------------
Eval num_timesteps=52040527, episode_reward=0.07 +/- 0.99
Episode length: 29.95 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 52040527    |
| train/                  |             |
|    approx_kl            | 0.036250584 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000144    |
|    loss                 | 0.0763      |
|    n_updates            | 4235        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 848      |
|    time_elapsed    | 320173   |
|    total_timesteps | 52101120 |
---------------------------------
Eval num_timesteps=52101968, episode_reward=-0.00 +/- 0.99
Episode length: 29.95 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.004     |
| time/                   |            |
|    total_timesteps      | 52101968   |
| train/                  |            |
|    approx_kl            | 0.03688907 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.000144   |
|    loss                 | 0.0639     |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.00291   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 849      |
|    time_elapsed    | 320522   |
|    total_timesteps | 52162560 |
---------------------------------
Eval num_timesteps=52163409, episode_reward=0.01 +/- 0.99
Episode length: 29.92 +/- 0.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.008      |
| time/                   |            |
|    total_timesteps      | 52163409   |
| train/                  |            |
|    approx_kl            | 0.03806674 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.714     |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.000144   |
|    loss                 | 0.106      |
|    n_updates            | 4245       |
|    policy_gradient_loss | -0.00302   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 850      |
|    time_elapsed    | 320872   |
|    total_timesteps | 52224000 |
---------------------------------
Eval num_timesteps=52224850, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 0.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 52224850    |
| train/                  |             |
|    approx_kl            | 0.038242213 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.13        |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.00363    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 851      |
|    time_elapsed    | 321221   |
|    total_timesteps | 52285440 |
---------------------------------
Eval num_timesteps=52286291, episode_reward=0.03 +/- 0.98
Episode length: 29.92 +/- 1.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 52286291   |
| train/                  |            |
|    approx_kl            | 0.03798007 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000143   |
|    loss                 | 0.151      |
|    n_updates            | 4255       |
|    policy_gradient_loss | -0.00314   |
|    value_loss           | 0.245      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 852      |
|    time_elapsed    | 321570   |
|    total_timesteps | 52346880 |
---------------------------------
Eval num_timesteps=52347732, episode_reward=0.04 +/- 0.99
Episode length: 29.95 +/- 0.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.04      |
| time/                   |           |
|    total_timesteps      | 52347732  |
| train/                  |           |
|    approx_kl            | 0.0382787 |
|    clip_fraction        | 0.216     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.709    |
|    explained_variance   | 0.355     |
|    learning_rate        | 0.000143  |
|    loss                 | 0.0624    |
|    n_updates            | 4260      |
|    policy_gradient_loss | -0.00492  |
|    value_loss           | 0.241     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 853      |
|    time_elapsed    | 321919   |
|    total_timesteps | 52408320 |
---------------------------------
Eval num_timesteps=52409173, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.132       |
| time/                   |             |
|    total_timesteps      | 52409173    |
| train/                  |             |
|    approx_kl            | 0.039069157 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.0509      |
|    n_updates            | 4265        |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 854      |
|    time_elapsed    | 322268   |
|    total_timesteps | 52469760 |
---------------------------------
Eval num_timesteps=52470614, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.008      |
| time/                   |            |
|    total_timesteps      | 52470614   |
| train/                  |            |
|    approx_kl            | 0.03887375 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.704     |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.000143   |
|    loss                 | 0.0517     |
|    n_updates            | 4270       |
|    policy_gradient_loss | -0.00336   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 855      |
|    time_elapsed    | 322617   |
|    total_timesteps | 52531200 |
---------------------------------
Eval num_timesteps=52532055, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 52532055    |
| train/                  |             |
|    approx_kl            | 0.039886784 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.117       |
|    n_updates            | 4275        |
|    policy_gradient_loss | -0.00447    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 856      |
|    time_elapsed    | 322967   |
|    total_timesteps | 52592640 |
---------------------------------
Eval num_timesteps=52593496, episode_reward=0.07 +/- 0.98
Episode length: 29.94 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 52593496    |
| train/                  |             |
|    approx_kl            | 0.037435334 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0557      |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.00338    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 857      |
|    time_elapsed    | 323318   |
|    total_timesteps | 52654080 |
---------------------------------
Eval num_timesteps=52654937, episode_reward=0.06 +/- 0.99
Episode length: 29.91 +/- 1.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 52654937   |
| train/                  |            |
|    approx_kl            | 0.03702135 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.692     |
|    explained_variance   | 0.338      |
|    learning_rate        | 0.000142   |
|    loss                 | 0.0412     |
|    n_updates            | 4285       |
|    policy_gradient_loss | -0.0035    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 858      |
|    time_elapsed    | 323668   |
|    total_timesteps | 52715520 |
---------------------------------
Eval num_timesteps=52716378, episode_reward=0.08 +/- 0.98
Episode length: 29.95 +/- 0.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 52716378    |
| train/                  |             |
|    approx_kl            | 0.037962485 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.691      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.148       |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00446    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 859      |
|    time_elapsed    | 324019   |
|    total_timesteps | 52776960 |
---------------------------------
Eval num_timesteps=52777819, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 52777819    |
| train/                  |             |
|    approx_kl            | 0.039470393 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.689      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0716      |
|    n_updates            | 4295        |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 860      |
|    time_elapsed    | 324368   |
|    total_timesteps | 52838400 |
---------------------------------
Eval num_timesteps=52839260, episode_reward=0.02 +/- 0.98
Episode length: 29.89 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 52839260    |
| train/                  |             |
|    approx_kl            | 0.036154725 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.691      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000141    |
|    loss                 | 0.176       |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.00437    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 861      |
|    time_elapsed    | 324717   |
|    total_timesteps | 52899840 |
---------------------------------
Eval num_timesteps=52900701, episode_reward=0.02 +/- 0.98
Episode length: 29.97 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 52900701   |
| train/                  |            |
|    approx_kl            | 0.03855611 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.689     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000141   |
|    loss                 | 0.0243     |
|    n_updates            | 4305       |
|    policy_gradient_loss | -0.00464   |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 862      |
|    time_elapsed    | 325067   |
|    total_timesteps | 52961280 |
---------------------------------
Eval num_timesteps=52962142, episode_reward=0.13 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.13        |
| time/                   |             |
|    total_timesteps      | 52962142    |
| train/                  |             |
|    approx_kl            | 0.037371747 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000141    |
|    loss                 | 0.0705      |
|    n_updates            | 4310        |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 863      |
|    time_elapsed    | 325417   |
|    total_timesteps | 53022720 |
---------------------------------
Eval num_timesteps=53023583, episode_reward=0.02 +/- 0.99
Episode length: 29.89 +/- 1.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 53023583   |
| train/                  |            |
|    approx_kl            | 0.03826131 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.689     |
|    explained_variance   | 0.36       |
|    learning_rate        | 0.000141   |
|    loss                 | 0.197      |
|    n_updates            | 4315       |
|    policy_gradient_loss | -0.00385   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 864      |
|    time_elapsed    | 325766   |
|    total_timesteps | 53084160 |
---------------------------------
Eval num_timesteps=53085024, episode_reward=0.08 +/- 0.98
Episode length: 29.90 +/- 1.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 53085024    |
| train/                  |             |
|    approx_kl            | 0.038680848 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.688      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.000141    |
|    loss                 | 0.0866      |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 865      |
|    time_elapsed    | 326114   |
|    total_timesteps | 53145600 |
---------------------------------
Eval num_timesteps=53146465, episode_reward=0.09 +/- 0.99
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 53146465    |
| train/                  |             |
|    approx_kl            | 0.036800314 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000141    |
|    loss                 | 0.0711      |
|    n_updates            | 4325        |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 866      |
|    time_elapsed    | 326463   |
|    total_timesteps | 53207040 |
---------------------------------
Eval num_timesteps=53207906, episode_reward=-0.03 +/- 0.99
Episode length: 29.92 +/- 1.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.032      |
| time/                   |             |
|    total_timesteps      | 53207906    |
| train/                  |             |
|    approx_kl            | 0.036837317 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.00014     |
|    loss                 | 0.0607      |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 867      |
|    time_elapsed    | 326813   |
|    total_timesteps | 53268480 |
---------------------------------
Eval num_timesteps=53269347, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.022      |
| time/                   |            |
|    total_timesteps      | 53269347   |
| train/                  |            |
|    approx_kl            | 0.03715665 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.684     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.00014    |
|    loss                 | 0.0978     |
|    n_updates            | 4335       |
|    policy_gradient_loss | -0.00441   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 868      |
|    time_elapsed    | 327164   |
|    total_timesteps | 53329920 |
---------------------------------
Eval num_timesteps=53330788, episode_reward=0.14 +/- 0.98
Episode length: 29.89 +/- 1.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 53330788    |
| train/                  |             |
|    approx_kl            | 0.038571596 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.00014     |
|    loss                 | 0.106       |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.00397    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 869      |
|    time_elapsed    | 327513   |
|    total_timesteps | 53391360 |
---------------------------------
Eval num_timesteps=53392229, episode_reward=0.07 +/- 0.99
Episode length: 29.86 +/- 1.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 53392229   |
| train/                  |            |
|    approx_kl            | 0.03831564 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.677     |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.00014    |
|    loss                 | 0.143      |
|    n_updates            | 4345       |
|    policy_gradient_loss | -0.0016    |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 870      |
|    time_elapsed    | 327863   |
|    total_timesteps | 53452800 |
---------------------------------
Eval num_timesteps=53453670, episode_reward=0.02 +/- 0.99
Episode length: 29.94 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.022      |
| time/                   |            |
|    total_timesteps      | 53453670   |
| train/                  |            |
|    approx_kl            | 0.03652215 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.675     |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.00014    |
|    loss                 | 0.149      |
|    n_updates            | 4350       |
|    policy_gradient_loss | -0.00399   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 871      |
|    time_elapsed    | 328212   |
|    total_timesteps | 53514240 |
---------------------------------
Eval num_timesteps=53515111, episode_reward=0.10 +/- 0.98
Episode length: 29.93 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 53515111    |
| train/                  |             |
|    approx_kl            | 0.037922993 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.682      |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000139    |
|    loss                 | 0.178       |
|    n_updates            | 4355        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 872      |
|    time_elapsed    | 328562   |
|    total_timesteps | 53575680 |
---------------------------------
Eval num_timesteps=53576552, episode_reward=0.09 +/- 0.98
Episode length: 29.95 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 53576552    |
| train/                  |             |
|    approx_kl            | 0.038352806 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.000139    |
|    loss                 | 0.108       |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 873      |
|    time_elapsed    | 328911   |
|    total_timesteps | 53637120 |
---------------------------------
Eval num_timesteps=53637993, episode_reward=0.03 +/- 0.99
Episode length: 29.90 +/- 1.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 53637993    |
| train/                  |             |
|    approx_kl            | 0.039297964 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.682      |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000139    |
|    loss                 | 0.102       |
|    n_updates            | 4365        |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 874      |
|    time_elapsed    | 329260   |
|    total_timesteps | 53698560 |
---------------------------------
Eval num_timesteps=53699434, episode_reward=0.11 +/- 0.99
Episode length: 29.95 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 53699434    |
| train/                  |             |
|    approx_kl            | 0.036404427 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000139    |
|    loss                 | 0.0642      |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 875      |
|    time_elapsed    | 329609   |
|    total_timesteps | 53760000 |
---------------------------------
Eval num_timesteps=53760875, episode_reward=0.05 +/- 1.00
Episode length: 29.92 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 53760875    |
| train/                  |             |
|    approx_kl            | 0.037382934 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000139    |
|    loss                 | 0.107       |
|    n_updates            | 4375        |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 876      |
|    time_elapsed    | 329958   |
|    total_timesteps | 53821440 |
---------------------------------
Eval num_timesteps=53822316, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 53822316    |
| train/                  |             |
|    approx_kl            | 0.039217018 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.689      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000139    |
|    loss                 | 0.163       |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.00382    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 877      |
|    time_elapsed    | 330307   |
|    total_timesteps | 53882880 |
---------------------------------
Eval num_timesteps=53883757, episode_reward=0.04 +/- 0.99
Episode length: 29.96 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 53883757   |
| train/                  |            |
|    approx_kl            | 0.03739021 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.682     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.000138   |
|    loss                 | 0.1        |
|    n_updates            | 4385       |
|    policy_gradient_loss | -0.00477   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 878      |
|    time_elapsed    | 330658   |
|    total_timesteps | 53944320 |
---------------------------------
Eval num_timesteps=53945198, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 53945198    |
| train/                  |             |
|    approx_kl            | 0.041330878 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.673      |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.000138    |
|    loss                 | 0.0228      |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 879      |
|    time_elapsed    | 331009   |
|    total_timesteps | 54005760 |
---------------------------------
Eval num_timesteps=54006639, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 54006639    |
| train/                  |             |
|    approx_kl            | 0.039407644 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000138    |
|    loss                 | 0.118       |
|    n_updates            | 4395        |
|    policy_gradient_loss | -0.00477    |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 880      |
|    time_elapsed    | 331359   |
|    total_timesteps | 54067200 |
---------------------------------
Eval num_timesteps=54068080, episode_reward=0.07 +/- 0.98
Episode length: 29.91 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 54068080    |
| train/                  |             |
|    approx_kl            | 0.037010707 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.667      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000138    |
|    loss                 | 0.126       |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 881      |
|    time_elapsed    | 331708   |
|    total_timesteps | 54128640 |
---------------------------------
Eval num_timesteps=54129521, episode_reward=0.09 +/- 0.98
Episode length: 29.96 +/- 1.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.09      |
| time/                   |           |
|    total_timesteps      | 54129521  |
| train/                  |           |
|    approx_kl            | 0.0375735 |
|    clip_fraction        | 0.207     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.669    |
|    explained_variance   | 0.357     |
|    learning_rate        | 0.000138  |
|    loss                 | 0.0452    |
|    n_updates            | 4405      |
|    policy_gradient_loss | -0.0035   |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 882      |
|    time_elapsed    | 332058   |
|    total_timesteps | 54190080 |
---------------------------------
Eval num_timesteps=54190962, episode_reward=0.08 +/- 0.98
Episode length: 29.97 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.078      |
| time/                   |            |
|    total_timesteps      | 54190962   |
| train/                  |            |
|    approx_kl            | 0.03840802 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.668     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000137   |
|    loss                 | 0.116      |
|    n_updates            | 4410       |
|    policy_gradient_loss | -0.00372   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 883      |
|    time_elapsed    | 332408   |
|    total_timesteps | 54251520 |
---------------------------------
Eval num_timesteps=54252403, episode_reward=0.02 +/- 0.97
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 54252403    |
| train/                  |             |
|    approx_kl            | 0.036887188 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000137    |
|    loss                 | 0.144       |
|    n_updates            | 4415        |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 884      |
|    time_elapsed    | 332757   |
|    total_timesteps | 54312960 |
---------------------------------
Eval num_timesteps=54313844, episode_reward=0.09 +/- 0.98
Episode length: 29.97 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 54313844   |
| train/                  |            |
|    approx_kl            | 0.03786607 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.669     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.000137   |
|    loss                 | 0.152      |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.00433   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 885      |
|    time_elapsed    | 333107   |
|    total_timesteps | 54374400 |
---------------------------------
Eval num_timesteps=54375285, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.074      |
| time/                   |            |
|    total_timesteps      | 54375285   |
| train/                  |            |
|    approx_kl            | 0.03707272 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.673     |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.000137   |
|    loss                 | 0.102      |
|    n_updates            | 4425       |
|    policy_gradient_loss | -0.00378   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 886      |
|    time_elapsed    | 333456   |
|    total_timesteps | 54435840 |
---------------------------------
Eval num_timesteps=54436726, episode_reward=0.16 +/- 0.97
Episode length: 29.92 +/- 1.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.164      |
| time/                   |            |
|    total_timesteps      | 54436726   |
| train/                  |            |
|    approx_kl            | 0.03936865 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.667     |
|    explained_variance   | 0.371      |
|    learning_rate        | 0.000137   |
|    loss                 | 0.0702     |
|    n_updates            | 4430       |
|    policy_gradient_loss | -0.00371   |
|    value_loss           | 0.237      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.164
SELFPLAY: new best model, bumping up generation to 44
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 887      |
|    time_elapsed    | 333805   |
|    total_timesteps | 54497280 |
---------------------------------
Eval num_timesteps=54498167, episode_reward=-0.05 +/- 0.99
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.05       |
| time/                   |             |
|    total_timesteps      | 54498167    |
| train/                  |             |
|    approx_kl            | 0.038099892 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000137    |
|    loss                 | 0.0763      |
|    n_updates            | 4435        |
|    policy_gradient_loss | -0.00273    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 888      |
|    time_elapsed    | 334156   |
|    total_timesteps | 54558720 |
---------------------------------
Eval num_timesteps=54559608, episode_reward=-0.02 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 54559608    |
| train/                  |             |
|    approx_kl            | 0.038450893 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0896      |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.00441    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 889      |
|    time_elapsed    | 334508   |
|    total_timesteps | 54620160 |
---------------------------------
Eval num_timesteps=54621049, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 54621049    |
| train/                  |             |
|    approx_kl            | 0.038579166 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.103       |
|    n_updates            | 4445        |
|    policy_gradient_loss | -0.00451    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 163      |
|    iterations      | 890      |
|    time_elapsed    | 334858   |
|    total_timesteps | 54681600 |
---------------------------------
Eval num_timesteps=54682490, episode_reward=0.03 +/- 0.98
Episode length: 29.94 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 54682490    |
| train/                  |             |
|    approx_kl            | 0.038636293 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.37        |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0859      |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 891      |
|    time_elapsed    | 335208   |
|    total_timesteps | 54743040 |
---------------------------------
Eval num_timesteps=54743931, episode_reward=0.04 +/- 0.98
Episode length: 29.92 +/- 1.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 54743931    |
| train/                  |             |
|    approx_kl            | 0.037729416 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0708      |
|    n_updates            | 4455        |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 892      |
|    time_elapsed    | 335558   |
|    total_timesteps | 54804480 |
---------------------------------
Eval num_timesteps=54805372, episode_reward=0.01 +/- 0.99
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 54805372    |
| train/                  |             |
|    approx_kl            | 0.037753996 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0773      |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 893      |
|    time_elapsed    | 335907   |
|    total_timesteps | 54865920 |
---------------------------------
Eval num_timesteps=54866813, episode_reward=0.01 +/- 0.99
Episode length: 30.00 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 54866813    |
| train/                  |             |
|    approx_kl            | 0.037775192 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.691      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000135    |
|    loss                 | 0.0988      |
|    n_updates            | 4465        |
|    policy_gradient_loss | -0.0047     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 894      |
|    time_elapsed    | 336257   |
|    total_timesteps | 54927360 |
---------------------------------
Eval num_timesteps=54928254, episode_reward=0.09 +/- 0.98
Episode length: 29.97 +/- 1.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 54928254    |
| train/                  |             |
|    approx_kl            | 0.039178837 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000135    |
|    loss                 | 0.0202      |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 895      |
|    time_elapsed    | 336607   |
|    total_timesteps | 54988800 |
---------------------------------
Eval num_timesteps=54989695, episode_reward=0.01 +/- 0.98
Episode length: 29.92 +/- 0.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.01       |
| time/                   |            |
|    total_timesteps      | 54989695   |
| train/                  |            |
|    approx_kl            | 0.03868042 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.683     |
|    explained_variance   | 0.37       |
|    learning_rate        | 0.000135   |
|    loss                 | 0.0801     |
|    n_updates            | 4475       |
|    policy_gradient_loss | -0.00409   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 896      |
|    time_elapsed    | 336955   |
|    total_timesteps | 55050240 |
---------------------------------
Eval num_timesteps=55051136, episode_reward=0.09 +/- 0.99
Episode length: 29.95 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 55051136    |
| train/                  |             |
|    approx_kl            | 0.038167518 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.672      |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.000135    |
|    loss                 | 0.0606      |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 897      |
|    time_elapsed    | 337304   |
|    total_timesteps | 55111680 |
---------------------------------
Eval num_timesteps=55112577, episode_reward=0.07 +/- 0.99
Episode length: 29.89 +/- 1.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.066      |
| time/                   |            |
|    total_timesteps      | 55112577   |
| train/                  |            |
|    approx_kl            | 0.03819124 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.674     |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.000135   |
|    loss                 | 0.0441     |
|    n_updates            | 4485       |
|    policy_gradient_loss | -0.00301   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 163      |
|    iterations      | 898      |
|    time_elapsed    | 337653   |
|    total_timesteps | 55173120 |
---------------------------------
Eval num_timesteps=55174018, episode_reward=0.07 +/- 0.98
Episode length: 29.88 +/- 1.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 55174018    |
| train/                  |             |
|    approx_kl            | 0.038561426 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.675      |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.000134    |
|    loss                 | 0.0478      |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 899      |
|    time_elapsed    | 338003   |
|    total_timesteps | 55234560 |
---------------------------------
Eval num_timesteps=55235459, episode_reward=-0.00 +/- 0.99
Episode length: 29.99 +/- 0.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 55235459    |
| train/                  |             |
|    approx_kl            | 0.037807006 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.000134    |
|    loss                 | 0.114       |
|    n_updates            | 4495        |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 900      |
|    time_elapsed    | 338355   |
|    total_timesteps | 55296000 |
---------------------------------
Eval num_timesteps=55296900, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 55296900   |
| train/                  |            |
|    approx_kl            | 0.03663267 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.664     |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.000134   |
|    loss                 | 0.101      |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.00479   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 901      |
|    time_elapsed    | 338705   |
|    total_timesteps | 55357440 |
---------------------------------
Eval num_timesteps=55358341, episode_reward=0.08 +/- 0.99
Episode length: 29.95 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.078      |
| time/                   |            |
|    total_timesteps      | 55358341   |
| train/                  |            |
|    approx_kl            | 0.03599936 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.665     |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.000134   |
|    loss                 | 0.119      |
|    n_updates            | 4505       |
|    policy_gradient_loss | -0.00347   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 902      |
|    time_elapsed    | 339055   |
|    total_timesteps | 55418880 |
---------------------------------
Eval num_timesteps=55419782, episode_reward=0.12 +/- 0.98
Episode length: 29.94 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 55419782   |
| train/                  |            |
|    approx_kl            | 0.03744804 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.664     |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.000134   |
|    loss                 | 0.0834     |
|    n_updates            | 4510       |
|    policy_gradient_loss | -0.00421   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 903      |
|    time_elapsed    | 339405   |
|    total_timesteps | 55480320 |
---------------------------------
Eval num_timesteps=55481223, episode_reward=0.13 +/- 0.98
Episode length: 29.95 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.126       |
| time/                   |             |
|    total_timesteps      | 55481223    |
| train/                  |             |
|    approx_kl            | 0.038200147 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.000134    |
|    loss                 | 0.117       |
|    n_updates            | 4515        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 904      |
|    time_elapsed    | 339754   |
|    total_timesteps | 55541760 |
---------------------------------
Eval num_timesteps=55542664, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 55542664   |
| train/                  |            |
|    approx_kl            | 0.03524456 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.656     |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.000133   |
|    loss                 | 0.0809     |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.00522   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 905      |
|    time_elapsed    | 340103   |
|    total_timesteps | 55603200 |
---------------------------------
Eval num_timesteps=55604105, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 55604105    |
| train/                  |             |
|    approx_kl            | 0.036208212 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.000133    |
|    loss                 | 0.131       |
|    n_updates            | 4525        |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 906      |
|    time_elapsed    | 340453   |
|    total_timesteps | 55664640 |
---------------------------------
Eval num_timesteps=55665546, episode_reward=0.12 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 55665546    |
| train/                  |             |
|    approx_kl            | 0.037074868 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.000133    |
|    loss                 | 0.108       |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 907      |
|    time_elapsed    | 340802   |
|    total_timesteps | 55726080 |
---------------------------------
Eval num_timesteps=55726987, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.112      |
| time/                   |            |
|    total_timesteps      | 55726987   |
| train/                  |            |
|    approx_kl            | 0.03880189 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.000133   |
|    loss                 | 0.145      |
|    n_updates            | 4535       |
|    policy_gradient_loss | -0.00381   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 908      |
|    time_elapsed    | 341152   |
|    total_timesteps | 55787520 |
---------------------------------
Eval num_timesteps=55788428, episode_reward=0.17 +/- 0.97
Episode length: 29.97 +/- 0.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.166      |
| time/                   |            |
|    total_timesteps      | 55788428   |
| train/                  |            |
|    approx_kl            | 0.03562115 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.652     |
|    explained_variance   | 0.368      |
|    learning_rate        | 0.000133   |
|    loss                 | 0.0503     |
|    n_updates            | 4540       |
|    policy_gradient_loss | -0.00442   |
|    value_loss           | 0.239      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.166
SELFPLAY: new best model, bumping up generation to 45
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 909      |
|    time_elapsed    | 341501   |
|    total_timesteps | 55848960 |
---------------------------------
Eval num_timesteps=55849869, episode_reward=-0.02 +/- 0.99
Episode length: 29.89 +/- 1.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 55849869    |
| train/                  |             |
|    approx_kl            | 0.039322212 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.000132    |
|    loss                 | 0.0802      |
|    n_updates            | 4545        |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 910      |
|    time_elapsed    | 341852   |
|    total_timesteps | 55910400 |
---------------------------------
Eval num_timesteps=55911310, episode_reward=-0.07 +/- 0.98
Episode length: 29.88 +/- 1.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.07       |
| time/                   |             |
|    total_timesteps      | 55911310    |
| train/                  |             |
|    approx_kl            | 0.039392725 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.37        |
|    learning_rate        | 0.000132    |
|    loss                 | 0.154       |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 911      |
|    time_elapsed    | 342202   |
|    total_timesteps | 55971840 |
---------------------------------
Eval num_timesteps=55972751, episode_reward=0.05 +/- 0.98
Episode length: 29.86 +/- 2.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 55972751    |
| train/                  |             |
|    approx_kl            | 0.037581537 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.688      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000132    |
|    loss                 | 0.0607      |
|    n_updates            | 4555        |
|    policy_gradient_loss | -0.00435    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 912      |
|    time_elapsed    | 342552   |
|    total_timesteps | 56033280 |
---------------------------------
Eval num_timesteps=56034192, episode_reward=0.08 +/- 0.98
Episode length: 29.69 +/- 2.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.7       |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 56034192   |
| train/                  |            |
|    approx_kl            | 0.03831172 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.684     |
|    explained_variance   | 0.373      |
|    learning_rate        | 0.000132   |
|    loss                 | 0.108      |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.00385   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 163      |
|    iterations      | 913      |
|    time_elapsed    | 342901   |
|    total_timesteps | 56094720 |
---------------------------------
Eval num_timesteps=56095633, episode_reward=0.05 +/- 0.99
Episode length: 29.53 +/- 3.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 56095633    |
| train/                  |             |
|    approx_kl            | 0.037372835 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.678      |
|    explained_variance   | 0.37        |
|    learning_rate        | 0.000132    |
|    loss                 | 0.0781      |
|    n_updates            | 4565        |
|    policy_gradient_loss | -0.00397    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 914      |
|    time_elapsed    | 343250   |
|    total_timesteps | 56156160 |
---------------------------------
Eval num_timesteps=56157074, episode_reward=-0.05 +/- 0.98
Episode length: 29.56 +/- 3.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.6       |
|    mean_reward          | -0.046     |
| time/                   |            |
|    total_timesteps      | 56157074   |
| train/                  |            |
|    approx_kl            | 0.03697469 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.676     |
|    explained_variance   | 0.356      |
|    learning_rate        | 0.000132   |
|    loss                 | 0.139      |
|    n_updates            | 4570       |
|    policy_gradient_loss | -0.00471   |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 915      |
|    time_elapsed    | 343599   |
|    total_timesteps | 56217600 |
---------------------------------
Eval num_timesteps=56218515, episode_reward=0.02 +/- 0.97
Episode length: 29.45 +/- 3.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.4        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 56218515    |
| train/                  |             |
|    approx_kl            | 0.036275912 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000131    |
|    loss                 | 0.0837      |
|    n_updates            | 4575        |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 916      |
|    time_elapsed    | 343947   |
|    total_timesteps | 56279040 |
---------------------------------
Eval num_timesteps=56279956, episode_reward=0.06 +/- 0.98
Episode length: 29.35 +/- 3.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.4        |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 56279956    |
| train/                  |             |
|    approx_kl            | 0.037019756 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.000131    |
|    loss                 | 0.164       |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.005      |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 917      |
|    time_elapsed    | 344295   |
|    total_timesteps | 56340480 |
---------------------------------
Eval num_timesteps=56341397, episode_reward=0.07 +/- 0.99
Episode length: 29.32 +/- 4.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.3      |
|    mean_reward          | 0.072     |
| time/                   |           |
|    total_timesteps      | 56341397  |
| train/                  |           |
|    approx_kl            | 0.0374868 |
|    clip_fraction        | 0.206     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.667    |
|    explained_variance   | 0.377     |
|    learning_rate        | 0.000131  |
|    loss                 | 0.0714    |
|    n_updates            | 4585      |
|    policy_gradient_loss | -0.00455  |
|    value_loss           | 0.236     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 918      |
|    time_elapsed    | 344642   |
|    total_timesteps | 56401920 |
---------------------------------
Eval num_timesteps=56402838, episode_reward=0.03 +/- 0.99
Episode length: 29.48 +/- 3.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 56402838    |
| train/                  |             |
|    approx_kl            | 0.038014427 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000131    |
|    loss                 | 0.0855      |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0049     |
|    value_loss           | 0.246       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 163      |
|    iterations      | 919      |
|    time_elapsed    | 344990   |
|    total_timesteps | 56463360 |
---------------------------------
Eval num_timesteps=56464279, episode_reward=0.04 +/- 0.99
Episode length: 29.62 +/- 3.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.6        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 56464279    |
| train/                  |             |
|    approx_kl            | 0.037824027 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.37        |
|    learning_rate        | 0.000131    |
|    loss                 | 0.19        |
|    n_updates            | 4595        |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.2     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 920      |
|    time_elapsed    | 345340   |
|    total_timesteps | 56524800 |
---------------------------------
Eval num_timesteps=56525720, episode_reward=0.06 +/- 0.98
Episode length: 29.40 +/- 3.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.4       |
|    mean_reward          | 0.062      |
| time/                   |            |
|    total_timesteps      | 56525720   |
| train/                  |            |
|    approx_kl            | 0.03427438 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.665     |
|    explained_variance   | 0.37       |
|    learning_rate        | 0.00013    |
|    loss                 | 0.134      |
|    n_updates            | 4600       |
|    policy_gradient_loss | -0.00381   |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 921      |
|    time_elapsed    | 345691   |
|    total_timesteps | 56586240 |
---------------------------------
Eval num_timesteps=56587161, episode_reward=0.09 +/- 0.98
Episode length: 29.37 +/- 3.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.4        |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 56587161    |
| train/                  |             |
|    approx_kl            | 0.036833875 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.00013     |
|    loss                 | 0.149       |
|    n_updates            | 4605        |
|    policy_gradient_loss | -0.00484    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.2     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 922      |
|    time_elapsed    | 346040   |
|    total_timesteps | 56647680 |
---------------------------------
Eval num_timesteps=56648602, episode_reward=0.14 +/- 0.97
Episode length: 29.51 +/- 3.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.5       |
|    mean_reward          | 0.142      |
| time/                   |            |
|    total_timesteps      | 56648602   |
| train/                  |            |
|    approx_kl            | 0.03648765 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.00013    |
|    loss                 | 0.106      |
|    n_updates            | 4610       |
|    policy_gradient_loss | -0.0043    |
|    value_loss           | 0.24       |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.142
SELFPLAY: new best model, bumping up generation to 46
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 923      |
|    time_elapsed    | 346389   |
|    total_timesteps | 56709120 |
---------------------------------
Eval num_timesteps=56710043, episode_reward=0.04 +/- 0.99
Episode length: 29.69 +/- 2.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.7       |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 56710043   |
| train/                  |            |
|    approx_kl            | 0.03798973 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.664     |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.00013    |
|    loss                 | 0.0972     |
|    n_updates            | 4615       |
|    policy_gradient_loss | -0.00421   |
|    value_loss           | 0.244      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 924      |
|    time_elapsed    | 346737   |
|    total_timesteps | 56770560 |
---------------------------------
Eval num_timesteps=56771484, episode_reward=0.01 +/- 0.98
Episode length: 29.87 +/- 1.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 56771484    |
| train/                  |             |
|    approx_kl            | 0.035773635 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.00013     |
|    loss                 | 0.0756      |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.005      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 925      |
|    time_elapsed    | 347087   |
|    total_timesteps | 56832000 |
---------------------------------
Eval num_timesteps=56832925, episode_reward=0.08 +/- 0.98
Episode length: 29.87 +/- 1.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.084      |
| time/                   |            |
|    total_timesteps      | 56832925   |
| train/                  |            |
|    approx_kl            | 0.03553529 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.00013    |
|    loss                 | 0.133      |
|    n_updates            | 4625       |
|    policy_gradient_loss | -0.0047    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 926      |
|    time_elapsed    | 347436   |
|    total_timesteps | 56893440 |
---------------------------------
Eval num_timesteps=56894366, episode_reward=0.05 +/- 0.99
Episode length: 29.70 +/- 2.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 56894366    |
| train/                  |             |
|    approx_kl            | 0.038320817 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.000129    |
|    loss                 | 0.0998      |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 927      |
|    time_elapsed    | 347785   |
|    total_timesteps | 56954880 |
---------------------------------
Eval num_timesteps=56955807, episode_reward=0.04 +/- 0.98
Episode length: 29.82 +/- 2.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 56955807    |
| train/                  |             |
|    approx_kl            | 0.036499403 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.000129    |
|    loss                 | 0.102       |
|    n_updates            | 4635        |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 928      |
|    time_elapsed    | 348134   |
|    total_timesteps | 57016320 |
---------------------------------
Eval num_timesteps=57017248, episode_reward=0.02 +/- 0.99
Episode length: 29.94 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 57017248    |
| train/                  |             |
|    approx_kl            | 0.038205814 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.000129    |
|    loss                 | 0.0717      |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 929      |
|    time_elapsed    | 348483   |
|    total_timesteps | 57077760 |
---------------------------------
Eval num_timesteps=57078689, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 57078689    |
| train/                  |             |
|    approx_kl            | 0.034415103 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.635      |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.000129    |
|    loss                 | 0.0243      |
|    n_updates            | 4645        |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 930      |
|    time_elapsed    | 348832   |
|    total_timesteps | 57139200 |
---------------------------------
Eval num_timesteps=57140130, episode_reward=0.08 +/- 0.98
Episode length: 29.75 +/- 2.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 57140130    |
| train/                  |             |
|    approx_kl            | 0.038959626 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.000129    |
|    loss                 | 0.102       |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 163      |
|    iterations      | 931      |
|    time_elapsed    | 349182   |
|    total_timesteps | 57200640 |
---------------------------------
Eval num_timesteps=57201571, episode_reward=0.06 +/- 0.98
Episode length: 29.79 +/- 2.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 57201571    |
| train/                  |             |
|    approx_kl            | 0.035420243 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.633      |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.000128    |
|    loss                 | 0.0704      |
|    n_updates            | 4655        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 932      |
|    time_elapsed    | 349533   |
|    total_timesteps | 57262080 |
---------------------------------
Eval num_timesteps=57263012, episode_reward=0.02 +/- 0.98
Episode length: 29.80 +/- 2.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.8      |
|    mean_reward          | 0.022     |
| time/                   |           |
|    total_timesteps      | 57263012  |
| train/                  |           |
|    approx_kl            | 0.0375852 |
|    clip_fraction        | 0.198     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.629    |
|    explained_variance   | 0.364     |
|    learning_rate        | 0.000128  |
|    loss                 | 0.162     |
|    n_updates            | 4660      |
|    policy_gradient_loss | -0.00521  |
|    value_loss           | 0.239     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 933      |
|    time_elapsed    | 349882   |
|    total_timesteps | 57323520 |
---------------------------------
Eval num_timesteps=57324453, episode_reward=0.14 +/- 0.97
Episode length: 29.92 +/- 1.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 57324453   |
| train/                  |            |
|    approx_kl            | 0.03547754 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.636     |
|    explained_variance   | 0.371      |
|    learning_rate        | 0.000128   |
|    loss                 | 0.121      |
|    n_updates            | 4665       |
|    policy_gradient_loss | -0.00454   |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 934      |
|    time_elapsed    | 350232   |
|    total_timesteps | 57384960 |
---------------------------------
Eval num_timesteps=57385894, episode_reward=0.05 +/- 0.98
Episode length: 29.79 +/- 2.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.8       |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 57385894   |
| train/                  |            |
|    approx_kl            | 0.03598557 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.377      |
|    learning_rate        | 0.000128   |
|    loss                 | 0.0792     |
|    n_updates            | 4670       |
|    policy_gradient_loss | -0.00412   |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 935      |
|    time_elapsed    | 350581   |
|    total_timesteps | 57446400 |
---------------------------------
Eval num_timesteps=57447335, episode_reward=0.09 +/- 0.98
Episode length: 29.94 +/- 1.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.094      |
| time/                   |            |
|    total_timesteps      | 57447335   |
| train/                  |            |
|    approx_kl            | 0.03745936 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.623     |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.000128   |
|    loss                 | 0.197      |
|    n_updates            | 4675       |
|    policy_gradient_loss | -0.00431   |
|    value_loss           | 0.245      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 936      |
|    time_elapsed    | 350930   |
|    total_timesteps | 57507840 |
---------------------------------
Eval num_timesteps=57508776, episode_reward=0.09 +/- 0.98
Episode length: 29.95 +/- 1.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.088      |
| time/                   |            |
|    total_timesteps      | 57508776   |
| train/                  |            |
|    approx_kl            | 0.03598159 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.627     |
|    explained_variance   | 0.379      |
|    learning_rate        | 0.000127   |
|    loss                 | 0.13       |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.00367   |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 937      |
|    time_elapsed    | 351279   |
|    total_timesteps | 57569280 |
---------------------------------
Eval num_timesteps=57570217, episode_reward=0.10 +/- 0.98
Episode length: 29.85 +/- 2.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 57570217    |
| train/                  |             |
|    approx_kl            | 0.035869118 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.000127    |
|    loss                 | 0.0748      |
|    n_updates            | 4685        |
|    policy_gradient_loss | -0.00468    |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 938      |
|    time_elapsed    | 351628   |
|    total_timesteps | 57630720 |
---------------------------------
Eval num_timesteps=57631658, episode_reward=0.17 +/- 0.97
Episode length: 29.85 +/- 2.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.172      |
| time/                   |            |
|    total_timesteps      | 57631658   |
| train/                  |            |
|    approx_kl            | 0.03700671 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.618     |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.000127   |
|    loss                 | 0.0841     |
|    n_updates            | 4690       |
|    policy_gradient_loss | -0.00411   |
|    value_loss           | 0.239      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.172
SELFPLAY: new best model, bumping up generation to 47
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 939      |
|    time_elapsed    | 351977   |
|    total_timesteps | 57692160 |
---------------------------------
Eval num_timesteps=57693099, episode_reward=-0.01 +/- 0.98
Episode length: 29.99 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.006     |
| time/                   |            |
|    total_timesteps      | 57693099   |
| train/                  |            |
|    approx_kl            | 0.03587067 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.659     |
|    explained_variance   | 0.386      |
|    learning_rate        | 0.000127   |
|    loss                 | 0.0654     |
|    n_updates            | 4695       |
|    policy_gradient_loss | -0.00467   |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 940      |
|    time_elapsed    | 352326   |
|    total_timesteps | 57753600 |
---------------------------------
Eval num_timesteps=57754540, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 57754540    |
| train/                  |             |
|    approx_kl            | 0.038017757 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.000127    |
|    loss                 | 0.0831      |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 941      |
|    time_elapsed    | 352675   |
|    total_timesteps | 57815040 |
---------------------------------
Eval num_timesteps=57815981, episode_reward=0.01 +/- 0.98
Episode length: 29.90 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 57815981    |
| train/                  |             |
|    approx_kl            | 0.037069514 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.000127    |
|    loss                 | 0.134       |
|    n_updates            | 4705        |
|    policy_gradient_loss | -0.00359    |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 942      |
|    time_elapsed    | 353027   |
|    total_timesteps | 57876480 |
---------------------------------
Eval num_timesteps=57877422, episode_reward=0.09 +/- 0.99
Episode length: 29.98 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 57877422    |
| train/                  |             |
|    approx_kl            | 0.037134513 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.000126    |
|    loss                 | 0.0654      |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.00462    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 943      |
|    time_elapsed    | 353378   |
|    total_timesteps | 57937920 |
---------------------------------
Eval num_timesteps=57938863, episode_reward=0.08 +/- 0.98
Episode length: 29.93 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 57938863   |
| train/                  |            |
|    approx_kl            | 0.03743234 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.636     |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.000126   |
|    loss                 | 0.122      |
|    n_updates            | 4715       |
|    policy_gradient_loss | -0.00506   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 944      |
|    time_elapsed    | 353728   |
|    total_timesteps | 57999360 |
---------------------------------
Eval num_timesteps=58000304, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.058      |
| time/                   |            |
|    total_timesteps      | 58000304   |
| train/                  |            |
|    approx_kl            | 0.03627212 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.633     |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.000126   |
|    loss                 | 0.0796     |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.00473   |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 945      |
|    time_elapsed    | 354078   |
|    total_timesteps | 58060800 |
---------------------------------
Eval num_timesteps=58061745, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 1.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 58061745    |
| train/                  |             |
|    approx_kl            | 0.036452353 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.629      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.000126    |
|    loss                 | 0.0707      |
|    n_updates            | 4725        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 946      |
|    time_elapsed    | 354427   |
|    total_timesteps | 58122240 |
---------------------------------
Eval num_timesteps=58123186, episode_reward=-0.03 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 58123186    |
| train/                  |             |
|    approx_kl            | 0.036612313 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.629      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000126    |
|    loss                 | 0.118       |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 947      |
|    time_elapsed    | 354777   |
|    total_timesteps | 58183680 |
---------------------------------
Eval num_timesteps=58184627, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 58184627    |
| train/                  |             |
|    approx_kl            | 0.034697104 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.63       |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.000125    |
|    loss                 | 0.0947      |
|    n_updates            | 4735        |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 948      |
|    time_elapsed    | 355127   |
|    total_timesteps | 58245120 |
---------------------------------
Eval num_timesteps=58246068, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 58246068   |
| train/                  |            |
|    approx_kl            | 0.03552448 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.634     |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.000125   |
|    loss                 | 0.091      |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.00435   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 949      |
|    time_elapsed    | 355476   |
|    total_timesteps | 58306560 |
---------------------------------
Eval num_timesteps=58307509, episode_reward=0.02 +/- 0.98
Episode length: 29.90 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 58307509    |
| train/                  |             |
|    approx_kl            | 0.036571044 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.636      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.000125    |
|    loss                 | 0.1         |
|    n_updates            | 4745        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 950      |
|    time_elapsed    | 355825   |
|    total_timesteps | 58368000 |
---------------------------------
Eval num_timesteps=58368950, episode_reward=0.05 +/- 0.99
Episode length: 29.95 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 58368950   |
| train/                  |            |
|    approx_kl            | 0.03727815 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.63      |
|    explained_variance   | 0.359      |
|    learning_rate        | 0.000125   |
|    loss                 | 0.0972     |
|    n_updates            | 4750       |
|    policy_gradient_loss | -0.00545   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 951      |
|    time_elapsed    | 356173   |
|    total_timesteps | 58429440 |
---------------------------------
Eval num_timesteps=58430391, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 58430391   |
| train/                  |            |
|    approx_kl            | 0.03482423 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.631     |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.000125   |
|    loss                 | 0.0744     |
|    n_updates            | 4755       |
|    policy_gradient_loss | -0.00521   |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 952      |
|    time_elapsed    | 356523   |
|    total_timesteps | 58490880 |
---------------------------------
Eval num_timesteps=58491832, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.08      |
| time/                   |           |
|    total_timesteps      | 58491832  |
| train/                  |           |
|    approx_kl            | 0.0354611 |
|    clip_fraction        | 0.196     |
|    clip_range           | 0.17      |
|    entropy_loss         | -0.626    |
|    explained_variance   | 0.364     |
|    learning_rate        | 0.000125  |
|    loss                 | 0.0827    |
|    n_updates            | 4760      |
|    policy_gradient_loss | -0.00461  |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 164      |
|    iterations      | 953      |
|    time_elapsed    | 356874   |
|    total_timesteps | 58552320 |
---------------------------------
Eval num_timesteps=58553273, episode_reward=0.14 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 58553273    |
| train/                  |             |
|    approx_kl            | 0.035902936 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.627      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.000124    |
|    loss                 | 0.0736      |
|    n_updates            | 4765        |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 954      |
|    time_elapsed    | 357224   |
|    total_timesteps | 58613760 |
---------------------------------
Eval num_timesteps=58614714, episode_reward=0.03 +/- 0.99
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 58614714    |
| train/                  |             |
|    approx_kl            | 0.034572244 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.621      |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.000124    |
|    loss                 | 0.125       |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00519    |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 955      |
|    time_elapsed    | 357574   |
|    total_timesteps | 58675200 |
---------------------------------
Eval num_timesteps=58676155, episode_reward=0.06 +/- 0.99
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 58676155   |
| train/                  |            |
|    approx_kl            | 0.03618501 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.619     |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.000124   |
|    loss                 | 0.113      |
|    n_updates            | 4775       |
|    policy_gradient_loss | -0.00474   |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 956      |
|    time_elapsed    | 357924   |
|    total_timesteps | 58736640 |
---------------------------------
Eval num_timesteps=58737596, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 58737596    |
| train/                  |             |
|    approx_kl            | 0.034914408 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.000124    |
|    loss                 | 0.106       |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 957      |
|    time_elapsed    | 358274   |
|    total_timesteps | 58798080 |
---------------------------------
Eval num_timesteps=58799037, episode_reward=0.07 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 58799037    |
| train/                  |             |
|    approx_kl            | 0.035129074 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.000124    |
|    loss                 | 0.0955      |
|    n_updates            | 4785        |
|    policy_gradient_loss | -0.00572    |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 958      |
|    time_elapsed    | 358624   |
|    total_timesteps | 58859520 |
---------------------------------
Eval num_timesteps=58860478, episode_reward=0.14 +/- 0.98
Episode length: 29.97 +/- 0.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 58860478    |
| train/                  |             |
|    approx_kl            | 0.035372242 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.17        |
|    entropy_loss         | -0.62       |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.000123    |
|    loss                 | 0.112       |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 48
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 959      |
|    time_elapsed    | 358973   |
|    total_timesteps | 58920960 |
---------------------------------
Eval num_timesteps=58921919, episode_reward=0.02 +/- 0.99
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 58921919   |
| train/                  |            |
|    approx_kl            | 0.03850871 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.637     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.000123   |
|    loss                 | 0.108      |
|    n_updates            | 4795       |
|    policy_gradient_loss | -0.00497   |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 960      |
|    time_elapsed    | 359322   |
|    total_timesteps | 58982400 |
---------------------------------
Eval num_timesteps=58983360, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 1.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.01       |
| time/                   |            |
|    total_timesteps      | 58983360   |
| train/                  |            |
|    approx_kl            | 0.03610796 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.633     |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.000123   |
|    loss                 | 0.0889     |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.0045    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 961      |
|    time_elapsed    | 359671   |
|    total_timesteps | 59043840 |
---------------------------------
Eval num_timesteps=59044801, episode_reward=0.07 +/- 0.99
Episode length: 30.03 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 59044801   |
| train/                  |            |
|    approx_kl            | 0.03497614 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.17       |
|    entropy_loss         | -0.635     |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.000123   |
|    loss                 | 0.133      |
|    n_updates            | 4805       |
|    policy_gradient_loss | -0.00379   |
|    value_loss           | 0.24       |
----------------------------------------
slurmstepd-n16: error: *** JOB 582 ON n16 CANCELLED AT 2024-06-14T03:20:57 DUE TO TIME LIMIT ***
