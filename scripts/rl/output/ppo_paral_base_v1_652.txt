CUDA Available: True
CPU Model: AMD EPYC 7313P 16-Core Processor
GPU Model: Tesla T4

num parallel processes: 4

CUDA available: True
net architecture - {'net_arch': {'pi': [128, 128, 128, 128, 128, 128, 128, 128], 'vf': [64, 64, 64, 64, 64, 64, 64, 64]}}
params: 
NUM_TIMESTEPS -4000000
EVAL_FREQ=15361
EVAL_EPISODES=250
BEST_THRESHOLD=0.45
LOGDIR=scripts/rl/output/paral/base/v1/
model params: 
 {'learning_rate': 8e-05, 'n_steps': 15360, 'n_epochs': 10, 'batch_size': 64, 'verbose': 1}
starting model: scripts/rl/output/paral/base/v0/history_0010
Ep done - 500.
Ep done - 500.
Ep done - 500.
Ep done - 500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 695      |
|    iterations      | 1        |
|    time_elapsed    | 88       |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61444, episode_reward=0.38 +/- 0.91
Episode length: 29.71 +/- 3.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.7       |
|    mean_reward          | 0.38       |
| time/                   |            |
|    total_timesteps      | 61444      |
| train/                  |            |
|    approx_kl            | 0.03075005 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.51      |
|    explained_variance   | 0.14       |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0221     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0588    |
|    value_loss           | 0.228      |
----------------------------------------
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 512      |
|    iterations      | 2        |
|    time_elapsed    | 239      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=122888, episode_reward=0.35 +/- 0.92
Episode length: 30.06 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.352       |
| time/                   |             |
|    total_timesteps      | 122888      |
| train/                  |             |
|    approx_kl            | 0.031861752 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.277       |
|    learning_rate        | 8e-05       |
|    loss                 | -0.0099     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0599     |
|    value_loss           | 0.216       |
-----------------------------------------
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 466      |
|    iterations      | 3        |
|    time_elapsed    | 395      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184332, episode_reward=0.42 +/- 0.90
Episode length: 30.04 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.416     |
| time/                   |           |
|    total_timesteps      | 184332    |
| train/                  |           |
|    approx_kl            | 0.0335984 |
|    clip_fraction        | 0.288     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.43     |
|    explained_variance   | 0.277     |
|    learning_rate        | 8e-05     |
|    loss                 | 0.0703    |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0598   |
|    value_loss           | 0.219     |
---------------------------------------
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 446      |
|    iterations      | 4        |
|    time_elapsed    | 550      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=245776, episode_reward=0.42 +/- 0.88
Episode length: 29.80 +/- 2.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.42        |
| time/                   |             |
|    total_timesteps      | 245776      |
| train/                  |             |
|    approx_kl            | 0.034737118 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.284       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.00598     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0593     |
|    value_loss           | 0.214       |
-----------------------------------------
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 435      |
|    iterations      | 5        |
|    time_elapsed    | 704      |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307220, episode_reward=0.54 +/- 0.81
Episode length: 29.92 +/- 2.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.544      |
| time/                   |            |
|    total_timesteps      | 307220     |
| train/                  |            |
|    approx_kl            | 0.03676912 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.267      |
|    learning_rate        | 8e-05      |
|    loss                 | -0.0237    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0589    |
|    value_loss           | 0.211      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.544
SELFPLAY: new best model, bumping up generation to 1
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 422      |
|    iterations      | 6        |
|    time_elapsed    | 872      |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=368664, episode_reward=0.31 +/- 0.94
Episode length: 29.54 +/- 3.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.312       |
| time/                   |             |
|    total_timesteps      | 368664      |
| train/                  |             |
|    approx_kl            | 0.038198926 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.285       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0237      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0599     |
|    value_loss           | 0.22        |
-----------------------------------------
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 418      |
|    iterations      | 7        |
|    time_elapsed    | 1027     |
|    total_timesteps | 430080   |
---------------------------------
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
Eval num_timesteps=430108, episode_reward=0.30 +/- 0.94
Episode length: 29.53 +/- 3.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.304       |
| time/                   |             |
|    total_timesteps      | 430108      |
| train/                  |             |
|    approx_kl            | 0.040001757 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.27        |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0135      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0597     |
|    value_loss           | 0.226       |
-----------------------------------------
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 415      |
|    iterations      | 8        |
|    time_elapsed    | 1183     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=491552, episode_reward=0.41 +/- 0.90
Episode length: 29.88 +/- 2.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.412       |
| time/                   |             |
|    total_timesteps      | 491552      |
| train/                  |             |
|    approx_kl            | 0.041701857 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.305       |
|    learning_rate        | 8e-05       |
|    loss                 | -0.0171     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0598     |
|    value_loss           | 0.218       |
-----------------------------------------
Ep done - 5000.
Ep done - 5000.
Ep done - 5000.
Ep done - 5000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 412      |
|    iterations      | 9        |
|    time_elapsed    | 1339     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=552996, episode_reward=0.27 +/- 0.95
Episode length: 29.63 +/- 3.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.6       |
|    mean_reward          | 0.268      |
| time/                   |            |
|    total_timesteps      | 552996     |
| train/                  |            |
|    approx_kl            | 0.04312273 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.266      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0389     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0586    |
|    value_loss           | 0.225      |
----------------------------------------
Ep done - 5500.
Ep done - 5500.
Ep done - 5500.
Ep done - 5500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 410      |
|    iterations      | 10       |
|    time_elapsed    | 1495     |
|    total_timesteps | 614400   |
---------------------------------
Eval num_timesteps=614440, episode_reward=0.42 +/- 0.90
Episode length: 29.56 +/- 3.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.6        |
|    mean_reward          | 0.424       |
| time/                   |             |
|    total_timesteps      | 614440      |
| train/                  |             |
|    approx_kl            | 0.044661667 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.293       |
|    learning_rate        | 8e-05       |
|    loss                 | -0.0288     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0583     |
|    value_loss           | 0.217       |
-----------------------------------------
Ep done - 6000.
Ep done - 6000.
Ep done - 6000.
Ep done - 6000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 409      |
|    iterations      | 11       |
|    time_elapsed    | 1650     |
|    total_timesteps | 675840   |
---------------------------------
Eval num_timesteps=675884, episode_reward=0.44 +/- 0.88
Episode length: 29.53 +/- 3.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.436       |
| time/                   |             |
|    total_timesteps      | 675884      |
| train/                  |             |
|    approx_kl            | 0.046594925 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.284       |
|    learning_rate        | 8e-05       |
|    loss                 | -0.0143     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0582     |
|    value_loss           | 0.216       |
-----------------------------------------
Ep done - 6500.
Ep done - 6500.
Ep done - 6500.
Ep done - 6500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 408      |
|    iterations      | 12       |
|    time_elapsed    | 1805     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=737328, episode_reward=0.44 +/- 0.87
Episode length: 29.66 +/- 3.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 0.444       |
| time/                   |             |
|    total_timesteps      | 737328      |
| train/                  |             |
|    approx_kl            | 0.048544865 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.32        |
|    learning_rate        | 8e-05       |
|    loss                 | -0.0136     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0582     |
|    value_loss           | 0.205       |
-----------------------------------------
Ep done - 7000.
Ep done - 7000.
Ep done - 7000.
Ep done - 7000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 407      |
|    iterations      | 13       |
|    time_elapsed    | 1961     |
|    total_timesteps | 798720   |
---------------------------------
Ep done - 7500.
Ep done - 7500.
Ep done - 7500.
Eval num_timesteps=798772, episode_reward=0.45 +/- 0.88
Episode length: 29.94 +/- 2.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.452       |
| time/                   |             |
|    total_timesteps      | 798772      |
| train/                  |             |
|    approx_kl            | 0.049065113 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.289       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0153      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0566     |
|    value_loss           | 0.214       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.452
SELFPLAY: new best model, bumping up generation to 2
Ep done - 7500.
Ep done - 8000.
Ep done - 8000.
Ep done - 8000.
Ep done - 8000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 404      |
|    iterations      | 14       |
|    time_elapsed    | 2128     |
|    total_timesteps | 860160   |
---------------------------------
Eval num_timesteps=860216, episode_reward=0.30 +/- 0.94
Episode length: 29.86 +/- 2.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.296      |
| time/                   |            |
|    total_timesteps      | 860216     |
| train/                  |            |
|    approx_kl            | 0.04979513 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.286      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0387     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0571    |
|    value_loss           | 0.228      |
----------------------------------------
Ep done - 8500.
Ep done - 8500.
Ep done - 8500.
Ep done - 8500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 403      |
|    iterations      | 15       |
|    time_elapsed    | 2284     |
|    total_timesteps | 921600   |
---------------------------------
Eval num_timesteps=921660, episode_reward=0.34 +/- 0.92
Episode length: 29.85 +/- 2.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.34      |
| time/                   |           |
|    total_timesteps      | 921660    |
| train/                  |           |
|    approx_kl            | 0.0516235 |
|    clip_fraction        | 0.323     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.1      |
|    explained_variance   | 0.306     |
|    learning_rate        | 8e-05     |
|    loss                 | 0.0148    |
|    n_updates            | 230       |
|    policy_gradient_loss | -0.0572   |
|    value_loss           | 0.227     |
---------------------------------------
Ep done - 9000.
Ep done - 9000.
Ep done - 9000.
Ep done - 9000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 402      |
|    iterations      | 16       |
|    time_elapsed    | 2439     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=983104, episode_reward=0.27 +/- 0.95
Episode length: 29.98 +/- 1.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.268      |
| time/                   |            |
|    total_timesteps      | 983104     |
| train/                  |            |
|    approx_kl            | 0.05357329 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.298      |
|    learning_rate        | 8e-05      |
|    loss                 | -0.00604   |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0577    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 9500.
Ep done - 9500.
Ep done - 9500.
Ep done - 9500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 402      |
|    iterations      | 17       |
|    time_elapsed    | 2594     |
|    total_timesteps | 1044480  |
---------------------------------
Eval num_timesteps=1044548, episode_reward=0.32 +/- 0.93
Episode length: 29.74 +/- 2.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 0.324       |
| time/                   |             |
|    total_timesteps      | 1044548     |
| train/                  |             |
|    approx_kl            | 0.054948617 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.3         |
|    learning_rate        | 8e-05       |
|    loss                 | 0.00309     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0558     |
|    value_loss           | 0.227       |
-----------------------------------------
Ep done - 10000.
Ep done - 10000.
Ep done - 10000.
Ep done - 10000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 402      |
|    iterations      | 18       |
|    time_elapsed    | 2748     |
|    total_timesteps | 1105920  |
---------------------------------
Eval num_timesteps=1105992, episode_reward=0.36 +/- 0.91
Episode length: 28.58 +/- 5.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.6        |
|    mean_reward          | 0.36        |
| time/                   |             |
|    total_timesteps      | 1105992     |
| train/                  |             |
|    approx_kl            | 0.055797156 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.332       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0137      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0562     |
|    value_loss           | 0.218       |
-----------------------------------------
Ep done - 10500.
Ep done - 10500.
Ep done - 10500.
Ep done - 10500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 402      |
|    iterations      | 19       |
|    time_elapsed    | 2903     |
|    total_timesteps | 1167360  |
---------------------------------
Eval num_timesteps=1167436, episode_reward=0.44 +/- 0.88
Episode length: 28.19 +/- 6.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.2        |
|    mean_reward          | 0.436       |
| time/                   |             |
|    total_timesteps      | 1167436     |
| train/                  |             |
|    approx_kl            | 0.055700812 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.321       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0237      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0553     |
|    value_loss           | 0.218       |
-----------------------------------------
Ep done - 11000.
Ep done - 11000.
Ep done - 11000.
Ep done - 11000.
Ep done - 11500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 401      |
|    iterations      | 20       |
|    time_elapsed    | 3057     |
|    total_timesteps | 1228800  |
---------------------------------
Ep done - 11500.
Ep done - 11500.
Ep done - 11500.
Eval num_timesteps=1228880, episode_reward=0.48 +/- 0.86
Episode length: 28.82 +/- 5.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.8       |
|    mean_reward          | 0.48       |
| time/                   |            |
|    total_timesteps      | 1228880    |
| train/                  |            |
|    approx_kl            | 0.05919795 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.284      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0135     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0555    |
|    value_loss           | 0.223      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.48
SELFPLAY: new best model, bumping up generation to 3
Ep done - 12000.
Ep done - 12000.
Ep done - 12000.
Ep done - 12000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 400      |
|    iterations      | 21       |
|    time_elapsed    | 3222     |
|    total_timesteps | 1290240  |
---------------------------------
Eval num_timesteps=1290324, episode_reward=0.22 +/- 0.97
Episode length: 29.65 +/- 3.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.6        |
|    mean_reward          | 0.216       |
| time/                   |             |
|    total_timesteps      | 1290324     |
| train/                  |             |
|    approx_kl            | 0.058779605 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.983      |
|    explained_variance   | 0.333       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.023       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.056      |
|    value_loss           | 0.223       |
-----------------------------------------
Ep done - 12500.
Ep done - 12500.
Ep done - 12500.
Ep done - 12500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.2     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 400      |
|    iterations      | 22       |
|    time_elapsed    | 3377     |
|    total_timesteps | 1351680  |
---------------------------------
Eval num_timesteps=1351768, episode_reward=0.29 +/- 0.95
Episode length: 29.48 +/- 3.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.5       |
|    mean_reward          | 0.292      |
| time/                   |            |
|    total_timesteps      | 1351768    |
| train/                  |            |
|    approx_kl            | 0.06199665 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.962     |
|    explained_variance   | 0.324      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0368     |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.229      |
----------------------------------------
Ep done - 13000.
Ep done - 13000.
Ep done - 13000.
Ep done - 13000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.2     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 399      |
|    iterations      | 23       |
|    time_elapsed    | 3533     |
|    total_timesteps | 1413120  |
---------------------------------
Eval num_timesteps=1413212, episode_reward=0.26 +/- 0.95
Episode length: 29.52 +/- 3.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.256       |
| time/                   |             |
|    total_timesteps      | 1413212     |
| train/                  |             |
|    approx_kl            | 0.062988676 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.948      |
|    explained_variance   | 0.321       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.03        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.055      |
|    value_loss           | 0.227       |
-----------------------------------------
Ep done - 13500.
Ep done - 13500.
Ep done - 13500.
Ep done - 13500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 399      |
|    iterations      | 24       |
|    time_elapsed    | 3688     |
|    total_timesteps | 1474560  |
---------------------------------
Eval num_timesteps=1474656, episode_reward=0.44 +/- 0.88
Episode length: 29.58 +/- 3.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.6        |
|    mean_reward          | 0.444       |
| time/                   |             |
|    total_timesteps      | 1474656     |
| train/                  |             |
|    approx_kl            | 0.062258005 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.933      |
|    explained_variance   | 0.317       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0276      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0544     |
|    value_loss           | 0.224       |
-----------------------------------------
Ep done - 14000.
Ep done - 14000.
Ep done - 14000.
Ep done - 14000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 399      |
|    iterations      | 25       |
|    time_elapsed    | 3843     |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1536100, episode_reward=0.28 +/- 0.94
Episode length: 29.49 +/- 3.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.5       |
|    mean_reward          | 0.28       |
| time/                   |            |
|    total_timesteps      | 1536100    |
| train/                  |            |
|    approx_kl            | 0.06456926 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.912     |
|    explained_variance   | 0.309      |
|    learning_rate        | 8e-05      |
|    loss                 | -0.00702   |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 14500.
Ep done - 14500.
Ep done - 14500.
Ep done - 14500.
Ep done - 15000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 399      |
|    iterations      | 26       |
|    time_elapsed    | 3998     |
|    total_timesteps | 1597440  |
---------------------------------
Ep done - 15000.
Ep done - 15000.
Ep done - 15000.
Eval num_timesteps=1597544, episode_reward=0.34 +/- 0.92
Episode length: 29.18 +/- 4.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 0.344       |
| time/                   |             |
|    total_timesteps      | 1597544     |
| train/                  |             |
|    approx_kl            | 0.064891934 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.893      |
|    explained_variance   | 0.341       |
|    learning_rate        | 8e-05       |
|    loss                 | -0.00362    |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0535     |
|    value_loss           | 0.226       |
-----------------------------------------
Ep done - 15500.
Ep done - 15500.
Ep done - 15500.
Ep done - 15500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 399      |
|    iterations      | 27       |
|    time_elapsed    | 4153     |
|    total_timesteps | 1658880  |
---------------------------------
Eval num_timesteps=1658988, episode_reward=0.26 +/- 0.95
Episode length: 29.76 +/- 2.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.8      |
|    mean_reward          | 0.264     |
| time/                   |           |
|    total_timesteps      | 1658988   |
| train/                  |           |
|    approx_kl            | 0.0666107 |
|    clip_fraction        | 0.321     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.883    |
|    explained_variance   | 0.343     |
|    learning_rate        | 8e-05     |
|    loss                 | 0.00567   |
|    n_updates            | 350       |
|    policy_gradient_loss | -0.0534   |
|    value_loss           | 0.219     |
---------------------------------------
Ep done - 16000.
Ep done - 16000.
Ep done - 16000.
Ep done - 16000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 399      |
|    iterations      | 28       |
|    time_elapsed    | 4307     |
|    total_timesteps | 1720320  |
---------------------------------
Eval num_timesteps=1720432, episode_reward=0.34 +/- 0.91
Episode length: 29.28 +/- 4.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.3       |
|    mean_reward          | 0.344      |
| time/                   |            |
|    total_timesteps      | 1720432    |
| train/                  |            |
|    approx_kl            | 0.06515679 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.864     |
|    explained_variance   | 0.315      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.023      |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0515    |
|    value_loss           | 0.218      |
----------------------------------------
Ep done - 16500.
Ep done - 16500.
Ep done - 16500.
Ep done - 16500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 399      |
|    iterations      | 29       |
|    time_elapsed    | 4455     |
|    total_timesteps | 1781760  |
---------------------------------
Eval num_timesteps=1781876, episode_reward=0.43 +/- 0.89
Episode length: 29.78 +/- 2.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.8       |
|    mean_reward          | 0.432      |
| time/                   |            |
|    total_timesteps      | 1781876    |
| train/                  |            |
|    approx_kl            | 0.06853043 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.847     |
|    explained_variance   | 0.327      |
|    learning_rate        | 8e-05      |
|    loss                 | -0.0144    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0517    |
|    value_loss           | 0.213      |
----------------------------------------
Ep done - 17000.
Ep done - 17000.
Ep done - 17000.
Ep done - 17000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 399      |
|    iterations      | 30       |
|    time_elapsed    | 4610     |
|    total_timesteps | 1843200  |
---------------------------------
Eval num_timesteps=1843320, episode_reward=0.30 +/- 0.93
Episode length: 29.87 +/- 2.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 1843320    |
| train/                  |            |
|    approx_kl            | 0.06904446 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.836     |
|    explained_variance   | 0.329      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0088     |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0506    |
|    value_loss           | 0.21       |
----------------------------------------
Ep done - 17500.
Ep done - 17500.
Ep done - 17500.
Ep done - 17500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.3     |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 399      |
|    iterations      | 31       |
|    time_elapsed    | 4764     |
|    total_timesteps | 1904640  |
---------------------------------
Eval num_timesteps=1904764, episode_reward=0.50 +/- 0.85
Episode length: 30.00 +/- 1.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 1904764    |
| train/                  |            |
|    approx_kl            | 0.07129865 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.83      |
|    explained_variance   | 0.311      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0439     |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.216      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5
SELFPLAY: new best model, bumping up generation to 4
Ep done - 18000.
Ep done - 18000.
Ep done - 18000.
Ep done - 18000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 393      |
|    iterations      | 32       |
|    time_elapsed    | 4991     |
|    total_timesteps | 1966080  |
---------------------------------
Ep done - 18500.
Ep done - 18500.
Ep done - 18500.
Ep done - 18500.
Eval num_timesteps=1966208, episode_reward=0.18 +/- 0.96
Episode length: 30.05 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.184      |
| time/                   |            |
|    total_timesteps      | 1966208    |
| train/                  |            |
|    approx_kl            | 0.07224504 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.811     |
|    explained_variance   | 0.317      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.00134    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0517    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 19000.
Ep done - 19000.
Ep done - 19000.
Ep done - 19000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 385      |
|    iterations      | 33       |
|    time_elapsed    | 5261     |
|    total_timesteps | 2027520  |
---------------------------------
Eval num_timesteps=2027652, episode_reward=0.19 +/- 0.97
Episode length: 30.04 +/- 0.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.188      |
| time/                   |            |
|    total_timesteps      | 2027652    |
| train/                  |            |
|    approx_kl            | 0.07528599 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.8       |
|    explained_variance   | 0.318      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0468     |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.233      |
----------------------------------------
Ep done - 19500.
Ep done - 19500.
Ep done - 19500.
Ep done - 19500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 377      |
|    iterations      | 34       |
|    time_elapsed    | 5526     |
|    total_timesteps | 2088960  |
---------------------------------
Eval num_timesteps=2089096, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 2089096     |
| train/                  |             |
|    approx_kl            | 0.073767595 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.799      |
|    explained_variance   | 0.314       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0274      |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0505     |
|    value_loss           | 0.233       |
-----------------------------------------
Ep done - 20000.
Ep done - 20000.
Ep done - 20000.
Ep done - 20000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 373      |
|    iterations      | 35       |
|    time_elapsed    | 5760     |
|    total_timesteps | 2150400  |
---------------------------------
Eval num_timesteps=2150540, episode_reward=0.24 +/- 0.96
Episode length: 30.04 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.24       |
| time/                   |            |
|    total_timesteps      | 2150540    |
| train/                  |            |
|    approx_kl            | 0.07625579 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.315      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.00487    |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 20500.
Ep done - 20500.
Ep done - 20500.
Ep done - 20500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 369      |
|    iterations      | 36       |
|    time_elapsed    | 5981     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2211984, episode_reward=0.16 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.156       |
| time/                   |             |
|    total_timesteps      | 2211984     |
| train/                  |             |
|    approx_kl            | 0.076089546 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.329       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.00932     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0502     |
|    value_loss           | 0.226       |
-----------------------------------------
Ep done - 21000.
Ep done - 21000.
Ep done - 21000.
Ep done - 21000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 363      |
|    iterations      | 37       |
|    time_elapsed    | 6258     |
|    total_timesteps | 2273280  |
---------------------------------
Eval num_timesteps=2273428, episode_reward=0.23 +/- 0.95
Episode length: 30.08 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.232      |
| time/                   |            |
|    total_timesteps      | 2273428    |
| train/                  |            |
|    approx_kl            | 0.07909725 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.766     |
|    explained_variance   | 0.339      |
|    learning_rate        | 8e-05      |
|    loss                 | -0.00479   |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0506    |
|    value_loss           | 0.225      |
----------------------------------------
Ep done - 21500.
Ep done - 21500.
Ep done - 21500.
Ep done - 21500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 359      |
|    iterations      | 38       |
|    time_elapsed    | 6487     |
|    total_timesteps | 2334720  |
---------------------------------
Ep done - 22000.
Eval num_timesteps=2334872, episode_reward=0.17 +/- 0.97
Episode length: 29.94 +/- 1.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.172      |
| time/                   |            |
|    total_timesteps      | 2334872    |
| train/                  |            |
|    approx_kl            | 0.08051166 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.748     |
|    explained_variance   | 0.322      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0597     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 22000.
Ep done - 22000.
Ep done - 22000.
Ep done - 22500.
Ep done - 22500.
Ep done - 22500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 357      |
|    iterations      | 39       |
|    time_elapsed    | 6702     |
|    total_timesteps | 2396160  |
---------------------------------
Ep done - 22500.
Eval num_timesteps=2396316, episode_reward=0.22 +/- 0.95
Episode length: 29.91 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.216       |
| time/                   |             |
|    total_timesteps      | 2396316     |
| train/                  |             |
|    approx_kl            | 0.080237165 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.327       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.048       |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0489     |
|    value_loss           | 0.223       |
-----------------------------------------
Ep done - 23000.
Ep done - 23000.
Ep done - 23000.
Ep done - 23000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 354      |
|    iterations      | 40       |
|    time_elapsed    | 6924     |
|    total_timesteps | 2457600  |
---------------------------------
Eval num_timesteps=2457760, episode_reward=0.27 +/- 0.95
Episode length: 30.03 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.272      |
| time/                   |            |
|    total_timesteps      | 2457760    |
| train/                  |            |
|    approx_kl            | 0.08150063 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.742     |
|    explained_variance   | 0.329      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0364     |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0492    |
|    value_loss           | 0.225      |
----------------------------------------
Ep done - 23500.
Ep done - 23500.
Ep done - 23500.
Ep done - 23500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 41       |
|    time_elapsed    | 7145     |
|    total_timesteps | 2519040  |
---------------------------------
Eval num_timesteps=2519204, episode_reward=0.23 +/- 0.96
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.228       |
| time/                   |             |
|    total_timesteps      | 2519204     |
| train/                  |             |
|    approx_kl            | 0.082300186 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0292      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0487     |
|    value_loss           | 0.229       |
-----------------------------------------
Ep done - 24000.
Ep done - 24000.
Ep done - 24000.
Ep done - 24000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 347      |
|    iterations      | 42       |
|    time_elapsed    | 7415     |
|    total_timesteps | 2580480  |
---------------------------------
Eval num_timesteps=2580648, episode_reward=0.36 +/- 0.92
Episode length: 29.94 +/- 1.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.36       |
| time/                   |            |
|    total_timesteps      | 2580648    |
| train/                  |            |
|    approx_kl            | 0.08500377 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.714     |
|    explained_variance   | 0.308      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0464     |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0487    |
|    value_loss           | 0.227      |
----------------------------------------
Ep done - 24500.
Ep done - 24500.
Ep done - 24500.
Ep done - 24500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 345      |
|    iterations      | 43       |
|    time_elapsed    | 7636     |
|    total_timesteps | 2641920  |
---------------------------------
Eval num_timesteps=2642092, episode_reward=0.32 +/- 0.94
Episode length: 30.07 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.324      |
| time/                   |            |
|    total_timesteps      | 2642092    |
| train/                  |            |
|    approx_kl            | 0.08364347 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.323      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0345     |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0479    |
|    value_loss           | 0.223      |
----------------------------------------
Ep done - 25000.
Ep done - 25000.
Ep done - 25000.
Ep done - 25000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 343      |
|    iterations      | 44       |
|    time_elapsed    | 7868     |
|    total_timesteps | 2703360  |
---------------------------------
Eval num_timesteps=2703536, episode_reward=0.29 +/- 0.94
Episode length: 30.00 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.292       |
| time/                   |             |
|    total_timesteps      | 2703536     |
| train/                  |             |
|    approx_kl            | 0.085306056 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.312       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0201      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0487     |
|    value_loss           | 0.228       |
-----------------------------------------
Ep done - 25500.
Ep done - 25500.
Ep done - 25500.
Ep done - 25500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 341      |
|    iterations      | 45       |
|    time_elapsed    | 8097     |
|    total_timesteps | 2764800  |
---------------------------------
Ep done - 26000.
Ep done - 26000.
Ep done - 26000.
Ep done - 26000.
Eval num_timesteps=2764980, episode_reward=0.29 +/- 0.94
Episode length: 30.06 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.292      |
| time/                   |            |
|    total_timesteps      | 2764980    |
| train/                  |            |
|    approx_kl            | 0.08650757 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.69      |
|    explained_variance   | 0.315      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0517     |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.048     |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 26500.
Ep done - 26500.
Ep done - 26500.
Ep done - 26500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 339      |
|    iterations      | 46       |
|    time_elapsed    | 8316     |
|    total_timesteps | 2826240  |
---------------------------------
Eval num_timesteps=2826424, episode_reward=0.36 +/- 0.92
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.36        |
| time/                   |             |
|    total_timesteps      | 2826424     |
| train/                  |             |
|    approx_kl            | 0.085534364 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0101      |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0467     |
|    value_loss           | 0.218       |
-----------------------------------------
Ep done - 27000.
Ep done - 27000.
Ep done - 27000.
Ep done - 27000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 338      |
|    iterations      | 47       |
|    time_elapsed    | 8540     |
|    total_timesteps | 2887680  |
---------------------------------
Eval num_timesteps=2887868, episode_reward=0.32 +/- 0.94
Episode length: 30.00 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.32       |
| time/                   |            |
|    total_timesteps      | 2887868    |
| train/                  |            |
|    approx_kl            | 0.08532715 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.675     |
|    explained_variance   | 0.332      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.000962   |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0474    |
|    value_loss           | 0.218      |
----------------------------------------
Ep done - 27500.
Ep done - 27500.
Ep done - 27500.
Ep done - 27500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 336      |
|    iterations      | 48       |
|    time_elapsed    | 8761     |
|    total_timesteps | 2949120  |
---------------------------------
Eval num_timesteps=2949312, episode_reward=0.37 +/- 0.92
Episode length: 30.04 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.372      |
| time/                   |            |
|    total_timesteps      | 2949312    |
| train/                  |            |
|    approx_kl            | 0.08813383 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.667     |
|    explained_variance   | 0.336      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0184     |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.216      |
----------------------------------------
Ep done - 28000.
Ep done - 28000.
Ep done - 28000.
Ep done - 28000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 335      |
|    iterations      | 49       |
|    time_elapsed    | 8978     |
|    total_timesteps | 3010560  |
---------------------------------
Eval num_timesteps=3010756, episode_reward=0.43 +/- 0.89
Episode length: 30.08 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.432     |
| time/                   |           |
|    total_timesteps      | 3010756   |
| train/                  |           |
|    approx_kl            | 0.0878263 |
|    clip_fraction        | 0.297     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.652    |
|    explained_variance   | 0.316     |
|    learning_rate        | 8e-05     |
|    loss                 | 0.0219    |
|    n_updates            | 570       |
|    policy_gradient_loss | -0.0451   |
|    value_loss           | 0.216     |
---------------------------------------
Ep done - 28500.
Ep done - 28500.
Ep done - 28500.
Ep done - 28500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 332      |
|    iterations      | 50       |
|    time_elapsed    | 9249     |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3072200, episode_reward=0.41 +/- 0.89
Episode length: 30.10 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.412      |
| time/                   |            |
|    total_timesteps      | 3072200    |
| train/                  |            |
|    approx_kl            | 0.09060038 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.324      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0742     |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0462    |
|    value_loss           | 0.22       |
----------------------------------------
Ep done - 29000.
Ep done - 29000.
Ep done - 29000.
Ep done - 29000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 330      |
|    iterations      | 51       |
|    time_elapsed    | 9470     |
|    total_timesteps | 3133440  |
---------------------------------
Eval num_timesteps=3133644, episode_reward=0.44 +/- 0.88
Episode length: 30.13 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.44       |
| time/                   |            |
|    total_timesteps      | 3133644    |
| train/                  |            |
|    approx_kl            | 0.08922382 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.641     |
|    explained_variance   | 0.327      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0312     |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0452    |
|    value_loss           | 0.218      |
----------------------------------------
Ep done - 29500.
Ep done - 29500.
Ep done - 29500.
Ep done - 29500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 327      |
|    iterations      | 52       |
|    time_elapsed    | 9740     |
|    total_timesteps | 3194880  |
---------------------------------
Ep done - 30000.
Ep done - 30000.
Ep done - 30000.
Ep done - 30000.
Eval num_timesteps=3195088, episode_reward=0.40 +/- 0.90
Episode length: 30.04 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.404      |
| time/                   |            |
|    total_timesteps      | 3195088    |
| train/                  |            |
|    approx_kl            | 0.08970515 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.642     |
|    explained_variance   | 0.313      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.052      |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0461    |
|    value_loss           | 0.217      |
----------------------------------------
Ep done - 30500.
Ep done - 30500.
Ep done - 30500.
Ep done - 30500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 326      |
|    iterations      | 53       |
|    time_elapsed    | 9959     |
|    total_timesteps | 3256320  |
---------------------------------
Eval num_timesteps=3256532, episode_reward=0.46 +/- 0.87
Episode length: 30.10 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.456      |
| time/                   |            |
|    total_timesteps      | 3256532    |
| train/                  |            |
|    approx_kl            | 0.09018795 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.631     |
|    explained_variance   | 0.329      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0109     |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.212      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.456
SELFPLAY: new best model, bumping up generation to 5
Ep done - 31000.
Ep done - 31000.
Ep done - 31000.
Ep done - 31000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.19    |
| time/              |          |
|    fps             | 325      |
|    iterations      | 54       |
|    time_elapsed    | 10197    |
|    total_timesteps | 3317760  |
---------------------------------
Eval num_timesteps=3317976, episode_reward=0.09 +/- 0.99
Episode length: 30.03 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 3317976     |
| train/                  |             |
|    approx_kl            | 0.097762235 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0643      |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0451     |
|    value_loss           | 0.233       |
-----------------------------------------
Ep done - 31500.
Ep done - 31500.
Ep done - 31500.
Ep done - 31500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 324      |
|    iterations      | 55       |
|    time_elapsed    | 10420    |
|    total_timesteps | 3379200  |
---------------------------------
Eval num_timesteps=3379420, episode_reward=0.24 +/- 0.94
Episode length: 30.04 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.24       |
| time/                   |            |
|    total_timesteps      | 3379420    |
| train/                  |            |
|    approx_kl            | 0.09678289 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.334      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.03       |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 32000.
Ep done - 32000.
Ep done - 32000.
Ep done - 32000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 322      |
|    iterations      | 56       |
|    time_elapsed    | 10685    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3440864, episode_reward=0.19 +/- 0.97
Episode length: 30.05 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.192      |
| time/                   |            |
|    total_timesteps      | 3440864    |
| train/                  |            |
|    approx_kl            | 0.10102557 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.653     |
|    explained_variance   | 0.348      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0229     |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.232      |
----------------------------------------
Ep done - 32500.
Ep done - 32500.
Ep done - 32500.
Ep done - 32500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 321      |
|    iterations      | 57       |
|    time_elapsed    | 10906    |
|    total_timesteps | 3502080  |
---------------------------------
Eval num_timesteps=3502308, episode_reward=0.08 +/- 0.99
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.084      |
| time/                   |            |
|    total_timesteps      | 3502308    |
| train/                  |            |
|    approx_kl            | 0.09898363 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.644     |
|    explained_variance   | 0.308      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0261     |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0452    |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 33000.
Ep done - 33000.
Ep done - 33000.
Ep done - 33000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 319      |
|    iterations      | 58       |
|    time_elapsed    | 11136    |
|    total_timesteps | 3563520  |
---------------------------------
Ep done - 33500.
Eval num_timesteps=3563752, episode_reward=0.30 +/- 0.94
Episode length: 30.08 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.304      |
| time/                   |            |
|    total_timesteps      | 3563752    |
| train/                  |            |
|    approx_kl            | 0.09556079 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.636     |
|    explained_variance   | 0.34       |
|    learning_rate        | 8e-05      |
|    loss                 | 0.05       |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.227      |
----------------------------------------
Ep done - 33500.
Ep done - 33500.
Ep done - 33500.
Ep done - 34000.
Ep done - 34000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 319      |
|    iterations      | 59       |
|    time_elapsed    | 11358    |
|    total_timesteps | 3624960  |
---------------------------------
Ep done - 34000.
Ep done - 34000.
Eval num_timesteps=3625196, episode_reward=0.19 +/- 0.97
Episode length: 30.06 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.188       |
| time/                   |             |
|    total_timesteps      | 3625196     |
| train/                  |             |
|    approx_kl            | 0.101271905 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.629      |
|    explained_variance   | 0.328       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0952      |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0452     |
|    value_loss           | 0.233       |
-----------------------------------------
Ep done - 34500.
Ep done - 34500.
Ep done - 34500.
Ep done - 34500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 317      |
|    iterations      | 60       |
|    time_elapsed    | 11626    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3686640, episode_reward=0.15 +/- 0.98
Episode length: 30.04 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.148      |
| time/                   |            |
|    total_timesteps      | 3686640    |
| train/                  |            |
|    approx_kl            | 0.10233311 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.334      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.00211    |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 35000.
Ep done - 35000.
Ep done - 35000.
Ep done - 35000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 316      |
|    iterations      | 61       |
|    time_elapsed    | 11843    |
|    total_timesteps | 3747840  |
---------------------------------
Eval num_timesteps=3748084, episode_reward=0.26 +/- 0.96
Episode length: 30.06 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.256       |
| time/                   |             |
|    total_timesteps      | 3748084     |
| train/                  |             |
|    approx_kl            | 0.104096845 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.342       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0386      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0454     |
|    value_loss           | 0.231       |
-----------------------------------------
Ep done - 35500.
Ep done - 35500.
Ep done - 35500.
Ep done - 35500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 315      |
|    iterations      | 62       |
|    time_elapsed    | 12064    |
|    total_timesteps | 3809280  |
---------------------------------
Eval num_timesteps=3809528, episode_reward=0.27 +/- 0.95
Episode length: 30.10 +/- 0.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.272      |
| time/                   |            |
|    total_timesteps      | 3809528    |
| train/                  |            |
|    approx_kl            | 0.10604933 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.617     |
|    explained_variance   | 0.34       |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0656     |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0445    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 36000.
Ep done - 36000.
Ep done - 36000.
Ep done - 36000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 315      |
|    iterations      | 63       |
|    time_elapsed    | 12287    |
|    total_timesteps | 3870720  |
---------------------------------
Eval num_timesteps=3870972, episode_reward=0.28 +/- 0.94
Episode length: 30.08 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.284       |
| time/                   |             |
|    total_timesteps      | 3870972     |
| train/                  |             |
|    approx_kl            | 0.104180954 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.335       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0354      |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0446     |
|    value_loss           | 0.233       |
-----------------------------------------
Ep done - 36500.
Ep done - 36500.
Ep done - 36500.
Ep done - 36500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 314      |
|    iterations      | 64       |
|    time_elapsed    | 12508    |
|    total_timesteps | 3932160  |
---------------------------------
Eval num_timesteps=3932416, episode_reward=0.37 +/- 0.91
Episode length: 30.04 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.368      |
| time/                   |            |
|    total_timesteps      | 3932416    |
| train/                  |            |
|    approx_kl            | 0.10389768 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.609     |
|    explained_variance   | 0.345      |
|    learning_rate        | 8e-05      |
|    loss                 | 0.0467     |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0443    |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 37000.
Ep done - 37000.
Ep done - 37000.
Ep done - 37000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 313      |
|    iterations      | 65       |
|    time_elapsed    | 12728    |
|    total_timesteps | 3993600  |
---------------------------------
Ep done - 37500.
Ep done - 37500.
Ep done - 37500.
Eval num_timesteps=3993860, episode_reward=0.24 +/- 0.95
Episode length: 30.07 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.24       |
| time/                   |            |
|    total_timesteps      | 3993860    |
| train/                  |            |
|    approx_kl            | 0.10300938 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.605     |
|    explained_variance   | 0.33       |
|    learning_rate        | 8e-05      |
|    loss                 | 0.102      |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 0.229      |
----------------------------------------
Ep done - 37500.
Ep done - 38000.
Ep done - 38000.
Ep done - 38000.
Ep done - 38000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 313      |
|    iterations      | 66       |
|    time_elapsed    | 12950    |
|    total_timesteps | 4055040  |
---------------------------------
Elapsed time: 3h 37m 19s
