CUDA Available: True
CPU Model: AMD EPYC 7313P 16-Core Processor
GPU Model: Tesla T4

num parallel processes: 8

CUDA available: True
params: 
NUM_TIMESTEPS=30000000
EVAL_FREQ=61441
EVAL_EPISODES=1000
BEST_THRESHOLD=0.125
LOGDIR=scripts/rl/output/paral/v3v3-1/
model params: 
 {'learning_rate': 5e-05, 'n_steps': 61440, 'n_epochs': 3, 'clip_range': 0.3, 'batch_size': 512, 'gamma': 1, 'verbose': 1}
starting model: scripts/rl/output/v3v3/history_0018
Ep done - 500.
Ep done - 500.
Ep done - 500.
Ep done - 500.
Ep done - 500.
Ep done - 500.
Ep done - 500.
Ep done - 500.
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
Ep done - 1000.
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
Ep done - 1500.
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
Ep done - 2000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.4     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 898      |
|    iterations      | 1        |
|    time_elapsed    | 546      |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=491528, episode_reward=0.14 +/- 0.97
Episode length: 29.17 +/- 4.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 0.141       |
| time/                   |             |
|    total_timesteps      | 491528      |
| train/                  |             |
|    approx_kl            | 0.008041748 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.29        |
|    learning_rate        | 5e-05       |
|    loss                 | 0.133       |
|    n_updates            | 7663        |
|    policy_gradient_loss | -0.00707    |
|    value_loss           | 0.311       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.141 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 1 ---------------------------------------
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
Ep done - 2500.
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
Ep done - 3000.
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
Ep done - 3500.
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
Ep done - 4000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.2     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 853      |
|    iterations      | 2        |
|    time_elapsed    | 1152     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=983056, episode_reward=0.13 +/- 0.97
Episode length: 29.76 +/- 2.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.8         |
|    mean_reward          | 0.129        |
| time/                   |              |
|    total_timesteps      | 983056       |
| train/                  |              |
|    approx_kl            | 0.0073061124 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.594       |
|    explained_variance   | 0.311        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.142        |
|    n_updates            | 7666         |
|    policy_gradient_loss | -0.0063      |
|    value_loss           | 0.304        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.129 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 2 ---------------------------------------
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
Ep done - 4500.
Ep done - 5000.
Ep done - 5000.
Ep done - 5000.
Ep done - 5000.
Ep done - 5000.
Ep done - 5000.
Ep done - 5000.
Ep done - 5000.
Ep done - 5500.
Ep done - 5500.
Ep done - 5500.
Ep done - 5500.
Ep done - 5500.
Ep done - 5500.
Ep done - 5500.
Ep done - 5500.
Ep done - 6000.
Ep done - 6000.
Ep done - 6000.
Ep done - 6000.
Ep done - 6000.
Ep done - 6000.
Ep done - 6000.
Ep done - 6000.
Ep done - 6500.
Ep done - 6500.
Ep done - 6500.
Ep done - 6500.
Ep done - 6500.
Ep done - 6500.
Ep done - 6500.
Ep done - 6500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 841      |
|    iterations      | 3        |
|    time_elapsed    | 1751     |
|    total_timesteps | 1474560  |
---------------------------------
Eval num_timesteps=1474584, episode_reward=0.12 +/- 0.97
Episode length: 29.96 +/- 1.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.125       |
| time/                   |             |
|    total_timesteps      | 1474584     |
| train/                  |             |
|    approx_kl            | 0.007649532 |
|    clip_fraction        | 0.028       |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.313       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.135       |
|    n_updates            | 7669        |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 0.303       |
-----------------------------------------
Ep done - 7000.
Ep done - 7000.
Ep done - 7000.
Ep done - 7000.
Ep done - 7000.
Ep done - 7000.
Ep done - 7000.
Ep done - 7000.
Ep done - 7500.
Ep done - 7500.
Ep done - 7500.
Ep done - 7500.
Ep done - 7500.
Ep done - 7500.
Ep done - 7500.
Ep done - 7500.
Ep done - 8000.
Ep done - 8000.
Ep done - 8000.
Ep done - 8000.
Ep done - 8000.
Ep done - 8000.
Ep done - 8000.
Ep done - 8000.
Ep done - 8500.
Ep done - 8500.
Ep done - 8500.
Ep done - 8500.
Ep done - 8500.
Ep done - 8500.
Ep done - 8500.
Ep done - 8500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 838      |
|    iterations      | 4        |
|    time_elapsed    | 2344     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1966112, episode_reward=0.13 +/- 0.97
Episode length: 30.02 +/- 1.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.132        |
| time/                   |              |
|    total_timesteps      | 1966112      |
| train/                  |              |
|    approx_kl            | 0.0075277546 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.585       |
|    explained_variance   | 0.316        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.141        |
|    n_updates            | 7672         |
|    policy_gradient_loss | -0.00605     |
|    value_loss           | 0.304        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.132 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 3 ---------------------------------------
Ep done - 9000.
Ep done - 9000.
Ep done - 9000.
Ep done - 9000.
Ep done - 9000.
Ep done - 9000.
Ep done - 9000.
Ep done - 9000.
Ep done - 9500.
Ep done - 9500.
Ep done - 9500.
Ep done - 9500.
Ep done - 9500.
Ep done - 9500.
Ep done - 9500.
Ep done - 9500.
Ep done - 10000.
Ep done - 10000.
Ep done - 10000.
Ep done - 10000.
Ep done - 10000.
Ep done - 10000.
Ep done - 10000.
Ep done - 10000.
Ep done - 10500.
Ep done - 10500.
Ep done - 10500.
Ep done - 10500.
Ep done - 10500.
Ep done - 10500.
Ep done - 10500.
Ep done - 10500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 5        |
|    time_elapsed    | 2943     |
|    total_timesteps | 2457600  |
---------------------------------
Ep done - 11000.
Ep done - 11000.
Ep done - 11000.
Ep done - 11000.
Ep done - 11000.
Eval num_timesteps=2457640, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.117        |
| time/                   |              |
|    total_timesteps      | 2457640      |
| train/                  |              |
|    approx_kl            | 0.0072246003 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.574       |
|    explained_variance   | 0.321        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.122        |
|    n_updates            | 7675         |
|    policy_gradient_loss | -0.00594     |
|    value_loss           | 0.302        |
------------------------------------------
Ep done - 11000.
Ep done - 11000.
Ep done - 11000.
Ep done - 11500.
Ep done - 11500.
Ep done - 11500.
Ep done - 11500.
Ep done - 11500.
Ep done - 11500.
Ep done - 11500.
Ep done - 11500.
Ep done - 12000.
Ep done - 12000.
Ep done - 12000.
Ep done - 12000.
Ep done - 12000.
Ep done - 12000.
Ep done - 12000.
Ep done - 12000.
Ep done - 12500.
Ep done - 12500.
Ep done - 12500.
Ep done - 12500.
Ep done - 12500.
Ep done - 12500.
Ep done - 12500.
Ep done - 12500.
Ep done - 13000.
Ep done - 13000.
Ep done - 13000.
Ep done - 13000.
Ep done - 13000.
Ep done - 13000.
Ep done - 13000.
Ep done - 13000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 834      |
|    iterations      | 6        |
|    time_elapsed    | 3533     |
|    total_timesteps | 2949120  |
---------------------------------
Eval num_timesteps=2949168, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.103        |
| time/                   |              |
|    total_timesteps      | 2949168      |
| train/                  |              |
|    approx_kl            | 0.0074237776 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.568       |
|    explained_variance   | 0.32         |
|    learning_rate        | 5e-05        |
|    loss                 | 0.15         |
|    n_updates            | 7678         |
|    policy_gradient_loss | -0.00601     |
|    value_loss           | 0.302        |
------------------------------------------
Ep done - 13500.
Ep done - 13500.
Ep done - 13500.
Ep done - 13500.
Ep done - 13500.
Ep done - 13500.
Ep done - 13500.
Ep done - 13500.
Ep done - 14000.
Ep done - 14000.
Ep done - 14000.
Ep done - 14000.
Ep done - 14000.
Ep done - 14000.
Ep done - 14000.
Ep done - 14000.
Ep done - 14500.
Ep done - 14500.
Ep done - 14500.
Ep done - 14500.
Ep done - 14500.
Ep done - 14500.
Ep done - 14500.
Ep done - 14500.
Ep done - 15000.
Ep done - 15000.
Ep done - 15000.
Ep done - 15000.
Ep done - 15000.
Ep done - 15000.
Ep done - 15000.
Ep done - 15000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 834      |
|    iterations      | 7        |
|    time_elapsed    | 4123     |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3440696, episode_reward=0.14 +/- 0.98
Episode length: 30.03 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.145       |
| time/                   |             |
|    total_timesteps      | 3440696     |
| train/                  |             |
|    approx_kl            | 0.007396577 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.326       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.12        |
|    n_updates            | 7681        |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 0.3         |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.145 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 4 ---------------------------------------
Ep done - 15500.
Ep done - 15500.
Ep done - 15500.
Ep done - 15500.
Ep done - 15500.
Ep done - 15500.
Ep done - 15500.
Ep done - 15500.
Ep done - 16000.
Ep done - 16000.
Ep done - 16000.
Ep done - 16000.
Ep done - 16000.
Ep done - 16000.
Ep done - 16000.
Ep done - 16000.
Ep done - 16500.
Ep done - 16500.
Ep done - 16500.
Ep done - 16500.
Ep done - 16500.
Ep done - 16500.
Ep done - 16500.
Ep done - 16500.
Ep done - 17000.
Ep done - 17000.
Ep done - 17000.
Ep done - 17000.
Ep done - 17000.
Ep done - 17000.
Ep done - 17000.
Ep done - 17000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 832      |
|    iterations      | 8        |
|    time_elapsed    | 4721     |
|    total_timesteps | 3932160  |
---------------------------------
Ep done - 17500.
Ep done - 17500.
Ep done - 17500.
Ep done - 17500.
Ep done - 17500.
Ep done - 17500.
Ep done - 17500.
Ep done - 17500.
Eval num_timesteps=3932224, episode_reward=0.19 +/- 0.97
Episode length: 30.09 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.186        |
| time/                   |              |
|    total_timesteps      | 3932224      |
| train/                  |              |
|    approx_kl            | 0.0071622194 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.551       |
|    explained_variance   | 0.323        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.124        |
|    n_updates            | 7684         |
|    policy_gradient_loss | -0.00604     |
|    value_loss           | 0.301        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.186 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 5 ---------------------------------------
Ep done - 18000.
Ep done - 18000.
Ep done - 18000.
Ep done - 18000.
Ep done - 18000.
Ep done - 18000.
Ep done - 18000.
Ep done - 18000.
Ep done - 18500.
Ep done - 18500.
Ep done - 18500.
Ep done - 18500.
Ep done - 18500.
Ep done - 18500.
Ep done - 18500.
Ep done - 18500.
Ep done - 19000.
Ep done - 19000.
Ep done - 19000.
Ep done - 19000.
Ep done - 19000.
Ep done - 19000.
Ep done - 19000.
Ep done - 19000.
Ep done - 19500.
Ep done - 19500.
Ep done - 19500.
Ep done - 19500.
Ep done - 19500.
Ep done - 19500.
Ep done - 19500.
Ep done - 19500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 831      |
|    iterations      | 9        |
|    time_elapsed    | 5320     |
|    total_timesteps | 4423680  |
---------------------------------
Eval num_timesteps=4423752, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 4423752     |
| train/                  |             |
|    approx_kl            | 0.007181661 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.325       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.138       |
|    n_updates            | 7687        |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 0.299       |
-----------------------------------------
Ep done - 20000.
Ep done - 20000.
Ep done - 20000.
Ep done - 20000.
Ep done - 20000.
Ep done - 20000.
Ep done - 20000.
Ep done - 20000.
Ep done - 20500.
Ep done - 20500.
Ep done - 20500.
Ep done - 20500.
Ep done - 20500.
Ep done - 20500.
Ep done - 20500.
Ep done - 20500.
Ep done - 21000.
Ep done - 21000.
Ep done - 21000.
Ep done - 21000.
Ep done - 21000.
Ep done - 21000.
Ep done - 21000.
Ep done - 21000.
Ep done - 21500.
Ep done - 21500.
Ep done - 21500.
Ep done - 21500.
Ep done - 21500.
Ep done - 21500.
Ep done - 21500.
Ep done - 21500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 830      |
|    iterations      | 10       |
|    time_elapsed    | 5916     |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4915280, episode_reward=0.20 +/- 0.96
Episode length: 30.13 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.203        |
| time/                   |              |
|    total_timesteps      | 4915280      |
| train/                  |              |
|    approx_kl            | 0.0069579124 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.329        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.14         |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.00605     |
|    value_loss           | 0.301        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.203 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 6 ---------------------------------------
Ep done - 22000.
Ep done - 22000.
Ep done - 22000.
Ep done - 22000.
Ep done - 22000.
Ep done - 22000.
Ep done - 22000.
Ep done - 22000.
Ep done - 22500.
Ep done - 22500.
Ep done - 22500.
Ep done - 22500.
Ep done - 22500.
Ep done - 22500.
Ep done - 22500.
Ep done - 22500.
Ep done - 23000.
Ep done - 23000.
Ep done - 23000.
Ep done - 23000.
Ep done - 23000.
Ep done - 23000.
Ep done - 23000.
Ep done - 23000.
Ep done - 23500.
Ep done - 23500.
Ep done - 23500.
Ep done - 23500.
Ep done - 23500.
Ep done - 23500.
Ep done - 23500.
Ep done - 23500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 829      |
|    iterations      | 11       |
|    time_elapsed    | 6519     |
|    total_timesteps | 5406720  |
---------------------------------
Ep done - 24000.
Ep done - 24000.
Ep done - 24000.
Ep done - 24000.
Ep done - 24000.
Ep done - 24000.
Ep done - 24000.
Ep done - 24000.
Eval num_timesteps=5406808, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.132        |
| time/                   |              |
|    total_timesteps      | 5406808      |
| train/                  |              |
|    approx_kl            | 0.0070974478 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.325        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.134        |
|    n_updates            | 7693         |
|    policy_gradient_loss | -0.0059      |
|    value_loss           | 0.302        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.132 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 7 ---------------------------------------
Ep done - 24500.
Ep done - 24500.
Ep done - 24500.
Ep done - 24500.
Ep done - 24500.
Ep done - 24500.
Ep done - 24500.
Ep done - 24500.
Ep done - 25000.
Ep done - 25000.
Ep done - 25000.
Ep done - 25000.
Ep done - 25000.
Ep done - 25000.
Ep done - 25000.
Ep done - 25000.
Ep done - 25500.
Ep done - 25500.
Ep done - 25500.
Ep done - 25500.
Ep done - 25500.
Ep done - 25500.
Ep done - 25500.
Ep done - 25500.
Ep done - 26000.
Ep done - 26000.
Ep done - 26000.
Ep done - 26000.
Ep done - 26000.
Ep done - 26000.
Ep done - 26000.
Ep done - 26000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 828      |
|    iterations      | 12       |
|    time_elapsed    | 7118     |
|    total_timesteps | 5898240  |
---------------------------------
Eval num_timesteps=5898336, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.125        |
| time/                   |              |
|    total_timesteps      | 5898336      |
| train/                  |              |
|    approx_kl            | 0.0068835597 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.549       |
|    explained_variance   | 0.327        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.134        |
|    n_updates            | 7696         |
|    policy_gradient_loss | -0.0057      |
|    value_loss           | 0.303        |
------------------------------------------
Ep done - 26500.
Ep done - 26500.
Ep done - 26500.
Ep done - 26500.
Ep done - 26500.
Ep done - 26500.
Ep done - 26500.
Ep done - 26500.
Ep done - 27000.
Ep done - 27000.
Ep done - 27000.
Ep done - 27000.
Ep done - 27000.
Ep done - 27000.
Ep done - 27000.
Ep done - 27000.
Ep done - 27500.
Ep done - 27500.
Ep done - 27500.
Ep done - 27500.
Ep done - 27500.
Ep done - 27500.
Ep done - 27500.
Ep done - 27500.
Ep done - 28000.
Ep done - 28000.
Ep done - 28000.
Ep done - 28000.
Ep done - 28000.
Ep done - 28000.
Ep done - 28000.
Ep done - 28000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 828      |
|    iterations      | 13       |
|    time_elapsed    | 7713     |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6389864, episode_reward=0.16 +/- 0.97
Episode length: 30.08 +/- 0.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.156        |
| time/                   |              |
|    total_timesteps      | 6389864      |
| train/                  |              |
|    approx_kl            | 0.0071254713 |
|    clip_fraction        | 0.0285       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.553       |
|    explained_variance   | 0.322        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.136        |
|    n_updates            | 7699         |
|    policy_gradient_loss | -0.00598     |
|    value_loss           | 0.3          |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.156 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 8 ---------------------------------------
Ep done - 28500.
Ep done - 28500.
Ep done - 28500.
Ep done - 28500.
Ep done - 28500.
Ep done - 28500.
Ep done - 28500.
Ep done - 28500.
Ep done - 29000.
Ep done - 29000.
Ep done - 29000.
Ep done - 29000.
Ep done - 29000.
Ep done - 29000.
Ep done - 29000.
Ep done - 29000.
Ep done - 29500.
Ep done - 29500.
Ep done - 29500.
Ep done - 29500.
Ep done - 29500.
Ep done - 29500.
Ep done - 29500.
Ep done - 29500.
Ep done - 30000.
Ep done - 30000.
Ep done - 30000.
Ep done - 30000.
Ep done - 30000.
Ep done - 30000.
Ep done - 30000.
Ep done - 30000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 14       |
|    time_elapsed    | 8315     |
|    total_timesteps | 6881280  |
---------------------------------
Ep done - 30500.
Ep done - 30500.
Ep done - 30500.
Ep done - 30500.
Ep done - 30500.
Ep done - 30500.
Ep done - 30500.
Ep done - 30500.
Eval num_timesteps=6881392, episode_reward=0.21 +/- 0.96
Episode length: 30.06 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.209       |
| time/                   |             |
|    total_timesteps      | 6881392     |
| train/                  |             |
|    approx_kl            | 0.007440561 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.324       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.124       |
|    n_updates            | 7702        |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 0.301       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.209 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 9 ---------------------------------------
Ep done - 31000.
Ep done - 31000.
Ep done - 31000.
Ep done - 31000.
Ep done - 31000.
Ep done - 31000.
Ep done - 31000.
Ep done - 31000.
Ep done - 31500.
Ep done - 31500.
Ep done - 31500.
Ep done - 31500.
Ep done - 31500.
Ep done - 31500.
Ep done - 31500.
Ep done - 31500.
Ep done - 32000.
Ep done - 32000.
Ep done - 32000.
Ep done - 32000.
Ep done - 32000.
Ep done - 32000.
Ep done - 32000.
Ep done - 32000.
Ep done - 32500.
Ep done - 32500.
Ep done - 32500.
Ep done - 32500.
Ep done - 32500.
Ep done - 32500.
Ep done - 32500.
Ep done - 32500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 15       |
|    time_elapsed    | 8911     |
|    total_timesteps | 7372800  |
---------------------------------
Eval num_timesteps=7372920, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.077        |
| time/                   |              |
|    total_timesteps      | 7372920      |
| train/                  |              |
|    approx_kl            | 0.0077906367 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.568       |
|    explained_variance   | 0.314        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.158        |
|    n_updates            | 7705         |
|    policy_gradient_loss | -0.0061      |
|    value_loss           | 0.305        |
------------------------------------------
Ep done - 33000.
Ep done - 33000.
Ep done - 33000.
Ep done - 33000.
Ep done - 33000.
Ep done - 33000.
Ep done - 33000.
Ep done - 33000.
Ep done - 33500.
Ep done - 33500.
Ep done - 33500.
Ep done - 33500.
Ep done - 33500.
Ep done - 33500.
Ep done - 33500.
Ep done - 33500.
Ep done - 34000.
Ep done - 34000.
Ep done - 34000.
Ep done - 34000.
Ep done - 34000.
Ep done - 34000.
Ep done - 34000.
Ep done - 34000.
Ep done - 34500.
Ep done - 34500.
Ep done - 34500.
Ep done - 34500.
Ep done - 34500.
Ep done - 34500.
Ep done - 34500.
Ep done - 34500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 827      |
|    iterations      | 16       |
|    time_elapsed    | 9500     |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7864448, episode_reward=0.09 +/- 0.98
Episode length: 30.08 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 7864448     |
| train/                  |             |
|    approx_kl            | 0.007053838 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.568      |
|    explained_variance   | 0.317       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.142       |
|    n_updates            | 7708        |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 0.303       |
-----------------------------------------
Ep done - 35000.
Ep done - 35000.
Ep done - 35000.
Ep done - 35000.
Ep done - 35000.
Ep done - 35000.
Ep done - 35000.
Ep done - 35000.
Ep done - 35500.
Ep done - 35500.
Ep done - 35500.
Ep done - 35500.
Ep done - 35500.
Ep done - 35500.
Ep done - 35500.
Ep done - 35500.
Ep done - 36000.
Ep done - 36000.
Ep done - 36000.
Ep done - 36000.
Ep done - 36000.
Ep done - 36000.
Ep done - 36000.
Ep done - 36000.
Ep done - 36500.
Ep done - 36500.
Ep done - 36500.
Ep done - 36500.
Ep done - 36500.
Ep done - 36500.
Ep done - 36500.
Ep done - 36500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 828      |
|    iterations      | 17       |
|    time_elapsed    | 10088    |
|    total_timesteps | 8355840  |
---------------------------------
Ep done - 37000.
Ep done - 37000.
Ep done - 37000.
Ep done - 37000.
Ep done - 37000.
Ep done - 37000.
Ep done - 37000.
Ep done - 37000.
Eval num_timesteps=8355976, episode_reward=0.17 +/- 0.97
Episode length: 30.08 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.171       |
| time/                   |             |
|    total_timesteps      | 8355976     |
| train/                  |             |
|    approx_kl            | 0.007330817 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.315       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.133       |
|    n_updates            | 7711        |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 0.303       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.171 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 10 ---------------------------------------
Ep done - 37500.
Ep done - 37500.
Ep done - 37500.
Ep done - 37500.
Ep done - 37500.
Ep done - 37500.
Ep done - 37500.
Ep done - 37500.
Ep done - 38000.
Ep done - 38000.
Ep done - 38000.
Ep done - 38000.
Ep done - 38000.
Ep done - 38000.
Ep done - 38000.
Ep done - 38000.
Ep done - 38500.
Ep done - 38500.
Ep done - 38500.
Ep done - 38500.
Ep done - 38500.
Ep done - 38500.
Ep done - 38500.
Ep done - 38500.
Ep done - 39000.
Ep done - 39000.
Ep done - 39000.
Ep done - 39000.
Ep done - 39000.
Ep done - 39000.
Ep done - 39000.
Ep done - 39000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 18       |
|    time_elapsed    | 10692    |
|    total_timesteps | 8847360  |
---------------------------------
Eval num_timesteps=8847504, episode_reward=0.12 +/- 0.97
Episode length: 30.00 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.119        |
| time/                   |              |
|    total_timesteps      | 8847504      |
| train/                  |              |
|    approx_kl            | 0.0073089856 |
|    clip_fraction        | 0.0261       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.569       |
|    explained_variance   | 0.317        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.125        |
|    n_updates            | 7714         |
|    policy_gradient_loss | -0.00564     |
|    value_loss           | 0.303        |
------------------------------------------
Ep done - 39500.
Ep done - 39500.
Ep done - 39500.
Ep done - 39500.
Ep done - 39500.
Ep done - 39500.
Ep done - 39500.
Ep done - 39500.
Ep done - 40000.
Ep done - 40000.
Ep done - 40000.
Ep done - 40000.
Ep done - 40000.
Ep done - 40000.
Ep done - 40000.
Ep done - 40000.
Ep done - 40500.
Ep done - 40500.
Ep done - 40500.
Ep done - 40500.
Ep done - 40500.
Ep done - 40500.
Ep done - 40500.
Ep done - 40500.
Ep done - 41000.
Ep done - 41000.
Ep done - 41000.
Ep done - 41000.
Ep done - 41000.
Ep done - 41000.
Ep done - 41000.
Ep done - 41000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 827      |
|    iterations      | 19       |
|    time_elapsed    | 11280    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9339032, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 9339032     |
| train/                  |             |
|    approx_kl            | 0.007213889 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.564      |
|    explained_variance   | 0.319       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.109       |
|    n_updates            | 7717        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 0.302       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.144 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 11 ---------------------------------------
Ep done - 41500.
Ep done - 41500.
Ep done - 41500.
Ep done - 41500.
Ep done - 41500.
Ep done - 41500.
Ep done - 41500.
Ep done - 41500.
Ep done - 42000.
Ep done - 42000.
Ep done - 42000.
Ep done - 42000.
Ep done - 42000.
Ep done - 42000.
Ep done - 42000.
Ep done - 42000.
Ep done - 42500.
Ep done - 42500.
Ep done - 42500.
Ep done - 42500.
Ep done - 42500.
Ep done - 42500.
Ep done - 42500.
Ep done - 42500.
Ep done - 43000.
Ep done - 43000.
Ep done - 43000.
Ep done - 43000.
Ep done - 43000.
Ep done - 43000.
Ep done - 43000.
Ep done - 43000.
Ep done - 43500.
Ep done - 43500.
Ep done - 43500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 20       |
|    time_elapsed    | 11885    |
|    total_timesteps | 9830400  |
---------------------------------
Ep done - 43500.
Ep done - 43500.
Ep done - 43500.
Ep done - 43500.
Ep done - 43500.
Eval num_timesteps=9830560, episode_reward=0.15 +/- 0.98
Episode length: 30.08 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.153        |
| time/                   |              |
|    total_timesteps      | 9830560      |
| train/                  |              |
|    approx_kl            | 0.0072052577 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.557       |
|    explained_variance   | 0.312        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.138        |
|    n_updates            | 7720         |
|    policy_gradient_loss | -0.00564     |
|    value_loss           | 0.305        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.153 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 12 ---------------------------------------
Ep done - 44000.
Ep done - 44000.
Ep done - 44000.
Ep done - 44000.
Ep done - 44000.
Ep done - 44000.
Ep done - 44000.
Ep done - 44000.
Ep done - 44500.
Ep done - 44500.
Ep done - 44500.
Ep done - 44500.
Ep done - 44500.
Ep done - 44500.
Ep done - 44500.
Ep done - 44500.
Ep done - 45000.
Ep done - 45000.
Ep done - 45000.
Ep done - 45000.
Ep done - 45000.
Ep done - 45000.
Ep done - 45000.
Ep done - 45000.
Ep done - 45500.
Ep done - 45500.
Ep done - 45500.
Ep done - 45500.
Ep done - 45500.
Ep done - 45500.
Ep done - 45500.
Ep done - 45500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 21       |
|    time_elapsed    | 12483    |
|    total_timesteps | 10321920 |
---------------------------------
Eval num_timesteps=10322088, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.111       |
| time/                   |             |
|    total_timesteps      | 10322088    |
| train/                  |             |
|    approx_kl            | 0.007115227 |
|    clip_fraction        | 0.0272      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.319       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.139       |
|    n_updates            | 7723        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 0.302       |
-----------------------------------------
Ep done - 46000.
Ep done - 46000.
Ep done - 46000.
Ep done - 46000.
Ep done - 46000.
Ep done - 46000.
Ep done - 46000.
Ep done - 46000.
Ep done - 46500.
Ep done - 46500.
Ep done - 46500.
Ep done - 46500.
Ep done - 46500.
Ep done - 46500.
Ep done - 46500.
Ep done - 46500.
Ep done - 47000.
Ep done - 47000.
Ep done - 47000.
Ep done - 47000.
Ep done - 47000.
Ep done - 47000.
Ep done - 47000.
Ep done - 47000.
Ep done - 47500.
Ep done - 47500.
Ep done - 47500.
Ep done - 47500.
Ep done - 47500.
Ep done - 47500.
Ep done - 47500.
Ep done - 47500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 22       |
|    time_elapsed    | 13073    |
|    total_timesteps | 10813440 |
---------------------------------
Eval num_timesteps=10813616, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.166        |
| time/                   |              |
|    total_timesteps      | 10813616     |
| train/                  |              |
|    approx_kl            | 0.0070839096 |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.318        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.149        |
|    n_updates            | 7726         |
|    policy_gradient_loss | -0.00567     |
|    value_loss           | 0.303        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.166 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 13 ---------------------------------------
Ep done - 48000.
Ep done - 48000.
Ep done - 48000.
Ep done - 48000.
Ep done - 48000.
Ep done - 48000.
Ep done - 48000.
Ep done - 48000.
Ep done - 48500.
Ep done - 48500.
Ep done - 48500.
Ep done - 48500.
Ep done - 48500.
Ep done - 48500.
Ep done - 48500.
Ep done - 48500.
Ep done - 49000.
Ep done - 49000.
Ep done - 49000.
Ep done - 49000.
Ep done - 49000.
Ep done - 49000.
Ep done - 49000.
Ep done - 49000.
Ep done - 49500.
Ep done - 49500.
Ep done - 49500.
Ep done - 49500.
Ep done - 49500.
Ep done - 49500.
Ep done - 49500.
Ep done - 49500.
Ep done - 50000.
Ep done - 50000.
Ep done - 50000.
Ep done - 50000.
Ep done - 50000.
Ep done - 50000.
Ep done - 50000.
Ep done - 50000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 827      |
|    iterations      | 23       |
|    time_elapsed    | 13669    |
|    total_timesteps | 11304960 |
---------------------------------
Eval num_timesteps=11305144, episode_reward=0.19 +/- 0.96
Episode length: 30.08 +/- 0.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.194        |
| time/                   |              |
|    total_timesteps      | 11305144     |
| train/                  |              |
|    approx_kl            | 0.0070626033 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.534       |
|    explained_variance   | 0.319        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.134        |
|    n_updates            | 7729         |
|    policy_gradient_loss | -0.00575     |
|    value_loss           | 0.304        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.194 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 14 ---------------------------------------
Ep done - 50500.
Ep done - 50500.
Ep done - 50500.
Ep done - 50500.
Ep done - 50500.
Ep done - 50500.
Ep done - 50500.
Ep done - 50500.
Ep done - 51000.
Ep done - 51000.
Ep done - 51000.
Ep done - 51000.
Ep done - 51000.
Ep done - 51000.
Ep done - 51000.
Ep done - 51000.
Ep done - 51500.
Ep done - 51500.
Ep done - 51500.
Ep done - 51500.
Ep done - 51500.
Ep done - 51500.
Ep done - 51500.
Ep done - 51500.
Ep done - 52000.
Ep done - 52000.
Ep done - 52000.
Ep done - 52000.
Ep done - 52000.
Ep done - 52000.
Ep done - 52000.
Ep done - 52000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 24       |
|    time_elapsed    | 14267    |
|    total_timesteps | 11796480 |
---------------------------------
Eval num_timesteps=11796672, episode_reward=0.10 +/- 0.98
Episode length: 30.03 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.105       |
| time/                   |             |
|    total_timesteps      | 11796672    |
| train/                  |             |
|    approx_kl            | 0.006792007 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.529      |
|    explained_variance   | 0.318       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.173       |
|    n_updates            | 7732        |
|    policy_gradient_loss | -0.00567    |
|    value_loss           | 0.302       |
-----------------------------------------
Ep done - 52500.
Ep done - 52500.
Ep done - 52500.
Ep done - 52500.
Ep done - 52500.
Ep done - 52500.
Ep done - 52500.
Ep done - 52500.
Ep done - 53000.
Ep done - 53000.
Ep done - 53000.
Ep done - 53000.
Ep done - 53000.
Ep done - 53000.
Ep done - 53000.
Ep done - 53000.
Ep done - 53500.
Ep done - 53500.
Ep done - 53500.
Ep done - 53500.
Ep done - 53500.
Ep done - 53500.
Ep done - 53500.
Ep done - 53500.
Ep done - 54000.
Ep done - 54000.
Ep done - 54000.
Ep done - 54000.
Ep done - 54000.
Ep done - 54000.
Ep done - 54000.
Ep done - 54000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 827      |
|    iterations      | 25       |
|    time_elapsed    | 14856    |
|    total_timesteps | 12288000 |
---------------------------------
Ep done - 54500.
Ep done - 54500.
Eval num_timesteps=12288200, episode_reward=0.17 +/- 0.96
Episode length: 30.06 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.173       |
| time/                   |             |
|    total_timesteps      | 12288200    |
| train/                  |             |
|    approx_kl            | 0.006976431 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.521      |
|    explained_variance   | 0.313       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.135       |
|    n_updates            | 7735        |
|    policy_gradient_loss | -0.00549    |
|    value_loss           | 0.303       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.173 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 15 ---------------------------------------
Ep done - 54500.
Ep done - 54500.
Ep done - 54500.
Ep done - 54500.
Ep done - 54500.
Ep done - 54500.
Ep done - 55000.
Ep done - 55000.
Ep done - 55000.
Ep done - 55000.
Ep done - 55000.
Ep done - 55000.
Ep done - 55000.
Ep done - 55000.
Ep done - 55500.
Ep done - 55500.
Ep done - 55500.
Ep done - 55500.
Ep done - 55500.
Ep done - 55500.
Ep done - 55500.
Ep done - 55500.
Ep done - 56000.
Ep done - 56000.
Ep done - 56000.
Ep done - 56000.
Ep done - 56000.
Ep done - 56000.
Ep done - 56000.
Ep done - 56000.
Ep done - 56500.
Ep done - 56500.
Ep done - 56500.
Ep done - 56500.
Ep done - 56500.
Ep done - 56500.
Ep done - 56500.
Ep done - 56500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 26       |
|    time_elapsed    | 15455    |
|    total_timesteps | 12779520 |
---------------------------------
Eval num_timesteps=12779728, episode_reward=0.20 +/- 0.96
Episode length: 30.11 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.197        |
| time/                   |              |
|    total_timesteps      | 12779728     |
| train/                  |              |
|    approx_kl            | 0.0069385627 |
|    clip_fraction        | 0.0261       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.513       |
|    explained_variance   | 0.316        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.129        |
|    n_updates            | 7738         |
|    policy_gradient_loss | -0.00556     |
|    value_loss           | 0.304        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.197 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 16 ---------------------------------------
Ep done - 57000.
Ep done - 57000.
Ep done - 57000.
Ep done - 57000.
Ep done - 57000.
Ep done - 57000.
Ep done - 57000.
Ep done - 57000.
Ep done - 57500.
Ep done - 57500.
Ep done - 57500.
Ep done - 57500.
Ep done - 57500.
Ep done - 57500.
Ep done - 57500.
Ep done - 57500.
Ep done - 58000.
Ep done - 58000.
Ep done - 58000.
Ep done - 58000.
Ep done - 58000.
Ep done - 58000.
Ep done - 58000.
Ep done - 58000.
Ep done - 58500.
Ep done - 58500.
Ep done - 58500.
Ep done - 58500.
Ep done - 58500.
Ep done - 58500.
Ep done - 58500.
Ep done - 58500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 826      |
|    iterations      | 27       |
|    time_elapsed    | 16058    |
|    total_timesteps | 13271040 |
---------------------------------
Eval num_timesteps=13271256, episode_reward=0.11 +/- 0.98
Episode length: 30.06 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.112        |
| time/                   |              |
|    total_timesteps      | 13271256     |
| train/                  |              |
|    approx_kl            | 0.0068288785 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.512       |
|    explained_variance   | 0.317        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.146        |
|    n_updates            | 7741         |
|    policy_gradient_loss | -0.00556     |
|    value_loss           | 0.301        |
------------------------------------------
Ep done - 59000.
Ep done - 59000.
Ep done - 59000.
Ep done - 59000.
Ep done - 59000.
Ep done - 59000.
Ep done - 59000.
Ep done - 59000.
Ep done - 59500.
Ep done - 59500.
Ep done - 59500.
Ep done - 59500.
Ep done - 59500.
Ep done - 59500.
Ep done - 59500.
Ep done - 59500.
Ep done - 60000.
Ep done - 60000.
Ep done - 60000.
Ep done - 60000.
Ep done - 60000.
Ep done - 60000.
Ep done - 60000.
Ep done - 60000.
Ep done - 60500.
Ep done - 60500.
Ep done - 60500.
Ep done - 60500.
Ep done - 60500.
Ep done - 60500.
Ep done - 60500.
Ep done - 60500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 28       |
|    time_elapsed    | 16652    |
|    total_timesteps | 13762560 |
---------------------------------
Ep done - 61000.
Ep done - 61000.
Ep done - 61000.
Ep done - 61000.
Ep done - 61000.
Ep done - 61000.
Ep done - 61000.
Ep done - 61000.
Eval num_timesteps=13762784, episode_reward=0.20 +/- 0.96
Episode length: 30.09 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.205       |
| time/                   |             |
|    total_timesteps      | 13762784    |
| train/                  |             |
|    approx_kl            | 0.006866646 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.312       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.145       |
|    n_updates            | 7744        |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 0.306       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.205 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 17 ---------------------------------------
Ep done - 61500.
Ep done - 61500.
Ep done - 61500.
Ep done - 61500.
Ep done - 61500.
Ep done - 61500.
Ep done - 61500.
Ep done - 61500.
Ep done - 62000.
Ep done - 62000.
Ep done - 62000.
Ep done - 62000.
Ep done - 62000.
Ep done - 62000.
Ep done - 62000.
Ep done - 62000.
Ep done - 62500.
Ep done - 62500.
Ep done - 62500.
Ep done - 62500.
Ep done - 62500.
Ep done - 62500.
Ep done - 62500.
Ep done - 62500.
Ep done - 63000.
Ep done - 63000.
Ep done - 63000.
Ep done - 63000.
Ep done - 63000.
Ep done - 63000.
Ep done - 63000.
Ep done - 63000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 826      |
|    iterations      | 29       |
|    time_elapsed    | 17255    |
|    total_timesteps | 14254080 |
---------------------------------
Eval num_timesteps=14254312, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.059       |
| time/                   |             |
|    total_timesteps      | 14254312    |
| train/                  |             |
|    approx_kl            | 0.006794988 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.513      |
|    explained_variance   | 0.315       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.137       |
|    n_updates            | 7747        |
|    policy_gradient_loss | -0.00567    |
|    value_loss           | 0.304       |
-----------------------------------------
Ep done - 63500.
Ep done - 63500.
Ep done - 63500.
Ep done - 63500.
Ep done - 63500.
Ep done - 63500.
Ep done - 63500.
Ep done - 63500.
Ep done - 64000.
Ep done - 64000.
Ep done - 64000.
Ep done - 64000.
Ep done - 64000.
Ep done - 64000.
Ep done - 64000.
Ep done - 64000.
Ep done - 64500.
Ep done - 64500.
Ep done - 64500.
Ep done - 64500.
Ep done - 64500.
Ep done - 64500.
Ep done - 64500.
Ep done - 64500.
Ep done - 65000.
Ep done - 65000.
Ep done - 65000.
Ep done - 65000.
Ep done - 65000.
Ep done - 65000.
Ep done - 65000.
Ep done - 65000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 826      |
|    iterations      | 30       |
|    time_elapsed    | 17850    |
|    total_timesteps | 14745600 |
---------------------------------
Eval num_timesteps=14745840, episode_reward=0.11 +/- 0.98
Episode length: 30.08 +/- 0.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.109        |
| time/                   |              |
|    total_timesteps      | 14745840     |
| train/                  |              |
|    approx_kl            | 0.0066539776 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0.319        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.142        |
|    n_updates            | 7750         |
|    policy_gradient_loss | -0.0055      |
|    value_loss           | 0.304        |
------------------------------------------
Ep done - 65500.
Ep done - 65500.
Ep done - 65500.
Ep done - 65500.
Ep done - 65500.
Ep done - 65500.
Ep done - 65500.
Ep done - 65500.
Ep done - 66000.
Ep done - 66000.
Ep done - 66000.
Ep done - 66000.
Ep done - 66000.
Ep done - 66000.
Ep done - 66000.
Ep done - 66000.
Ep done - 66500.
Ep done - 66500.
Ep done - 66500.
Ep done - 66500.
Ep done - 66500.
Ep done - 66500.
Ep done - 66500.
Ep done - 66500.
Ep done - 67000.
Ep done - 67000.
Ep done - 67000.
Ep done - 67000.
Ep done - 67000.
Ep done - 67000.
Ep done - 67000.
Ep done - 67000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 31       |
|    time_elapsed    | 18444    |
|    total_timesteps | 15237120 |
---------------------------------
Ep done - 67500.
Ep done - 67500.
Ep done - 67500.
Ep done - 67500.
Ep done - 67500.
Ep done - 67500.
Ep done - 67500.
Ep done - 67500.
Eval num_timesteps=15237368, episode_reward=0.19 +/- 0.97
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.193       |
| time/                   |             |
|    total_timesteps      | 15237368    |
| train/                  |             |
|    approx_kl            | 0.007019559 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.516      |
|    explained_variance   | 0.313       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.144       |
|    n_updates            | 7753        |
|    policy_gradient_loss | -0.00551    |
|    value_loss           | 0.303       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.193 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 18 ---------------------------------------
Ep done - 68000.
Ep done - 68000.
Ep done - 68000.
Ep done - 68000.
Ep done - 68000.
Ep done - 68000.
Ep done - 68000.
Ep done - 68000.
Ep done - 68500.
Ep done - 68500.
Ep done - 68500.
Ep done - 68500.
Ep done - 68500.
Ep done - 68500.
Ep done - 68500.
Ep done - 68500.
Ep done - 69000.
Ep done - 69000.
Ep done - 69000.
Ep done - 69000.
Ep done - 69000.
Ep done - 69000.
Ep done - 69000.
Ep done - 69000.
Ep done - 69500.
Ep done - 69500.
Ep done - 69500.
Ep done - 69500.
Ep done - 69500.
Ep done - 69500.
Ep done - 69500.
Ep done - 69500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 32       |
|    time_elapsed    | 19048    |
|    total_timesteps | 15728640 |
---------------------------------
Eval num_timesteps=15728896, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.118        |
| time/                   |              |
|    total_timesteps      | 15728896     |
| train/                  |              |
|    approx_kl            | 0.0071401927 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.524       |
|    explained_variance   | 0.317        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.132        |
|    n_updates            | 7756         |
|    policy_gradient_loss | -0.00559     |
|    value_loss           | 0.302        |
------------------------------------------
Ep done - 70000.
Ep done - 70000.
Ep done - 70000.
Ep done - 70000.
Ep done - 70000.
Ep done - 70000.
Ep done - 70000.
Ep done - 70000.
Ep done - 70500.
Ep done - 70500.
Ep done - 70500.
Ep done - 70500.
Ep done - 70500.
Ep done - 70500.
Ep done - 70500.
Ep done - 70500.
Ep done - 71000.
Ep done - 71000.
Ep done - 71000.
Ep done - 71000.
Ep done - 71000.
Ep done - 71000.
Ep done - 71000.
Ep done - 71000.
Ep done - 71500.
Ep done - 71500.
Ep done - 71500.
Ep done - 71500.
Ep done - 71500.
Ep done - 71500.
Ep done - 71500.
Ep done - 71500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 33       |
|    time_elapsed    | 19636    |
|    total_timesteps | 16220160 |
---------------------------------
Eval num_timesteps=16220424, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.137       |
| time/                   |             |
|    total_timesteps      | 16220424    |
| train/                  |             |
|    approx_kl            | 0.007027535 |
|    clip_fraction        | 0.0256      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.319       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.129       |
|    n_updates            | 7759        |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 0.303       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.137 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 19 ---------------------------------------
Ep done - 72000.
Ep done - 72000.
Ep done - 72000.
Ep done - 72000.
Ep done - 72000.
Ep done - 72000.
Ep done - 72000.
Ep done - 72000.
Ep done - 72500.
Ep done - 72500.
Ep done - 72500.
Ep done - 72500.
Ep done - 72500.
Ep done - 72500.
Ep done - 72500.
Ep done - 72500.
Ep done - 73000.
Ep done - 73000.
Ep done - 73000.
Ep done - 73000.
Ep done - 73000.
Ep done - 73000.
Ep done - 73000.
Ep done - 73000.
Ep done - 73500.
Ep done - 73500.
Ep done - 73500.
Ep done - 73500.
Ep done - 73500.
Ep done - 73500.
Ep done - 73500.
Ep done - 73500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 34       |
|    time_elapsed    | 20240    |
|    total_timesteps | 16711680 |
---------------------------------
Ep done - 74000.
Ep done - 74000.
Ep done - 74000.
Ep done - 74000.
Ep done - 74000.
Ep done - 74000.
Ep done - 74000.
Ep done - 74000.
Eval num_timesteps=16711952, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.168       |
| time/                   |             |
|    total_timesteps      | 16711952    |
| train/                  |             |
|    approx_kl            | 0.006942011 |
|    clip_fraction        | 0.0261      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.532      |
|    explained_variance   | 0.315       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.142       |
|    n_updates            | 7762        |
|    policy_gradient_loss | -0.00554    |
|    value_loss           | 0.304       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.168 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 20 ---------------------------------------
Ep done - 74500.
Ep done - 74500.
Ep done - 74500.
Ep done - 74500.
Ep done - 74500.
Ep done - 74500.
Ep done - 74500.
Ep done - 74500.
Ep done - 75000.
Ep done - 75000.
Ep done - 75000.
Ep done - 75000.
Ep done - 75000.
Ep done - 75000.
Ep done - 75000.
Ep done - 75000.
Ep done - 75500.
Ep done - 75500.
Ep done - 75500.
Ep done - 75500.
Ep done - 75500.
Ep done - 75500.
Ep done - 75500.
Ep done - 75500.
Ep done - 76000.
Ep done - 76000.
Ep done - 76000.
Ep done - 76000.
Ep done - 76000.
Ep done - 76000.
Ep done - 76000.
Ep done - 76000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 825      |
|    iterations      | 35       |
|    time_elapsed    | 20838    |
|    total_timesteps | 17203200 |
---------------------------------
Eval num_timesteps=17203480, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.141        |
| time/                   |              |
|    total_timesteps      | 17203480     |
| train/                  |              |
|    approx_kl            | 0.0069066165 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.316        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.137        |
|    n_updates            | 7765         |
|    policy_gradient_loss | -0.00554     |
|    value_loss           | 0.303        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.141 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 21 ---------------------------------------
Ep done - 76500.
Ep done - 76500.
Ep done - 76500.
Ep done - 76500.
Ep done - 76500.
Ep done - 76500.
Ep done - 76500.
Ep done - 76500.
Ep done - 77000.
Ep done - 77000.
Ep done - 77000.
Ep done - 77000.
Ep done - 77000.
Ep done - 77000.
Ep done - 77000.
Ep done - 77000.
Ep done - 77500.
Ep done - 77500.
Ep done - 77500.
Ep done - 77500.
Ep done - 77500.
Ep done - 77500.
Ep done - 77500.
Ep done - 77500.
Ep done - 78000.
Ep done - 78000.
Ep done - 78000.
Ep done - 78000.
Ep done - 78000.
Ep done - 78000.
Ep done - 78000.
Ep done - 78000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 36       |
|    time_elapsed    | 21437    |
|    total_timesteps | 17694720 |
---------------------------------
Eval num_timesteps=17695008, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.122        |
| time/                   |              |
|    total_timesteps      | 17695008     |
| train/                  |              |
|    approx_kl            | 0.0068606706 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.314        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.161        |
|    n_updates            | 7768         |
|    policy_gradient_loss | -0.00541     |
|    value_loss           | 0.302        |
------------------------------------------
Ep done - 78500.
Ep done - 78500.
Ep done - 78500.
Ep done - 78500.
Ep done - 78500.
Ep done - 78500.
Ep done - 78500.
Ep done - 78500.
Ep done - 79000.
Ep done - 79000.
Ep done - 79000.
Ep done - 79000.
Ep done - 79000.
Ep done - 79000.
Ep done - 79000.
Ep done - 79000.
Ep done - 79500.
Ep done - 79500.
Ep done - 79500.
Ep done - 79500.
Ep done - 79500.
Ep done - 79500.
Ep done - 79500.
Ep done - 79500.
Ep done - 80000.
Ep done - 80000.
Ep done - 80000.
Ep done - 80000.
Ep done - 80000.
Ep done - 80000.
Ep done - 80000.
Ep done - 80000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 825      |
|    iterations      | 37       |
|    time_elapsed    | 22025    |
|    total_timesteps | 18186240 |
---------------------------------
Ep done - 80500.
Ep done - 80500.
Ep done - 80500.
Ep done - 80500.
Ep done - 80500.
Ep done - 80500.
Ep done - 80500.
Ep done - 80500.
Eval num_timesteps=18186536, episode_reward=0.10 +/- 0.97
Episode length: 30.04 +/- 0.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 18186536    |
| train/                  |             |
|    approx_kl            | 0.006828536 |
|    clip_fraction        | 0.0255      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.536      |
|    explained_variance   | 0.313       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.125       |
|    n_updates            | 7771        |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 0.302       |
-----------------------------------------
Ep done - 81000.
Ep done - 81000.
Ep done - 81000.
Ep done - 81000.
Ep done - 81000.
Ep done - 81000.
Ep done - 81000.
Ep done - 81000.
Ep done - 81500.
Ep done - 81500.
Ep done - 81500.
Ep done - 81500.
Ep done - 81500.
Ep done - 81500.
Ep done - 81500.
Ep done - 81500.
Ep done - 82000.
Ep done - 82000.
Ep done - 82000.
Ep done - 82000.
Ep done - 82000.
Ep done - 82000.
Ep done - 82000.
Ep done - 82000.
Ep done - 82500.
Ep done - 82500.
Ep done - 82500.
Ep done - 82500.
Ep done - 82500.
Ep done - 82500.
Ep done - 82500.
Ep done - 82500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 38       |
|    time_elapsed    | 22619    |
|    total_timesteps | 18677760 |
---------------------------------
Eval num_timesteps=18678064, episode_reward=0.11 +/- 0.97
Episode length: 29.99 +/- 0.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.111        |
| time/                   |              |
|    total_timesteps      | 18678064     |
| train/                  |              |
|    approx_kl            | 0.0068471897 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.317        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.132        |
|    n_updates            | 7774         |
|    policy_gradient_loss | -0.00547     |
|    value_loss           | 0.303        |
------------------------------------------
Ep done - 83000.
Ep done - 83000.
Ep done - 83000.
Ep done - 83000.
Ep done - 83000.
Ep done - 83000.
Ep done - 83000.
Ep done - 83000.
Ep done - 83500.
Ep done - 83500.
Ep done - 83500.
Ep done - 83500.
Ep done - 83500.
Ep done - 83500.
Ep done - 83500.
Ep done - 83500.
Ep done - 84000.
Ep done - 84000.
Ep done - 84000.
Ep done - 84000.
Ep done - 84000.
Ep done - 84000.
Ep done - 84000.
Ep done - 84000.
Ep done - 84500.
Ep done - 84500.
Ep done - 84500.
Ep done - 84500.
Ep done - 84500.
Ep done - 84500.
Ep done - 84500.
Ep done - 84500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 39       |
|    time_elapsed    | 23208    |
|    total_timesteps | 19169280 |
---------------------------------
Eval num_timesteps=19169592, episode_reward=0.18 +/- 0.97
Episode length: 30.05 +/- 0.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.183        |
| time/                   |              |
|    total_timesteps      | 19169592     |
| train/                  |              |
|    approx_kl            | 0.0068243514 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.317        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.103        |
|    n_updates            | 7777         |
|    policy_gradient_loss | -0.00532     |
|    value_loss           | 0.304        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.183 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 22 ---------------------------------------
Ep done - 85000.
Ep done - 85000.
Ep done - 85000.
Ep done - 85000.
Ep done - 85000.
Ep done - 85000.
Ep done - 85000.
Ep done - 85000.
Ep done - 85500.
Ep done - 85500.
Ep done - 85500.
Ep done - 85500.
Ep done - 85500.
Ep done - 85500.
Ep done - 85500.
Ep done - 85500.
Ep done - 86000.
Ep done - 86000.
Ep done - 86000.
Ep done - 86000.
Ep done - 86000.
Ep done - 86000.
Ep done - 86000.
Ep done - 86000.
Ep done - 86500.
Ep done - 86500.
Ep done - 86500.
Ep done - 86500.
Ep done - 86500.
Ep done - 86500.
Ep done - 86500.
Ep done - 86500.
Ep done - 87000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 40       |
|    time_elapsed    | 23808    |
|    total_timesteps | 19660800 |
---------------------------------
Ep done - 87000.
Ep done - 87000.
Ep done - 87000.
Ep done - 87000.
Ep done - 87000.
Ep done - 87000.
Ep done - 87000.
Eval num_timesteps=19661120, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.127       |
| time/                   |             |
|    total_timesteps      | 19661120    |
| train/                  |             |
|    approx_kl            | 0.007059924 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.536      |
|    explained_variance   | 0.316       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.139       |
|    n_updates            | 7780        |
|    policy_gradient_loss | -0.00551    |
|    value_loss           | 0.304       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.127 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 23 ---------------------------------------
Ep done - 87500.
Ep done - 87500.
Ep done - 87500.
Ep done - 87500.
Ep done - 87500.
Ep done - 87500.
Ep done - 87500.
Ep done - 87500.
Ep done - 88000.
Ep done - 88000.
Ep done - 88000.
Ep done - 88000.
Ep done - 88000.
Ep done - 88000.
Ep done - 88000.
Ep done - 88000.
Ep done - 88500.
Ep done - 88500.
Ep done - 88500.
Ep done - 88500.
Ep done - 88500.
Ep done - 88500.
Ep done - 88500.
Ep done - 88500.
Ep done - 89000.
Ep done - 89000.
Ep done - 89000.
Ep done - 89000.
Ep done - 89000.
Ep done - 89000.
Ep done - 89000.
Ep done - 89000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 41       |
|    time_elapsed    | 24409    |
|    total_timesteps | 20152320 |
---------------------------------
Eval num_timesteps=20152648, episode_reward=0.18 +/- 0.97
Episode length: 30.04 +/- 0.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.178        |
| time/                   |              |
|    total_timesteps      | 20152648     |
| train/                  |              |
|    approx_kl            | 0.0068866564 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.533       |
|    explained_variance   | 0.315        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.139        |
|    n_updates            | 7783         |
|    policy_gradient_loss | -0.00552     |
|    value_loss           | 0.304        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.178 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 24 ---------------------------------------
Ep done - 89500.
Ep done - 89500.
Ep done - 89500.
Ep done - 89500.
Ep done - 89500.
Ep done - 89500.
Ep done - 89500.
Ep done - 89500.
Ep done - 90000.
Ep done - 90000.
Ep done - 90000.
Ep done - 90000.
Ep done - 90000.
Ep done - 90000.
Ep done - 90000.
Ep done - 90000.
Ep done - 90500.
Ep done - 90500.
Ep done - 90500.
Ep done - 90500.
Ep done - 90500.
Ep done - 90500.
Ep done - 90500.
Ep done - 90500.
Ep done - 91000.
Ep done - 91000.
Ep done - 91000.
Ep done - 91000.
Ep done - 91000.
Ep done - 91000.
Ep done - 91000.
Ep done - 91000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 825      |
|    iterations      | 42       |
|    time_elapsed    | 25011    |
|    total_timesteps | 20643840 |
---------------------------------
Eval num_timesteps=20644176, episode_reward=0.12 +/- 0.98
Episode length: 30.05 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.117       |
| time/                   |             |
|    total_timesteps      | 20644176    |
| train/                  |             |
|    approx_kl            | 0.007076309 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.314       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.129       |
|    n_updates            | 7786        |
|    policy_gradient_loss | -0.00533    |
|    value_loss           | 0.304       |
-----------------------------------------
Ep done - 91500.
Ep done - 91500.
Ep done - 91500.
Ep done - 91500.
Ep done - 91500.
Ep done - 91500.
Ep done - 91500.
Ep done - 91500.
Ep done - 92000.
Ep done - 92000.
Ep done - 92000.
Ep done - 92000.
Ep done - 92000.
Ep done - 92000.
Ep done - 92000.
Ep done - 92000.
Ep done - 92500.
Ep done - 92500.
Ep done - 92500.
Ep done - 92500.
Ep done - 92500.
Ep done - 92500.
Ep done - 92500.
Ep done - 92500.
Ep done - 93000.
Ep done - 93000.
Ep done - 93000.
Ep done - 93000.
Ep done - 93000.
Ep done - 93000.
Ep done - 93000.
Ep done - 93000.
Ep done - 93500.
Ep done - 93500.
Ep done - 93500.
Ep done - 93500.
Ep done - 93500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 825      |
|    iterations      | 43       |
|    time_elapsed    | 25605    |
|    total_timesteps | 21135360 |
---------------------------------
Ep done - 93500.
Ep done - 93500.
Ep done - 93500.
Eval num_timesteps=21135704, episode_reward=0.12 +/- 0.98
Episode length: 30.04 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.116        |
| time/                   |              |
|    total_timesteps      | 21135704     |
| train/                  |              |
|    approx_kl            | 0.0069988673 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.318        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.144        |
|    n_updates            | 7789         |
|    policy_gradient_loss | -0.00552     |
|    value_loss           | 0.302        |
------------------------------------------
Ep done - 94000.
Ep done - 94000.
Ep done - 94000.
Ep done - 94000.
Ep done - 94000.
Ep done - 94000.
Ep done - 94000.
Ep done - 94000.
Ep done - 94500.
Ep done - 94500.
Ep done - 94500.
Ep done - 94500.
Ep done - 94500.
Ep done - 94500.
Ep done - 94500.
Ep done - 94500.
Ep done - 95000.
Ep done - 95000.
Ep done - 95000.
Ep done - 95000.
Ep done - 95000.
Ep done - 95000.
Ep done - 95000.
Ep done - 95000.
Ep done - 95500.
Ep done - 95500.
Ep done - 95500.
Ep done - 95500.
Ep done - 95500.
Ep done - 95500.
Ep done - 95500.
Ep done - 95500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 44       |
|    time_elapsed    | 26194    |
|    total_timesteps | 21626880 |
---------------------------------
Eval num_timesteps=21627232, episode_reward=0.12 +/- 0.98
Episode length: 30.03 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 21627232    |
| train/                  |             |
|    approx_kl            | 0.006883206 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.529      |
|    explained_variance   | 0.317       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.138       |
|    n_updates            | 7792        |
|    policy_gradient_loss | -0.00552    |
|    value_loss           | 0.303       |
-----------------------------------------
Ep done - 96000.
Ep done - 96000.
Ep done - 96000.
Ep done - 96000.
Ep done - 96000.
Ep done - 96000.
Ep done - 96000.
Ep done - 96000.
Ep done - 96500.
Ep done - 96500.
Ep done - 96500.
Ep done - 96500.
Ep done - 96500.
Ep done - 96500.
Ep done - 96500.
Ep done - 96500.
Ep done - 97000.
Ep done - 97000.
Ep done - 97000.
Ep done - 97000.
Ep done - 97000.
Ep done - 97000.
Ep done - 97000.
Ep done - 97000.
Ep done - 97500.
Ep done - 97500.
Ep done - 97500.
Ep done - 97500.
Ep done - 97500.
Ep done - 97500.
Ep done - 97500.
Ep done - 97500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 45       |
|    time_elapsed    | 26781    |
|    total_timesteps | 22118400 |
---------------------------------
Ep done - 98000.
Eval num_timesteps=22118760, episode_reward=0.21 +/- 0.96
Episode length: 30.06 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.211       |
| time/                   |             |
|    total_timesteps      | 22118760    |
| train/                  |             |
|    approx_kl            | 0.007016194 |
|    clip_fraction        | 0.0256      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.525      |
|    explained_variance   | 0.319       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.14        |
|    n_updates            | 7795        |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 0.301       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.211 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 25 ---------------------------------------
Ep done - 98000.
Ep done - 98000.
Ep done - 98000.
Ep done - 98000.
Ep done - 98000.
Ep done - 98000.
Ep done - 98000.
Ep done - 98500.
Ep done - 98500.
Ep done - 98500.
Ep done - 98500.
Ep done - 98500.
Ep done - 98500.
Ep done - 98500.
Ep done - 98500.
Ep done - 99000.
Ep done - 99000.
Ep done - 99000.
Ep done - 99000.
Ep done - 99000.
Ep done - 99000.
Ep done - 99000.
Ep done - 99000.
Ep done - 99500.
Ep done - 99500.
Ep done - 99500.
Ep done - 99500.
Ep done - 99500.
Ep done - 99500.
Ep done - 99500.
Ep done - 99500.
Ep done - 100000.
Ep done - 100000.
Ep done - 100000.
Ep done - 100000.
Ep done - 100000.
Ep done - 100000.
Ep done - 100000.
Ep done - 100000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 46       |
|    time_elapsed    | 27379    |
|    total_timesteps | 22609920 |
---------------------------------
Eval num_timesteps=22610288, episode_reward=0.14 +/- 0.98
Episode length: 30.06 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.135       |
| time/                   |             |
|    total_timesteps      | 22610288    |
| train/                  |             |
|    approx_kl            | 0.007334314 |
|    clip_fraction        | 0.0268      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.314       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.129       |
|    n_updates            | 7798        |
|    policy_gradient_loss | -0.00555    |
|    value_loss           | 0.305       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.135 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 26 ---------------------------------------
Ep done - 100500.
Ep done - 100500.
Ep done - 100500.
Ep done - 100500.
Ep done - 100500.
Ep done - 100500.
Ep done - 100500.
Ep done - 100500.
Ep done - 101000.
Ep done - 101000.
Ep done - 101000.
Ep done - 101000.
Ep done - 101000.
Ep done - 101000.
Ep done - 101000.
Ep done - 101000.
Ep done - 101500.
Ep done - 101500.
Ep done - 101500.
Ep done - 101500.
Ep done - 101500.
Ep done - 101500.
Ep done - 101500.
Ep done - 101500.
Ep done - 102000.
Ep done - 102000.
Ep done - 102000.
Ep done - 102000.
Ep done - 102000.
Ep done - 102000.
Ep done - 102000.
Ep done - 102000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 47       |
|    time_elapsed    | 27978    |
|    total_timesteps | 23101440 |
---------------------------------
Eval num_timesteps=23101816, episode_reward=0.09 +/- 0.97
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.087       |
| time/                   |             |
|    total_timesteps      | 23101816    |
| train/                  |             |
|    approx_kl            | 0.006958247 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.311       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.136       |
|    n_updates            | 7801        |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 0.305       |
-----------------------------------------
Ep done - 102500.
Ep done - 102500.
Ep done - 102500.
Ep done - 102500.
Ep done - 102500.
Ep done - 102500.
Ep done - 102500.
Ep done - 102500.
Ep done - 103000.
Ep done - 103000.
Ep done - 103000.
Ep done - 103000.
Ep done - 103000.
Ep done - 103000.
Ep done - 103000.
Ep done - 103000.
Ep done - 103500.
Ep done - 103500.
Ep done - 103500.
Ep done - 103500.
Ep done - 103500.
Ep done - 103500.
Ep done - 103500.
Ep done - 103500.
Ep done - 104000.
Ep done - 104000.
Ep done - 104000.
Ep done - 104000.
Ep done - 104000.
Ep done - 104000.
Ep done - 104000.
Ep done - 104000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 48       |
|    time_elapsed    | 28571    |
|    total_timesteps | 23592960 |
---------------------------------
Ep done - 104500.
Ep done - 104500.
Ep done - 104500.
Ep done - 104500.
Eval num_timesteps=23593344, episode_reward=0.13 +/- 0.98
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 23593344    |
| train/                  |             |
|    approx_kl            | 0.007211859 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.317       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.132       |
|    n_updates            | 7804        |
|    policy_gradient_loss | -0.0056     |
|    value_loss           | 0.304       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.128 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 27 ---------------------------------------
Ep done - 104500.
Ep done - 104500.
Ep done - 104500.
Ep done - 104500.
Ep done - 105000.
Ep done - 105000.
Ep done - 105000.
Ep done - 105000.
Ep done - 105000.
Ep done - 105000.
Ep done - 105000.
Ep done - 105000.
Ep done - 105500.
Ep done - 105500.
Ep done - 105500.
Ep done - 105500.
Ep done - 105500.
Ep done - 105500.
Ep done - 105500.
Ep done - 105500.
Ep done - 106000.
Ep done - 106000.
Ep done - 106000.
Ep done - 106000.
Ep done - 106000.
Ep done - 106000.
Ep done - 106000.
Ep done - 106000.
Ep done - 106500.
Ep done - 106500.
Ep done - 106500.
Ep done - 106500.
Ep done - 106500.
Ep done - 106500.
Ep done - 106500.
Ep done - 106500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 825      |
|    iterations      | 49       |
|    time_elapsed    | 29169    |
|    total_timesteps | 24084480 |
---------------------------------
Eval num_timesteps=24084872, episode_reward=0.08 +/- 0.99
Episode length: 30.01 +/- 1.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.079       |
| time/                   |             |
|    total_timesteps      | 24084872    |
| train/                  |             |
|    approx_kl            | 0.006993363 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.318       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.131       |
|    n_updates            | 7807        |
|    policy_gradient_loss | -0.00544    |
|    value_loss           | 0.304       |
-----------------------------------------
Ep done - 107000.
Ep done - 107000.
Ep done - 107000.
Ep done - 107000.
Ep done - 107000.
Ep done - 107000.
Ep done - 107000.
Ep done - 107000.
Ep done - 107500.
Ep done - 107500.
Ep done - 107500.
Ep done - 107500.
Ep done - 107500.
Ep done - 107500.
Ep done - 107500.
Ep done - 107500.
Ep done - 108000.
Ep done - 108000.
Ep done - 108000.
Ep done - 108000.
Ep done - 108000.
Ep done - 108000.
Ep done - 108000.
Ep done - 108000.
Ep done - 108500.
Ep done - 108500.
Ep done - 108500.
Ep done - 108500.
Ep done - 108500.
Ep done - 108500.
Ep done - 108500.
Ep done - 108500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 825      |
|    iterations      | 50       |
|    time_elapsed    | 29764    |
|    total_timesteps | 24576000 |
---------------------------------
Eval num_timesteps=24576400, episode_reward=0.16 +/- 0.97
Episode length: 30.05 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.162        |
| time/                   |              |
|    total_timesteps      | 24576400     |
| train/                  |              |
|    approx_kl            | 0.0070727644 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.512       |
|    explained_variance   | 0.316        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.146        |
|    n_updates            | 7810         |
|    policy_gradient_loss | -0.00556     |
|    value_loss           | 0.302        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.162 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 28 ---------------------------------------
Ep done - 109000.
Ep done - 109000.
Ep done - 109000.
Ep done - 109000.
Ep done - 109000.
Ep done - 109000.
Ep done - 109000.
Ep done - 109000.
Ep done - 109500.
Ep done - 109500.
Ep done - 109500.
Ep done - 109500.
Ep done - 109500.
Ep done - 109500.
Ep done - 109500.
Ep done - 109500.
Ep done - 110000.
Ep done - 110000.
Ep done - 110000.
Ep done - 110000.
Ep done - 110000.
Ep done - 110000.
Ep done - 110000.
Ep done - 110000.
Ep done - 110500.
Ep done - 110500.
Ep done - 110500.
Ep done - 110500.
Ep done - 110500.
Ep done - 110500.
Ep done - 110500.
Ep done - 110500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 51       |
|    time_elapsed    | 30365    |
|    total_timesteps | 25067520 |
---------------------------------
Ep done - 111000.
Ep done - 111000.
Ep done - 111000.
Ep done - 111000.
Ep done - 111000.
Ep done - 111000.
Ep done - 111000.
Ep done - 111000.
Eval num_timesteps=25067928, episode_reward=0.16 +/- 0.97
Episode length: 30.04 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.161        |
| time/                   |              |
|    total_timesteps      | 25067928     |
| train/                  |              |
|    approx_kl            | 0.0068264543 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.509       |
|    explained_variance   | 0.314        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.146        |
|    n_updates            | 7813         |
|    policy_gradient_loss | -0.00539     |
|    value_loss           | 0.303        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.161 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 29 ---------------------------------------
Ep done - 111500.
Ep done - 111500.
Ep done - 111500.
Ep done - 111500.
Ep done - 111500.
Ep done - 111500.
Ep done - 111500.
Ep done - 111500.
Ep done - 112000.
Ep done - 112000.
Ep done - 112000.
Ep done - 112000.
Ep done - 112000.
Ep done - 112000.
Ep done - 112000.
Ep done - 112000.
Ep done - 112500.
Ep done - 112500.
Ep done - 112500.
Ep done - 112500.
Ep done - 112500.
Ep done - 112500.
Ep done - 112500.
Ep done - 112500.
Ep done - 113000.
Ep done - 113000.
Ep done - 113000.
Ep done - 113000.
Ep done - 113000.
Ep done - 113000.
Ep done - 113000.
Ep done - 113000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 52       |
|    time_elapsed    | 30967    |
|    total_timesteps | 25559040 |
---------------------------------
Eval num_timesteps=25559456, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.135        |
| time/                   |              |
|    total_timesteps      | 25559456     |
| train/                  |              |
|    approx_kl            | 0.0064419457 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.504       |
|    explained_variance   | 0.318        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.116        |
|    n_updates            | 7816         |
|    policy_gradient_loss | -0.0051      |
|    value_loss           | 0.302        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.135 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 30 ---------------------------------------
Ep done - 113500.
Ep done - 113500.
Ep done - 113500.
Ep done - 113500.
Ep done - 113500.
Ep done - 113500.
Ep done - 113500.
Ep done - 113500.
Ep done - 114000.
Ep done - 114000.
Ep done - 114000.
Ep done - 114000.
Ep done - 114000.
Ep done - 114000.
Ep done - 114000.
Ep done - 114000.
Ep done - 114500.
Ep done - 114500.
Ep done - 114500.
Ep done - 114500.
Ep done - 114500.
Ep done - 114500.
Ep done - 114500.
Ep done - 114500.
Ep done - 115000.
Ep done - 115000.
Ep done - 115000.
Ep done - 115000.
Ep done - 115000.
Ep done - 115000.
Ep done - 115000.
Ep done - 115000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 53       |
|    time_elapsed    | 31569    |
|    total_timesteps | 26050560 |
---------------------------------
Eval num_timesteps=26050984, episode_reward=0.14 +/- 0.97
Episode length: 30.04 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.144        |
| time/                   |              |
|    total_timesteps      | 26050984     |
| train/                  |              |
|    approx_kl            | 0.0066608996 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.502       |
|    explained_variance   | 0.316        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.133        |
|    n_updates            | 7819         |
|    policy_gradient_loss | -0.00525     |
|    value_loss           | 0.303        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.144 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 31 ---------------------------------------
Ep done - 115500.
Ep done - 115500.
Ep done - 115500.
Ep done - 115500.
Ep done - 115500.
Ep done - 115500.
Ep done - 115500.
Ep done - 115500.
Ep done - 116000.
Ep done - 116000.
Ep done - 116000.
Ep done - 116000.
Ep done - 116000.
Ep done - 116000.
Ep done - 116000.
Ep done - 116000.
Ep done - 116500.
Ep done - 116500.
Ep done - 116500.
Ep done - 116500.
Ep done - 116500.
Ep done - 116500.
Ep done - 116500.
Ep done - 116500.
Ep done - 117000.
Ep done - 117000.
Ep done - 117000.
Ep done - 117000.
Ep done - 117000.
Ep done - 117000.
Ep done - 117000.
Ep done - 117000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 825      |
|    iterations      | 54       |
|    time_elapsed    | 32170    |
|    total_timesteps | 26542080 |
---------------------------------
Ep done - 117500.
Ep done - 117500.
Ep done - 117500.
Ep done - 117500.
Ep done - 117500.
Ep done - 117500.
Ep done - 117500.
Ep done - 117500.
Eval num_timesteps=26542512, episode_reward=0.16 +/- 0.97
Episode length: 30.08 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.156       |
| time/                   |             |
|    total_timesteps      | 26542512    |
| train/                  |             |
|    approx_kl            | 0.006556736 |
|    clip_fraction        | 0.0251      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.499      |
|    explained_variance   | 0.314       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.128       |
|    n_updates            | 7822        |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 0.303       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.156 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 32 ---------------------------------------
Ep done - 118000.
Ep done - 118000.
Ep done - 118000.
Ep done - 118000.
Ep done - 118000.
Ep done - 118000.
Ep done - 118000.
Ep done - 118000.
Ep done - 118500.
Ep done - 118500.
Ep done - 118500.
Ep done - 118500.
Ep done - 118500.
Ep done - 118500.
Ep done - 118500.
Ep done - 118500.
Ep done - 119000.
Ep done - 119000.
Ep done - 119000.
Ep done - 119000.
Ep done - 119000.
Ep done - 119000.
Ep done - 119000.
Ep done - 119000.
Ep done - 119500.
Ep done - 119500.
Ep done - 119500.
Ep done - 119500.
Ep done - 119500.
Ep done - 119500.
Ep done - 119500.
Ep done - 119500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 824      |
|    iterations      | 55       |
|    time_elapsed    | 32768    |
|    total_timesteps | 27033600 |
---------------------------------
Eval num_timesteps=27034040, episode_reward=0.09 +/- 0.98
Episode length: 30.03 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.092        |
| time/                   |              |
|    total_timesteps      | 27034040     |
| train/                  |              |
|    approx_kl            | 0.0064859083 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.496       |
|    explained_variance   | 0.315        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.159        |
|    n_updates            | 7825         |
|    policy_gradient_loss | -0.00516     |
|    value_loss           | 0.305        |
------------------------------------------
Ep done - 120000.
Ep done - 120000.
Ep done - 120000.
Ep done - 120000.
Ep done - 120000.
Ep done - 120000.
Ep done - 120000.
Ep done - 120000.
Ep done - 120500.
Ep done - 120500.
Ep done - 120500.
Ep done - 120500.
Ep done - 120500.
Ep done - 120500.
Ep done - 120500.
Ep done - 120500.
Ep done - 121000.
Ep done - 121000.
Ep done - 121000.
Ep done - 121000.
Ep done - 121000.
Ep done - 121000.
Ep done - 121000.
Ep done - 121000.
Ep done - 121500.
Ep done - 121500.
Ep done - 121500.
Ep done - 121500.
Ep done - 121500.
Ep done - 121500.
Ep done - 121500.
Ep done - 121500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 56       |
|    time_elapsed    | 33361    |
|    total_timesteps | 27525120 |
---------------------------------
Eval num_timesteps=27525568, episode_reward=0.15 +/- 0.97
Episode length: 30.07 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 27525568    |
| train/                  |             |
|    approx_kl            | 0.006637419 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.492      |
|    explained_variance   | 0.314       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.149       |
|    n_updates            | 7828        |
|    policy_gradient_loss | -0.0051     |
|    value_loss           | 0.304       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.15 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 33 ---------------------------------------
Ep done - 122000.
Ep done - 122000.
Ep done - 122000.
Ep done - 122000.
Ep done - 122000.
Ep done - 122000.
Ep done - 122000.
Ep done - 122000.
Ep done - 122500.
Ep done - 122500.
Ep done - 122500.
Ep done - 122500.
Ep done - 122500.
Ep done - 122500.
Ep done - 122500.
Ep done - 122500.
Ep done - 123000.
Ep done - 123000.
Ep done - 123000.
Ep done - 123000.
Ep done - 123000.
Ep done - 123000.
Ep done - 123000.
Ep done - 123000.
Ep done - 123500.
Ep done - 123500.
Ep done - 123500.
Ep done - 123500.
Ep done - 123500.
Ep done - 123500.
Ep done - 123500.
Ep done - 123500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 825      |
|    iterations      | 57       |
|    time_elapsed    | 33959    |
|    total_timesteps | 28016640 |
---------------------------------
Ep done - 124000.
Ep done - 124000.
Ep done - 124000.
Ep done - 124000.
Ep done - 124000.
Ep done - 124000.
Ep done - 124000.
Ep done - 124000.
Eval num_timesteps=28017096, episode_reward=0.09 +/- 0.97
Episode length: 30.01 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.094        |
| time/                   |              |
|    total_timesteps      | 28017096     |
| train/                  |              |
|    approx_kl            | 0.0065805926 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.313        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.143        |
|    n_updates            | 7831         |
|    policy_gradient_loss | -0.00528     |
|    value_loss           | 0.304        |
------------------------------------------
Ep done - 124500.
Ep done - 124500.
Ep done - 124500.
Ep done - 124500.
Ep done - 124500.
Ep done - 124500.
Ep done - 124500.
Ep done - 124500.
Ep done - 125000.
Ep done - 125000.
Ep done - 125000.
Ep done - 125000.
Ep done - 125000.
Ep done - 125000.
Ep done - 125000.
Ep done - 125000.
Ep done - 125500.
Ep done - 125500.
Ep done - 125500.
Ep done - 125500.
Ep done - 125500.
Ep done - 125500.
Ep done - 125500.
Ep done - 125500.
Ep done - 126000.
Ep done - 126000.
Ep done - 126000.
Ep done - 126000.
Ep done - 126000.
Ep done - 126000.
Ep done - 126000.
Ep done - 126000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 825      |
|    iterations      | 58       |
|    time_elapsed    | 34553    |
|    total_timesteps | 28508160 |
---------------------------------
Eval num_timesteps=28508624, episode_reward=0.16 +/- 0.97
Episode length: 30.09 +/- 0.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.162        |
| time/                   |              |
|    total_timesteps      | 28508624     |
| train/                  |              |
|    approx_kl            | 0.0066265883 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.484       |
|    explained_variance   | 0.315        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.145        |
|    n_updates            | 7834         |
|    policy_gradient_loss | -0.00522     |
|    value_loss           | 0.304        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.162 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 34 ---------------------------------------
Ep done - 126500.
Ep done - 126500.
Ep done - 126500.
Ep done - 126500.
Ep done - 126500.
Ep done - 126500.
Ep done - 126500.
Ep done - 126500.
Ep done - 127000.
Ep done - 127000.
Ep done - 127000.
Ep done - 127000.
Ep done - 127000.
Ep done - 127000.
Ep done - 127000.
Ep done - 127000.
Ep done - 127500.
Ep done - 127500.
Ep done - 127500.
Ep done - 127500.
Ep done - 127500.
Ep done - 127500.
Ep done - 127500.
Ep done - 127500.
Ep done - 128000.
Ep done - 128000.
Ep done - 128000.
Ep done - 128000.
Ep done - 128000.
Ep done - 128000.
Ep done - 128000.
Ep done - 128000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 59       |
|    time_elapsed    | 35150    |
|    total_timesteps | 28999680 |
---------------------------------
Eval num_timesteps=29000152, episode_reward=0.13 +/- 0.98
Episode length: 30.05 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.129       |
| time/                   |             |
|    total_timesteps      | 29000152    |
| train/                  |             |
|    approx_kl            | 0.006903879 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.482      |
|    explained_variance   | 0.321       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.143       |
|    n_updates            | 7837        |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 0.301       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.129 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 35 ---------------------------------------
Ep done - 128500.
Ep done - 128500.
Ep done - 128500.
Ep done - 128500.
Ep done - 128500.
Ep done - 128500.
Ep done - 128500.
Ep done - 128500.
Ep done - 129000.
Ep done - 129000.
Ep done - 129000.
Ep done - 129000.
Ep done - 129000.
Ep done - 129000.
Ep done - 129000.
Ep done - 129000.
Ep done - 129500.
Ep done - 129500.
Ep done - 129500.
Ep done - 129500.
Ep done - 129500.
Ep done - 129500.
Ep done - 129500.
Ep done - 129500.
Ep done - 130000.
Ep done - 130000.
Ep done - 130000.
Ep done - 130000.
Ep done - 130000.
Ep done - 130000.
Ep done - 130000.
Ep done - 130000.
Ep done - 130500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 824      |
|    iterations      | 60       |
|    time_elapsed    | 35749    |
|    total_timesteps | 29491200 |
---------------------------------
Ep done - 130500.
Ep done - 130500.
Ep done - 130500.
Ep done - 130500.
Ep done - 130500.
Ep done - 130500.
Ep done - 130500.
Eval num_timesteps=29491680, episode_reward=0.13 +/- 0.97
Episode length: 30.03 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.131        |
| time/                   |              |
|    total_timesteps      | 29491680     |
| train/                  |              |
|    approx_kl            | 0.0066453493 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.3          |
|    entropy_loss         | -0.473       |
|    explained_variance   | 0.316        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.133        |
|    n_updates            | 7840         |
|    policy_gradient_loss | -0.00534     |
|    value_loss           | 0.303        |
------------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.131 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 36 ---------------------------------------
Ep done - 131000.
Ep done - 131000.
Ep done - 131000.
Ep done - 131000.
Ep done - 131000.
Ep done - 131000.
Ep done - 131000.
Ep done - 131000.
Ep done - 131500.
Ep done - 131500.
Ep done - 131500.
Ep done - 131500.
Ep done - 131500.
Ep done - 131500.
Ep done - 131500.
Ep done - 131500.
Ep done - 132000.
Ep done - 132000.
Ep done - 132000.
Ep done - 132000.
Ep done - 132000.
Ep done - 132000.
Ep done - 132000.
Ep done - 132000.
Ep done - 132500.
Ep done - 132500.
Ep done - 132500.
Ep done - 132500.
Ep done - 132500.
Ep done - 132500.
Ep done - 132500.
Ep done - 132500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 824      |
|    iterations      | 61       |
|    time_elapsed    | 36348    |
|    total_timesteps | 29982720 |
---------------------------------
Eval num_timesteps=29983208, episode_reward=0.16 +/- 0.97
Episode length: 30.06 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.158       |
| time/                   |             |
|    total_timesteps      | 29983208    |
| train/                  |             |
|    approx_kl            | 0.006350136 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.3         |
|    entropy_loss         | -0.465      |
|    explained_variance   | 0.318       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.137       |
|    n_updates            | 7843        |
|    policy_gradient_loss | -0.00519    |
|    value_loss           | 0.303       |
-----------------------------------------
New best mean reward!
------------------SELFPLAY: mean_reward achieved: 0.158 ---------------------------------------
------------------SELFPLAY: new best model, bumping up generation to 37 ---------------------------------------
Ep done - 133000.
Ep done - 133000.
Ep done - 133000.
Ep done - 133000.
Ep done - 133000.
Ep done - 133000.
Ep done - 133000.
Ep done - 133000.
Ep done - 133500.
Ep done - 133500.
Ep done - 133500.
Ep done - 133500.
Ep done - 133500.
Ep done - 133500.
Ep done - 133500.
Ep done - 133500.
Ep done - 134000.
Ep done - 134000.
Ep done - 134000.
Ep done - 134000.
Ep done - 134000.
Ep done - 134000.
Ep done - 134000.
Ep done - 134000.
Ep done - 134500.
Ep done - 134500.
Ep done - 134500.
Ep done - 134500.
Ep done - 134500.
Ep done - 134500.
Ep done - 134500.
Ep done - 134500.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 824      |
|    iterations      | 62       |
|    time_elapsed    | 36946    |
|    total_timesteps | 30474240 |
---------------------------------
Elapsed time: 10h 16m 24s
