CUDA Available: True
CPU Model: AMD EPYC 7313P 16-Core Processor
GPU Model: Tesla T4
seed: 129 
num_timesteps: 10000000 
eval_freq: 26000 
eval_episoded: 200 
best_threshold: 0.3 
logdir: scripts/rl/output/phase2/ars/mlp/base/ 
continueFrom_model: None
CUDA available: True
Using cuda device

params: {'n_delta': 30, 'n_top': 12, 'zero_policy': False, 'n_eval_episodes': 10, 'delta_std': 0.05, 'learning_rate': <__main__.LinearSchedule object at 0x7fb97cf91d00>, 'verbose': 2, 'seed': 129, 'policy_kwargs': {'net_arch': [64, 64, 64, 64, 64, 64, 64, 64]}}

Ep done - 1000.
Eval num_timesteps=26000, episode_reward=0.52 +/- 0.85
Episode length: 30.00 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.52     |
| rollout/           |          |
|    return_std      | 2.55     |
| time/              |          |
|    total_timesteps | 26000    |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 0        |
|    learning_rate   | 0.04     |
|    step_size       | 0.00131  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.52
SELFPLAY: new best model, bumping up generation to 1
Ep done - 2000.
Eval num_timesteps=52000, episode_reward=0.42 +/- 0.90
Episode length: 30.09 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.425    |
| rollout/           |          |
|    return_std      | 3.91     |
| time/              |          |
|    total_timesteps | 52000    |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1        |
|    learning_rate   | 0.0399   |
|    step_size       | 0.000852 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.425
SELFPLAY: new best model, bumping up generation to 2
Ep done - 3000.
Eval num_timesteps=78000, episode_reward=0.42 +/- 0.89
Episode length: 29.99 +/- 1.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.425    |
| rollout/           |          |
|    return_std      | 3.54     |
| time/              |          |
|    total_timesteps | 78000    |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 3        |
|    learning_rate   | 0.0398   |
|    step_size       | 0.000937 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.425
SELFPLAY: new best model, bumping up generation to 3
Ep done - 4000.
Eval num_timesteps=104000, episode_reward=0.56 +/- 0.80
Episode length: 30.12 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.565    |
| rollout/           |          |
|    return_std      | 2.7      |
| time/              |          |
|    total_timesteps | 104000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 4        |
|    learning_rate   | 0.0397   |
|    step_size       | 0.00122  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.565
SELFPLAY: new best model, bumping up generation to 4
Ep done - 5000.
Eval num_timesteps=130000, episode_reward=0.65 +/- 0.75
Episode length: 30.11 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.65     |
| rollout/           |          |
|    return_std      | 2.94     |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 6        |
|    learning_rate   | 0.0396   |
|    step_size       | 0.00112  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.65
SELFPLAY: new best model, bumping up generation to 5
Ep done - 6000.
Eval num_timesteps=156000, episode_reward=0.57 +/- 0.79
Episode length: 30.00 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.57     |
| rollout/           |          |
|    return_std      | 1.79     |
| time/              |          |
|    total_timesteps | 156000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 7        |
|    learning_rate   | 0.0395   |
|    step_size       | 0.00184  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.57
SELFPLAY: new best model, bumping up generation to 6
Ep done - 7000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.465    |
|    return_std      | 2.52     |
| time/              |          |
|    fps             | 658      |
|    time_elapsed    | 274      |
|    total_timesteps | 180437   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 9        |
|    learning_rate   | 0.0394   |
|    step_size       | 0.0013   |
---------------------------------
Eval num_timesteps=182000, episode_reward=0.68 +/- 0.72
Episode length: 30.05 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.68     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.68
SELFPLAY: new best model, bumping up generation to 7
Ep done - 8000.
Eval num_timesteps=208000, episode_reward=0.66 +/- 0.74
Episode length: 30.07 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.66     |
| rollout/           |          |
|    return_std      | 1.98     |
| time/              |          |
|    total_timesteps | 208000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 10       |
|    learning_rate   | 0.0393   |
|    step_size       | 0.00165  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.66
SELFPLAY: new best model, bumping up generation to 8
Ep done - 9000.
Eval num_timesteps=234000, episode_reward=0.63 +/- 0.76
Episode length: 30.01 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.63     |
| rollout/           |          |
|    return_std      | 3.15     |
| time/              |          |
|    total_timesteps | 234000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 11       |
|    learning_rate   | 0.0392   |
|    step_size       | 0.00104  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.63
SELFPLAY: new best model, bumping up generation to 9
Ep done - 10000.
Eval num_timesteps=260000, episode_reward=0.67 +/- 0.74
Episode length: 30.07 +/- 0.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.67     |
| rollout/           |          |
|    return_std      | 2.7      |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 13       |
|    learning_rate   | 0.0391   |
|    step_size       | 0.0012   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.67
SELFPLAY: new best model, bumping up generation to 10
Ep done - 11000.
Eval num_timesteps=286000, episode_reward=0.69 +/- 0.70
Episode length: 30.21 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.685    |
| rollout/           |          |
|    return_std      | 3.09     |
| time/              |          |
|    total_timesteps | 286000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 14       |
|    learning_rate   | 0.039    |
|    step_size       | 0.00105  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.685
SELFPLAY: new best model, bumping up generation to 11
Ep done - 12000.
Eval num_timesteps=312000, episode_reward=0.73 +/- 0.67
Episode length: 30.08 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.735    |
| rollout/           |          |
|    return_std      | 2.35     |
| time/              |          |
|    total_timesteps | 312000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 16       |
|    learning_rate   | 0.0388   |
|    step_size       | 0.00138  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.735
SELFPLAY: new best model, bumping up generation to 12
Ep done - 13000.
Eval num_timesteps=338000, episode_reward=0.62 +/- 0.77
Episode length: 30.13 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.625    |
| rollout/           |          |
|    return_std      | 2.07     |
| time/              |          |
|    total_timesteps | 338000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 17       |
|    learning_rate   | 0.0388   |
|    step_size       | 0.00156  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.625
SELFPLAY: new best model, bumping up generation to 13
Ep done - 14000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.597    |
|    return_std      | 2.24     |
| time/              |          |
|    fps             | 650      |
|    time_elapsed    | 554      |
|    total_timesteps | 360756   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 19       |
|    learning_rate   | 0.0386   |
|    step_size       | 0.00144  |
---------------------------------
Eval num_timesteps=364000, episode_reward=0.77 +/- 0.62
Episode length: 30.16 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.765    |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.765
SELFPLAY: new best model, bumping up generation to 14
Ep done - 15000.
Eval num_timesteps=390000, episode_reward=0.64 +/- 0.75
Episode length: 30.12 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.635    |
| rollout/           |          |
|    return_std      | 2.5      |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 20       |
|    learning_rate   | 0.0386   |
|    step_size       | 0.00128  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.635
SELFPLAY: new best model, bumping up generation to 15
Ep done - 16000.
Ep done - 17000.
Eval num_timesteps=416000, episode_reward=0.72 +/- 0.67
Episode length: 30.14 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.725    |
| rollout/           |          |
|    return_std      | 2.58     |
| time/              |          |
|    total_timesteps | 416000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 22       |
|    learning_rate   | 0.0384   |
|    step_size       | 0.00124  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.725
SELFPLAY: new best model, bumping up generation to 16
Ep done - 18000.
Eval num_timesteps=442000, episode_reward=0.78 +/- 0.61
Episode length: 30.08 +/- 0.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.775    |
| rollout/           |          |
|    return_std      | 1.94     |
| time/              |          |
|    total_timesteps | 442000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 23       |
|    learning_rate   | 0.0383   |
|    step_size       | 0.00165  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.775
SELFPLAY: new best model, bumping up generation to 17
Ep done - 19000.
Eval num_timesteps=468000, episode_reward=0.84 +/- 0.52
Episode length: 30.00 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.84     |
| rollout/           |          |
|    return_std      | 2        |
| time/              |          |
|    total_timesteps | 468000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 24       |
|    learning_rate   | 0.0383   |
|    step_size       | 0.0016   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.84
SELFPLAY: new best model, bumping up generation to 18
Ep done - 20000.
Eval num_timesteps=494000, episode_reward=0.80 +/- 0.59
Episode length: 30.12 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.8      |
| rollout/           |          |
|    return_std      | 2.38     |
| time/              |          |
|    total_timesteps | 494000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 26       |
|    learning_rate   | 0.0381   |
|    step_size       | 0.00134  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8
SELFPLAY: new best model, bumping up generation to 19
Ep done - 21000.
Eval num_timesteps=520000, episode_reward=0.74 +/- 0.66
Episode length: 30.18 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.745    |
| rollout/           |          |
|    return_std      | 1.83     |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 27       |
|    learning_rate   | 0.0381   |
|    step_size       | 0.00173  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.745
SELFPLAY: new best model, bumping up generation to 20
Ep done - 22000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.693    |
|    return_std      | 3.3      |
| time/              |          |
|    fps             | 648      |
|    time_elapsed    | 835      |
|    total_timesteps | 541237   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 29       |
|    learning_rate   | 0.0379   |
|    step_size       | 0.000958 |
---------------------------------
Eval num_timesteps=546000, episode_reward=0.81 +/- 0.57
Episode length: 30.11 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.815    |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.815
SELFPLAY: new best model, bumping up generation to 21
Ep done - 23000.
Eval num_timesteps=572000, episode_reward=0.85 +/- 0.52
Episode length: 30.19 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.85     |
| rollout/           |          |
|    return_std      | 1.86     |
| time/              |          |
|    total_timesteps | 572000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 30       |
|    learning_rate   | 0.0378   |
|    step_size       | 0.00169  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.85
SELFPLAY: new best model, bumping up generation to 22
Ep done - 24000.
Eval num_timesteps=598000, episode_reward=0.81 +/- 0.58
Episode length: 30.19 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.81     |
| rollout/           |          |
|    return_std      | 2.3      |
| time/              |          |
|    total_timesteps | 598000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 32       |
|    learning_rate   | 0.0377   |
|    step_size       | 0.00137  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.81
SELFPLAY: new best model, bumping up generation to 23
Ep done - 25000.
Eval num_timesteps=624000, episode_reward=0.79 +/- 0.61
Episode length: 30.05 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.785    |
| rollout/           |          |
|    return_std      | 3.24     |
| time/              |          |
|    total_timesteps | 624000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 33       |
|    learning_rate   | 0.0376   |
|    step_size       | 0.000968 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.785
SELFPLAY: new best model, bumping up generation to 24
Ep done - 26000.
Eval num_timesteps=650000, episode_reward=0.82 +/- 0.55
Episode length: 30.07 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.825    |
| rollout/           |          |
|    return_std      | 2.35     |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 35       |
|    learning_rate   | 0.0375   |
|    step_size       | 0.00133  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.825
SELFPLAY: new best model, bumping up generation to 25
Ep done - 27000.
Eval num_timesteps=676000, episode_reward=0.76 +/- 0.63
Episode length: 30.16 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.755    |
| rollout/           |          |
|    return_std      | 2.32     |
| time/              |          |
|    total_timesteps | 676000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 36       |
|    learning_rate   | 0.0374   |
|    step_size       | 0.00134  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.755
SELFPLAY: new best model, bumping up generation to 26
Ep done - 28000.
Eval num_timesteps=702000, episode_reward=0.72 +/- 0.67
Episode length: 30.16 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.72     |
| rollout/           |          |
|    return_std      | 2.48     |
| time/              |          |
|    total_timesteps | 702000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 37       |
|    learning_rate   | 0.0373   |
|    step_size       | 0.00126  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.72
SELFPLAY: new best model, bumping up generation to 27
Ep done - 29000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.665    |
|    return_std      | 1.8      |
| time/              |          |
|    fps             | 647      |
|    time_elapsed    | 1115     |
|    total_timesteps | 721766   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 39       |
|    learning_rate   | 0.0372   |
|    step_size       | 0.00172  |
---------------------------------
Eval num_timesteps=728000, episode_reward=0.89 +/- 0.44
Episode length: 30.17 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 728000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 28
Ep done - 30000.
Eval num_timesteps=754000, episode_reward=0.83 +/- 0.56
Episode length: 30.16 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.83     |
| rollout/           |          |
|    return_std      | 2.75     |
| time/              |          |
|    total_timesteps | 754000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 40       |
|    learning_rate   | 0.0371   |
|    step_size       | 0.00112  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.83
SELFPLAY: new best model, bumping up generation to 29
Ep done - 31000.
Eval num_timesteps=780000, episode_reward=0.80 +/- 0.57
Episode length: 30.20 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.8      |
| rollout/           |          |
|    return_std      | 2.37     |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 42       |
|    learning_rate   | 0.037    |
|    step_size       | 0.0013   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8
SELFPLAY: new best model, bumping up generation to 30
Ep done - 32000.
Ep done - 33000.
Eval num_timesteps=806000, episode_reward=0.83 +/- 0.56
Episode length: 30.16 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.83     |
| rollout/           |          |
|    return_std      | 2.3      |
| time/              |          |
|    total_timesteps | 806000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 43       |
|    learning_rate   | 0.0369   |
|    step_size       | 0.00134  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.83
SELFPLAY: new best model, bumping up generation to 31
Ep done - 34000.
Eval num_timesteps=832000, episode_reward=0.87 +/- 0.48
Episode length: 30.09 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.87     |
| rollout/           |          |
|    return_std      | 1.51     |
| time/              |          |
|    total_timesteps | 832000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 45       |
|    learning_rate   | 0.0368   |
|    step_size       | 0.00202  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.87
SELFPLAY: new best model, bumping up generation to 32
Ep done - 35000.
Eval num_timesteps=858000, episode_reward=0.69 +/- 0.71
Episode length: 30.11 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.69     |
| rollout/           |          |
|    return_std      | 1.88     |
| time/              |          |
|    total_timesteps | 858000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 46       |
|    learning_rate   | 0.0367   |
|    step_size       | 0.00163  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.69
SELFPLAY: new best model, bumping up generation to 33
Ep done - 36000.
Eval num_timesteps=884000, episode_reward=0.87 +/- 0.47
Episode length: 30.11 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.87     |
| rollout/           |          |
|    return_std      | 2.96     |
| time/              |          |
|    total_timesteps | 884000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 47       |
|    learning_rate   | 0.0366   |
|    step_size       | 0.00103  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.87
SELFPLAY: new best model, bumping up generation to 34
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.697    |
|    return_std      | 1.79     |
| time/              |          |
|    fps             | 646      |
|    time_elapsed    | 1396     |
|    total_timesteps | 902330   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 49       |
|    learning_rate   | 0.0365   |
|    step_size       | 0.0017   |
---------------------------------
Ep done - 37000.
Eval num_timesteps=910000, episode_reward=0.81 +/- 0.57
Episode length: 30.14 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.81     |
| time/              |          |
|    total_timesteps | 910000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.81
SELFPLAY: new best model, bumping up generation to 35
Ep done - 38000.
Eval num_timesteps=936000, episode_reward=0.77 +/- 0.62
Episode length: 30.16 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.765    |
| rollout/           |          |
|    return_std      | 2.12     |
| time/              |          |
|    total_timesteps | 936000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 50       |
|    learning_rate   | 0.0364   |
|    step_size       | 0.00143  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.765
SELFPLAY: new best model, bumping up generation to 36
Ep done - 39000.
Eval num_timesteps=962000, episode_reward=0.72 +/- 0.66
Episode length: 30.05 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.725    |
| rollout/           |          |
|    return_std      | 2.08     |
| time/              |          |
|    total_timesteps | 962000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 52       |
|    learning_rate   | 0.0362   |
|    step_size       | 0.00145  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.725
SELFPLAY: new best model, bumping up generation to 37
Ep done - 40000.
Eval num_timesteps=988000, episode_reward=0.71 +/- 0.69
Episode length: 30.04 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.715    |
| rollout/           |          |
|    return_std      | 2.48     |
| time/              |          |
|    total_timesteps | 988000   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 53       |
|    learning_rate   | 0.0362   |
|    step_size       | 0.00122  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.715
SELFPLAY: new best model, bumping up generation to 38
Ep done - 41000.
Eval num_timesteps=1014000, episode_reward=0.74 +/- 0.66
Episode length: 30.11 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.745    |
| rollout/           |          |
|    return_std      | 2.04     |
| time/              |          |
|    total_timesteps | 1014000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 55       |
|    learning_rate   | 0.036    |
|    step_size       | 0.00147  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.745
SELFPLAY: new best model, bumping up generation to 39
Ep done - 42000.
Eval num_timesteps=1040000, episode_reward=0.76 +/- 0.64
Episode length: 30.07 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.755    |
| rollout/           |          |
|    return_std      | 2.37     |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 56       |
|    learning_rate   | 0.036    |
|    step_size       | 0.00126  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.755
SELFPLAY: new best model, bumping up generation to 40
Ep done - 43000.
Eval num_timesteps=1066000, episode_reward=0.81 +/- 0.57
Episode length: 30.12 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.805    |
| rollout/           |          |
|    return_std      | 3.82     |
| time/              |          |
|    total_timesteps | 1066000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 58       |
|    learning_rate   | 0.0358   |
|    step_size       | 0.000781 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.805
SELFPLAY: new best model, bumping up generation to 41
Ep done - 44000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.672    |
|    return_std      | 2.54     |
| time/              |          |
|    fps             | 645      |
|    time_elapsed    | 1677     |
|    total_timesteps | 1082779  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 59       |
|    learning_rate   | 0.0357   |
|    step_size       | 0.00117  |
---------------------------------
Eval num_timesteps=1092000, episode_reward=0.78 +/- 0.61
Episode length: 30.09 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.78     |
| time/              |          |
|    total_timesteps | 1092000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.78
SELFPLAY: new best model, bumping up generation to 42
Ep done - 45000.
Eval num_timesteps=1118000, episode_reward=0.71 +/- 0.68
Episode length: 30.02 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.71     |
| rollout/           |          |
|    return_std      | 2.25     |
| time/              |          |
|    total_timesteps | 1118000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 60       |
|    learning_rate   | 0.0357   |
|    step_size       | 0.00132  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.71
SELFPLAY: new best model, bumping up generation to 43
Ep done - 46000.
Eval num_timesteps=1144000, episode_reward=0.74 +/- 0.65
Episode length: 30.05 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.745    |
| rollout/           |          |
|    return_std      | 2.84     |
| time/              |          |
|    total_timesteps | 1144000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 62       |
|    learning_rate   | 0.0355   |
|    step_size       | 0.00104  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.745
SELFPLAY: new best model, bumping up generation to 44
Ep done - 47000.
Eval num_timesteps=1170000, episode_reward=0.78 +/- 0.61
Episode length: 30.08 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.78     |
| rollout/           |          |
|    return_std      | 2.55     |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 63       |
|    learning_rate   | 0.0355   |
|    step_size       | 0.00116  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.78
SELFPLAY: new best model, bumping up generation to 45
Ep done - 48000.
Eval num_timesteps=1196000, episode_reward=0.66 +/- 0.74
Episode length: 30.11 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.655    |
| rollout/           |          |
|    return_std      | 3.2      |
| time/              |          |
|    total_timesteps | 1196000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 65       |
|    learning_rate   | 0.0353   |
|    step_size       | 0.000919 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.655
SELFPLAY: new best model, bumping up generation to 46
Ep done - 49000.
Ep done - 50000.
Eval num_timesteps=1222000, episode_reward=0.64 +/- 0.76
Episode length: 30.10 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.635    |
| rollout/           |          |
|    return_std      | 2.42     |
| time/              |          |
|    total_timesteps | 1222000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 66       |
|    learning_rate   | 0.0352   |
|    step_size       | 0.00121  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.635
SELFPLAY: new best model, bumping up generation to 47
Ep done - 51000.
Eval num_timesteps=1248000, episode_reward=0.74 +/- 0.66
Episode length: 30.08 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.745    |
| rollout/           |          |
|    return_std      | 2.76     |
| time/              |          |
|    total_timesteps | 1248000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 68       |
|    learning_rate   | 0.0351   |
|    step_size       | 0.00106  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.745
SELFPLAY: new best model, bumping up generation to 48
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.617    |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 644      |
|    time_elapsed    | 1958     |
|    total_timesteps | 1263174  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 69       |
|    learning_rate   | 0.035    |
|    step_size       | 0.00108  |
---------------------------------
Ep done - 52000.
Eval num_timesteps=1274000, episode_reward=0.74 +/- 0.65
Episode length: 30.09 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.745    |
| time/              |          |
|    total_timesteps | 1274000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.745
SELFPLAY: new best model, bumping up generation to 49
Ep done - 53000.
Eval num_timesteps=1300000, episode_reward=0.68 +/- 0.72
Episode length: 30.07 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.675    |
| rollout/           |          |
|    return_std      | 2.07     |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 71       |
|    learning_rate   | 0.0349   |
|    step_size       | 0.0014   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.675
SELFPLAY: new best model, bumping up generation to 50
Ep done - 54000.
Eval num_timesteps=1326000, episode_reward=0.57 +/- 0.80
Episode length: 30.05 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.57     |
| rollout/           |          |
|    return_std      | 2.86     |
| time/              |          |
|    total_timesteps | 1326000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 72       |
|    learning_rate   | 0.0348   |
|    step_size       | 0.00101  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.57
SELFPLAY: new best model, bumping up generation to 51
Ep done - 55000.
Eval num_timesteps=1352000, episode_reward=0.71 +/- 0.69
Episode length: 29.89 +/- 1.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.71     |
| rollout/           |          |
|    return_std      | 2.26     |
| time/              |          |
|    total_timesteps | 1352000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 73       |
|    learning_rate   | 0.0347   |
|    step_size       | 0.00128  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.71
SELFPLAY: new best model, bumping up generation to 52
Ep done - 56000.
Eval num_timesteps=1378000, episode_reward=0.70 +/- 0.69
Episode length: 30.04 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.705    |
| rollout/           |          |
|    return_std      | 1.77     |
| time/              |          |
|    total_timesteps | 1378000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 75       |
|    learning_rate   | 0.0346   |
|    step_size       | 0.00163  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.705
SELFPLAY: new best model, bumping up generation to 53
Ep done - 57000.
Eval num_timesteps=1404000, episode_reward=0.65 +/- 0.75
Episode length: 30.08 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.65     |
| rollout/           |          |
|    return_std      | 2.55     |
| time/              |          |
|    total_timesteps | 1404000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 76       |
|    learning_rate   | 0.0345   |
|    step_size       | 0.00113  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.65
SELFPLAY: new best model, bumping up generation to 54
Ep done - 58000.
Eval num_timesteps=1430000, episode_reward=0.72 +/- 0.67
Episode length: 30.04 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.725    |
| rollout/           |          |
|    return_std      | 2.59     |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 78       |
|    learning_rate   | 0.0344   |
|    step_size       | 0.00111  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.725
SELFPLAY: new best model, bumping up generation to 55
Ep done - 59000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.592    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 644      |
|    time_elapsed    | 2240     |
|    total_timesteps | 1443453  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 79       |
|    learning_rate   | 0.0343   |
|    step_size       | 0.00133  |
---------------------------------
Eval num_timesteps=1456000, episode_reward=0.61 +/- 0.77
Episode length: 30.08 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.615    |
| time/              |          |
|    total_timesteps | 1456000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.615
SELFPLAY: new best model, bumping up generation to 56
Ep done - 60000.
Eval num_timesteps=1482000, episode_reward=0.72 +/- 0.68
Episode length: 30.02 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.72     |
| rollout/           |          |
|    return_std      | 2.57     |
| time/              |          |
|    total_timesteps | 1482000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 81       |
|    learning_rate   | 0.0342   |
|    step_size       | 0.00111  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.72
SELFPLAY: new best model, bumping up generation to 57
Ep done - 61000.
Eval num_timesteps=1508000, episode_reward=0.41 +/- 0.89
Episode length: 29.95 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.405    |
| rollout/           |          |
|    return_std      | 3.46     |
| time/              |          |
|    total_timesteps | 1508000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 82       |
|    learning_rate   | 0.0341   |
|    step_size       | 0.000821 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.405
SELFPLAY: new best model, bumping up generation to 58
Ep done - 62000.
Eval num_timesteps=1534000, episode_reward=0.56 +/- 0.82
Episode length: 29.99 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.565    |
| rollout/           |          |
|    return_std      | 2.53     |
| time/              |          |
|    total_timesteps | 1534000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 84       |
|    learning_rate   | 0.0339   |
|    step_size       | 0.00112  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.565
SELFPLAY: new best model, bumping up generation to 59
Ep done - 63000.
Eval num_timesteps=1560000, episode_reward=0.59 +/- 0.79
Episode length: 30.05 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.59     |
| rollout/           |          |
|    return_std      | 3.38     |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 85       |
|    learning_rate   | 0.0339   |
|    step_size       | 0.000836 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.59
SELFPLAY: new best model, bumping up generation to 60
Ep done - 64000.
Eval num_timesteps=1586000, episode_reward=0.62 +/- 0.77
Episode length: 30.05 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.62     |
| rollout/           |          |
|    return_std      | 2.95     |
| time/              |          |
|    total_timesteps | 1586000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 86       |
|    learning_rate   | 0.0338   |
|    step_size       | 0.000956 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.62
SELFPLAY: new best model, bumping up generation to 61
Ep done - 65000.
Ep done - 66000.
Eval num_timesteps=1612000, episode_reward=0.53 +/- 0.84
Episode length: 30.03 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.53     |
| rollout/           |          |
|    return_std      | 2.54     |
| time/              |          |
|    total_timesteps | 1612000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 88       |
|    learning_rate   | 0.0336   |
|    step_size       | 0.00111  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.53
SELFPLAY: new best model, bumping up generation to 62
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.307    |
|    return_std      | 3.11     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 2521     |
|    total_timesteps | 1623748  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 89       |
|    learning_rate   | 0.0336   |
|    step_size       | 0.000901 |
---------------------------------
Ep done - 67000.
Eval num_timesteps=1638000, episode_reward=0.49 +/- 0.85
Episode length: 29.90 +/- 1.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.495    |
| time/              |          |
|    total_timesteps | 1638000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.495
SELFPLAY: new best model, bumping up generation to 63
Ep done - 68000.
Eval num_timesteps=1664000, episode_reward=0.49 +/- 0.87
Episode length: 30.07 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.49     |
| rollout/           |          |
|    return_std      | 2.27     |
| time/              |          |
|    total_timesteps | 1664000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 91       |
|    learning_rate   | 0.0334   |
|    step_size       | 0.00123  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.49
SELFPLAY: new best model, bumping up generation to 64
Ep done - 69000.
Eval num_timesteps=1690000, episode_reward=0.49 +/- 0.86
Episode length: 30.09 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.49     |
| rollout/           |          |
|    return_std      | 3.82     |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 92       |
|    learning_rate   | 0.0334   |
|    step_size       | 0.000728 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.49
SELFPLAY: new best model, bumping up generation to 65
Ep done - 70000.
Eval num_timesteps=1716000, episode_reward=0.43 +/- 0.89
Episode length: 30.07 +/- 0.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.435    |
| rollout/           |          |
|    return_std      | 2.51     |
| time/              |          |
|    total_timesteps | 1716000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 94       |
|    learning_rate   | 0.0332   |
|    step_size       | 0.0011   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.435
SELFPLAY: new best model, bumping up generation to 66
Ep done - 71000.
Eval num_timesteps=1742000, episode_reward=0.51 +/- 0.84
Episode length: 30.09 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.51     |
| rollout/           |          |
|    return_std      | 2.2      |
| time/              |          |
|    total_timesteps | 1742000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 95       |
|    learning_rate   | 0.0331   |
|    step_size       | 0.00126  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.51
SELFPLAY: new best model, bumping up generation to 67
Ep done - 72000.
Eval num_timesteps=1768000, episode_reward=0.40 +/- 0.89
Episode length: 29.93 +/- 1.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.4      |
| rollout/           |          |
|    return_std      | 3.22     |
| time/              |          |
|    total_timesteps | 1768000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 97       |
|    learning_rate   | 0.033    |
|    step_size       | 0.000854 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.4
SELFPLAY: new best model, bumping up generation to 68
Ep done - 73000.
Eval num_timesteps=1794000, episode_reward=0.61 +/- 0.77
Episode length: 30.10 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.61     |
| rollout/           |          |
|    return_std      | 2.18     |
| time/              |          |
|    total_timesteps | 1794000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 98       |
|    learning_rate   | 0.0329   |
|    step_size       | 0.00126  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.61
SELFPLAY: new best model, bumping up generation to 69
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.343    |
|    return_std      | 4.05     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 2802     |
|    total_timesteps | 1803981  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 99       |
|    learning_rate   | 0.0329   |
|    step_size       | 0.000675 |
---------------------------------
Ep done - 74000.
Eval num_timesteps=1820000, episode_reward=0.33 +/- 0.93
Episode length: 30.04 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.325    |
| time/              |          |
|    total_timesteps | 1820000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.325
SELFPLAY: new best model, bumping up generation to 70
Ep done - 75000.
Eval num_timesteps=1846000, episode_reward=0.48 +/- 0.87
Episode length: 30.09 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.485    |
| rollout/           |          |
|    return_std      | 2.65     |
| time/              |          |
|    total_timesteps | 1846000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 101      |
|    learning_rate   | 0.0327   |
|    step_size       | 0.00103  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.485
SELFPLAY: new best model, bumping up generation to 71
Ep done - 76000.
Eval num_timesteps=1872000, episode_reward=0.47 +/- 0.88
Episode length: 30.11 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.475    |
| rollout/           |          |
|    return_std      | 3.6      |
| time/              |          |
|    total_timesteps | 1872000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 102      |
|    learning_rate   | 0.0326   |
|    step_size       | 0.000756 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.475
SELFPLAY: new best model, bumping up generation to 72
Ep done - 77000.
Eval num_timesteps=1898000, episode_reward=0.51 +/- 0.84
Episode length: 30.10 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.51     |
| rollout/           |          |
|    return_std      | 3.24     |
| time/              |          |
|    total_timesteps | 1898000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 104      |
|    learning_rate   | 0.0325   |
|    step_size       | 0.000835 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.51
SELFPLAY: new best model, bumping up generation to 73
Ep done - 78000.
Eval num_timesteps=1924000, episode_reward=0.40 +/- 0.90
Episode length: 29.97 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.395    |
| rollout/           |          |
|    return_std      | 3.48     |
| time/              |          |
|    total_timesteps | 1924000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 105      |
|    learning_rate   | 0.0324   |
|    step_size       | 0.000777 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.395
SELFPLAY: new best model, bumping up generation to 74
Ep done - 79000.
Eval num_timesteps=1950000, episode_reward=0.37 +/- 0.92
Episode length: 30.00 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.37     |
| rollout/           |          |
|    return_std      | 2.75     |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 107      |
|    learning_rate   | 0.0323   |
|    step_size       | 0.000978 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.37
SELFPLAY: new best model, bumping up generation to 75
Ep done - 80000.
Eval num_timesteps=1976000, episode_reward=0.42 +/- 0.89
Episode length: 29.99 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.425    |
| rollout/           |          |
|    return_std      | 3.09     |
| time/              |          |
|    total_timesteps | 1976000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 108      |
|    learning_rate   | 0.0322   |
|    step_size       | 0.000869 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.425
SELFPLAY: new best model, bumping up generation to 76
Ep done - 81000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.243    |
|    return_std      | 3.35     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 3082     |
|    total_timesteps | 1984069  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 109      |
|    learning_rate   | 0.0321   |
|    step_size       | 0.000799 |
---------------------------------
Eval num_timesteps=2002000, episode_reward=0.30 +/- 0.93
Episode length: 30.04 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 2002000  |
---------------------------------
Ep done - 82000.
Ep done - 83000.
Eval num_timesteps=2028000, episode_reward=0.43 +/- 0.87
Episode length: 30.09 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.43     |
| rollout/           |          |
|    return_std      | 3.69     |
| time/              |          |
|    total_timesteps | 2028000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 111      |
|    learning_rate   | 0.032    |
|    step_size       | 0.000723 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.43
SELFPLAY: new best model, bumping up generation to 77
Ep done - 84000.
Eval num_timesteps=2054000, episode_reward=0.34 +/- 0.93
Episode length: 30.05 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.345    |
| rollout/           |          |
|    return_std      | 3.35     |
| time/              |          |
|    total_timesteps | 2054000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 112      |
|    learning_rate   | 0.0319   |
|    step_size       | 0.000794 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.345
SELFPLAY: new best model, bumping up generation to 78
Ep done - 85000.
Eval num_timesteps=2080000, episode_reward=0.36 +/- 0.91
Episode length: 30.19 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.365    |
| rollout/           |          |
|    return_std      | 3.69     |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 114      |
|    learning_rate   | 0.0318   |
|    step_size       | 0.000717 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.365
SELFPLAY: new best model, bumping up generation to 79
Ep done - 86000.
Eval num_timesteps=2106000, episode_reward=0.14 +/- 0.97
Episode length: 30.09 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.135    |
| rollout/           |          |
|    return_std      | 3.61     |
| time/              |          |
|    total_timesteps | 2106000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 115      |
|    learning_rate   | 0.0317   |
|    step_size       | 0.000732 |
---------------------------------
Ep done - 87000.
Eval num_timesteps=2132000, episode_reward=0.23 +/- 0.97
Episode length: 30.09 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.225    |
| rollout/           |          |
|    return_std      | 4.12     |
| time/              |          |
|    total_timesteps | 2132000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 117      |
|    learning_rate   | 0.0316   |
|    step_size       | 0.000639 |
---------------------------------
Ep done - 88000.
Eval num_timesteps=2158000, episode_reward=0.26 +/- 0.94
Episode length: 30.05 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.255    |
| rollout/           |          |
|    return_std      | 3.55     |
| time/              |          |
|    total_timesteps | 2158000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 118      |
|    learning_rate   | 0.0315   |
|    step_size       | 0.000739 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.177    |
|    return_std      | 3.41     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 3362     |
|    total_timesteps | 2164191  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 119      |
|    learning_rate   | 0.0314   |
|    step_size       | 0.000768 |
---------------------------------
Ep done - 89000.
Eval num_timesteps=2184000, episode_reward=0.27 +/- 0.94
Episode length: 30.07 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.265    |
| rollout/           |          |
|    return_std      | 2.98     |
| time/              |          |
|    total_timesteps | 2184000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 120      |
|    learning_rate   | 0.0313   |
|    step_size       | 0.000877 |
---------------------------------
Ep done - 90000.
Eval num_timesteps=2210000, episode_reward=0.47 +/- 0.88
Episode length: 30.08 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.465    |
| rollout/           |          |
|    return_std      | 3.78     |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 121      |
|    learning_rate   | 0.0313   |
|    step_size       | 0.000689 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.465
SELFPLAY: new best model, bumping up generation to 80
Ep done - 91000.
Eval num_timesteps=2236000, episode_reward=0.24 +/- 0.96
Episode length: 30.02 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.24     |
| rollout/           |          |
|    return_std      | 3.95     |
| time/              |          |
|    total_timesteps | 2236000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 122      |
|    learning_rate   | 0.0312   |
|    step_size       | 0.000658 |
---------------------------------
Ep done - 92000.
Eval num_timesteps=2262000, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.13     |
| rollout/           |          |
|    return_std      | 3.39     |
| time/              |          |
|    total_timesteps | 2262000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 124      |
|    learning_rate   | 0.0311   |
|    step_size       | 0.000764 |
---------------------------------
Ep done - 93000.
Eval num_timesteps=2288000, episode_reward=0.13 +/- 0.98
Episode length: 30.04 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.13     |
| rollout/           |          |
|    return_std      | 3.2      |
| time/              |          |
|    total_timesteps | 2288000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 125      |
|    learning_rate   | 0.031    |
|    step_size       | 0.000806 |
---------------------------------
Ep done - 94000.
Eval num_timesteps=2314000, episode_reward=0.30 +/- 0.94
Episode length: 30.05 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.305    |
| rollout/           |          |
|    return_std      | 3.44     |
| time/              |          |
|    total_timesteps | 2314000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 127      |
|    learning_rate   | 0.0308   |
|    step_size       | 0.000748 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.305
SELFPLAY: new best model, bumping up generation to 81
Ep done - 95000.
Eval num_timesteps=2340000, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.12     |
| rollout/           |          |
|    return_std      | 3.71     |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 128      |
|    learning_rate   | 0.0308   |
|    step_size       | 0.000691 |
---------------------------------
Ep done - 96000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.0433   |
|    return_std      | 4.11     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 3643     |
|    total_timesteps | 2344248  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 129      |
|    learning_rate   | 0.0307   |
|    step_size       | 0.000623 |
---------------------------------
Eval num_timesteps=2366000, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.11     |
| rollout/           |          |
|    return_std      | 3.78     |
| time/              |          |
|    total_timesteps | 2366000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 130      |
|    learning_rate   | 0.0306   |
|    step_size       | 0.000675 |
---------------------------------
Ep done - 97000.
Eval num_timesteps=2392000, episode_reward=0.26 +/- 0.94
Episode length: 30.02 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.255    |
| rollout/           |          |
|    return_std      | 2.59     |
| time/              |          |
|    total_timesteps | 2392000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 131      |
|    learning_rate   | 0.0306   |
|    step_size       | 0.000984 |
---------------------------------
Ep done - 98000.
Ep done - 99000.
Eval num_timesteps=2418000, episode_reward=0.23 +/- 0.95
Episode length: 30.02 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.23     |
| rollout/           |          |
|    return_std      | 2.75     |
| time/              |          |
|    total_timesteps | 2418000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 133      |
|    learning_rate   | 0.0304   |
|    step_size       | 0.000921 |
---------------------------------
Ep done - 100000.
Eval num_timesteps=2444000, episode_reward=0.21 +/- 0.95
Episode length: 29.97 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.215    |
| rollout/           |          |
|    return_std      | 3.07     |
| time/              |          |
|    total_timesteps | 2444000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 134      |
|    learning_rate   | 0.0303   |
|    step_size       | 0.000822 |
---------------------------------
Ep done - 101000.
Eval num_timesteps=2470000, episode_reward=0.40 +/- 0.90
Episode length: 30.00 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.395    |
| rollout/           |          |
|    return_std      | 3.36     |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 135      |
|    learning_rate   | 0.0303   |
|    step_size       | 0.000751 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.395
SELFPLAY: new best model, bumping up generation to 82
Ep done - 102000.
Eval num_timesteps=2496000, episode_reward=-0.01 +/- 0.99
Episode length: 30.02 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.005   |
| rollout/           |          |
|    return_std      | 3.65     |
| time/              |          |
|    total_timesteps | 2496000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 137      |
|    learning_rate   | 0.0301   |
|    step_size       | 0.000688 |
---------------------------------
Ep done - 103000.
Eval num_timesteps=2522000, episode_reward=0.12 +/- 0.98
Episode length: 29.95 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.115    |
| rollout/           |          |
|    return_std      | 3.65     |
| time/              |          |
|    total_timesteps | 2522000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 138      |
|    learning_rate   | 0.03     |
|    step_size       | 0.000685 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.045    |
|    return_std      | 3.38     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 3925     |
|    total_timesteps | 2524126  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 139      |
|    learning_rate   | 0.03     |
|    step_size       | 0.000739 |
---------------------------------
Ep done - 104000.
Eval num_timesteps=2548000, episode_reward=0.18 +/- 0.97
Episode length: 30.00 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.18     |
| rollout/           |          |
|    return_std      | 3.08     |
| time/              |          |
|    total_timesteps | 2548000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 140      |
|    learning_rate   | 0.0299   |
|    step_size       | 0.00081  |
---------------------------------
Ep done - 105000.
Eval num_timesteps=2574000, episode_reward=0.20 +/- 0.96
Episode length: 30.00 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.195    |
| rollout/           |          |
|    return_std      | 2.82     |
| time/              |          |
|    total_timesteps | 2574000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 141      |
|    learning_rate   | 0.0298   |
|    step_size       | 0.000882 |
---------------------------------
Ep done - 106000.
Eval num_timesteps=2600000, episode_reward=0.18 +/- 0.97
Episode length: 30.01 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.18     |
| rollout/           |          |
|    return_std      | 2.97     |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 143      |
|    learning_rate   | 0.0297   |
|    step_size       | 0.000834 |
---------------------------------
Ep done - 107000.
Eval num_timesteps=2626000, episode_reward=0.20 +/- 0.97
Episode length: 30.00 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.205    |
| rollout/           |          |
|    return_std      | 4.2      |
| time/              |          |
|    total_timesteps | 2626000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 144      |
|    learning_rate   | 0.0296   |
|    step_size       | 0.000587 |
---------------------------------
Ep done - 108000.
Eval num_timesteps=2652000, episode_reward=0.25 +/- 0.95
Episode length: 30.04 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.25     |
| rollout/           |          |
|    return_std      | 2.77     |
| time/              |          |
|    total_timesteps | 2652000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 146      |
|    learning_rate   | 0.0295   |
|    step_size       | 0.000887 |
---------------------------------
Ep done - 109000.
Eval num_timesteps=2678000, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.12     |
| rollout/           |          |
|    return_std      | 3.32     |
| time/              |          |
|    total_timesteps | 2678000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 147      |
|    learning_rate   | 0.0294   |
|    step_size       | 0.000737 |
---------------------------------
Ep done - 110000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.0117   |
|    return_std      | 3.13     |
| time/              |          |
|    fps             | 644      |
|    time_elapsed    | 4198     |
|    total_timesteps | 2703936  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 149      |
|    learning_rate   | 0.0293   |
|    step_size       | 0.000778 |
---------------------------------
Eval num_timesteps=2704000, episode_reward=0.27 +/- 0.95
Episode length: 29.98 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.27     |
| time/              |          |
|    total_timesteps | 2704000  |
---------------------------------
Ep done - 111000.
Eval num_timesteps=2730000, episode_reward=0.32 +/- 0.93
Episode length: 30.00 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.32     |
| rollout/           |          |
|    return_std      | 3.28     |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 150      |
|    learning_rate   | 0.0292   |
|    step_size       | 0.000741 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.32
SELFPLAY: new best model, bumping up generation to 83
Ep done - 112000.
Eval num_timesteps=2756000, episode_reward=-0.01 +/- 0.99
Episode length: 30.04 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.01    |
| rollout/           |          |
|    return_std      | 3.71     |
| time/              |          |
|    total_timesteps | 2756000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 151      |
|    learning_rate   | 0.0291   |
|    step_size       | 0.000655 |
---------------------------------
Ep done - 113000.
Ep done - 114000.
Eval num_timesteps=2782000, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.055    |
| rollout/           |          |
|    return_std      | 3.66     |
| time/              |          |
|    total_timesteps | 2782000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 153      |
|    learning_rate   | 0.029    |
|    step_size       | 0.000659 |
---------------------------------
Ep done - 115000.
Eval num_timesteps=2808000, episode_reward=0.33 +/- 0.92
Episode length: 30.04 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.325    |
| rollout/           |          |
|    return_std      | 3.08     |
| time/              |          |
|    total_timesteps | 2808000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 154      |
|    learning_rate   | 0.0289   |
|    step_size       | 0.000783 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.325
SELFPLAY: new best model, bumping up generation to 84
Ep done - 116000.
Eval num_timesteps=2834000, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.05     |
| rollout/           |          |
|    return_std      | 2.7      |
| time/              |          |
|    total_timesteps | 2834000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 156      |
|    learning_rate   | 0.0288   |
|    step_size       | 0.000887 |
---------------------------------
Ep done - 117000.
Eval num_timesteps=2860000, episode_reward=-0.04 +/- 0.99
Episode length: 29.98 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.045   |
| rollout/           |          |
|    return_std      | 3.73     |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 157      |
|    learning_rate   | 0.0287   |
|    step_size       | 0.000641 |
---------------------------------
Ep done - 118000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.175   |
|    return_std      | 3.51     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 4478     |
|    total_timesteps | 2883861  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 159      |
|    learning_rate   | 0.0285   |
|    step_size       | 0.000678 |
---------------------------------
Eval num_timesteps=2886000, episode_reward=0.10 +/- 0.99
Episode length: 30.00 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.105    |
| time/              |          |
|    total_timesteps | 2886000  |
---------------------------------
Ep done - 119000.
Eval num_timesteps=2912000, episode_reward=0.15 +/- 0.96
Episode length: 29.98 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.15     |
| rollout/           |          |
|    return_std      | 4.28     |
| time/              |          |
|    total_timesteps | 2912000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 160      |
|    learning_rate   | 0.0285   |
|    step_size       | 0.000554 |
---------------------------------
Ep done - 120000.
Eval num_timesteps=2938000, episode_reward=0.30 +/- 0.94
Episode length: 30.02 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.305    |
| rollout/           |          |
|    return_std      | 3.07     |
| time/              |          |
|    total_timesteps | 2938000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 162      |
|    learning_rate   | 0.0283   |
|    step_size       | 0.000769 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.305
SELFPLAY: new best model, bumping up generation to 85
Ep done - 121000.
Eval num_timesteps=2964000, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.04     |
| rollout/           |          |
|    return_std      | 3.61     |
| time/              |          |
|    total_timesteps | 2964000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 163      |
|    learning_rate   | 0.0282   |
|    step_size       | 0.000652 |
---------------------------------
Ep done - 122000.
Eval num_timesteps=2990000, episode_reward=0.24 +/- 0.96
Episode length: 30.13 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.24     |
| rollout/           |          |
|    return_std      | 2.84     |
| time/              |          |
|    total_timesteps | 2990000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 164      |
|    learning_rate   | 0.0282   |
|    step_size       | 0.000826 |
---------------------------------
Ep done - 123000.
Eval num_timesteps=3016000, episode_reward=0.20 +/- 0.96
Episode length: 30.07 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.205    |
| rollout/           |          |
|    return_std      | 3.18     |
| time/              |          |
|    total_timesteps | 3016000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 166      |
|    learning_rate   | 0.028    |
|    step_size       | 0.000736 |
---------------------------------
Ep done - 124000.
Eval num_timesteps=3042000, episode_reward=0.17 +/- 0.97
Episode length: 30.09 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.175    |
| rollout/           |          |
|    return_std      | 3.47     |
| time/              |          |
|    total_timesteps | 3042000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 167      |
|    learning_rate   | 0.028    |
|    step_size       | 0.000671 |
---------------------------------
Ep done - 125000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.125   |
|    return_std      | 3.97     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 4759     |
|    total_timesteps | 3063749  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 169      |
|    learning_rate   | 0.0278   |
|    step_size       | 0.000584 |
---------------------------------
Eval num_timesteps=3068000, episode_reward=0.15 +/- 0.97
Episode length: 30.07 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.155    |
| time/              |          |
|    total_timesteps | 3068000  |
---------------------------------
Ep done - 126000.
Eval num_timesteps=3094000, episode_reward=0.19 +/- 0.97
Episode length: 30.05 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.19     |
| rollout/           |          |
|    return_std      | 3.75     |
| time/              |          |
|    total_timesteps | 3094000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 170      |
|    learning_rate   | 0.0277   |
|    step_size       | 0.000617 |
---------------------------------
Ep done - 127000.
Eval num_timesteps=3120000, episode_reward=0.26 +/- 0.96
Episode length: 30.05 +/- 0.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.26     |
| rollout/           |          |
|    return_std      | 2.75     |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 172      |
|    learning_rate   | 0.0276   |
|    step_size       | 0.000837 |
---------------------------------
Ep done - 128000.
Eval num_timesteps=3146000, episode_reward=0.19 +/- 0.96
Episode length: 30.04 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.19     |
| rollout/           |          |
|    return_std      | 2.9      |
| time/              |          |
|    total_timesteps | 3146000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 173      |
|    learning_rate   | 0.0275   |
|    step_size       | 0.00079  |
---------------------------------
Ep done - 129000.
Ep done - 130000.
Eval num_timesteps=3172000, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.05     |
| rollout/           |          |
|    return_std      | 3.53     |
| time/              |          |
|    total_timesteps | 3172000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 175      |
|    learning_rate   | 0.0274   |
|    step_size       | 0.000647 |
---------------------------------
Ep done - 131000.
Eval num_timesteps=3198000, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.09     |
| rollout/           |          |
|    return_std      | 3.57     |
| time/              |          |
|    total_timesteps | 3198000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 176      |
|    learning_rate   | 0.0273   |
|    step_size       | 0.000638 |
---------------------------------
Ep done - 132000.
Eval num_timesteps=3224000, episode_reward=0.09 +/- 0.98
Episode length: 30.05 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.09     |
| rollout/           |          |
|    return_std      | 2.98     |
| time/              |          |
|    total_timesteps | 3224000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 177      |
|    learning_rate   | 0.0272   |
|    step_size       | 0.000762 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
|    return_std      | 3.08     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 5039     |
|    total_timesteps | 3243705  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 179      |
|    learning_rate   | 0.0271   |
|    step_size       | 0.000733 |
---------------------------------
Ep done - 133000.
Eval num_timesteps=3250000, episode_reward=0.23 +/- 0.95
Episode length: 30.00 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.225    |
| time/              |          |
|    total_timesteps | 3250000  |
---------------------------------
Ep done - 134000.
Eval num_timesteps=3276000, episode_reward=0.12 +/- 0.98
Episode length: 30.08 +/- 0.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.125    |
| rollout/           |          |
|    return_std      | 3.04     |
| time/              |          |
|    total_timesteps | 3276000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 180      |
|    learning_rate   | 0.027    |
|    step_size       | 0.000742 |
---------------------------------
Ep done - 135000.
Eval num_timesteps=3302000, episode_reward=0.06 +/- 0.97
Episode length: 30.10 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.06     |
| rollout/           |          |
|    return_std      | 4.12     |
| time/              |          |
|    total_timesteps | 3302000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 182      |
|    learning_rate   | 0.0269   |
|    step_size       | 0.000544 |
---------------------------------
Ep done - 136000.
Eval num_timesteps=3328000, episode_reward=0.25 +/- 0.95
Episode length: 30.08 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.25     |
| rollout/           |          |
|    return_std      | 2.26     |
| time/              |          |
|    total_timesteps | 3328000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 183      |
|    learning_rate   | 0.0268   |
|    step_size       | 0.000987 |
---------------------------------
Ep done - 137000.
Eval num_timesteps=3354000, episode_reward=0.23 +/- 0.97
Episode length: 30.07 +/- 0.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.225    |
| rollout/           |          |
|    return_std      | 4.05     |
| time/              |          |
|    total_timesteps | 3354000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 185      |
|    learning_rate   | 0.0267   |
|    step_size       | 0.000548 |
---------------------------------
Ep done - 138000.
Eval num_timesteps=3380000, episode_reward=0.28 +/- 0.95
Episode length: 30.09 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.285    |
| rollout/           |          |
|    return_std      | 3.59     |
| time/              |          |
|    total_timesteps | 3380000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 186      |
|    learning_rate   | 0.0266   |
|    step_size       | 0.000618 |
---------------------------------
Ep done - 139000.
Eval num_timesteps=3406000, episode_reward=0.15 +/- 0.98
Episode length: 30.00 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.155    |
| rollout/           |          |
|    return_std      | 4.14     |
| time/              |          |
|    total_timesteps | 3406000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 188      |
|    learning_rate   | 0.0264   |
|    step_size       | 0.000532 |
---------------------------------
Ep done - 140000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.015   |
|    return_std      | 4.44     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 5320     |
|    total_timesteps | 3423827  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 189      |
|    learning_rate   | 0.0264   |
|    step_size       | 0.000495 |
---------------------------------
Eval num_timesteps=3432000, episode_reward=0.49 +/- 0.85
Episode length: 30.09 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.495    |
| time/              |          |
|    total_timesteps | 3432000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.495
SELFPLAY: new best model, bumping up generation to 86
Ep done - 141000.
Eval num_timesteps=3458000, episode_reward=0.07 +/- 1.00
Episode length: 30.23 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.065    |
| rollout/           |          |
|    return_std      | 3.5      |
| time/              |          |
|    total_timesteps | 3458000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 190      |
|    learning_rate   | 0.0263   |
|    step_size       | 0.000626 |
---------------------------------
Ep done - 142000.
Eval num_timesteps=3484000, episode_reward=0.15 +/- 0.98
Episode length: 29.98 +/- 0.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.155    |
| rollout/           |          |
|    return_std      | 3.22     |
| time/              |          |
|    total_timesteps | 3484000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 192      |
|    learning_rate   | 0.0262   |
|    step_size       | 0.000676 |
---------------------------------
Ep done - 143000.
Eval num_timesteps=3510000, episode_reward=0.33 +/- 0.92
Episode length: 30.00 +/- 0.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.325    |
| rollout/           |          |
|    return_std      | 4.97     |
| time/              |          |
|    total_timesteps | 3510000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 193      |
|    learning_rate   | 0.0261   |
|    step_size       | 0.000438 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.325
SELFPLAY: new best model, bumping up generation to 87
Ep done - 144000.
Eval num_timesteps=3536000, episode_reward=0.27 +/- 0.94
Episode length: 30.13 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.27     |
| rollout/           |          |
|    return_std      | 4.44     |
| time/              |          |
|    total_timesteps | 3536000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 195      |
|    learning_rate   | 0.0259   |
|    step_size       | 0.000487 |
---------------------------------
Ep done - 145000.
Ep done - 146000.
Eval num_timesteps=3562000, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 0.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.12     |
| rollout/           |          |
|    return_std      | 3.12     |
| time/              |          |
|    total_timesteps | 3562000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 196      |
|    learning_rate   | 0.0259   |
|    step_size       | 0.00069  |
---------------------------------
Ep done - 147000.
Eval num_timesteps=3588000, episode_reward=-0.11 +/- 0.98
Episode length: 30.02 +/- 0.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.11    |
| rollout/           |          |
|    return_std      | 4.1      |
| time/              |          |
|    total_timesteps | 3588000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 198      |
|    learning_rate   | 0.0257   |
|    step_size       | 0.000523 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
|    return_std      | 3.49     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 5600     |
|    total_timesteps | 3603859  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 199      |
|    learning_rate   | 0.0257   |
|    step_size       | 0.000612 |
---------------------------------
Ep done - 148000.
Eval num_timesteps=3614000, episode_reward=0.14 +/- 0.99
Episode length: 30.21 +/- 0.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.14     |
| time/              |          |
|    total_timesteps | 3614000  |
---------------------------------
Ep done - 149000.
Eval num_timesteps=3640000, episode_reward=0.54 +/- 0.84
Episode length: 30.02 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.535    |
| rollout/           |          |
|    return_std      | 3.91     |
| time/              |          |
|    total_timesteps | 3640000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 201      |
|    learning_rate   | 0.0255   |
|    step_size       | 0.000544 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.535
SELFPLAY: new best model, bumping up generation to 88
Ep done - 150000.
Eval num_timesteps=3666000, episode_reward=-0.01 +/- 1.00
Episode length: 29.95 +/- 0.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.005   |
| rollout/           |          |
|    return_std      | 3.83     |
| time/              |          |
|    total_timesteps | 3666000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 202      |
|    learning_rate   | 0.0254   |
|    step_size       | 0.000554 |
---------------------------------
Ep done - 151000.
Eval num_timesteps=3692000, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.135    |
| rollout/           |          |
|    return_std      | 3.71     |
| time/              |          |
|    total_timesteps | 3692000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 203      |
|    learning_rate   | 0.0254   |
|    step_size       | 0.000569 |
---------------------------------
Ep done - 152000.
Eval num_timesteps=3718000, episode_reward=0.29 +/- 0.95
Episode length: 30.08 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.29     |
| rollout/           |          |
|    return_std      | 4.01     |
| time/              |          |
|    total_timesteps | 3718000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 205      |
|    learning_rate   | 0.0252   |
|    step_size       | 0.000525 |
---------------------------------
Ep done - 153000.
Eval num_timesteps=3744000, episode_reward=0.11 +/- 0.99
Episode length: 30.11 +/- 0.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.11     |
| rollout/           |          |
|    return_std      | 3.58     |
| time/              |          |
|    total_timesteps | 3744000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 206      |
|    learning_rate   | 0.0252   |
|    step_size       | 0.000586 |
---------------------------------
Ep done - 154000.
Eval num_timesteps=3770000, episode_reward=-0.03 +/- 0.95
Episode length: 30.07 +/- 0.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | -0.03    |
| rollout/           |          |
|    return_std      | 4.4      |
| time/              |          |
|    total_timesteps | 3770000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 208      |
|    learning_rate   | 0.025    |
|    step_size       | 0.000473 |
---------------------------------
Ep done - 155000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.0383  |
|    return_std      | 3.11     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 5882     |
|    total_timesteps | 3783911  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 209      |
|    learning_rate   | 0.0249   |
|    step_size       | 0.000669 |
---------------------------------
Eval num_timesteps=3796000, episode_reward=0.17 +/- 0.97
Episode length: 29.94 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.165    |
| time/              |          |
|    total_timesteps | 3796000  |
---------------------------------
Ep done - 156000.
Eval num_timesteps=3822000, episode_reward=0.07 +/- 0.98
Episode length: 30.11 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.07     |
| rollout/           |          |
|    return_std      | 3.26     |
| time/              |          |
|    total_timesteps | 3822000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 211      |
|    learning_rate   | 0.0248   |
|    step_size       | 0.000634 |
---------------------------------
Ep done - 157000.
Eval num_timesteps=3848000, episode_reward=0.15 +/- 0.99
Episode length: 29.87 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.15     |
| rollout/           |          |
|    return_std      | 3.51     |
| time/              |          |
|    total_timesteps | 3848000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 212      |
|    learning_rate   | 0.0247   |
|    step_size       | 0.000587 |
---------------------------------
Ep done - 158000.
Eval num_timesteps=3874000, episode_reward=0.64 +/- 0.76
Episode length: 30.17 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.64     |
| rollout/           |          |
|    return_std      | 4.61     |
| time/              |          |
|    total_timesteps | 3874000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 214      |
|    learning_rate   | 0.0246   |
|    step_size       | 0.000444 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.64
SELFPLAY: new best model, bumping up generation to 89
Ep done - 159000.
Eval num_timesteps=3900000, episode_reward=-0.34 +/- 0.94
Episode length: 29.91 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.335   |
| rollout/           |          |
|    return_std      | 3.37     |
| time/              |          |
|    total_timesteps | 3900000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 215      |
|    learning_rate   | 0.0245   |
|    step_size       | 0.000605 |
---------------------------------
Ep done - 160000.
Eval num_timesteps=3926000, episode_reward=-0.47 +/- 0.88
Episode length: 29.94 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.465   |
| rollout/           |          |
|    return_std      | 4.56     |
| time/              |          |
|    total_timesteps | 3926000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 216      |
|    learning_rate   | 0.0244   |
|    step_size       | 0.000447 |
---------------------------------
Ep done - 161000.
Ep done - 162000.
Eval num_timesteps=3952000, episode_reward=0.17 +/- 0.96
Episode length: 30.18 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.17     |
| rollout/           |          |
|    return_std      | 4.08     |
| time/              |          |
|    total_timesteps | 3952000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 218      |
|    learning_rate   | 0.0243   |
|    step_size       | 0.000497 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.148   |
|    return_std      | 3.7      |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 6163     |
|    total_timesteps | 3963875  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 219      |
|    learning_rate   | 0.0242   |
|    step_size       | 0.000546 |
---------------------------------
Ep done - 163000.
Eval num_timesteps=3978000, episode_reward=-0.26 +/- 0.94
Episode length: 30.06 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | -0.26    |
| time/              |          |
|    total_timesteps | 3978000  |
---------------------------------
Ep done - 164000.
Eval num_timesteps=4004000, episode_reward=0.18 +/- 0.97
Episode length: 30.03 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.185    |
| rollout/           |          |
|    return_std      | 4.39     |
| time/              |          |
|    total_timesteps | 4004000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 221      |
|    learning_rate   | 0.0241   |
|    step_size       | 0.000457 |
---------------------------------
Ep done - 165000.
Eval num_timesteps=4030000, episode_reward=0.09 +/- 0.98
Episode length: 29.96 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.09     |
| rollout/           |          |
|    return_std      | 5.38     |
| time/              |          |
|    total_timesteps | 4030000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 222      |
|    learning_rate   | 0.024    |
|    step_size       | 0.000371 |
---------------------------------
Ep done - 166000.
Eval num_timesteps=4056000, episode_reward=0.30 +/- 0.94
Episode length: 30.04 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.3      |
| rollout/           |          |
|    return_std      | 4.66     |
| time/              |          |
|    total_timesteps | 4056000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 224      |
|    learning_rate   | 0.0239   |
|    step_size       | 0.000427 |
---------------------------------
Ep done - 167000.
Eval num_timesteps=4082000, episode_reward=-0.25 +/- 0.93
Episode length: 29.88 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.25    |
| rollout/           |          |
|    return_std      | 4.03     |
| time/              |          |
|    total_timesteps | 4082000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 225      |
|    learning_rate   | 0.0238   |
|    step_size       | 0.000492 |
---------------------------------
Ep done - 168000.
Eval num_timesteps=4108000, episode_reward=-0.12 +/- 0.98
Episode length: 30.02 +/- 0.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.115   |
| rollout/           |          |
|    return_std      | 3.51     |
| time/              |          |
|    total_timesteps | 4108000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 226      |
|    learning_rate   | 0.0237   |
|    step_size       | 0.000563 |
---------------------------------
Ep done - 169000.
Eval num_timesteps=4134000, episode_reward=0.34 +/- 0.94
Episode length: 30.08 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.335    |
| rollout/           |          |
|    return_std      | 4.16     |
| time/              |          |
|    total_timesteps | 4134000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 228      |
|    learning_rate   | 0.0236   |
|    step_size       | 0.000472 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.335
SELFPLAY: new best model, bumping up generation to 90
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.0583  |
|    return_std      | 5.52     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 6444     |
|    total_timesteps | 4144046  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 229      |
|    learning_rate   | 0.0235   |
|    step_size       | 0.000355 |
---------------------------------
Ep done - 170000.
Eval num_timesteps=4160000, episode_reward=-0.23 +/- 0.96
Episode length: 29.77 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | -0.23    |
| time/              |          |
|    total_timesteps | 4160000  |
---------------------------------
Ep done - 171000.
Eval num_timesteps=4186000, episode_reward=-0.20 +/- 0.97
Episode length: 29.84 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | -0.195   |
| rollout/           |          |
|    return_std      | 4.31     |
| time/              |          |
|    total_timesteps | 4186000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 231      |
|    learning_rate   | 0.0234   |
|    step_size       | 0.000451 |
---------------------------------
Ep done - 172000.
Eval num_timesteps=4212000, episode_reward=-0.20 +/- 0.97
Episode length: 29.96 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.2     |
| rollout/           |          |
|    return_std      | 4.62     |
| time/              |          |
|    total_timesteps | 4212000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 232      |
|    learning_rate   | 0.0233   |
|    step_size       | 0.00042  |
---------------------------------
Ep done - 173000.
Eval num_timesteps=4238000, episode_reward=-0.27 +/- 0.96
Episode length: 30.00 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.265   |
| rollout/           |          |
|    return_std      | 4.1      |
| time/              |          |
|    total_timesteps | 4238000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 234      |
|    learning_rate   | 0.0231   |
|    step_size       | 0.00047  |
---------------------------------
Ep done - 174000.
Eval num_timesteps=4264000, episode_reward=0.40 +/- 0.84
Episode length: 30.02 +/- 0.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.395    |
| rollout/           |          |
|    return_std      | 4.04     |
| time/              |          |
|    total_timesteps | 4264000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 235      |
|    learning_rate   | 0.0231   |
|    step_size       | 0.000475 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.395
SELFPLAY: new best model, bumping up generation to 91
Ep done - 175000.
Eval num_timesteps=4290000, episode_reward=0.48 +/- 0.88
Episode length: 29.98 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.48     |
| rollout/           |          |
|    return_std      | 4.66     |
| time/              |          |
|    total_timesteps | 4290000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 237      |
|    learning_rate   | 0.0229   |
|    step_size       | 0.00041  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.48
SELFPLAY: new best model, bumping up generation to 92
Ep done - 176000.
Eval num_timesteps=4316000, episode_reward=0.38 +/- 0.92
Episode length: 30.02 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.375    |
| rollout/           |          |
|    return_std      | 4.18     |
| time/              |          |
|    total_timesteps | 4316000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 238      |
|    learning_rate   | 0.0228   |
|    step_size       | 0.000455 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.375
SELFPLAY: new best model, bumping up generation to 93
Ep done - 177000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.172   |
|    return_std      | 5.61     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 6725     |
|    total_timesteps | 4323868  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 239      |
|    learning_rate   | 0.0228   |
|    step_size       | 0.000338 |
---------------------------------
Ep done - 178000.
Eval num_timesteps=4342000, episode_reward=0.01 +/- 0.81
Episode length: 29.99 +/- 0.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.015    |
| rollout/           |          |
|    return_std      | 5.6      |
| time/              |          |
|    total_timesteps | 4342000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 240      |
|    learning_rate   | 0.0227   |
|    step_size       | 0.000338 |
---------------------------------
Ep done - 179000.
Eval num_timesteps=4368000, episode_reward=0.16 +/- 0.98
Episode length: 30.07 +/- 0.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.16     |
| rollout/           |          |
|    return_std      | 3.84     |
| time/              |          |
|    total_timesteps | 4368000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 241      |
|    learning_rate   | 0.0226   |
|    step_size       | 0.000491 |
---------------------------------
Ep done - 180000.
Eval num_timesteps=4394000, episode_reward=0.24 +/- 0.93
Episode length: 29.97 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.24     |
| rollout/           |          |
|    return_std      | 3.59     |
| time/              |          |
|    total_timesteps | 4394000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 242      |
|    learning_rate   | 0.0226   |
|    step_size       | 0.000524 |
---------------------------------
Ep done - 181000.
Eval num_timesteps=4420000, episode_reward=0.41 +/- 0.87
Episode length: 29.88 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.405    |
| rollout/           |          |
|    return_std      | 4.42     |
| time/              |          |
|    total_timesteps | 4420000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 244      |
|    learning_rate   | 0.0224   |
|    step_size       | 0.000423 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.405
SELFPLAY: new best model, bumping up generation to 94
Ep done - 182000.
Eval num_timesteps=4446000, episode_reward=0.13 +/- 0.98
Episode length: 30.01 +/- 0.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.13     |
| rollout/           |          |
|    return_std      | 5.19     |
| time/              |          |
|    total_timesteps | 4446000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 245      |
|    learning_rate   | 0.0223   |
|    step_size       | 0.000358 |
---------------------------------
Ep done - 183000.
Eval num_timesteps=4472000, episode_reward=0.59 +/- 0.81
Episode length: 30.27 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.59     |
| rollout/           |          |
|    return_std      | 5.37     |
| time/              |          |
|    total_timesteps | 4472000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 247      |
|    learning_rate   | 0.0222   |
|    step_size       | 0.000344 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.59
SELFPLAY: new best model, bumping up generation to 95
Ep done - 184000.
Eval num_timesteps=4498000, episode_reward=-0.14 +/- 0.99
Episode length: 30.05 +/- 0.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.135   |
| rollout/           |          |
|    return_std      | 5.38     |
| time/              |          |
|    total_timesteps | 4498000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 248      |
|    learning_rate   | 0.0221   |
|    step_size       | 0.000342 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.15    |
|    return_std      | 5.39     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 7005     |
|    total_timesteps | 4503727  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 249      |
|    learning_rate   | 0.0221   |
|    step_size       | 0.000341 |
---------------------------------
Ep done - 185000.
Eval num_timesteps=4524000, episode_reward=0.03 +/- 0.98
Episode length: 29.91 +/- 0.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.03     |
| rollout/           |          |
|    return_std      | 4.98     |
| time/              |          |
|    total_timesteps | 4524000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 250      |
|    learning_rate   | 0.022    |
|    step_size       | 0.000368 |
---------------------------------
Ep done - 186000.
Eval num_timesteps=4550000, episode_reward=0.64 +/- 0.76
Episode length: 30.11 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.64     |
| rollout/           |          |
|    return_std      | 4.52     |
| time/              |          |
|    total_timesteps | 4550000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 251      |
|    learning_rate   | 0.0219   |
|    step_size       | 0.000404 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.64
SELFPLAY: new best model, bumping up generation to 96
Ep done - 187000.
Eval num_timesteps=4576000, episode_reward=0.33 +/- 0.93
Episode length: 30.32 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.325    |
| rollout/           |          |
|    return_std      | 5.35     |
| time/              |          |
|    total_timesteps | 4576000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 253      |
|    learning_rate   | 0.0218   |
|    step_size       | 0.000339 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.325
SELFPLAY: new best model, bumping up generation to 97
Ep done - 188000.
Eval num_timesteps=4602000, episode_reward=-0.71 +/- 0.70
Episode length: 29.61 +/- 0.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.71    |
| rollout/           |          |
|    return_std      | 5.1      |
| time/              |          |
|    total_timesteps | 4602000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 254      |
|    learning_rate   | 0.0217   |
|    step_size       | 0.000355 |
---------------------------------
Ep done - 189000.
Eval num_timesteps=4628000, episode_reward=0.41 +/- 0.90
Episode length: 30.27 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.405    |
| rollout/           |          |
|    return_std      | 5.12     |
| time/              |          |
|    total_timesteps | 4628000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 255      |
|    learning_rate   | 0.0216   |
|    step_size       | 0.000352 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.405
SELFPLAY: new best model, bumping up generation to 98
Ep done - 190000.
Eval num_timesteps=4654000, episode_reward=-0.61 +/- 0.79
Episode length: 29.28 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.3     |
|    mean_reward     | -0.615   |
| rollout/           |          |
|    return_std      | 4.77     |
| time/              |          |
|    total_timesteps | 4654000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 257      |
|    learning_rate   | 0.0215   |
|    step_size       | 0.000375 |
---------------------------------
Ep done - 191000.
Eval num_timesteps=4680000, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.025    |
| rollout/           |          |
|    return_std      | 3.84     |
| time/              |          |
|    total_timesteps | 4680000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 258      |
|    learning_rate   | 0.0214   |
|    step_size       | 0.000464 |
---------------------------------
Ep done - 192000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.105   |
|    return_std      | 4.84     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 7287     |
|    total_timesteps | 4683576  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 259      |
|    learning_rate   | 0.0213   |
|    step_size       | 0.000367 |
---------------------------------
Eval num_timesteps=4706000, episode_reward=0.69 +/- 0.72
Episode length: 30.00 +/- 0.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.69     |
| rollout/           |          |
|    return_std      | 4.18     |
| time/              |          |
|    total_timesteps | 4706000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 260      |
|    learning_rate   | 0.0213   |
|    step_size       | 0.000424 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.69
SELFPLAY: new best model, bumping up generation to 99
Ep done - 193000.
Ep done - 194000.
Eval num_timesteps=4732000, episode_reward=-0.33 +/- 0.93
Episode length: 30.07 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | -0.33    |
| rollout/           |          |
|    return_std      | 4.72     |
| time/              |          |
|    total_timesteps | 4732000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 261      |
|    learning_rate   | 0.0212   |
|    step_size       | 0.000374 |
---------------------------------
Ep done - 195000.
Eval num_timesteps=4758000, episode_reward=-0.54 +/- 0.84
Episode length: 30.09 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | -0.54    |
| rollout/           |          |
|    return_std      | 4.61     |
| time/              |          |
|    total_timesteps | 4758000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 263      |
|    learning_rate   | 0.021    |
|    step_size       | 0.000381 |
---------------------------------
Ep done - 196000.
Eval num_timesteps=4784000, episode_reward=0.33 +/- 0.93
Episode length: 30.02 +/- 0.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.325    |
| rollout/           |          |
|    return_std      | 5.49     |
| time/              |          |
|    total_timesteps | 4784000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 264      |
|    learning_rate   | 0.021    |
|    step_size       | 0.000318 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.325
SELFPLAY: new best model, bumping up generation to 100
Ep done - 197000.
Eval num_timesteps=4810000, episode_reward=0.51 +/- 0.84
Episode length: 30.32 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.51     |
| rollout/           |          |
|    return_std      | 5.67     |
| time/              |          |
|    total_timesteps | 4810000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 266      |
|    learning_rate   | 0.0208   |
|    step_size       | 0.000306 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.51
SELFPLAY: new best model, bumping up generation to 101
Ep done - 198000.
Eval num_timesteps=4836000, episode_reward=-0.32 +/- 0.95
Episode length: 29.91 +/- 0.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.315   |
| rollout/           |          |
|    return_std      | 4.42     |
| time/              |          |
|    total_timesteps | 4836000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 267      |
|    learning_rate   | 0.0208   |
|    step_size       | 0.000391 |
---------------------------------
Ep done - 199000.
Eval num_timesteps=4862000, episode_reward=0.46 +/- 0.89
Episode length: 30.07 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.46     |
| rollout/           |          |
|    return_std      | 4.61     |
| time/              |          |
|    total_timesteps | 4862000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 268      |
|    learning_rate   | 0.0207   |
|    step_size       | 0.000374 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.46
SELFPLAY: new best model, bumping up generation to 102
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.0133  |
|    return_std      | 4.85     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 7568     |
|    total_timesteps | 4863564  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 269      |
|    learning_rate   | 0.0206   |
|    step_size       | 0.000354 |
---------------------------------
Ep done - 200000.
Eval num_timesteps=4888000, episode_reward=0.54 +/- 0.64
Episode length: 30.05 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.535    |
| rollout/           |          |
|    return_std      | 4.94     |
| time/              |          |
|    total_timesteps | 4888000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 270      |
|    learning_rate   | 0.0205   |
|    step_size       | 0.000347 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.535
SELFPLAY: new best model, bumping up generation to 103
Ep done - 201000.
Eval num_timesteps=4914000, episode_reward=0.27 +/- 0.96
Episode length: 29.89 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.27     |
| rollout/           |          |
|    return_std      | 5.66     |
| time/              |          |
|    total_timesteps | 4914000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 271      |
|    learning_rate   | 0.0205   |
|    step_size       | 0.000302 |
---------------------------------
Ep done - 202000.
Eval num_timesteps=4940000, episode_reward=0.06 +/- 1.00
Episode length: 29.76 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | 0.06     |
| rollout/           |          |
|    return_std      | 5.41     |
| time/              |          |
|    total_timesteps | 4940000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 273      |
|    learning_rate   | 0.0203   |
|    step_size       | 0.000313 |
---------------------------------
Ep done - 203000.
Eval num_timesteps=4966000, episode_reward=0.15 +/- 0.96
Episode length: 29.93 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.15     |
| rollout/           |          |
|    return_std      | 5.39     |
| time/              |          |
|    total_timesteps | 4966000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 274      |
|    learning_rate   | 0.0203   |
|    step_size       | 0.000313 |
---------------------------------
Ep done - 204000.
Eval num_timesteps=4992000, episode_reward=-0.03 +/- 0.81
Episode length: 29.98 +/- 0.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.03    |
| rollout/           |          |
|    return_std      | 5.73     |
| time/              |          |
|    total_timesteps | 4992000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 276      |
|    learning_rate   | 0.0201   |
|    step_size       | 0.000292 |
---------------------------------
Ep done - 205000.
Eval num_timesteps=5018000, episode_reward=0.28 +/- 0.90
Episode length: 30.08 +/- 0.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.285    |
| rollout/           |          |
|    return_std      | 5.54     |
| time/              |          |
|    total_timesteps | 5018000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 277      |
|    learning_rate   | 0.02     |
|    step_size       | 0.000301 |
---------------------------------
Ep done - 206000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.0217   |
|    return_std      | 5.5      |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 7840     |
|    total_timesteps | 5043157  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 279      |
|    learning_rate   | 0.0199   |
|    step_size       | 0.000301 |
---------------------------------
Eval num_timesteps=5044000, episode_reward=0.28 +/- 0.95
Episode length: 30.00 +/- 0.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.28     |
| time/              |          |
|    total_timesteps | 5044000  |
---------------------------------
Ep done - 207000.
Eval num_timesteps=5070000, episode_reward=0.57 +/- 0.70
Episode length: 29.93 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.57     |
| rollout/           |          |
|    return_std      | 4.53     |
| time/              |          |
|    total_timesteps | 5070000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 280      |
|    learning_rate   | 0.0198   |
|    step_size       | 0.000365 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.57
SELFPLAY: new best model, bumping up generation to 104
Ep done - 208000.
Eval num_timesteps=5096000, episode_reward=-0.12 +/- 0.99
Episode length: 29.80 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | -0.115   |
| rollout/           |          |
|    return_std      | 5.71     |
| time/              |          |
|    total_timesteps | 5096000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 281      |
|    learning_rate   | 0.0198   |
|    step_size       | 0.000288 |
---------------------------------
Ep done - 209000.
Ep done - 210000.
Eval num_timesteps=5122000, episode_reward=0.78 +/- 0.61
Episode length: 30.05 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.775    |
| rollout/           |          |
|    return_std      | 5.12     |
| time/              |          |
|    total_timesteps | 5122000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 283      |
|    learning_rate   | 0.0196   |
|    step_size       | 0.000319 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.775
SELFPLAY: new best model, bumping up generation to 105
Ep done - 211000.
Eval num_timesteps=5148000, episode_reward=-0.49 +/- 0.87
Episode length: 29.78 +/- 0.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | -0.49    |
| rollout/           |          |
|    return_std      | 5.44     |
| time/              |          |
|    total_timesteps | 5148000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 284      |
|    learning_rate   | 0.0195   |
|    step_size       | 0.000299 |
---------------------------------
Ep done - 212000.
Eval num_timesteps=5174000, episode_reward=0.83 +/- 0.55
Episode length: 29.96 +/- 0.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.835    |
| rollout/           |          |
|    return_std      | 6.12     |
| time/              |          |
|    total_timesteps | 5174000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 286      |
|    learning_rate   | 0.0194   |
|    step_size       | 0.000264 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.835
SELFPLAY: new best model, bumping up generation to 106
Ep done - 213000.
Eval num_timesteps=5200000, episode_reward=0.15 +/- 0.87
Episode length: 30.07 +/- 0.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.155    |
| rollout/           |          |
|    return_std      | 5.02     |
| time/              |          |
|    total_timesteps | 5200000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 287      |
|    learning_rate   | 0.0193   |
|    step_size       | 0.000321 |
---------------------------------
Ep done - 214000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.187   |
|    return_std      | 5.87     |
| time/              |          |
|    fps             | 643      |
|    time_elapsed    | 8121     |
|    total_timesteps | 5223016  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 289      |
|    learning_rate   | 0.0192   |
|    step_size       | 0.000272 |
---------------------------------
Eval num_timesteps=5226000, episode_reward=0.13 +/- 0.99
Episode length: 30.22 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.13     |
| time/              |          |
|    total_timesteps | 5226000  |
---------------------------------
Ep done - 215000.
Eval num_timesteps=5252000, episode_reward=0.30 +/- 0.93
Episode length: 30.02 +/- 0.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.3      |
| rollout/           |          |
|    return_std      | 6.17     |
| time/              |          |
|    total_timesteps | 5252000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 290      |
|    learning_rate   | 0.0191   |
|    step_size       | 0.000258 |
---------------------------------
Ep done - 216000.
Eval num_timesteps=5278000, episode_reward=-0.10 +/- 0.94
Episode length: 29.86 +/- 0.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.095   |
| rollout/           |          |
|    return_std      | 5.77     |
| time/              |          |
|    total_timesteps | 5278000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 292      |
|    learning_rate   | 0.019    |
|    step_size       | 0.000274 |
---------------------------------
Ep done - 217000.
Eval num_timesteps=5304000, episode_reward=-0.69 +/- 0.68
Episode length: 29.79 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | -0.685   |
| rollout/           |          |
|    return_std      | 5.71     |
| time/              |          |
|    total_timesteps | 5304000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 293      |
|    learning_rate   | 0.0189   |
|    step_size       | 0.000276 |
---------------------------------
Ep done - 218000.
Eval num_timesteps=5330000, episode_reward=-0.56 +/- 0.82
Episode length: 29.80 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | -0.56    |
| rollout/           |          |
|    return_std      | 6.38     |
| time/              |          |
|    total_timesteps | 5330000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 294      |
|    learning_rate   | 0.0188   |
|    step_size       | 0.000246 |
---------------------------------
Ep done - 219000.
Eval num_timesteps=5356000, episode_reward=-0.29 +/- 0.96
Episode length: 29.71 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.7     |
|    mean_reward     | -0.29    |
| rollout/           |          |
|    return_std      | 5.42     |
| time/              |          |
|    total_timesteps | 5356000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 296      |
|    learning_rate   | 0.0187   |
|    step_size       | 0.000287 |
---------------------------------
Ep done - 220000.
Eval num_timesteps=5382000, episode_reward=0.08 +/- 1.00
Episode length: 30.00 +/- 0.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.08     |
| rollout/           |          |
|    return_std      | 6.42     |
| time/              |          |
|    total_timesteps | 5382000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 297      |
|    learning_rate   | 0.0186   |
|    step_size       | 0.000241 |
---------------------------------
Ep done - 221000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.0517  |
|    return_std      | 6.23     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 8402     |
|    total_timesteps | 5402843  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 299      |
|    learning_rate   | 0.0185   |
|    step_size       | 0.000247 |
---------------------------------
Eval num_timesteps=5408000, episode_reward=0.32 +/- 0.95
Episode length: 30.06 +/- 0.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.32     |
| time/              |          |
|    total_timesteps | 5408000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.32
SELFPLAY: new best model, bumping up generation to 107
Ep done - 222000.
Eval num_timesteps=5434000, episode_reward=0.03 +/- 1.00
Episode length: 29.95 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.03     |
| rollout/           |          |
|    return_std      | 5.11     |
| time/              |          |
|    total_timesteps | 5434000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 300      |
|    learning_rate   | 0.0184   |
|    step_size       | 0.0003   |
---------------------------------
Ep done - 223000.
Eval num_timesteps=5460000, episode_reward=-0.39 +/- 0.92
Episode length: 29.32 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.3     |
|    mean_reward     | -0.39    |
| rollout/           |          |
|    return_std      | 4.82     |
| time/              |          |
|    total_timesteps | 5460000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 302      |
|    learning_rate   | 0.0182   |
|    step_size       | 0.000316 |
---------------------------------
Ep done - 224000.
Eval num_timesteps=5486000, episode_reward=0.21 +/- 0.97
Episode length: 29.97 +/- 0.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.21     |
| rollout/           |          |
|    return_std      | 4.62     |
| time/              |          |
|    total_timesteps | 5486000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 303      |
|    learning_rate   | 0.0182   |
|    step_size       | 0.000328 |
---------------------------------
Ep done - 225000.
Ep done - 226000.
Eval num_timesteps=5512000, episode_reward=0.19 +/- 0.98
Episode length: 30.34 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.19     |
| rollout/           |          |
|    return_std      | 4.53     |
| time/              |          |
|    total_timesteps | 5512000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 305      |
|    learning_rate   | 0.018    |
|    step_size       | 0.000332 |
---------------------------------
Ep done - 227000.
Eval num_timesteps=5538000, episode_reward=0.71 +/- 0.68
Episode length: 30.04 +/- 0.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.715    |
| rollout/           |          |
|    return_std      | 4.67     |
| time/              |          |
|    total_timesteps | 5538000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 306      |
|    learning_rate   | 0.018    |
|    step_size       | 0.000321 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.715
SELFPLAY: new best model, bumping up generation to 108
Ep done - 228000.
Eval num_timesteps=5564000, episode_reward=0.68 +/- 0.73
Episode length: 30.16 +/- 0.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.68     |
| rollout/           |          |
|    return_std      | 5.05     |
| time/              |          |
|    total_timesteps | 5564000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 307      |
|    learning_rate   | 0.0179   |
|    step_size       | 0.000295 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.68
SELFPLAY: new best model, bumping up generation to 109
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.152   |
|    return_std      | 5.35     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 8683     |
|    total_timesteps | 5582740  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 309      |
|    learning_rate   | 0.0177   |
|    step_size       | 0.000276 |
---------------------------------
Ep done - 229000.
Eval num_timesteps=5590000, episode_reward=0.56 +/- 0.80
Episode length: 29.98 +/- 0.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.565    |
| time/              |          |
|    total_timesteps | 5590000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.565
SELFPLAY: new best model, bumping up generation to 110
Ep done - 230000.
Eval num_timesteps=5616000, episode_reward=-0.18 +/- 0.95
Episode length: 29.97 +/- 0.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.18    |
| rollout/           |          |
|    return_std      | 6.21     |
| time/              |          |
|    total_timesteps | 5616000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 310      |
|    learning_rate   | 0.0177   |
|    step_size       | 0.000237 |
---------------------------------
Ep done - 231000.
Eval num_timesteps=5642000, episode_reward=0.33 +/- 0.94
Episode length: 29.79 +/- 0.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | 0.33     |
| rollout/           |          |
|    return_std      | 5.55     |
| time/              |          |
|    total_timesteps | 5642000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 312      |
|    learning_rate   | 0.0175   |
|    step_size       | 0.000263 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.33
SELFPLAY: new best model, bumping up generation to 111
Ep done - 232000.
Eval num_timesteps=5668000, episode_reward=0.15 +/- 0.99
Episode length: 30.07 +/- 0.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.15     |
| rollout/           |          |
|    return_std      | 4.69     |
| time/              |          |
|    total_timesteps | 5668000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 313      |
|    learning_rate   | 0.0175   |
|    step_size       | 0.00031  |
---------------------------------
Ep done - 233000.
Eval num_timesteps=5694000, episode_reward=-0.68 +/- 0.68
Episode length: 29.99 +/- 0.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.675   |
| rollout/           |          |
|    return_std      | 5.61     |
| time/              |          |
|    total_timesteps | 5694000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 315      |
|    learning_rate   | 0.0173   |
|    step_size       | 0.000257 |
---------------------------------
Ep done - 234000.
Eval num_timesteps=5720000, episode_reward=-0.75 +/- 0.66
Episode length: 30.00 +/- 0.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.75    |
| rollout/           |          |
|    return_std      | 5.91     |
| time/              |          |
|    total_timesteps | 5720000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 316      |
|    learning_rate   | 0.0172   |
|    step_size       | 0.000243 |
---------------------------------
Ep done - 235000.
Eval num_timesteps=5746000, episode_reward=0.42 +/- 0.78
Episode length: 30.11 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.42     |
| rollout/           |          |
|    return_std      | 5.28     |
| time/              |          |
|    total_timesteps | 5746000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 318      |
|    learning_rate   | 0.0171   |
|    step_size       | 0.00027  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.42
SELFPLAY: new best model, bumping up generation to 112
Ep done - 236000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.0117   |
|    return_std      | 6.65     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 8964     |
|    total_timesteps | 5762659  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 319      |
|    learning_rate   | 0.017    |
|    step_size       | 0.000213 |
---------------------------------
Eval num_timesteps=5772000, episode_reward=-0.13 +/- 0.99
Episode length: 30.41 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | -0.13    |
| time/              |          |
|    total_timesteps | 5772000  |
---------------------------------
Ep done - 237000.
Eval num_timesteps=5798000, episode_reward=-0.78 +/- 0.63
Episode length: 29.61 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.78    |
| rollout/           |          |
|    return_std      | 6.92     |
| time/              |          |
|    total_timesteps | 5798000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 320      |
|    learning_rate   | 0.0169   |
|    step_size       | 0.000204 |
---------------------------------
Ep done - 238000.
Eval num_timesteps=5824000, episode_reward=0.00 +/- 1.00
Episode length: 30.43 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | 0        |
| rollout/           |          |
|    return_std      | 5.01     |
| time/              |          |
|    total_timesteps | 5824000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 322      |
|    learning_rate   | 0.0168   |
|    step_size       | 0.000279 |
---------------------------------
Ep done - 239000.
Eval num_timesteps=5850000, episode_reward=-0.56 +/- 0.58
Episode length: 29.89 +/- 0.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.56    |
| rollout/           |          |
|    return_std      | 5.23     |
| time/              |          |
|    total_timesteps | 5850000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 323      |
|    learning_rate   | 0.0167   |
|    step_size       | 0.000267 |
---------------------------------
Ep done - 240000.
Eval num_timesteps=5876000, episode_reward=-0.86 +/- 0.51
Episode length: 29.82 +/- 0.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | -0.86    |
| rollout/           |          |
|    return_std      | 6.22     |
| time/              |          |
|    total_timesteps | 5876000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 325      |
|    learning_rate   | 0.0166   |
|    step_size       | 0.000222 |
---------------------------------
Ep done - 241000.
Ep done - 242000.
Eval num_timesteps=5902000, episode_reward=0.00 +/- 1.00
Episode length: 29.55 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | 0        |
| rollout/           |          |
|    return_std      | 6.72     |
| time/              |          |
|    total_timesteps | 5902000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 326      |
|    learning_rate   | 0.0165   |
|    step_size       | 0.000205 |
---------------------------------
Ep done - 243000.
Eval num_timesteps=5928000, episode_reward=0.62 +/- 0.78
Episode length: 30.30 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.62     |
| rollout/           |          |
|    return_std      | 6.53     |
| time/              |          |
|    total_timesteps | 5928000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 328      |
|    learning_rate   | 0.0164   |
|    step_size       | 0.000209 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.62
SELFPLAY: new best model, bumping up generation to 113
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.278   |
|    return_std      | 6.22     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 9247     |
|    total_timesteps | 5942472  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 329      |
|    learning_rate   | 0.0163   |
|    step_size       | 0.000219 |
---------------------------------
Ep done - 244000.
Eval num_timesteps=5954000, episode_reward=0.00 +/- 1.00
Episode length: 29.58 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 5954000  |
---------------------------------
Ep done - 245000.
Eval num_timesteps=5980000, episode_reward=0.81 +/- 0.58
Episode length: 29.68 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.7     |
|    mean_reward     | 0.815    |
| rollout/           |          |
|    return_std      | 6.16     |
| time/              |          |
|    total_timesteps | 5980000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 331      |
|    learning_rate   | 0.0162   |
|    step_size       | 0.000218 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.815
SELFPLAY: new best model, bumping up generation to 114
Ep done - 246000.
Eval num_timesteps=6006000, episode_reward=0.12 +/- 0.77
Episode length: 30.00 +/- 0.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.115    |
| rollout/           |          |
|    return_std      | 6.36     |
| time/              |          |
|    total_timesteps | 6006000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 332      |
|    learning_rate   | 0.0161   |
|    step_size       | 0.000211 |
---------------------------------
Ep done - 247000.
Eval num_timesteps=6032000, episode_reward=-0.36 +/- 0.89
Episode length: 29.98 +/- 0.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.36    |
| rollout/           |          |
|    return_std      | 6.64     |
| time/              |          |
|    total_timesteps | 6032000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 333      |
|    learning_rate   | 0.016    |
|    step_size       | 0.000201 |
---------------------------------
Ep done - 248000.
Eval num_timesteps=6058000, episode_reward=-0.30 +/- 0.77
Episode length: 29.77 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | -0.305   |
| rollout/           |          |
|    return_std      | 5.75     |
| time/              |          |
|    total_timesteps | 6058000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 335      |
|    learning_rate   | 0.0159   |
|    step_size       | 0.00023  |
---------------------------------
Ep done - 249000.
Eval num_timesteps=6084000, episode_reward=-0.05 +/- 0.77
Episode length: 29.91 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.05    |
| rollout/           |          |
|    return_std      | 5.74     |
| time/              |          |
|    total_timesteps | 6084000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 336      |
|    learning_rate   | 0.0158   |
|    step_size       | 0.000229 |
---------------------------------
Ep done - 250000.
Eval num_timesteps=6110000, episode_reward=0.19 +/- 0.98
Episode length: 30.16 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.19     |
| rollout/           |          |
|    return_std      | 4.43     |
| time/              |          |
|    total_timesteps | 6110000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 338      |
|    learning_rate   | 0.0157   |
|    step_size       | 0.000294 |
---------------------------------
Ep done - 251000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.0833  |
|    return_std      | 5.5      |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 9528     |
|    total_timesteps | 6122285  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 339      |
|    learning_rate   | 0.0156   |
|    step_size       | 0.000236 |
---------------------------------
Eval num_timesteps=6136000, episode_reward=0.41 +/- 0.88
Episode length: 29.92 +/- 0.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.415    |
| time/              |          |
|    total_timesteps | 6136000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.415
SELFPLAY: new best model, bumping up generation to 115
Ep done - 252000.
Eval num_timesteps=6162000, episode_reward=-0.09 +/- 0.98
Episode length: 29.75 +/- 0.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.7     |
|    mean_reward     | -0.085   |
| rollout/           |          |
|    return_std      | 5.82     |
| time/              |          |
|    total_timesteps | 6162000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 341      |
|    learning_rate   | 0.0154   |
|    step_size       | 0.000221 |
---------------------------------
Ep done - 253000.
Eval num_timesteps=6188000, episode_reward=0.53 +/- 0.83
Episode length: 30.43 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | 0.53     |
| rollout/           |          |
|    return_std      | 6.81     |
| time/              |          |
|    total_timesteps | 6188000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 342      |
|    learning_rate   | 0.0154   |
|    step_size       | 0.000188 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.53
SELFPLAY: new best model, bumping up generation to 116
Ep done - 254000.
Eval num_timesteps=6214000, episode_reward=-0.08 +/- 1.00
Episode length: 30.03 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.08    |
| rollout/           |          |
|    return_std      | 5.48     |
| time/              |          |
|    total_timesteps | 6214000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 344      |
|    learning_rate   | 0.0152   |
|    step_size       | 0.000231 |
---------------------------------
Ep done - 255000.
Eval num_timesteps=6240000, episode_reward=0.53 +/- 0.85
Episode length: 30.20 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.525    |
| rollout/           |          |
|    return_std      | 6.07     |
| time/              |          |
|    total_timesteps | 6240000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 345      |
|    learning_rate   | 0.0152   |
|    step_size       | 0.000208 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.525
SELFPLAY: new best model, bumping up generation to 117
Ep done - 256000.
Eval num_timesteps=6266000, episode_reward=0.43 +/- 0.61
Episode length: 29.96 +/- 0.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.43     |
| rollout/           |          |
|    return_std      | 4.22     |
| time/              |          |
|    total_timesteps | 6266000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 346      |
|    learning_rate   | 0.0151   |
|    step_size       | 0.000298 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.43
SELFPLAY: new best model, bumping up generation to 118
Ep done - 257000.
Ep done - 258000.
Eval num_timesteps=6292000, episode_reward=-0.38 +/- 0.88
Episode length: 29.54 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.375   |
| rollout/           |          |
|    return_std      | 5.22     |
| time/              |          |
|    total_timesteps | 6292000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 348      |
|    learning_rate   | 0.0149   |
|    step_size       | 0.000238 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.138   |
|    return_std      | 4.48     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 9808     |
|    total_timesteps | 6302146  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 349      |
|    learning_rate   | 0.0149   |
|    step_size       | 0.000276 |
---------------------------------
Ep done - 259000.
Eval num_timesteps=6318000, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.035    |
| time/              |          |
|    total_timesteps | 6318000  |
---------------------------------
Ep done - 260000.
Eval num_timesteps=6344000, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 0.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.075    |
| rollout/           |          |
|    return_std      | 5.03     |
| time/              |          |
|    total_timesteps | 6344000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 351      |
|    learning_rate   | 0.0147   |
|    step_size       | 0.000244 |
---------------------------------
Ep done - 261000.
Eval num_timesteps=6370000, episode_reward=0.20 +/- 0.98
Episode length: 30.25 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.205    |
| rollout/           |          |
|    return_std      | 5.29     |
| time/              |          |
|    total_timesteps | 6370000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 352      |
|    learning_rate   | 0.0146   |
|    step_size       | 0.000231 |
---------------------------------
Ep done - 262000.
Eval num_timesteps=6396000, episode_reward=0.64 +/- 0.77
Episode length: 30.29 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.64     |
| rollout/           |          |
|    return_std      | 4.44     |
| time/              |          |
|    total_timesteps | 6396000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 354      |
|    learning_rate   | 0.0145   |
|    step_size       | 0.000272 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.64
SELFPLAY: new best model, bumping up generation to 119
Ep done - 263000.
Eval num_timesteps=6422000, episode_reward=-0.11 +/- 0.99
Episode length: 29.94 +/- 0.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.11    |
| rollout/           |          |
|    return_std      | 6.04     |
| time/              |          |
|    total_timesteps | 6422000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 355      |
|    learning_rate   | 0.0144   |
|    step_size       | 0.000199 |
---------------------------------
Ep done - 264000.
Eval num_timesteps=6448000, episode_reward=-0.90 +/- 0.44
Episode length: 29.55 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.9     |
| rollout/           |          |
|    return_std      | 5.75     |
| time/              |          |
|    total_timesteps | 6448000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 357      |
|    learning_rate   | 0.0143   |
|    step_size       | 0.000207 |
---------------------------------
Ep done - 265000.
Eval num_timesteps=6474000, episode_reward=0.04 +/- 1.00
Episode length: 29.61 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | 0.04     |
| rollout/           |          |
|    return_std      | 4.78     |
| time/              |          |
|    total_timesteps | 6474000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 358      |
|    learning_rate   | 0.0142   |
|    step_size       | 0.000248 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.0533   |
|    return_std      | 6.67     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 10089    |
|    total_timesteps | 6482013  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 359      |
|    learning_rate   | 0.0141   |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 266000.
Eval num_timesteps=6500000, episode_reward=0.05 +/- 0.95
Episode length: 29.71 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.7     |
|    mean_reward     | 0.05     |
| rollout/           |          |
|    return_std      | 5.62     |
| time/              |          |
|    total_timesteps | 6500000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 360      |
|    learning_rate   | 0.0141   |
|    step_size       | 0.000209 |
---------------------------------
Ep done - 267000.
Eval num_timesteps=6526000, episode_reward=0.04 +/- 1.00
Episode length: 29.54 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | 0.04     |
| rollout/           |          |
|    return_std      | 5.02     |
| time/              |          |
|    total_timesteps | 6526000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 361      |
|    learning_rate   | 0.014    |
|    step_size       | 0.000232 |
---------------------------------
Ep done - 268000.
Eval num_timesteps=6552000, episode_reward=-0.92 +/- 0.39
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.92    |
| rollout/           |          |
|    return_std      | 4.39     |
| time/              |          |
|    total_timesteps | 6552000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 362      |
|    learning_rate   | 0.0139   |
|    step_size       | 0.000264 |
---------------------------------
Ep done - 269000.
Eval num_timesteps=6578000, episode_reward=0.37 +/- 0.93
Episode length: 30.15 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.37     |
| rollout/           |          |
|    return_std      | 5.97     |
| time/              |          |
|    total_timesteps | 6578000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 364      |
|    learning_rate   | 0.0138   |
|    step_size       | 0.000192 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.37
SELFPLAY: new best model, bumping up generation to 120
Ep done - 270000.
Eval num_timesteps=6604000, episode_reward=0.58 +/- 0.81
Episode length: 29.93 +/- 0.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.58     |
| rollout/           |          |
|    return_std      | 6.37     |
| time/              |          |
|    total_timesteps | 6604000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 365      |
|    learning_rate   | 0.0137   |
|    step_size       | 0.000179 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.58
SELFPLAY: new best model, bumping up generation to 121
Ep done - 271000.
Eval num_timesteps=6630000, episode_reward=0.81 +/- 0.59
Episode length: 30.39 +/- 0.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | 0.81     |
| rollout/           |          |
|    return_std      | 5.07     |
| time/              |          |
|    total_timesteps | 6630000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 367      |
|    learning_rate   | 0.0136   |
|    step_size       | 0.000223 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.81
SELFPLAY: new best model, bumping up generation to 122
Ep done - 272000.
Eval num_timesteps=6656000, episode_reward=0.32 +/- 0.89
Episode length: 30.10 +/- 0.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.32     |
| rollout/           |          |
|    return_std      | 5.27     |
| time/              |          |
|    total_timesteps | 6656000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 368      |
|    learning_rate   | 0.0135   |
|    step_size       | 0.000213 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.32
SELFPLAY: new best model, bumping up generation to 123
Ep done - 273000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.118    |
|    return_std      | 5.2      |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 10370    |
|    total_timesteps | 6662061  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 369      |
|    learning_rate   | 0.0134   |
|    step_size       | 0.000215 |
---------------------------------
Ep done - 274000.
Eval num_timesteps=6682000, episode_reward=-0.21 +/- 0.95
Episode length: 29.96 +/- 0.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.21    |
| rollout/           |          |
|    return_std      | 6.43     |
| time/              |          |
|    total_timesteps | 6682000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 370      |
|    learning_rate   | 0.0134   |
|    step_size       | 0.000173 |
---------------------------------
Ep done - 275000.
Eval num_timesteps=6708000, episode_reward=-0.22 +/- 0.98
Episode length: 29.98 +/- 0.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.22    |
| rollout/           |          |
|    return_std      | 5.66     |
| time/              |          |
|    total_timesteps | 6708000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 371      |
|    learning_rate   | 0.0133   |
|    step_size       | 0.000195 |
---------------------------------
Ep done - 276000.
Eval num_timesteps=6734000, episode_reward=0.30 +/- 0.95
Episode length: 30.12 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.3      |
| rollout/           |          |
|    return_std      | 6.1      |
| time/              |          |
|    total_timesteps | 6734000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 372      |
|    learning_rate   | 0.0132   |
|    step_size       | 0.00018  |
---------------------------------
Ep done - 277000.
Eval num_timesteps=6760000, episode_reward=-0.20 +/- 0.98
Episode length: 29.64 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.2     |
| rollout/           |          |
|    return_std      | 6.21     |
| time/              |          |
|    total_timesteps | 6760000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 374      |
|    learning_rate   | 0.0131   |
|    step_size       | 0.000175 |
---------------------------------
Ep done - 278000.
Eval num_timesteps=6786000, episode_reward=0.14 +/- 0.96
Episode length: 29.64 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | 0.14     |
| rollout/           |          |
|    return_std      | 5.25     |
| time/              |          |
|    total_timesteps | 6786000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 375      |
|    learning_rate   | 0.013    |
|    step_size       | 0.000206 |
---------------------------------
Ep done - 279000.
Eval num_timesteps=6812000, episode_reward=0.56 +/- 0.83
Episode length: 29.98 +/- 0.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.555    |
| rollout/           |          |
|    return_std      | 5.96     |
| time/              |          |
|    total_timesteps | 6812000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 377      |
|    learning_rate   | 0.0128   |
|    step_size       | 0.00018  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.555
SELFPLAY: new best model, bumping up generation to 124
Ep done - 280000.
Eval num_timesteps=6838000, episode_reward=-0.45 +/- 0.71
Episode length: 29.89 +/- 0.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.45    |
| rollout/           |          |
|    return_std      | 7.11     |
| time/              |          |
|    total_timesteps | 6838000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 378      |
|    learning_rate   | 0.0128   |
|    step_size       | 0.00015  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.065   |
|    return_std      | 5.38     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 10651    |
|    total_timesteps | 6842187  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 379      |
|    learning_rate   | 0.0127   |
|    step_size       | 0.000197 |
---------------------------------
Ep done - 281000.
Eval num_timesteps=6864000, episode_reward=0.42 +/- 0.91
Episode length: 29.73 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.7     |
|    mean_reward     | 0.42     |
| rollout/           |          |
|    return_std      | 6.5      |
| time/              |          |
|    total_timesteps | 6864000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 380      |
|    learning_rate   | 0.0126   |
|    step_size       | 0.000162 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.42
SELFPLAY: new best model, bumping up generation to 125
Ep done - 282000.
Eval num_timesteps=6890000, episode_reward=-0.83 +/- 0.56
Episode length: 30.05 +/- 0.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | -0.83    |
| rollout/           |          |
|    return_std      | 4.74     |
| time/              |          |
|    total_timesteps | 6890000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 381      |
|    learning_rate   | 0.0126   |
|    step_size       | 0.000221 |
---------------------------------
Ep done - 283000.
Eval num_timesteps=6916000, episode_reward=0.20 +/- 0.81
Episode length: 29.68 +/- 0.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.7     |
|    mean_reward     | 0.2      |
| rollout/           |          |
|    return_std      | 5.96     |
| time/              |          |
|    total_timesteps | 6916000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 383      |
|    learning_rate   | 0.0124   |
|    step_size       | 0.000174 |
---------------------------------
Ep done - 284000.
Eval num_timesteps=6942000, episode_reward=0.76 +/- 0.64
Episode length: 30.11 +/- 0.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.755    |
| rollout/           |          |
|    return_std      | 5.27     |
| time/              |          |
|    total_timesteps | 6942000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 384      |
|    learning_rate   | 0.0123   |
|    step_size       | 0.000195 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.755
SELFPLAY: new best model, bumping up generation to 126
Ep done - 285000.
Eval num_timesteps=6968000, episode_reward=0.04 +/- 0.99
Episode length: 29.82 +/- 0.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | 0.04     |
| rollout/           |          |
|    return_std      | 5.73     |
| time/              |          |
|    total_timesteps | 6968000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 385      |
|    learning_rate   | 0.0123   |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 286000.
Eval num_timesteps=6994000, episode_reward=0.15 +/- 0.99
Episode length: 30.11 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.15     |
| rollout/           |          |
|    return_std      | 4.82     |
| time/              |          |
|    total_timesteps | 6994000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 387      |
|    learning_rate   | 0.0121   |
|    step_size       | 0.00021  |
---------------------------------
Ep done - 287000.
Eval num_timesteps=7020000, episode_reward=0.51 +/- 0.73
Episode length: 30.20 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.51     |
| rollout/           |          |
|    return_std      | 6.47     |
| time/              |          |
|    total_timesteps | 7020000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 388      |
|    learning_rate   | 0.0121   |
|    step_size       | 0.000155 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.51
SELFPLAY: new best model, bumping up generation to 127
Ep done - 288000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
|    return_std      | 5.28     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 10931    |
|    total_timesteps | 7022071  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 389      |
|    learning_rate   | 0.012    |
|    step_size       | 0.000189 |
---------------------------------
Eval num_timesteps=7046000, episode_reward=0.16 +/- 0.99
Episode length: 30.06 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.16     |
| rollout/           |          |
|    return_std      | 4.88     |
| time/              |          |
|    total_timesteps | 7046000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 390      |
|    learning_rate   | 0.0119   |
|    step_size       | 0.000203 |
---------------------------------
Ep done - 289000.
Ep done - 290000.
Eval num_timesteps=7072000, episode_reward=0.92 +/- 0.39
Episode length: 30.02 +/- 0.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.92     |
| rollout/           |          |
|    return_std      | 6.49     |
| time/              |          |
|    total_timesteps | 7072000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 391      |
|    learning_rate   | 0.0118   |
|    step_size       | 0.000152 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 128
Ep done - 291000.
Eval num_timesteps=7098000, episode_reward=-0.84 +/- 0.54
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.84    |
| rollout/           |          |
|    return_std      | 3.82     |
| time/              |          |
|    total_timesteps | 7098000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 393      |
|    learning_rate   | 0.0117   |
|    step_size       | 0.000255 |
---------------------------------
Ep done - 292000.
Eval num_timesteps=7124000, episode_reward=-0.56 +/- 0.83
Episode length: 29.98 +/- 0.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.555   |
| rollout/           |          |
|    return_std      | 5.66     |
| time/              |          |
|    total_timesteps | 7124000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 394      |
|    learning_rate   | 0.0116   |
|    step_size       | 0.000171 |
---------------------------------
Ep done - 293000.
Eval num_timesteps=7150000, episode_reward=-0.60 +/- 0.79
Episode length: 30.02 +/- 0.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.605   |
| rollout/           |          |
|    return_std      | 7.06     |
| time/              |          |
|    total_timesteps | 7150000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 396      |
|    learning_rate   | 0.0115   |
|    step_size       | 0.000135 |
---------------------------------
Ep done - 294000.
Eval num_timesteps=7176000, episode_reward=-0.96 +/- 0.28
Episode length: 29.98 +/- 0.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.96    |
| rollout/           |          |
|    return_std      | 6.84     |
| time/              |          |
|    total_timesteps | 7176000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 397      |
|    learning_rate   | 0.0114   |
|    step_size       | 0.000139 |
---------------------------------
Ep done - 295000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.0817  |
|    return_std      | 6.16     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 11204    |
|    total_timesteps | 7201885  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 399      |
|    learning_rate   | 0.0113   |
|    step_size       | 0.000152 |
---------------------------------
Eval num_timesteps=7202000, episode_reward=-0.35 +/- 0.93
Episode length: 29.98 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.35    |
| time/              |          |
|    total_timesteps | 7202000  |
---------------------------------
Ep done - 296000.
Eval num_timesteps=7228000, episode_reward=0.59 +/- 0.80
Episode length: 30.00 +/- 0.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.59     |
| rollout/           |          |
|    return_std      | 5.2      |
| time/              |          |
|    total_timesteps | 7228000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 400      |
|    learning_rate   | 0.0112   |
|    step_size       | 0.000179 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.59
SELFPLAY: new best model, bumping up generation to 129
Ep done - 297000.
Eval num_timesteps=7254000, episode_reward=-0.01 +/- 0.99
Episode length: 30.01 +/- 0.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.015   |
| rollout/           |          |
|    return_std      | 6.2      |
| time/              |          |
|    total_timesteps | 7254000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 401      |
|    learning_rate   | 0.0111   |
|    step_size       | 0.000149 |
---------------------------------
Ep done - 298000.
Eval num_timesteps=7280000, episode_reward=0.01 +/- 1.00
Episode length: 29.86 +/- 0.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.005    |
| rollout/           |          |
|    return_std      | 6.2      |
| time/              |          |
|    total_timesteps | 7280000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 403      |
|    learning_rate   | 0.011    |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 299000.
Eval num_timesteps=7306000, episode_reward=-0.42 +/- 0.91
Episode length: 29.73 +/- 0.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.7     |
|    mean_reward     | -0.42    |
| rollout/           |          |
|    return_std      | 6.02     |
| time/              |          |
|    total_timesteps | 7306000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 404      |
|    learning_rate   | 0.0109   |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 300000.
Eval num_timesteps=7332000, episode_reward=0.25 +/- 0.97
Episode length: 30.25 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.25     |
| rollout/           |          |
|    return_std      | 5.46     |
| time/              |          |
|    total_timesteps | 7332000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 406      |
|    learning_rate   | 0.0108   |
|    step_size       | 0.000164 |
---------------------------------
Ep done - 301000.
Eval num_timesteps=7358000, episode_reward=0.64 +/- 0.77
Episode length: 30.21 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.635    |
| rollout/           |          |
|    return_std      | 6.09     |
| time/              |          |
|    total_timesteps | 7358000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 407      |
|    learning_rate   | 0.0107   |
|    step_size       | 0.000146 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.635
SELFPLAY: new best model, bumping up generation to 130
Ep done - 302000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.233   |
|    return_std      | 6.34     |
| time/              |          |
|    fps             | 642      |
|    time_elapsed    | 11492    |
|    total_timesteps | 7381963  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 409      |
|    learning_rate   | 0.0105   |
|    step_size       | 0.000139 |
---------------------------------
Eval num_timesteps=7384000, episode_reward=-1.00 +/- 0.00
Episode length: 29.50 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 7384000  |
---------------------------------
Ep done - 303000.
Eval num_timesteps=7410000, episode_reward=-0.99 +/- 0.14
Episode length: 29.50 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.99    |
| rollout/           |          |
|    return_std      | 6.36     |
| time/              |          |
|    total_timesteps | 7410000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 410      |
|    learning_rate   | 0.0105   |
|    step_size       | 0.000137 |
---------------------------------
Ep done - 304000.
Ep done - 305000.
Eval num_timesteps=7436000, episode_reward=-0.02 +/- 1.00
Episode length: 29.95 +/- 0.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.02    |
| rollout/           |          |
|    return_std      | 6.64     |
| time/              |          |
|    total_timesteps | 7436000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 412      |
|    learning_rate   | 0.0103   |
|    step_size       | 0.00013  |
---------------------------------
Ep done - 306000.
Eval num_timesteps=7462000, episode_reward=0.97 +/- 0.21
Episode length: 30.46 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.5     |
|    mean_reward     | 0.975    |
| rollout/           |          |
|    return_std      | 6.28     |
| time/              |          |
|    total_timesteps | 7462000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 413      |
|    learning_rate   | 0.0103   |
|    step_size       | 0.000136 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.975
SELFPLAY: new best model, bumping up generation to 131
Ep done - 307000.
Eval num_timesteps=7488000, episode_reward=0.84 +/- 0.54
Episode length: 30.30 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.84     |
| rollout/           |          |
|    return_std      | 6.65     |
| time/              |          |
|    total_timesteps | 7488000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 414      |
|    learning_rate   | 0.0102   |
|    step_size       | 0.000128 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.84
SELFPLAY: new best model, bumping up generation to 132
Ep done - 308000.
Eval num_timesteps=7514000, episode_reward=-0.97 +/- 0.24
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.97    |
| rollout/           |          |
|    return_std      | 6.78     |
| time/              |          |
|    total_timesteps | 7514000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 416      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000123 |
---------------------------------
Ep done - 309000.
Eval num_timesteps=7540000, episode_reward=-0.91 +/- 0.41
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.91    |
| rollout/           |          |
|    return_std      | 7.08     |
| time/              |          |
|    total_timesteps | 7540000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 417      |
|    learning_rate   | 0.00997  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 310000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.237   |
|    return_std      | 6.27     |
| time/              |          |
|    fps             | 641      |
|    time_elapsed    | 11784    |
|    total_timesteps | 7561763  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 419      |
|    learning_rate   | 0.00982  |
|    step_size       | 0.000131 |
---------------------------------
Eval num_timesteps=7566000, episode_reward=-0.39 +/- 0.59
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.39    |
| time/              |          |
|    total_timesteps | 7566000  |
---------------------------------
Ep done - 311000.
Eval num_timesteps=7592000, episode_reward=-0.35 +/- 0.61
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.35    |
| rollout/           |          |
|    return_std      | 6.21     |
| time/              |          |
|    total_timesteps | 7592000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 420      |
|    learning_rate   | 0.00975  |
|    step_size       | 0.000131 |
---------------------------------
Ep done - 312000.
Eval num_timesteps=7618000, episode_reward=0.67 +/- 0.74
Episode length: 30.02 +/- 0.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.67     |
| rollout/           |          |
|    return_std      | 5.89     |
| time/              |          |
|    total_timesteps | 7618000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 422      |
|    learning_rate   | 0.00961  |
|    step_size       | 0.000136 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.67
SELFPLAY: new best model, bumping up generation to 133
Ep done - 313000.
Eval num_timesteps=7644000, episode_reward=0.05 +/- 1.00
Episode length: 30.00 +/- 0.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.05     |
| rollout/           |          |
|    return_std      | 7.17     |
| time/              |          |
|    total_timesteps | 7644000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 423      |
|    learning_rate   | 0.00954  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 314000.
Eval num_timesteps=7670000, episode_reward=0.67 +/- 0.74
Episode length: 30.00 +/- 0.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.67     |
| rollout/           |          |
|    return_std      | 6.84     |
| time/              |          |
|    total_timesteps | 7670000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 425      |
|    learning_rate   | 0.00939  |
|    step_size       | 0.000114 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.67
SELFPLAY: new best model, bumping up generation to 134
Ep done - 315000.
Eval num_timesteps=7696000, episode_reward=-0.01 +/- 1.00
Episode length: 29.98 +/- 0.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.01    |
| rollout/           |          |
|    return_std      | 5.5      |
| time/              |          |
|    total_timesteps | 7696000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 426      |
|    learning_rate   | 0.00932  |
|    step_size       | 0.000141 |
---------------------------------
Ep done - 316000.
Eval num_timesteps=7722000, episode_reward=0.03 +/- 0.97
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.03     |
| rollout/           |          |
|    return_std      | 6.09     |
| time/              |          |
|    total_timesteps | 7722000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 427      |
|    learning_rate   | 0.00925  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 317000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.0767  |
|    return_std      | 6.34     |
| time/              |          |
|    fps             | 641      |
|    time_elapsed    | 12065    |
|    total_timesteps | 7741466  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 429      |
|    learning_rate   | 0.00911  |
|    step_size       | 0.00012  |
---------------------------------
Eval num_timesteps=7748000, episode_reward=-0.02 +/- 0.96
Episode length: 29.96 +/- 0.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.02    |
| time/              |          |
|    total_timesteps | 7748000  |
---------------------------------
Ep done - 318000.
Eval num_timesteps=7774000, episode_reward=-0.64 +/- 0.60
Episode length: 29.48 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.64    |
| rollout/           |          |
|    return_std      | 6.55     |
| time/              |          |
|    total_timesteps | 7774000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 430      |
|    learning_rate   | 0.00903  |
|    step_size       | 0.000115 |
---------------------------------
Ep done - 319000.
Eval num_timesteps=7800000, episode_reward=0.14 +/- 0.99
Episode length: 29.98 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.14     |
| rollout/           |          |
|    return_std      | 4.96     |
| time/              |          |
|    total_timesteps | 7800000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 432      |
|    learning_rate   | 0.00889  |
|    step_size       | 0.000149 |
---------------------------------
Ep done - 320000.
Ep done - 321000.
Eval num_timesteps=7826000, episode_reward=0.29 +/- 0.84
Episode length: 30.02 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.295    |
| rollout/           |          |
|    return_std      | 6.75     |
| time/              |          |
|    total_timesteps | 7826000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 433      |
|    learning_rate   | 0.00882  |
|    step_size       | 0.000109 |
---------------------------------
Ep done - 322000.
Eval num_timesteps=7852000, episode_reward=0.60 +/- 0.80
Episode length: 29.50 +/- 0.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | 0.6      |
| rollout/           |          |
|    return_std      | 5.94     |
| time/              |          |
|    total_timesteps | 7852000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 435      |
|    learning_rate   | 0.00867  |
|    step_size       | 0.000122 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6
SELFPLAY: new best model, bumping up generation to 135
Ep done - 323000.
Eval num_timesteps=7878000, episode_reward=-0.04 +/- 1.00
Episode length: 30.01 +/- 0.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.04    |
| rollout/           |          |
|    return_std      | 7.3      |
| time/              |          |
|    total_timesteps | 7878000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 436      |
|    learning_rate   | 0.0086   |
|    step_size       | 9.82e-05 |
---------------------------------
Ep done - 324000.
Eval num_timesteps=7904000, episode_reward=0.94 +/- 0.34
Episode length: 30.04 +/- 0.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.94     |
| rollout/           |          |
|    return_std      | 6.81     |
| time/              |          |
|    total_timesteps | 7904000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 438      |
|    learning_rate   | 0.00846  |
|    step_size       | 0.000104 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.94
SELFPLAY: new best model, bumping up generation to 136
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.183   |
|    return_std      | 6.23     |
| time/              |          |
|    fps             | 641      |
|    time_elapsed    | 12345    |
|    total_timesteps | 7921234  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 439      |
|    learning_rate   | 0.00839  |
|    step_size       | 0.000112 |
---------------------------------
Ep done - 325000.
Eval num_timesteps=7930000, episode_reward=-0.06 +/- 1.00
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.06    |
| time/              |          |
|    total_timesteps | 7930000  |
---------------------------------
Ep done - 326000.
Eval num_timesteps=7956000, episode_reward=-0.74 +/- 0.67
Episode length: 29.61 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.74    |
| rollout/           |          |
|    return_std      | 6.4      |
| time/              |          |
|    total_timesteps | 7956000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 440      |
|    learning_rate   | 0.00832  |
|    step_size       | 0.000108 |
---------------------------------
Ep done - 327000.
Eval num_timesteps=7982000, episode_reward=-0.76 +/- 0.65
Episode length: 29.61 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.76    |
| rollout/           |          |
|    return_std      | 5.96     |
| time/              |          |
|    total_timesteps | 7982000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 442      |
|    learning_rate   | 0.00817  |
|    step_size       | 0.000114 |
---------------------------------
Ep done - 328000.
Eval num_timesteps=8008000, episode_reward=-0.64 +/- 0.77
Episode length: 29.65 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.64    |
| rollout/           |          |
|    return_std      | 6.77     |
| time/              |          |
|    total_timesteps | 8008000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 443      |
|    learning_rate   | 0.0081   |
|    step_size       | 9.98e-05 |
---------------------------------
Ep done - 329000.
Eval num_timesteps=8034000, episode_reward=0.19 +/- 0.98
Episode length: 29.98 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.19     |
| rollout/           |          |
|    return_std      | 6.4      |
| time/              |          |
|    total_timesteps | 8034000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 445      |
|    learning_rate   | 0.00796  |
|    step_size       | 0.000104 |
---------------------------------
Ep done - 330000.
Eval num_timesteps=8060000, episode_reward=0.07 +/- 1.00
Episode length: 29.93 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.07     |
| rollout/           |          |
|    return_std      | 5.95     |
| time/              |          |
|    total_timesteps | 8060000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 446      |
|    learning_rate   | 0.00788  |
|    step_size       | 0.00011  |
---------------------------------
Ep done - 331000.
Eval num_timesteps=8086000, episode_reward=-0.06 +/- 1.00
Episode length: 29.64 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.06    |
| rollout/           |          |
|    return_std      | 7.58     |
| time/              |          |
|    total_timesteps | 8086000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 448      |
|    learning_rate   | 0.00774  |
|    step_size       | 8.51e-05 |
---------------------------------
Ep done - 332000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.0333   |
|    return_std      | 5.29     |
| time/              |          |
|    fps             | 641      |
|    time_elapsed    | 12626    |
|    total_timesteps | 8100574  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 449      |
|    learning_rate   | 0.00767  |
|    step_size       | 0.000121 |
---------------------------------
Eval num_timesteps=8112000, episode_reward=0.11 +/- 0.99
Episode length: 29.95 +/- 0.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.11     |
| time/              |          |
|    total_timesteps | 8112000  |
---------------------------------
Ep done - 333000.
Eval num_timesteps=8138000, episode_reward=0.48 +/- 0.88
Episode length: 30.12 +/- 0.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.48     |
| rollout/           |          |
|    return_std      | 6.25     |
| time/              |          |
|    total_timesteps | 8138000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 451      |
|    learning_rate   | 0.00753  |
|    step_size       | 0.0001   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.48
SELFPLAY: new best model, bumping up generation to 137
Ep done - 334000.
Eval num_timesteps=8164000, episode_reward=0.04 +/- 1.00
Episode length: 30.05 +/- 0.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.035    |
| rollout/           |          |
|    return_std      | 5.51     |
| time/              |          |
|    total_timesteps | 8164000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 452      |
|    learning_rate   | 0.00745  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 335000.
Eval num_timesteps=8190000, episode_reward=0.21 +/- 0.98
Episode length: 30.06 +/- 0.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.21     |
| rollout/           |          |
|    return_std      | 6.99     |
| time/              |          |
|    total_timesteps | 8190000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 453      |
|    learning_rate   | 0.00738  |
|    step_size       | 8.8e-05  |
---------------------------------
Ep done - 336000.
Ep done - 337000.
Eval num_timesteps=8216000, episode_reward=0.27 +/- 0.96
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.27     |
| rollout/           |          |
|    return_std      | 5.57     |
| time/              |          |
|    total_timesteps | 8216000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 455      |
|    learning_rate   | 0.00724  |
|    step_size       | 0.000108 |
---------------------------------
Ep done - 338000.
Eval num_timesteps=8242000, episode_reward=0.94 +/- 0.33
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.935    |
| rollout/           |          |
|    return_std      | 5.49     |
| time/              |          |
|    total_timesteps | 8242000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 456      |
|    learning_rate   | 0.00717  |
|    step_size       | 0.000109 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 138
Ep done - 339000.
Eval num_timesteps=8268000, episode_reward=-0.35 +/- 0.89
Episode length: 29.98 +/- 0.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.35    |
| rollout/           |          |
|    return_std      | 6.14     |
| time/              |          |
|    total_timesteps | 8268000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 458      |
|    learning_rate   | 0.00702  |
|    step_size       | 9.53e-05 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.00833 |
|    return_std      | 6.71     |
| time/              |          |
|    fps             | 641      |
|    time_elapsed    | 12906    |
|    total_timesteps | 8280221  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 459      |
|    learning_rate   | 0.00695  |
|    step_size       | 8.63e-05 |
---------------------------------
Ep done - 340000.
Eval num_timesteps=8294000, episode_reward=0.01 +/- 1.00
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.01     |
| time/              |          |
|    total_timesteps | 8294000  |
---------------------------------
Ep done - 341000.
Eval num_timesteps=8320000, episode_reward=0.06 +/- 1.00
Episode length: 29.81 +/- 0.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | 0.06     |
| rollout/           |          |
|    return_std      | 5.96     |
| time/              |          |
|    total_timesteps | 8320000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 461      |
|    learning_rate   | 0.00681  |
|    step_size       | 9.51e-05 |
---------------------------------
Ep done - 342000.
Eval num_timesteps=8346000, episode_reward=0.13 +/- 0.99
Episode length: 29.98 +/- 0.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.13     |
| rollout/           |          |
|    return_std      | 4.58     |
| time/              |          |
|    total_timesteps | 8346000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 462      |
|    learning_rate   | 0.00674  |
|    step_size       | 0.000122 |
---------------------------------
Ep done - 343000.
Eval num_timesteps=8372000, episode_reward=0.25 +/- 0.91
Episode length: 29.83 +/- 0.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | 0.25     |
| rollout/           |          |
|    return_std      | 4.04     |
| time/              |          |
|    total_timesteps | 8372000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 464      |
|    learning_rate   | 0.00659  |
|    step_size       | 0.000136 |
---------------------------------
Ep done - 344000.
Eval num_timesteps=8398000, episode_reward=-0.06 +/- 0.96
Episode length: 29.93 +/- 0.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | -0.06    |
| rollout/           |          |
|    return_std      | 6.88     |
| time/              |          |
|    total_timesteps | 8398000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 465      |
|    learning_rate   | 0.00652  |
|    step_size       | 7.9e-05  |
---------------------------------
Ep done - 345000.
Eval num_timesteps=8424000, episode_reward=0.69 +/- 0.72
Episode length: 30.32 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.695    |
| rollout/           |          |
|    return_std      | 5.66     |
| time/              |          |
|    total_timesteps | 8424000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 466      |
|    learning_rate   | 0.00645  |
|    step_size       | 9.49e-05 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.695
SELFPLAY: new best model, bumping up generation to 139
Ep done - 346000.
Eval num_timesteps=8450000, episode_reward=-0.10 +/- 0.89
Episode length: 29.99 +/- 0.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.095   |
| rollout/           |          |
|    return_std      | 6.99     |
| time/              |          |
|    total_timesteps | 8450000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 468      |
|    learning_rate   | 0.0063   |
|    step_size       | 7.51e-05 |
---------------------------------
Ep done - 347000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.0317  |
|    return_std      | 6.55     |
| time/              |          |
|    fps             | 641      |
|    time_elapsed    | 13192    |
|    total_timesteps | 8460063  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 469      |
|    learning_rate   | 0.00623  |
|    step_size       | 7.92e-05 |
---------------------------------
Eval num_timesteps=8476000, episode_reward=0.66 +/- 0.75
Episode length: 29.75 +/- 0.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.7     |
|    mean_reward     | 0.66     |
| time/              |          |
|    total_timesteps | 8476000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.66
SELFPLAY: new best model, bumping up generation to 140
Ep done - 348000.
Eval num_timesteps=8502000, episode_reward=-0.23 +/- 0.97
Episode length: 29.57 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.23    |
| rollout/           |          |
|    return_std      | 7.4      |
| time/              |          |
|    total_timesteps | 8502000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 471      |
|    learning_rate   | 0.00609  |
|    step_size       | 6.86e-05 |
---------------------------------
Ep done - 349000.
Eval num_timesteps=8528000, episode_reward=0.81 +/- 0.59
Episode length: 30.05 +/- 0.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.81     |
| rollout/           |          |
|    return_std      | 6.39     |
| time/              |          |
|    total_timesteps | 8528000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 472      |
|    learning_rate   | 0.00602  |
|    step_size       | 7.84e-05 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.81
SELFPLAY: new best model, bumping up generation to 141
Ep done - 350000.
Eval num_timesteps=8554000, episode_reward=-0.08 +/- 1.00
Episode length: 30.09 +/- 0.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | -0.08    |
| rollout/           |          |
|    return_std      | 7.45     |
| time/              |          |
|    total_timesteps | 8554000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 474      |
|    learning_rate   | 0.00587  |
|    step_size       | 6.57e-05 |
---------------------------------
Ep done - 351000.
Ep done - 352000.
Eval num_timesteps=8580000, episode_reward=-0.01 +/- 1.00
Episode length: 30.14 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | -0.01    |
| rollout/           |          |
|    return_std      | 7.12     |
| time/              |          |
|    total_timesteps | 8580000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 475      |
|    learning_rate   | 0.0058   |
|    step_size       | 6.79e-05 |
---------------------------------
Ep done - 353000.
Eval num_timesteps=8606000, episode_reward=0.31 +/- 0.95
Episode length: 29.91 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.31     |
| rollout/           |          |
|    return_std      | 5.86     |
| time/              |          |
|    total_timesteps | 8606000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 477      |
|    learning_rate   | 0.00566  |
|    step_size       | 8.05e-05 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 142
Ep done - 354000.
Eval num_timesteps=8632000, episode_reward=0.10 +/- 0.99
Episode length: 30.08 +/- 0.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.1      |
| rollout/           |          |
|    return_std      | 5.95     |
| time/              |          |
|    total_timesteps | 8632000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 478      |
|    learning_rate   | 0.00558  |
|    step_size       | 7.82e-05 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
|    return_std      | 6.73     |
| time/              |          |
|    fps             | 640      |
|    time_elapsed    | 13486    |
|    total_timesteps | 8639810  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 479      |
|    learning_rate   | 0.00551  |
|    step_size       | 6.83e-05 |
---------------------------------
Ep done - 355000.
Eval num_timesteps=8658000, episode_reward=-0.87 +/- 0.49
Episode length: 29.57 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.87    |
| rollout/           |          |
|    return_std      | 6.35     |
| time/              |          |
|    total_timesteps | 8658000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 480      |
|    learning_rate   | 0.00544  |
|    step_size       | 7.14e-05 |
---------------------------------
Ep done - 356000.
Eval num_timesteps=8684000, episode_reward=-0.91 +/- 0.41
Episode length: 29.55 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.91    |
| rollout/           |          |
|    return_std      | 5.91     |
| time/              |          |
|    total_timesteps | 8684000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 481      |
|    learning_rate   | 0.00537  |
|    step_size       | 7.57e-05 |
---------------------------------
Ep done - 357000.
Eval num_timesteps=8710000, episode_reward=-0.89 +/- 0.46
Episode length: 29.55 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.89    |
| rollout/           |          |
|    return_std      | 5.45     |
| time/              |          |
|    total_timesteps | 8710000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 482      |
|    learning_rate   | 0.0053   |
|    step_size       | 8.1e-05  |
---------------------------------
Ep done - 358000.
Eval num_timesteps=8736000, episode_reward=-0.76 +/- 0.65
Episode length: 29.61 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.6     |
|    mean_reward     | -0.76    |
| rollout/           |          |
|    return_std      | 6.79     |
| time/              |          |
|    total_timesteps | 8736000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 484      |
|    learning_rate   | 0.00515  |
|    step_size       | 6.32e-05 |
---------------------------------
Ep done - 359000.
Eval num_timesteps=8762000, episode_reward=0.09 +/- 1.00
Episode length: 30.05 +/- 0.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.09     |
| rollout/           |          |
|    return_std      | 5.12     |
| time/              |          |
|    total_timesteps | 8762000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 485      |
|    learning_rate   | 0.00508  |
|    step_size       | 8.26e-05 |
---------------------------------
Ep done - 360000.
Eval num_timesteps=8788000, episode_reward=0.74 +/- 0.67
Episode length: 30.37 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | 0.74     |
| rollout/           |          |
|    return_std      | 6.81     |
| time/              |          |
|    total_timesteps | 8788000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 487      |
|    learning_rate   | 0.00494  |
|    step_size       | 6.04e-05 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.74
SELFPLAY: new best model, bumping up generation to 143
Ep done - 361000.
Eval num_timesteps=8814000, episode_reward=0.03 +/- 0.99
Episode length: 29.93 +/- 0.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.025    |
| rollout/           |          |
|    return_std      | 5.72     |
| time/              |          |
|    total_timesteps | 8814000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 488      |
|    learning_rate   | 0.00487  |
|    step_size       | 7.09e-05 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.0967  |
|    return_std      | 5.97     |
| time/              |          |
|    fps             | 639      |
|    time_elapsed    | 13780    |
|    total_timesteps | 8819197  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 489      |
|    learning_rate   | 0.0048   |
|    step_size       | 6.69e-05 |
---------------------------------
Ep done - 362000.
Eval num_timesteps=8840000, episode_reward=0.06 +/- 1.00
Episode length: 30.02 +/- 0.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.055    |
| rollout/           |          |
|    return_std      | 6.27     |
| time/              |          |
|    total_timesteps | 8840000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 490      |
|    learning_rate   | 0.00472  |
|    step_size       | 6.27e-05 |
---------------------------------
Ep done - 363000.
Eval num_timesteps=8866000, episode_reward=0.23 +/- 0.96
Episode length: 30.11 +/- 0.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.23     |
| rollout/           |          |
|    return_std      | 6.19     |
| time/              |          |
|    total_timesteps | 8866000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 491      |
|    learning_rate   | 0.00465  |
|    step_size       | 6.26e-05 |
---------------------------------
Ep done - 364000.
Eval num_timesteps=8892000, episode_reward=-0.47 +/- 0.53
Episode length: 30.02 +/- 0.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.47    |
| rollout/           |          |
|    return_std      | 6.65     |
| time/              |          |
|    total_timesteps | 8892000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 493      |
|    learning_rate   | 0.00451  |
|    step_size       | 5.65e-05 |
---------------------------------
Ep done - 365000.
Eval num_timesteps=8918000, episode_reward=-0.46 +/- 0.54
Episode length: 30.02 +/- 0.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.455   |
| rollout/           |          |
|    return_std      | 6.18     |
| time/              |          |
|    total_timesteps | 8918000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 494      |
|    learning_rate   | 0.00444  |
|    step_size       | 5.98e-05 |
---------------------------------
Ep done - 366000.
Eval num_timesteps=8944000, episode_reward=0.17 +/- 0.98
Episode length: 30.59 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.6     |
|    mean_reward     | 0.175    |
| rollout/           |          |
|    return_std      | 7.69     |
| time/              |          |
|    total_timesteps | 8944000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 495      |
|    learning_rate   | 0.00436  |
|    step_size       | 4.73e-05 |
---------------------------------
Ep done - 367000.
Ep done - 368000.
Eval num_timesteps=8970000, episode_reward=0.13 +/- 0.99
Episode length: 30.07 +/- 0.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.13     |
| rollout/           |          |
|    return_std      | 6.61     |
| time/              |          |
|    total_timesteps | 8970000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 497      |
|    learning_rate   | 0.00422  |
|    step_size       | 5.32e-05 |
---------------------------------
Ep done - 369000.
Eval num_timesteps=8996000, episode_reward=0.10 +/- 0.99
Episode length: 30.05 +/- 0.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.1      |
| rollout/           |          |
|    return_std      | 6.47     |
| time/              |          |
|    total_timesteps | 8996000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 498      |
|    learning_rate   | 0.00415  |
|    step_size       | 5.35e-05 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.00167  |
|    return_std      | 6.32     |
| time/              |          |
|    fps             | 639      |
|    time_elapsed    | 14081    |
|    total_timesteps | 8998662  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 499      |
|    learning_rate   | 0.00408  |
|    step_size       | 5.38e-05 |
---------------------------------
Ep done - 370000.
Eval num_timesteps=9022000, episode_reward=0.01 +/- 1.00
Episode length: 30.04 +/- 0.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.015    |
| rollout/           |          |
|    return_std      | 6.68     |
| time/              |          |
|    total_timesteps | 9022000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 500      |
|    learning_rate   | 0.00401  |
|    step_size       | 5e-05    |
---------------------------------
Ep done - 371000.
Eval num_timesteps=9048000, episode_reward=0.67 +/- 0.74
Episode length: 30.02 +/- 0.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.67     |
| rollout/           |          |
|    return_std      | 7.27     |
| time/              |          |
|    total_timesteps | 9048000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 501      |
|    learning_rate   | 0.00393  |
|    step_size       | 4.51e-05 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.67
SELFPLAY: new best model, bumping up generation to 144
Ep done - 372000.
Eval num_timesteps=9074000, episode_reward=0.14 +/- 0.99
Episode length: 30.00 +/- 0.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.14     |
| rollout/           |          |
|    return_std      | 5.79     |
| time/              |          |
|    total_timesteps | 9074000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 503      |
|    learning_rate   | 0.00379  |
|    step_size       | 5.45e-05 |
---------------------------------
Ep done - 373000.
Eval num_timesteps=9100000, episode_reward=-0.01 +/- 1.00
Episode length: 29.98 +/- 0.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.01    |
| rollout/           |          |
|    return_std      | 6.45     |
| time/              |          |
|    total_timesteps | 9100000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 504      |
|    learning_rate   | 0.00372  |
|    step_size       | 4.8e-05  |
---------------------------------
Ep done - 374000.
Eval num_timesteps=9126000, episode_reward=0.00 +/- 1.00
Episode length: 29.98 +/- 0.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0        |
| rollout/           |          |
|    return_std      | 7.34     |
| time/              |          |
|    total_timesteps | 9126000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 506      |
|    learning_rate   | 0.00357  |
|    step_size       | 4.06e-05 |
---------------------------------
Ep done - 375000.
Eval num_timesteps=9152000, episode_reward=-0.02 +/- 1.00
Episode length: 29.52 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.02    |
| rollout/           |          |
|    return_std      | 6.87     |
| time/              |          |
|    total_timesteps | 9152000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 507      |
|    learning_rate   | 0.0035   |
|    step_size       | 4.25e-05 |
---------------------------------
Ep done - 376000.
Eval num_timesteps=9178000, episode_reward=-0.01 +/- 1.00
Episode length: 29.50 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.01    |
| rollout/           |          |
|    return_std      | 6.74     |
| time/              |          |
|    total_timesteps | 9178000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 508      |
|    learning_rate   | 0.00343  |
|    step_size       | 4.24e-05 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.162    |
|    return_std      | 7.6      |
| time/              |          |
|    fps             | 637      |
|    time_elapsed    | 14387    |
|    total_timesteps | 9178361  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 509      |
|    learning_rate   | 0.00336  |
|    step_size       | 3.68e-05 |
---------------------------------
Ep done - 377000.
Eval num_timesteps=9204000, episode_reward=-0.12 +/- 0.99
Episode length: 29.49 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.12    |
| rollout/           |          |
|    return_std      | 8.06     |
| time/              |          |
|    total_timesteps | 9204000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 510      |
|    learning_rate   | 0.00329  |
|    step_size       | 3.4e-05  |
---------------------------------
Ep done - 378000.
Eval num_timesteps=9230000, episode_reward=-0.03 +/- 1.00
Episode length: 29.52 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.03    |
| rollout/           |          |
|    return_std      | 6.71     |
| time/              |          |
|    total_timesteps | 9230000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 511      |
|    learning_rate   | 0.00321  |
|    step_size       | 3.99e-05 |
---------------------------------
Ep done - 379000.
Eval num_timesteps=9256000, episode_reward=-0.01 +/- 1.00
Episode length: 29.52 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.01    |
| rollout/           |          |
|    return_std      | 6.72     |
| time/              |          |
|    total_timesteps | 9256000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 513      |
|    learning_rate   | 0.00307  |
|    step_size       | 3.81e-05 |
---------------------------------
Ep done - 380000.
Eval num_timesteps=9282000, episode_reward=-0.06 +/- 1.00
Episode length: 29.48 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.06    |
| rollout/           |          |
|    return_std      | 5.91     |
| time/              |          |
|    total_timesteps | 9282000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 514      |
|    learning_rate   | 0.003    |
|    step_size       | 4.23e-05 |
---------------------------------
Ep done - 381000.
Eval num_timesteps=9308000, episode_reward=-0.04 +/- 1.00
Episode length: 29.52 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.04    |
| rollout/           |          |
|    return_std      | 5.48     |
| time/              |          |
|    total_timesteps | 9308000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 516      |
|    learning_rate   | 0.00286  |
|    step_size       | 4.34e-05 |
---------------------------------
Ep done - 382000.
Eval num_timesteps=9334000, episode_reward=-0.02 +/- 1.00
Episode length: 29.52 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -0.02    |
| rollout/           |          |
|    return_std      | 6.62     |
| time/              |          |
|    total_timesteps | 9334000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 517      |
|    learning_rate   | 0.00278  |
|    step_size       | 3.5e-05  |
---------------------------------
Ep done - 383000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.075    |
|    return_std      | 4.97     |
| time/              |          |
|    fps             | 636      |
|    time_elapsed    | 14694    |
|    total_timesteps | 9358082  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 519      |
|    learning_rate   | 0.00264  |
|    step_size       | 4.43e-05 |
---------------------------------
Ep done - 384000.
Eval num_timesteps=9360000, episode_reward=0.90 +/- 0.44
Episode length: 30.16 +/- 0.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 9360000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 145
Ep done - 385000.
Eval num_timesteps=9386000, episode_reward=0.29 +/- 0.47
Episode length: 30.00 +/- 0.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.295    |
| rollout/           |          |
|    return_std      | 6.88     |
| time/              |          |
|    total_timesteps | 9386000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 520      |
|    learning_rate   | 0.00257  |
|    step_size       | 3.11e-05 |
---------------------------------
Ep done - 386000.
Eval num_timesteps=9412000, episode_reward=-0.04 +/- 0.77
Episode length: 29.98 +/- 0.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.035   |
| rollout/           |          |
|    return_std      | 6.54     |
| time/              |          |
|    total_timesteps | 9412000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 522      |
|    learning_rate   | 0.00242  |
|    step_size       | 3.09e-05 |
---------------------------------
Ep done - 387000.
Eval num_timesteps=9438000, episode_reward=-0.07 +/- 0.76
Episode length: 30.01 +/- 0.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.065   |
| rollout/           |          |
|    return_std      | 6.88     |
| time/              |          |
|    total_timesteps | 9438000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 523      |
|    learning_rate   | 0.00235  |
|    step_size       | 2.85e-05 |
---------------------------------
Ep done - 388000.
Eval num_timesteps=9464000, episode_reward=0.01 +/- 0.75
Episode length: 30.00 +/- 0.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.015    |
| rollout/           |          |
|    return_std      | 6.71     |
| time/              |          |
|    total_timesteps | 9464000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 524      |
|    learning_rate   | 0.00228  |
|    step_size       | 2.83e-05 |
---------------------------------
Ep done - 389000.
Eval num_timesteps=9490000, episode_reward=0.12 +/- 0.77
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.12     |
| rollout/           |          |
|    return_std      | 6.85     |
| time/              |          |
|    total_timesteps | 9490000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 526      |
|    learning_rate   | 0.00214  |
|    step_size       | 2.6e-05  |
---------------------------------
Ep done - 390000.
Eval num_timesteps=9516000, episode_reward=-0.04 +/- 0.75
Episode length: 30.00 +/- 0.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.045   |
| rollout/           |          |
|    return_std      | 6.57     |
| time/              |          |
|    total_timesteps | 9516000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 527      |
|    learning_rate   | 0.00206  |
|    step_size       | 2.62e-05 |
---------------------------------
Ep done - 391000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.00167  |
|    return_std      | 5.37     |
| time/              |          |
|    fps             | 636      |
|    time_elapsed    | 14988    |
|    total_timesteps | 9537874  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 529      |
|    learning_rate   | 0.00192  |
|    step_size       | 2.98e-05 |
---------------------------------
Eval num_timesteps=9542000, episode_reward=0.04 +/- 0.78
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.035    |
| time/              |          |
|    total_timesteps | 9542000  |
---------------------------------
Ep done - 392000.
Eval num_timesteps=9568000, episode_reward=-0.10 +/- 0.74
Episode length: 30.00 +/- 0.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.095   |
| rollout/           |          |
|    return_std      | 6.14     |
| time/              |          |
|    total_timesteps | 9568000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 530      |
|    learning_rate   | 0.00185  |
|    step_size       | 2.51e-05 |
---------------------------------
Ep done - 393000.
Eval num_timesteps=9594000, episode_reward=-0.06 +/- 0.77
Episode length: 30.00 +/- 0.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.055   |
| rollout/           |          |
|    return_std      | 6.49     |
| time/              |          |
|    total_timesteps | 9594000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 532      |
|    learning_rate   | 0.0017   |
|    step_size       | 2.19e-05 |
---------------------------------
Ep done - 394000.
Eval num_timesteps=9620000, episode_reward=-0.04 +/- 0.76
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.04    |
| rollout/           |          |
|    return_std      | 6.75     |
| time/              |          |
|    total_timesteps | 9620000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 533      |
|    learning_rate   | 0.00163  |
|    step_size       | 2.02e-05 |
---------------------------------
Ep done - 395000.
Eval num_timesteps=9646000, episode_reward=-0.04 +/- 0.77
Episode length: 30.01 +/- 0.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.04    |
| rollout/           |          |
|    return_std      | 4.9      |
| time/              |          |
|    total_timesteps | 9646000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 535      |
|    learning_rate   | 0.00149  |
|    step_size       | 2.53e-05 |
---------------------------------
Ep done - 396000.
Eval num_timesteps=9672000, episode_reward=-0.03 +/- 0.76
Episode length: 30.01 +/- 0.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.025   |
| rollout/           |          |
|    return_std      | 6.32     |
| time/              |          |
|    total_timesteps | 9672000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 536      |
|    learning_rate   | 0.00142  |
|    step_size       | 1.87e-05 |
---------------------------------
Ep done - 397000.
Eval num_timesteps=9698000, episode_reward=0.02 +/- 0.75
Episode length: 30.00 +/- 0.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.02     |
| rollout/           |          |
|    return_std      | 5.63     |
| time/              |          |
|    total_timesteps | 9698000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 537      |
|    learning_rate   | 0.00135  |
|    step_size       | 1.99e-05 |
---------------------------------
Ep done - 398000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.132   |
|    return_std      | 5.09     |
| time/              |          |
|    fps             | 635      |
|    time_elapsed    | 15289    |
|    total_timesteps | 9717440  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 539      |
|    learning_rate   | 0.0012   |
|    step_size       | 1.97e-05 |
---------------------------------
Ep done - 399000.
Eval num_timesteps=9724000, episode_reward=-0.03 +/- 0.75
Episode length: 30.02 +/- 0.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.03    |
| time/              |          |
|    total_timesteps | 9724000  |
---------------------------------
Ep done - 400000.
Eval num_timesteps=9750000, episode_reward=-0.05 +/- 0.75
Episode length: 29.99 +/- 0.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.05    |
| rollout/           |          |
|    return_std      | 6.59     |
| time/              |          |
|    total_timesteps | 9750000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 540      |
|    learning_rate   | 0.00113  |
|    step_size       | 1.43e-05 |
---------------------------------
Ep done - 401000.
Eval num_timesteps=9776000, episode_reward=-0.06 +/- 0.77
Episode length: 30.02 +/- 0.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.06    |
| rollout/           |          |
|    return_std      | 5.25     |
| time/              |          |
|    total_timesteps | 9776000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 542      |
|    learning_rate   | 0.000987 |
|    step_size       | 1.56e-05 |
---------------------------------
Ep done - 402000.
Eval num_timesteps=9802000, episode_reward=-0.10 +/- 0.76
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.095   |
| rollout/           |          |
|    return_std      | 5.68     |
| time/              |          |
|    total_timesteps | 9802000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 543      |
|    learning_rate   | 0.000915 |
|    step_size       | 1.34e-05 |
---------------------------------
Ep done - 403000.
Eval num_timesteps=9828000, episode_reward=-0.03 +/- 0.75
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | -0.03    |
| rollout/           |          |
|    return_std      | 6.69     |
| time/              |          |
|    total_timesteps | 9828000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 545      |
|    learning_rate   | 0.000771 |
|    step_size       | 9.6e-06  |
---------------------------------
Ep done - 404000.
Eval num_timesteps=9854000, episode_reward=0.26 +/- 0.44
Episode length: 30.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.255    |
| rollout/           |          |
|    return_std      | 6.4      |
| time/              |          |
|    total_timesteps | 9854000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 546      |
|    learning_rate   | 0.000699 |
|    step_size       | 9.1e-06  |
---------------------------------
Ep done - 405000.
Eval num_timesteps=9880000, episode_reward=0.21 +/- 0.43
Episode length: 30.00 +/- 0.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.215    |
| rollout/           |          |
|    return_std      | 6.03     |
| time/              |          |
|    total_timesteps | 9880000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 548      |
|    learning_rate   | 0.000555 |
|    step_size       | 7.67e-06 |
---------------------------------
Ep done - 406000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.00333 |
|    return_std      | 7.8      |
| time/              |          |
|    fps             | 634      |
|    time_elapsed    | 15595    |
|    total_timesteps | 9897103  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 549      |
|    learning_rate   | 0.000483 |
|    step_size       | 5.16e-06 |
---------------------------------
Eval num_timesteps=9906000, episode_reward=0.29 +/- 0.46
Episode length: 30.00 +/- 0.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.295    |
| time/              |          |
|    total_timesteps | 9906000  |
---------------------------------
Ep done - 407000.
Eval num_timesteps=9932000, episode_reward=0.27 +/- 0.46
Episode length: 29.98 +/- 0.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.27     |
| rollout/           |          |
|    return_std      | 6.09     |
| time/              |          |
|    total_timesteps | 9932000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 550      |
|    learning_rate   | 0.000412 |
|    step_size       | 5.63e-06 |
---------------------------------
Ep done - 408000.
Eval num_timesteps=9958000, episode_reward=0.23 +/- 0.44
Episode length: 30.00 +/- 0.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.225    |
| rollout/           |          |
|    return_std      | 6.16     |
| time/              |          |
|    total_timesteps | 9958000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 552      |
|    learning_rate   | 0.000268 |
|    step_size       | 3.62e-06 |
---------------------------------
Ep done - 409000.
Eval num_timesteps=9984000, episode_reward=0.24 +/- 0.46
Episode length: 29.98 +/- 0.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.245    |
| rollout/           |          |
|    return_std      | 5.83     |
| time/              |          |
|    total_timesteps | 9984000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 553      |
|    learning_rate   | 0.000196 |
|    step_size       | 2.8e-06  |
---------------------------------
/home/student/pantrasa/project/venv2/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.[0m
  logger.warn(
Ep done - 410000.
Elapsed time: 4h 23m 13s
