CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
seed: 369 
num_timesteps: 50000000 
eval_freq: 50000 
eval_episoded: 400 
best_threshold: 0.4 
logdir: scripts/rl/output/phase2/ars/mlp/base-v2/ 
continueFrom_model: scripts/rl/output/phase2/ars/mlp/base/history_0140
CUDA available: True

params: {'n_delta': 30, 'n_top': 6, 'zero_policy': False, 'n_eval_episodes': 20, 'delta_std': 0.03, 'learning_rate': <__main__.LinearSchedule object at 0x7fc58f750b50>, 'verbose': 1, 'seed': 369, 'policy_class': <class '__main__.CustomMlpPolicy'>}

Ep done - 1000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.532   |
|    return_std      | 13.9     |
| time/              |          |
|    fps             | 588      |
|    time_elapsed    | 61       |
|    total_timesteps | 35961    |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 470      |
|    learning_rate   | 0.004    |
|    step_size       | 4.8e-05  |
---------------------------------
Ep done - 2000.
Eval num_timesteps=50000, episode_reward=-1.00 +/- 0.00
Episode length: 29.54 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.5     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.481   |
|    return_std      | 12.5     |
| time/              |          |
|    fps             | 498      |
|    time_elapsed    | 144      |
|    total_timesteps | 71874    |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 471      |
|    learning_rate   | 0.004    |
|    step_size       | 5.32e-05 |
---------------------------------
Ep done - 3000.
Ep done - 4000.
Eval num_timesteps=100000, episode_reward=0.68 +/- 0.72
Episode length: 29.96 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.677    |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6775
SELFPLAY: new best model, bumping up generation to 1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.419   |
|    return_std      | 13.3     |
| time/              |          |
|    fps             | 479      |
|    time_elapsed    | 224      |
|    total_timesteps | 107785   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 472      |
|    learning_rate   | 0.00399  |
|    step_size       | 5.02e-05 |
---------------------------------
Ep done - 5000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.102    |
|    return_std      | 7.74     |
| time/              |          |
|    fps             | 504      |
|    time_elapsed    | 284      |
|    total_timesteps | 143743   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 473      |
|    learning_rate   | 0.00399  |
|    step_size       | 8.6e-05  |
---------------------------------
Ep done - 6000.
Eval num_timesteps=150000, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.065    |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
Ep done - 7000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.0608   |
|    return_std      | 6.45     |
| time/              |          |
|    fps             | 487      |
|    time_elapsed    | 368      |
|    total_timesteps | 179731   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 474      |
|    learning_rate   | 0.00399  |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 8000.
Eval num_timesteps=200000, episode_reward=0.24 +/- 0.95
Episode length: 29.94 +/- 1.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.242    |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.0817   |
|    return_std      | 6.7      |
| time/              |          |
|    fps             | 479      |
|    time_elapsed    | 449      |
|    total_timesteps | 215676   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 475      |
|    learning_rate   | 0.00399  |
|    step_size       | 9.91e-05 |
---------------------------------
Ep done - 9000.
Ep done - 10000.
Eval num_timesteps=250000, episode_reward=0.17 +/- 0.96
Episode length: 29.94 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.175    |
| time/              |          |
|    total_timesteps | 250000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.102    |
|    return_std      | 5.9      |
| time/              |          |
|    fps             | 475      |
|    time_elapsed    | 528      |
|    total_timesteps | 251618   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 476      |
|    learning_rate   | 0.00398  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 11000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.105    |
|    return_std      | 5.78     |
| time/              |          |
|    fps             | 488      |
|    time_elapsed    | 588      |
|    total_timesteps | 287522   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 477      |
|    learning_rate   | 0.00398  |
|    step_size       | 0.000115 |
---------------------------------
Ep done - 12000.
Eval num_timesteps=300000, episode_reward=0.18 +/- 0.96
Episode length: 29.98 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.182    |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Ep done - 13000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.156    |
|    return_std      | 5.65     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 670      |
|    total_timesteps | 323521   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 478      |
|    learning_rate   | 0.00398  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 14000.
Eval num_timesteps=350000, episode_reward=0.29 +/- 0.93
Episode length: 29.96 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.295    |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
|    return_std      | 5.77     |
| time/              |          |
|    fps             | 480      |
|    time_elapsed    | 748      |
|    total_timesteps | 359501   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 479      |
|    learning_rate   | 0.00397  |
|    step_size       | 0.000115 |
---------------------------------
Ep done - 15000.
Ep done - 16000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.109    |
|    return_std      | 5.94     |
| time/              |          |
|    fps             | 487      |
|    time_elapsed    | 810      |
|    total_timesteps | 395450   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 480      |
|    learning_rate   | 0.00397  |
|    step_size       | 0.000111 |
---------------------------------
Eval num_timesteps=400000, episode_reward=0.23 +/- 0.95
Episode length: 29.97 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.235    |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Ep done - 17000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.203    |
|    return_std      | 6.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 890      |
|    total_timesteps | 431464   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 481      |
|    learning_rate   | 0.00397  |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 18000.
Eval num_timesteps=450000, episode_reward=0.27 +/- 0.95
Episode length: 29.93 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.268    |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Ep done - 19000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.278    |
|    return_std      | 7.07     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 969      |
|    total_timesteps | 467475   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 482      |
|    learning_rate   | 0.00397  |
|    step_size       | 9.34e-05 |
---------------------------------
Ep done - 20000.
Eval num_timesteps=500000, episode_reward=0.35 +/- 0.92
Episode length: 29.88 +/- 1.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.35     |
| time/              |          |
|    total_timesteps | 500000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.193    |
|    return_std      | 5.55     |
| time/              |          |
|    fps             | 478      |
|    time_elapsed    | 1051     |
|    total_timesteps | 503469   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 483      |
|    learning_rate   | 0.00396  |
|    step_size       | 0.000119 |
---------------------------------
Ep done - 21000.
Ep done - 22000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.284    |
|    return_std      | 7.46     |
| time/              |          |
|    fps             | 485      |
|    time_elapsed    | 1110     |
|    total_timesteps | 539501   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 484      |
|    learning_rate   | 0.00396  |
|    step_size       | 8.84e-05 |
---------------------------------
Eval num_timesteps=550000, episode_reward=0.36 +/- 0.92
Episode length: 29.97 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.357    |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Ep done - 23000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.212    |
|    return_std      | 5.55     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 1189     |
|    total_timesteps | 575511   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 485      |
|    learning_rate   | 0.00396  |
|    step_size       | 0.000119 |
---------------------------------
Ep done - 24000.
Eval num_timesteps=600000, episode_reward=0.32 +/- 0.93
Episode length: 29.96 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.32     |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
Ep done - 25000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.268    |
|    return_std      | 7.65     |
| time/              |          |
|    fps             | 481      |
|    time_elapsed    | 1270     |
|    total_timesteps | 611552   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 486      |
|    learning_rate   | 0.00395  |
|    step_size       | 8.61e-05 |
---------------------------------
Ep done - 26000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.272    |
|    return_std      | 7.26     |
| time/              |          |
|    fps             | 486      |
|    time_elapsed    | 1329     |
|    total_timesteps | 647546   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 487      |
|    learning_rate   | 0.00395  |
|    step_size       | 9.07e-05 |
---------------------------------
Eval num_timesteps=650000, episode_reward=0.41 +/- 0.88
Episode length: 30.00 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.41     |
| time/              |          |
|    total_timesteps | 650000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.41
SELFPLAY: new best model, bumping up generation to 2
Ep done - 27000.
Ep done - 28000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.285    |
|    return_std      | 3.59     |
| time/              |          |
|    fps             | 485      |
|    time_elapsed    | 1409     |
|    total_timesteps | 683552   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 488      |
|    learning_rate   | 0.00395  |
|    step_size       | 0.000183 |
---------------------------------
Eval num_timesteps=700000, episode_reward=0.34 +/- 0.92
Episode length: 29.94 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.9     |
|    mean_reward     | 0.343    |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
Ep done - 29000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.287    |
|    return_std      | 6.44     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 1491     |
|    total_timesteps | 719533   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 489      |
|    learning_rate   | 0.00395  |
|    step_size       | 0.000102 |
---------------------------------
Ep done - 30000.
Ep done - 31000.
Eval num_timesteps=750000, episode_reward=0.41 +/- 0.90
Episode length: 29.98 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.405    |
| time/              |          |
|    total_timesteps | 750000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.405
SELFPLAY: new best model, bumping up generation to 3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.285    |
|    return_std      | 6.04     |
| time/              |          |
|    fps             | 481      |
|    time_elapsed    | 1569     |
|    total_timesteps | 755526   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 490      |
|    learning_rate   | 0.00394  |
|    step_size       | 0.000109 |
---------------------------------
Ep done - 32000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.263    |
|    return_std      | 5.65     |
| time/              |          |
|    fps             | 485      |
|    time_elapsed    | 1629     |
|    total_timesteps | 791517   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 491      |
|    learning_rate   | 0.00394  |
|    step_size       | 0.000116 |
---------------------------------
Ep done - 33000.
Eval num_timesteps=800000, episode_reward=0.40 +/- 0.90
Episode length: 29.95 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.398    |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
Ep done - 34000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.344    |
|    return_std      | 3.63     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 1710     |
|    total_timesteps | 827518   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 492      |
|    learning_rate   | 0.00394  |
|    step_size       | 0.000181 |
---------------------------------
Ep done - 35000.
Eval num_timesteps=850000, episode_reward=0.41 +/- 0.89
Episode length: 29.95 +/- 0.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.405    |
| time/              |          |
|    total_timesteps | 850000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.405
SELFPLAY: new best model, bumping up generation to 4
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.311    |
|    return_std      | 5.74     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 1789     |
|    total_timesteps | 863494   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 493      |
|    learning_rate   | 0.00393  |
|    step_size       | 0.000114 |
---------------------------------
Ep done - 36000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.284    |
|    return_std      | 7.4      |
| time/              |          |
|    fps             | 486      |
|    time_elapsed    | 1849     |
|    total_timesteps | 899467   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 494      |
|    learning_rate   | 0.00393  |
|    step_size       | 8.85e-05 |
---------------------------------
Ep done - 37000.
Eval num_timesteps=900000, episode_reward=0.43 +/- 0.89
Episode length: 29.99 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.432    |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.4325
SELFPLAY: new best model, bumping up generation to 5
Ep done - 38000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.312    |
|    return_std      | 5.6      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 1931     |
|    total_timesteps | 935489   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 495      |
|    learning_rate   | 0.00393  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 39000.
Eval num_timesteps=950000, episode_reward=0.40 +/- 0.90
Episode length: 30.00 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
Ep done - 40000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.376    |
|    return_std      | 5.09     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 2009     |
|    total_timesteps | 971502   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 496      |
|    learning_rate   | 0.00393  |
|    step_size       | 0.000129 |
---------------------------------
Ep done - 41000.
Eval num_timesteps=1000000, episode_reward=0.52 +/- 0.83
Episode length: 30.04 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.522    |
| time/              |          |
|    total_timesteps | 1000000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5225
SELFPLAY: new best model, bumping up generation to 6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.333    |
|    return_std      | 3.7      |
| time/              |          |
|    fps             | 481      |
|    time_elapsed    | 2091     |
|    total_timesteps | 1007468  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 497      |
|    learning_rate   | 0.00392  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 42000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.338    |
|    return_std      | 4.12     |
| time/              |          |
|    fps             | 485      |
|    time_elapsed    | 2150     |
|    total_timesteps | 1043502  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 498      |
|    learning_rate   | 0.00392  |
|    step_size       | 0.000158 |
---------------------------------
Ep done - 43000.
Eval num_timesteps=1050000, episode_reward=0.48 +/- 0.86
Episode length: 29.98 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.485    |
| time/              |          |
|    total_timesteps | 1050000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.485
SELFPLAY: new best model, bumping up generation to 7
Ep done - 44000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.369    |
|    return_std      | 6.61     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 2229     |
|    total_timesteps | 1079518  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 499      |
|    learning_rate   | 0.00392  |
|    step_size       | 9.88e-05 |
---------------------------------
Ep done - 45000.
Eval num_timesteps=1100000, episode_reward=0.45 +/- 0.87
Episode length: 30.00 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.45     |
| time/              |          |
|    total_timesteps | 1100000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.45
SELFPLAY: new best model, bumping up generation to 8
Ep done - 46000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.351    |
|    return_std      | 5.5      |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 2311     |
|    total_timesteps | 1115570  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 500      |
|    learning_rate   | 0.00391  |
|    step_size       | 0.000119 |
---------------------------------
Ep done - 47000.
Eval num_timesteps=1150000, episode_reward=0.48 +/- 0.86
Episode length: 30.00 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.485    |
| time/              |          |
|    total_timesteps | 1150000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.485
SELFPLAY: new best model, bumping up generation to 9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.357    |
|    return_std      | 3.41     |
| time/              |          |
|    fps             | 481      |
|    time_elapsed    | 2390     |
|    total_timesteps | 1151555  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 501      |
|    learning_rate   | 0.00391  |
|    step_size       | 0.000191 |
---------------------------------
Ep done - 48000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.388    |
|    return_std      | 5.55     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 2449     |
|    total_timesteps | 1187552  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 502      |
|    learning_rate   | 0.00391  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 49000.
Eval num_timesteps=1200000, episode_reward=0.47 +/- 0.87
Episode length: 30.01 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.47     |
| time/              |          |
|    total_timesteps | 1200000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.47
SELFPLAY: new best model, bumping up generation to 10
Ep done - 50000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.354    |
|    return_std      | 5.68     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 2531     |
|    total_timesteps | 1223556  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 503      |
|    learning_rate   | 0.0039   |
|    step_size       | 0.000115 |
---------------------------------
Ep done - 51000.
Eval num_timesteps=1250000, episode_reward=0.44 +/- 0.88
Episode length: 30.07 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.438    |
| time/              |          |
|    total_timesteps | 1250000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.4375
SELFPLAY: new best model, bumping up generation to 11
Ep done - 52000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.377    |
|    return_std      | 4.21     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 2611     |
|    total_timesteps | 1259577  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 504      |
|    learning_rate   | 0.0039   |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 53000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.333    |
|    return_std      | 4.89     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 2671     |
|    total_timesteps | 1295554  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 505      |
|    learning_rate   | 0.0039   |
|    step_size       | 0.000133 |
---------------------------------
Eval num_timesteps=1300000, episode_reward=0.51 +/- 0.84
Episode length: 30.02 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.505    |
| time/              |          |
|    total_timesteps | 1300000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.505
SELFPLAY: new best model, bumping up generation to 12
Ep done - 54000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.367    |
|    return_std      | 5.4      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 2750     |
|    total_timesteps | 1331547  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 506      |
|    learning_rate   | 0.0039   |
|    step_size       | 0.00012  |
---------------------------------
Ep done - 55000.
Eval num_timesteps=1350000, episode_reward=0.45 +/- 0.88
Episode length: 29.99 +/- 1.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.445    |
| time/              |          |
|    total_timesteps | 1350000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.445
SELFPLAY: new best model, bumping up generation to 13
Ep done - 56000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.4      |
|    return_std      | 4.4      |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 2832     |
|    total_timesteps | 1367594  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 507      |
|    learning_rate   | 0.00389  |
|    step_size       | 0.000147 |
---------------------------------
Ep done - 57000.
Eval num_timesteps=1400000, episode_reward=0.51 +/- 0.85
Episode length: 30.00 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.507    |
| time/              |          |
|    total_timesteps | 1400000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5075
SELFPLAY: new best model, bumping up generation to 14
Ep done - 58000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.363    |
|    return_std      | 5.11     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 2911     |
|    total_timesteps | 1403623  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 508      |
|    learning_rate   | 0.00389  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 59000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.403    |
|    return_std      | 6.12     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 2973     |
|    total_timesteps | 1439661  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 509      |
|    learning_rate   | 0.00389  |
|    step_size       | 0.000106 |
---------------------------------
Eval num_timesteps=1450000, episode_reward=0.54 +/- 0.82
Episode length: 30.04 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.54     |
| time/              |          |
|    total_timesteps | 1450000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.54
SELFPLAY: new best model, bumping up generation to 15
Ep done - 60000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.403    |
|    return_std      | 4.7      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 3053     |
|    total_timesteps | 1475729  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 510      |
|    learning_rate   | 0.00388  |
|    step_size       | 0.000138 |
---------------------------------
Ep done - 61000.
Ep done - 62000.
Eval num_timesteps=1500000, episode_reward=0.57 +/- 0.81
Episode length: 30.04 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.57     |
| time/              |          |
|    total_timesteps | 1500000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.57
SELFPLAY: new best model, bumping up generation to 16
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.378    |
|    return_std      | 3.92     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 3132     |
|    total_timesteps | 1511802  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 511      |
|    learning_rate   | 0.00388  |
|    step_size       | 0.000165 |
---------------------------------
Ep done - 63000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.362    |
|    return_std      | 6.49     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 3193     |
|    total_timesteps | 1547844  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 512      |
|    learning_rate   | 0.00388  |
|    step_size       | 9.97e-05 |
---------------------------------
Ep done - 64000.
Eval num_timesteps=1550000, episode_reward=0.56 +/- 0.82
Episode length: 30.07 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.555    |
| time/              |          |
|    total_timesteps | 1550000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.555
SELFPLAY: new best model, bumping up generation to 17
Ep done - 65000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.356    |
|    return_std      | 4.43     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 3272     |
|    total_timesteps | 1583859  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 513      |
|    learning_rate   | 0.00388  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 66000.
Eval num_timesteps=1600000, episode_reward=0.49 +/- 0.85
Episode length: 29.98 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.487    |
| time/              |          |
|    total_timesteps | 1600000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.4875
SELFPLAY: new best model, bumping up generation to 18
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.388    |
|    return_std      | 4.56     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 3351     |
|    total_timesteps | 1619900  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 514      |
|    learning_rate   | 0.00387  |
|    step_size       | 0.000142 |
---------------------------------
Ep done - 67000.
Ep done - 68000.
Eval num_timesteps=1650000, episode_reward=0.53 +/- 0.83
Episode length: 30.07 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.527    |
| time/              |          |
|    total_timesteps | 1650000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5275
SELFPLAY: new best model, bumping up generation to 19
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.351    |
|    return_std      | 5.47     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 3433     |
|    total_timesteps | 1655923  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 515      |
|    learning_rate   | 0.00387  |
|    step_size       | 0.000118 |
---------------------------------
Ep done - 69000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.394    |
|    return_std      | 5.33     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 3492     |
|    total_timesteps | 1691972  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 516      |
|    learning_rate   | 0.00387  |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 70000.
Eval num_timesteps=1700000, episode_reward=0.57 +/- 0.81
Episode length: 29.96 +/- 1.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.573    |
| time/              |          |
|    total_timesteps | 1700000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5725
SELFPLAY: new best model, bumping up generation to 20
Ep done - 71000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.457    |
|    return_std      | 4.81     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 3571     |
|    total_timesteps | 1727997  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 517      |
|    learning_rate   | 0.00386  |
|    step_size       | 0.000134 |
---------------------------------
Ep done - 72000.
Eval num_timesteps=1750000, episode_reward=0.59 +/- 0.79
Episode length: 29.99 +/- 1.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.593    |
| time/              |          |
|    total_timesteps | 1750000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5925
SELFPLAY: new best model, bumping up generation to 21
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.459    |
|    return_std      | 5.42     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 3652     |
|    total_timesteps | 1764005  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 518      |
|    learning_rate   | 0.00386  |
|    step_size       | 0.000119 |
---------------------------------
Ep done - 73000.
Ep done - 74000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.396    |
|    return_std      | 6.02     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 3712     |
|    total_timesteps | 1799986  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 519      |
|    learning_rate   | 0.00386  |
|    step_size       | 0.000107 |
---------------------------------
Eval num_timesteps=1800000, episode_reward=0.55 +/- 0.83
Episode length: 30.09 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.547    |
| time/              |          |
|    total_timesteps | 1800000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5475
SELFPLAY: new best model, bumping up generation to 22
Ep done - 75000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.403    |
|    return_std      | 4.66     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 3791     |
|    total_timesteps | 1836055  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 520      |
|    learning_rate   | 0.00386  |
|    step_size       | 0.000138 |
---------------------------------
Ep done - 76000.
Eval num_timesteps=1850000, episode_reward=0.57 +/- 0.80
Episode length: 30.09 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.575    |
| time/              |          |
|    total_timesteps | 1850000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.575
SELFPLAY: new best model, bumping up generation to 23
Ep done - 77000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.466    |
|    return_std      | 6        |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 3872     |
|    total_timesteps | 1872087  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 521      |
|    learning_rate   | 0.00385  |
|    step_size       | 0.000107 |
---------------------------------
Ep done - 78000.
Eval num_timesteps=1900000, episode_reward=0.56 +/- 0.80
Episode length: 30.02 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.56     |
| time/              |          |
|    total_timesteps | 1900000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.56
SELFPLAY: new best model, bumping up generation to 24
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.438    |
|    return_std      | 5.05     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 3951     |
|    total_timesteps | 1908131  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 522      |
|    learning_rate   | 0.00385  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 79000.
Ep done - 80000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.482    |
|    return_std      | 3.89     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 4010     |
|    total_timesteps | 1944193  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 523      |
|    learning_rate   | 0.00385  |
|    step_size       | 0.000165 |
---------------------------------
Eval num_timesteps=1950000, episode_reward=0.60 +/- 0.78
Episode length: 30.02 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.598    |
| time/              |          |
|    total_timesteps | 1950000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5975
SELFPLAY: new best model, bumping up generation to 25
Ep done - 81000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.461    |
|    return_std      | 4.8      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 4092     |
|    total_timesteps | 1980181  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 524      |
|    learning_rate   | 0.00384  |
|    step_size       | 0.000133 |
---------------------------------
Ep done - 82000.
Eval num_timesteps=2000000, episode_reward=0.58 +/- 0.80
Episode length: 29.98 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.58     |
| time/              |          |
|    total_timesteps | 2000000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.58
SELFPLAY: new best model, bumping up generation to 26
Ep done - 83000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.45     |
|    return_std      | 4.54     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 4171     |
|    total_timesteps | 2016217  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 525      |
|    learning_rate   | 0.00384  |
|    step_size       | 0.000141 |
---------------------------------
Ep done - 84000.
Eval num_timesteps=2050000, episode_reward=0.65 +/- 0.75
Episode length: 30.06 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.65     |
| time/              |          |
|    total_timesteps | 2050000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.65
SELFPLAY: new best model, bumping up generation to 27
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.501    |
|    return_std      | 3.03     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 4252     |
|    total_timesteps | 2052281  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 526      |
|    learning_rate   | 0.00384  |
|    step_size       | 0.000211 |
---------------------------------
Ep done - 85000.
Ep done - 86000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.448    |
|    return_std      | 5.35     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 4312     |
|    total_timesteps | 2088336  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 527      |
|    learning_rate   | 0.00384  |
|    step_size       | 0.000119 |
---------------------------------
Eval num_timesteps=2100000, episode_reward=0.55 +/- 0.81
Episode length: 30.05 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.547    |
| time/              |          |
|    total_timesteps | 2100000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5475
SELFPLAY: new best model, bumping up generation to 28
Ep done - 87000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.496    |
|    return_std      | 4.89     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 4390     |
|    total_timesteps | 2124389  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 528      |
|    learning_rate   | 0.00383  |
|    step_size       | 0.000131 |
---------------------------------
Ep done - 88000.
Eval num_timesteps=2150000, episode_reward=0.58 +/- 0.80
Episode length: 30.06 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.578    |
| time/              |          |
|    total_timesteps | 2150000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5775
SELFPLAY: new best model, bumping up generation to 29
Ep done - 89000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.421    |
|    return_std      | 5.05     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 4472     |
|    total_timesteps | 2160434  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 529      |
|    learning_rate   | 0.00383  |
|    step_size       | 0.000126 |
---------------------------------
Ep done - 90000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.475    |
|    return_std      | 6.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 4531     |
|    total_timesteps | 2196412  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 530      |
|    learning_rate   | 0.00383  |
|    step_size       | 0.000104 |
---------------------------------
Eval num_timesteps=2200000, episode_reward=0.58 +/- 0.80
Episode length: 30.00 +/- 1.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.585    |
| time/              |          |
|    total_timesteps | 2200000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.585
SELFPLAY: new best model, bumping up generation to 30
Ep done - 91000.
Ep done - 92000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.434    |
|    return_std      | 3.65     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 4610     |
|    total_timesteps | 2232444  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 531      |
|    learning_rate   | 0.00382  |
|    step_size       | 0.000174 |
---------------------------------
Eval num_timesteps=2250000, episode_reward=0.68 +/- 0.73
Episode length: 30.11 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.675    |
| time/              |          |
|    total_timesteps | 2250000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.675
SELFPLAY: new best model, bumping up generation to 31
Ep done - 93000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.413    |
|    return_std      | 4.36     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 4693     |
|    total_timesteps | 2268480  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 532      |
|    learning_rate   | 0.00382  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 94000.
Ep done - 95000.
Eval num_timesteps=2300000, episode_reward=0.60 +/- 0.78
Episode length: 30.03 +/- 1.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.598    |
| time/              |          |
|    total_timesteps | 2300000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5975
SELFPLAY: new best model, bumping up generation to 32
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.462    |
|    return_std      | 5.57     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 4773     |
|    total_timesteps | 2304539  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 533      |
|    learning_rate   | 0.00382  |
|    step_size       | 0.000114 |
---------------------------------
Ep done - 96000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.461    |
|    return_std      | 4.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 4833     |
|    total_timesteps | 2340595  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 534      |
|    learning_rate   | 0.00382  |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 97000.
Eval num_timesteps=2350000, episode_reward=0.60 +/- 0.77
Episode length: 30.04 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 2350000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6
SELFPLAY: new best model, bumping up generation to 33
Ep done - 98000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.477    |
|    return_std      | 4.26     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 4915     |
|    total_timesteps | 2376634  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 535      |
|    learning_rate   | 0.00381  |
|    step_size       | 0.000149 |
---------------------------------
Ep done - 99000.
Eval num_timesteps=2400000, episode_reward=0.61 +/- 0.78
Episode length: 30.07 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.608    |
| time/              |          |
|    total_timesteps | 2400000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6075
SELFPLAY: new best model, bumping up generation to 34
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.53     |
|    return_std      | 4.34     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 4994     |
|    total_timesteps | 2412654  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 536      |
|    learning_rate   | 0.00381  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 100000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.473    |
|    return_std      | 4.12     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 5053     |
|    total_timesteps | 2448674  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 537      |
|    learning_rate   | 0.00381  |
|    step_size       | 0.000154 |
---------------------------------
Ep done - 101000.
Eval num_timesteps=2450000, episode_reward=0.59 +/- 0.78
Episode length: 30.07 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.593    |
| time/              |          |
|    total_timesteps | 2450000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5925
SELFPLAY: new best model, bumping up generation to 35
Ep done - 102000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.487    |
|    return_std      | 6.27     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 5135     |
|    total_timesteps | 2484734  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 538      |
|    learning_rate   | 0.0038   |
|    step_size       | 0.000101 |
---------------------------------
Ep done - 103000.
Eval num_timesteps=2500000, episode_reward=0.60 +/- 0.78
Episode length: 30.03 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.598    |
| time/              |          |
|    total_timesteps | 2500000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5975
SELFPLAY: new best model, bumping up generation to 36
Ep done - 104000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.5      |
|    return_std      | 3.28     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 5214     |
|    total_timesteps | 2520834  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 539      |
|    learning_rate   | 0.0038   |
|    step_size       | 0.000193 |
---------------------------------
Ep done - 105000.
Eval num_timesteps=2550000, episode_reward=0.57 +/- 0.81
Episode length: 29.98 +/- 1.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.568    |
| time/              |          |
|    total_timesteps | 2550000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5675
SELFPLAY: new best model, bumping up generation to 37
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.502    |
|    return_std      | 5        |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 5293     |
|    total_timesteps | 2556905  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 540      |
|    learning_rate   | 0.0038   |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 106000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.465    |
|    return_std      | 3.81     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 5356     |
|    total_timesteps | 2592942  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 541      |
|    learning_rate   | 0.0038   |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 107000.
Eval num_timesteps=2600000, episode_reward=0.59 +/- 0.79
Episode length: 30.06 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.588    |
| time/              |          |
|    total_timesteps | 2600000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5875
SELFPLAY: new best model, bumping up generation to 38
Ep done - 108000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.469    |
|    return_std      | 3.18     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 5435     |
|    total_timesteps | 2629006  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 542      |
|    learning_rate   | 0.00379  |
|    step_size       | 0.000199 |
---------------------------------
Ep done - 109000.
Eval num_timesteps=2650000, episode_reward=0.65 +/- 0.75
Episode length: 30.09 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.645    |
| time/              |          |
|    total_timesteps | 2650000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.645
SELFPLAY: new best model, bumping up generation to 39
Ep done - 110000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.512    |
|    return_std      | 3.76     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 5514     |
|    total_timesteps | 2665033  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 543      |
|    learning_rate   | 0.00379  |
|    step_size       | 0.000168 |
---------------------------------
Ep done - 111000.
Eval num_timesteps=2700000, episode_reward=0.60 +/- 0.77
Episode length: 30.03 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.598    |
| time/              |          |
|    total_timesteps | 2700000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5975
SELFPLAY: new best model, bumping up generation to 40
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.45     |
|    return_std      | 4.75     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 5596     |
|    total_timesteps | 2701113  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 544      |
|    learning_rate   | 0.00379  |
|    step_size       | 0.000133 |
---------------------------------
Ep done - 112000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.474    |
|    return_std      | 6.14     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 5655     |
|    total_timesteps | 2737094  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 545      |
|    learning_rate   | 0.00378  |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 113000.
Eval num_timesteps=2750000, episode_reward=0.64 +/- 0.75
Episode length: 30.11 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.637    |
| time/              |          |
|    total_timesteps | 2750000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6375
SELFPLAY: new best model, bumping up generation to 41
Ep done - 114000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.473    |
|    return_std      | 5.62     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 5734     |
|    total_timesteps | 2773136  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 546      |
|    learning_rate   | 0.00378  |
|    step_size       | 0.000112 |
---------------------------------
Ep done - 115000.
Eval num_timesteps=2800000, episode_reward=0.60 +/- 0.79
Episode length: 30.09 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.598    |
| time/              |          |
|    total_timesteps | 2800000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5975
SELFPLAY: new best model, bumping up generation to 42
Ep done - 116000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.529    |
|    return_std      | 4.2      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 5816     |
|    total_timesteps | 2809214  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 547      |
|    learning_rate   | 0.00378  |
|    step_size       | 0.00015  |
---------------------------------
Ep done - 117000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.502    |
|    return_std      | 4.76     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 5874     |
|    total_timesteps | 2845262  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 548      |
|    learning_rate   | 0.00378  |
|    step_size       | 0.000132 |
---------------------------------
Eval num_timesteps=2850000, episode_reward=0.65 +/- 0.74
Episode length: 30.09 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.65     |
| time/              |          |
|    total_timesteps | 2850000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.65
SELFPLAY: new best model, bumping up generation to 43
Ep done - 118000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.507    |
|    return_std      | 5.55     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 5953     |
|    total_timesteps | 2881334  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 549      |
|    learning_rate   | 0.00377  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 119000.
Eval num_timesteps=2900000, episode_reward=0.61 +/- 0.78
Episode length: 30.13 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.61     |
| time/              |          |
|    total_timesteps | 2900000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.61
SELFPLAY: new best model, bumping up generation to 44
Ep done - 120000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.499    |
|    return_std      | 4.82     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6035     |
|    total_timesteps | 2917420  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 550      |
|    learning_rate   | 0.00377  |
|    step_size       | 0.00013  |
---------------------------------
Ep done - 121000.
Eval num_timesteps=2950000, episode_reward=0.60 +/- 0.79
Episode length: 30.05 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.598    |
| time/              |          |
|    total_timesteps | 2950000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5975
SELFPLAY: new best model, bumping up generation to 45
Ep done - 122000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.502    |
|    return_std      | 4.1      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6114     |
|    total_timesteps | 2953528  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 551      |
|    learning_rate   | 0.00377  |
|    step_size       | 0.000153 |
---------------------------------
Ep done - 123000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.501    |
|    return_std      | 5.61     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 6173     |
|    total_timesteps | 2989595  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 552      |
|    learning_rate   | 0.00376  |
|    step_size       | 0.000112 |
---------------------------------
Eval num_timesteps=3000000, episode_reward=0.68 +/- 0.72
Episode length: 30.12 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.682    |
| time/              |          |
|    total_timesteps | 3000000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6825
SELFPLAY: new best model, bumping up generation to 46
Ep done - 124000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.534    |
|    return_std      | 3.51     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6255     |
|    total_timesteps | 3025709  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 553      |
|    learning_rate   | 0.00376  |
|    step_size       | 0.000178 |
---------------------------------
Ep done - 125000.
Ep done - 126000.
Eval num_timesteps=3050000, episode_reward=0.69 +/- 0.70
Episode length: 30.14 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.695    |
| time/              |          |
|    total_timesteps | 3050000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.695
SELFPLAY: new best model, bumping up generation to 47
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.517    |
|    return_std      | 4.24     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6334     |
|    total_timesteps | 3061844  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 554      |
|    learning_rate   | 0.00376  |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 127000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.531    |
|    return_std      | 4.8      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 6396     |
|    total_timesteps | 3097842  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 555      |
|    learning_rate   | 0.00376  |
|    step_size       | 0.00013  |
---------------------------------
Ep done - 128000.
Eval num_timesteps=3100000, episode_reward=0.68 +/- 0.71
Episode length: 30.14 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.677    |
| time/              |          |
|    total_timesteps | 3100000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6775
SELFPLAY: new best model, bumping up generation to 48
Ep done - 129000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.526    |
|    return_std      | 6.14     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6475     |
|    total_timesteps | 3133904  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 556      |
|    learning_rate   | 0.00375  |
|    step_size       | 0.000102 |
---------------------------------
Ep done - 130000.
Eval num_timesteps=3150000, episode_reward=0.69 +/- 0.70
Episode length: 30.09 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.688    |
| time/              |          |
|    total_timesteps | 3150000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6875
SELFPLAY: new best model, bumping up generation to 49
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.519    |
|    return_std      | 4.76     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6554     |
|    total_timesteps | 3169965  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 557      |
|    learning_rate   | 0.00375  |
|    step_size       | 0.000131 |
---------------------------------
Ep done - 131000.
Ep done - 132000.
Eval num_timesteps=3200000, episode_reward=0.64 +/- 0.75
Episode length: 30.07 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.635    |
| time/              |          |
|    total_timesteps | 3200000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.635
SELFPLAY: new best model, bumping up generation to 50
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.498    |
|    return_std      | 5.75     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6636     |
|    total_timesteps | 3206044  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 558      |
|    learning_rate   | 0.00375  |
|    step_size       | 0.000109 |
---------------------------------
Ep done - 133000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.507    |
|    return_std      | 3.73     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 6696     |
|    total_timesteps | 3242110  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 559      |
|    learning_rate   | 0.00374  |
|    step_size       | 0.000167 |
---------------------------------
Ep done - 134000.
Eval num_timesteps=3250000, episode_reward=0.59 +/- 0.78
Episode length: 30.07 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.595    |
| time/              |          |
|    total_timesteps | 3250000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.595
SELFPLAY: new best model, bumping up generation to 51
Ep done - 135000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.527    |
|    return_std      | 3.54     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6774     |
|    total_timesteps | 3278218  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 560      |
|    learning_rate   | 0.00374  |
|    step_size       | 0.000176 |
---------------------------------
Ep done - 136000.
Eval num_timesteps=3300000, episode_reward=0.61 +/- 0.76
Episode length: 30.08 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.613    |
| time/              |          |
|    total_timesteps | 3300000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6125
SELFPLAY: new best model, bumping up generation to 52
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.537    |
|    return_std      | 5.06     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6857     |
|    total_timesteps | 3314311  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 561      |
|    learning_rate   | 0.00374  |
|    step_size       | 0.000123 |
---------------------------------
Ep done - 137000.
Ep done - 138000.
Eval num_timesteps=3350000, episode_reward=0.59 +/- 0.79
Episode length: 30.08 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.595    |
| time/              |          |
|    total_timesteps | 3350000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.595
SELFPLAY: new best model, bumping up generation to 53
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.51     |
|    return_std      | 4.85     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 6937     |
|    total_timesteps | 3350394  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 562      |
|    learning_rate   | 0.00373  |
|    step_size       | 0.000128 |
---------------------------------
Ep done - 139000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.522    |
|    return_std      | 4.31     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 6997     |
|    total_timesteps | 3386528  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 563      |
|    learning_rate   | 0.00373  |
|    step_size       | 0.000144 |
---------------------------------
Ep done - 140000.
Eval num_timesteps=3400000, episode_reward=0.61 +/- 0.77
Episode length: 30.05 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.615    |
| time/              |          |
|    total_timesteps | 3400000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.615
SELFPLAY: new best model, bumping up generation to 54
Ep done - 141000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.518    |
|    return_std      | 4.22     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7078     |
|    total_timesteps | 3422586  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 564      |
|    learning_rate   | 0.00373  |
|    step_size       | 0.000147 |
---------------------------------
Ep done - 142000.
Eval num_timesteps=3450000, episode_reward=0.65 +/- 0.74
Episode length: 30.09 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.645    |
| time/              |          |
|    total_timesteps | 3450000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.645
SELFPLAY: new best model, bumping up generation to 55
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.547    |
|    return_std      | 3.6      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7157     |
|    total_timesteps | 3458690  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 565      |
|    learning_rate   | 0.00373  |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 143000.
Ep done - 144000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.532    |
|    return_std      | 4.55     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 7216     |
|    total_timesteps | 3494715  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 566      |
|    learning_rate   | 0.00372  |
|    step_size       | 0.000136 |
---------------------------------
Eval num_timesteps=3500000, episode_reward=0.62 +/- 0.77
Episode length: 30.04 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.623    |
| time/              |          |
|    total_timesteps | 3500000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6225
SELFPLAY: new best model, bumping up generation to 56
Ep done - 145000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.497    |
|    return_std      | 3.73     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7298     |
|    total_timesteps | 3530781  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 567      |
|    learning_rate   | 0.00372  |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 146000.
Eval num_timesteps=3550000, episode_reward=0.56 +/- 0.81
Episode length: 30.04 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.562    |
| time/              |          |
|    total_timesteps | 3550000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5625
SELFPLAY: new best model, bumping up generation to 57
Ep done - 147000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.522    |
|    return_std      | 3.61     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7378     |
|    total_timesteps | 3566893  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 568      |
|    learning_rate   | 0.00372  |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 148000.
Eval num_timesteps=3600000, episode_reward=0.65 +/- 0.74
Episode length: 30.00 +/- 1.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.647    |
| time/              |          |
|    total_timesteps | 3600000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6475
SELFPLAY: new best model, bumping up generation to 58
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.533    |
|    return_std      | 4.69     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7458     |
|    total_timesteps | 3602967  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 569      |
|    learning_rate   | 0.00371  |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 149000.
Ep done - 150000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.557    |
|    return_std      | 3.96     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7521     |
|    total_timesteps | 3639025  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 570      |
|    learning_rate   | 0.00371  |
|    step_size       | 0.000156 |
---------------------------------
Eval num_timesteps=3650000, episode_reward=0.58 +/- 0.80
Episode length: 30.10 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.578    |
| time/              |          |
|    total_timesteps | 3650000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5775
SELFPLAY: new best model, bumping up generation to 59
Ep done - 151000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.516    |
|    return_std      | 4.86     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7601     |
|    total_timesteps | 3675103  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 571      |
|    learning_rate   | 0.00371  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 152000.
Eval num_timesteps=3700000, episode_reward=0.68 +/- 0.70
Episode length: 30.13 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.677    |
| time/              |          |
|    total_timesteps | 3700000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6775
SELFPLAY: new best model, bumping up generation to 60
Ep done - 153000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.503    |
|    return_std      | 4.48     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7681     |
|    total_timesteps | 3711205  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 572      |
|    learning_rate   | 0.00371  |
|    step_size       | 0.000138 |
---------------------------------
Ep done - 154000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.505    |
|    return_std      | 5.9      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7744     |
|    total_timesteps | 3747254  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 573      |
|    learning_rate   | 0.0037   |
|    step_size       | 0.000105 |
---------------------------------
Eval num_timesteps=3750000, episode_reward=0.70 +/- 0.69
Episode length: 30.09 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.698    |
| time/              |          |
|    total_timesteps | 3750000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6975
SELFPLAY: new best model, bumping up generation to 61
Ep done - 155000.
Ep done - 156000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.537    |
|    return_std      | 4.64     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7823     |
|    total_timesteps | 3783326  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 574      |
|    learning_rate   | 0.0037   |
|    step_size       | 0.000133 |
---------------------------------
Eval num_timesteps=3800000, episode_reward=0.66 +/- 0.73
Episode length: 30.11 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.657    |
| time/              |          |
|    total_timesteps | 3800000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6575
SELFPLAY: new best model, bumping up generation to 62
Ep done - 157000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.525    |
|    return_std      | 4.65     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 7903     |
|    total_timesteps | 3819397  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 575      |
|    learning_rate   | 0.0037   |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 158000.
Ep done - 159000.
Eval num_timesteps=3850000, episode_reward=0.65 +/- 0.74
Episode length: 30.06 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.645    |
| time/              |          |
|    total_timesteps | 3850000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.645
SELFPLAY: new best model, bumping up generation to 63
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.538    |
|    return_std      | 4.38     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 7985     |
|    total_timesteps | 3855442  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 576      |
|    learning_rate   | 0.00369  |
|    step_size       | 0.000141 |
---------------------------------
Ep done - 160000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.502    |
|    return_std      | 5.75     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8044     |
|    total_timesteps | 3891454  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 577      |
|    learning_rate   | 0.00369  |
|    step_size       | 0.000107 |
---------------------------------
Ep done - 161000.
Eval num_timesteps=3900000, episode_reward=0.62 +/- 0.77
Episode length: 30.05 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.618    |
| time/              |          |
|    total_timesteps | 3900000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6175
SELFPLAY: new best model, bumping up generation to 64
Ep done - 162000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.572    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8126     |
|    total_timesteps | 3927534  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 578      |
|    learning_rate   | 0.00369  |
|    step_size       | 0.000253 |
---------------------------------
Ep done - 163000.
Eval num_timesteps=3950000, episode_reward=0.69 +/- 0.70
Episode length: 30.14 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 3950000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.69
SELFPLAY: new best model, bumping up generation to 65
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.555    |
|    return_std      | 4.54     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8205     |
|    total_timesteps | 3963631  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 579      |
|    learning_rate   | 0.00369  |
|    step_size       | 0.000135 |
---------------------------------
Ep done - 164000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.545    |
|    return_std      | 2.81     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8264     |
|    total_timesteps | 3999738  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 580      |
|    learning_rate   | 0.00368  |
|    step_size       | 0.000218 |
---------------------------------
Ep done - 165000.
Eval num_timesteps=4000000, episode_reward=0.55 +/- 0.82
Episode length: 30.09 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.55     |
| time/              |          |
|    total_timesteps | 4000000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.55
SELFPLAY: new best model, bumping up generation to 66
Ep done - 166000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.528    |
|    return_std      | 4.08     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8346     |
|    total_timesteps | 4035875  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 581      |
|    learning_rate   | 0.00368  |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 167000.
Eval num_timesteps=4050000, episode_reward=0.65 +/- 0.75
Episode length: 30.13 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.65     |
| time/              |          |
|    total_timesteps | 4050000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.65
SELFPLAY: new best model, bumping up generation to 67
Ep done - 168000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.567    |
|    return_std      | 2.66     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8425     |
|    total_timesteps | 4071961  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 582      |
|    learning_rate   | 0.00368  |
|    step_size       | 0.000231 |
---------------------------------
Ep done - 169000.
Eval num_timesteps=4100000, episode_reward=0.61 +/- 0.78
Episode length: 30.13 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.613    |
| time/              |          |
|    total_timesteps | 4100000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6125
SELFPLAY: new best model, bumping up generation to 68
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.524    |
|    return_std      | 3.77     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8504     |
|    total_timesteps | 4108022  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 583      |
|    learning_rate   | 0.00367  |
|    step_size       | 0.000162 |
---------------------------------
Ep done - 170000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.538    |
|    return_std      | 4.83     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8566     |
|    total_timesteps | 4144083  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 584      |
|    learning_rate   | 0.00367  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 171000.
Eval num_timesteps=4150000, episode_reward=0.67 +/- 0.74
Episode length: 30.07 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.665    |
| time/              |          |
|    total_timesteps | 4150000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.665
SELFPLAY: new best model, bumping up generation to 69
Ep done - 172000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.519    |
|    return_std      | 4.13     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8645     |
|    total_timesteps | 4180163  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 585      |
|    learning_rate   | 0.00367  |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 173000.
Eval num_timesteps=4200000, episode_reward=0.67 +/- 0.73
Episode length: 30.05 +/- 1.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.665    |
| time/              |          |
|    total_timesteps | 4200000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.665
SELFPLAY: new best model, bumping up generation to 70
Ep done - 174000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.547    |
|    return_std      | 5.42     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8724     |
|    total_timesteps | 4216202  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 586      |
|    learning_rate   | 0.00367  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 175000.
Eval num_timesteps=4250000, episode_reward=0.66 +/- 0.75
Episode length: 30.13 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.655    |
| time/              |          |
|    total_timesteps | 4250000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.655
SELFPLAY: new best model, bumping up generation to 71
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.544    |
|    return_std      | 3.96     |
| time/              |          |
|    fps             | 482      |
|    time_elapsed    | 8806     |
|    total_timesteps | 4252324  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 587      |
|    learning_rate   | 0.00366  |
|    step_size       | 0.000154 |
---------------------------------
Ep done - 176000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.552    |
|    return_std      | 3.62     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8865     |
|    total_timesteps | 4288442  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 588      |
|    learning_rate   | 0.00366  |
|    step_size       | 0.000168 |
---------------------------------
Ep done - 177000.
Eval num_timesteps=4300000, episode_reward=0.70 +/- 0.69
Episode length: 30.13 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.705    |
| time/              |          |
|    total_timesteps | 4300000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.705
SELFPLAY: new best model, bumping up generation to 72
Ep done - 178000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.552    |
|    return_std      | 3.85     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 8944     |
|    total_timesteps | 4324562  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 589      |
|    learning_rate   | 0.00366  |
|    step_size       | 0.000158 |
---------------------------------
Ep done - 179000.
Eval num_timesteps=4350000, episode_reward=0.67 +/- 0.74
Episode length: 30.10 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.67     |
| time/              |          |
|    total_timesteps | 4350000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.67
SELFPLAY: new best model, bumping up generation to 73
Ep done - 180000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.532    |
|    return_std      | 4.8      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9026     |
|    total_timesteps | 4360657  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 590      |
|    learning_rate   | 0.00365  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 181000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.571    |
|    return_std      | 3.79     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9086     |
|    total_timesteps | 4396707  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 591      |
|    learning_rate   | 0.00365  |
|    step_size       | 0.000161 |
---------------------------------
Eval num_timesteps=4400000, episode_reward=0.64 +/- 0.75
Episode length: 30.08 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.64     |
| time/              |          |
|    total_timesteps | 4400000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.64
SELFPLAY: new best model, bumping up generation to 74
Ep done - 182000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.572    |
|    return_std      | 4.18     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9166     |
|    total_timesteps | 4432806  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 592      |
|    learning_rate   | 0.00365  |
|    step_size       | 0.000145 |
---------------------------------
Ep done - 183000.
Eval num_timesteps=4450000, episode_reward=0.69 +/- 0.71
Episode length: 30.11 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.693    |
| time/              |          |
|    total_timesteps | 4450000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6925
SELFPLAY: new best model, bumping up generation to 75
Ep done - 184000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.56     |
|    return_std      | 3.62     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9247     |
|    total_timesteps | 4468930  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 593      |
|    learning_rate   | 0.00365  |
|    step_size       | 0.000168 |
---------------------------------
Ep done - 185000.
Eval num_timesteps=4500000, episode_reward=0.69 +/- 0.71
Episode length: 30.12 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 4500000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.69
SELFPLAY: new best model, bumping up generation to 76
Ep done - 186000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.596    |
|    return_std      | 3.45     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9327     |
|    total_timesteps | 4505023  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 594      |
|    learning_rate   | 0.00364  |
|    step_size       | 0.000176 |
---------------------------------
Ep done - 187000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.581    |
|    return_std      | 3.82     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9386     |
|    total_timesteps | 4541137  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 595      |
|    learning_rate   | 0.00364  |
|    step_size       | 0.000159 |
---------------------------------
Eval num_timesteps=4550000, episode_reward=0.63 +/- 0.75
Episode length: 30.08 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.632    |
| time/              |          |
|    total_timesteps | 4550000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6325
SELFPLAY: new best model, bumping up generation to 77
Ep done - 188000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.573    |
|    return_std      | 5.37     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9468     |
|    total_timesteps | 4577196  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 596      |
|    learning_rate   | 0.00364  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 189000.
Eval num_timesteps=4600000, episode_reward=0.65 +/- 0.74
Episode length: 30.10 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.652    |
| time/              |          |
|    total_timesteps | 4600000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6525
SELFPLAY: new best model, bumping up generation to 78
Ep done - 190000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.545    |
|    return_std      | 3.96     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9547     |
|    total_timesteps | 4613277  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 597      |
|    learning_rate   | 0.00363  |
|    step_size       | 0.000153 |
---------------------------------
Ep done - 191000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.586    |
|    return_std      | 4.38     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9606     |
|    total_timesteps | 4649374  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 598      |
|    learning_rate   | 0.00363  |
|    step_size       | 0.000138 |
---------------------------------
Ep done - 192000.
Eval num_timesteps=4650000, episode_reward=0.70 +/- 0.69
Episode length: 30.09 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 4650000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7
SELFPLAY: new best model, bumping up generation to 79
Ep done - 193000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.591    |
|    return_std      | 3.6      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9688     |
|    total_timesteps | 4685441  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 599      |
|    learning_rate   | 0.00363  |
|    step_size       | 0.000168 |
---------------------------------
Ep done - 194000.
Eval num_timesteps=4700000, episode_reward=0.66 +/- 0.73
Episode length: 30.09 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.66     |
| time/              |          |
|    total_timesteps | 4700000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.66
SELFPLAY: new best model, bumping up generation to 80
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.568    |
|    return_std      | 3.82     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9767     |
|    total_timesteps | 4721540  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 600      |
|    learning_rate   | 0.00363  |
|    step_size       | 0.000158 |
---------------------------------
Ep done - 195000.
Ep done - 196000.
Eval num_timesteps=4750000, episode_reward=0.72 +/- 0.68
Episode length: 30.09 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.725    |
| time/              |          |
|    total_timesteps | 4750000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.725
SELFPLAY: new best model, bumping up generation to 81
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.526    |
|    return_std      | 5.57     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9846     |
|    total_timesteps | 4757619  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 601      |
|    learning_rate   | 0.00362  |
|    step_size       | 0.000108 |
---------------------------------
Ep done - 197000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.588    |
|    return_std      | 5.21     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9908     |
|    total_timesteps | 4793663  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 602      |
|    learning_rate   | 0.00362  |
|    step_size       | 0.000116 |
---------------------------------
Ep done - 198000.
Eval num_timesteps=4800000, episode_reward=0.70 +/- 0.69
Episode length: 30.14 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.703    |
| time/              |          |
|    total_timesteps | 4800000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7025
SELFPLAY: new best model, bumping up generation to 82
Ep done - 199000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.574    |
|    return_std      | 4.75     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 9986     |
|    total_timesteps | 4829753  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 603      |
|    learning_rate   | 0.00362  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 200000.
Eval num_timesteps=4850000, episode_reward=0.68 +/- 0.71
Episode length: 30.11 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.682    |
| time/              |          |
|    total_timesteps | 4850000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6825
SELFPLAY: new best model, bumping up generation to 83
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.623    |
|    return_std      | 3.44     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10067    |
|    total_timesteps | 4865818  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 604      |
|    learning_rate   | 0.00361  |
|    step_size       | 0.000175 |
---------------------------------
Ep done - 201000.
Ep done - 202000.
Eval num_timesteps=4900000, episode_reward=0.71 +/- 0.68
Episode length: 30.08 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.715    |
| time/              |          |
|    total_timesteps | 4900000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.715
SELFPLAY: new best model, bumping up generation to 84
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.564    |
|    return_std      | 5.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10147    |
|    total_timesteps | 4901858  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 605      |
|    learning_rate   | 0.00361  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 203000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.578    |
|    return_std      | 4.32     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10206    |
|    total_timesteps | 4937875  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 606      |
|    learning_rate   | 0.00361  |
|    step_size       | 0.000139 |
---------------------------------
Ep done - 204000.
Eval num_timesteps=4950000, episode_reward=0.69 +/- 0.71
Episode length: 30.05 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.693    |
| time/              |          |
|    total_timesteps | 4950000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6925
SELFPLAY: new best model, bumping up generation to 85
Ep done - 205000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.579    |
|    return_std      | 3.69     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10288    |
|    total_timesteps | 4973964  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 607      |
|    learning_rate   | 0.0036   |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 206000.
Eval num_timesteps=5000000, episode_reward=0.73 +/- 0.67
Episode length: 30.10 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.728    |
| time/              |          |
|    total_timesteps | 5000000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7275
SELFPLAY: new best model, bumping up generation to 86
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.554    |
|    return_std      | 3.71     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10367    |
|    total_timesteps | 5010060  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 608      |
|    learning_rate   | 0.0036   |
|    step_size       | 0.000162 |
---------------------------------
Ep done - 207000.
Ep done - 208000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.557    |
|    return_std      | 3.13     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10426    |
|    total_timesteps | 5046135  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 609      |
|    learning_rate   | 0.0036   |
|    step_size       | 0.000192 |
---------------------------------
Eval num_timesteps=5050000, episode_reward=0.69 +/- 0.70
Episode length: 30.12 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.693    |
| time/              |          |
|    total_timesteps | 5050000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6925
SELFPLAY: new best model, bumping up generation to 87
Ep done - 209000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.591    |
|    return_std      | 4.54     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10509    |
|    total_timesteps | 5082220  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 610      |
|    learning_rate   | 0.0036   |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 210000.
Eval num_timesteps=5100000, episode_reward=0.70 +/- 0.70
Episode length: 30.07 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 5100000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7
SELFPLAY: new best model, bumping up generation to 88
Ep done - 211000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.567    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10588    |
|    total_timesteps | 5118292  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 611      |
|    learning_rate   | 0.00359  |
|    step_size       | 0.000264 |
---------------------------------
Ep done - 212000.
Eval num_timesteps=5150000, episode_reward=0.70 +/- 0.70
Episode length: 30.11 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.698    |
| time/              |          |
|    total_timesteps | 5150000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6975
SELFPLAY: new best model, bumping up generation to 89
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.571    |
|    return_std      | 4.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10667    |
|    total_timesteps | 5154431  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 612      |
|    learning_rate   | 0.00359  |
|    step_size       | 0.000144 |
---------------------------------
Ep done - 213000.
Ep done - 214000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.623    |
|    return_std      | 3.69     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10729    |
|    total_timesteps | 5190542  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 613      |
|    learning_rate   | 0.00359  |
|    step_size       | 0.000162 |
---------------------------------
Eval num_timesteps=5200000, episode_reward=0.69 +/- 0.72
Episode length: 30.09 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.685    |
| time/              |          |
|    total_timesteps | 5200000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.685
SELFPLAY: new best model, bumping up generation to 90
Ep done - 215000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.597    |
|    return_std      | 4.85     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10808    |
|    total_timesteps | 5226628  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 614      |
|    learning_rate   | 0.00358  |
|    step_size       | 0.000123 |
---------------------------------
Ep done - 216000.
Eval num_timesteps=5250000, episode_reward=0.70 +/- 0.71
Episode length: 30.08 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 5250000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7
SELFPLAY: new best model, bumping up generation to 91
Ep done - 217000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.594    |
|    return_std      | 3.67     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10887    |
|    total_timesteps | 5262707  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 615      |
|    learning_rate   | 0.00358  |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 218000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.588    |
|    return_std      | 3.9      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 10949    |
|    total_timesteps | 5298791  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 616      |
|    learning_rate   | 0.00358  |
|    step_size       | 0.000153 |
---------------------------------
Eval num_timesteps=5300000, episode_reward=0.69 +/- 0.70
Episode length: 30.10 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 5300000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.69
SELFPLAY: new best model, bumping up generation to 92
Ep done - 219000.
Ep done - 220000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.623    |
|    return_std      | 3.67     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11028    |
|    total_timesteps | 5334890  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 617      |
|    learning_rate   | 0.00358  |
|    step_size       | 0.000162 |
---------------------------------
Eval num_timesteps=5350000, episode_reward=0.67 +/- 0.73
Episode length: 30.07 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.665    |
| time/              |          |
|    total_timesteps | 5350000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.665
SELFPLAY: new best model, bumping up generation to 93
Ep done - 221000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.552    |
|    return_std      | 4.7      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11107    |
|    total_timesteps | 5370992  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 618      |
|    learning_rate   | 0.00357  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 222000.
Eval num_timesteps=5400000, episode_reward=0.65 +/- 0.75
Episode length: 30.12 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.645    |
| time/              |          |
|    total_timesteps | 5400000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.645
SELFPLAY: new best model, bumping up generation to 94
Ep done - 223000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.6      |
|    return_std      | 3.81     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11190    |
|    total_timesteps | 5407074  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 619      |
|    learning_rate   | 0.00357  |
|    step_size       | 0.000156 |
---------------------------------
Ep done - 224000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.623    |
|    return_std      | 3.74     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11250    |
|    total_timesteps | 5443181  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 620      |
|    learning_rate   | 0.00357  |
|    step_size       | 0.000159 |
---------------------------------
Ep done - 225000.
Eval num_timesteps=5450000, episode_reward=0.69 +/- 0.71
Episode length: 30.11 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.695    |
| time/              |          |
|    total_timesteps | 5450000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.695
SELFPLAY: new best model, bumping up generation to 95
Ep done - 226000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.586    |
|    return_std      | 3.69     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11329    |
|    total_timesteps | 5479282  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 621      |
|    learning_rate   | 0.00356  |
|    step_size       | 0.000161 |
---------------------------------
Ep done - 227000.
Eval num_timesteps=5500000, episode_reward=0.70 +/- 0.69
Episode length: 30.05 +/- 0.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.703    |
| time/              |          |
|    total_timesteps | 5500000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7025
SELFPLAY: new best model, bumping up generation to 96
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.572    |
|    return_std      | 5.41     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11411    |
|    total_timesteps | 5515422  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 622      |
|    learning_rate   | 0.00356  |
|    step_size       | 0.00011  |
---------------------------------
Ep done - 228000.
Ep done - 229000.
Eval num_timesteps=5550000, episode_reward=0.61 +/- 0.77
Episode length: 30.07 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.608    |
| time/              |          |
|    total_timesteps | 5550000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6075
SELFPLAY: new best model, bumping up generation to 97
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.603    |
|    return_std      | 4.89     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11490    |
|    total_timesteps | 5551499  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 623      |
|    learning_rate   | 0.00356  |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 230000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.611    |
|    return_std      | 5.34     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11549    |
|    total_timesteps | 5587556  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 624      |
|    learning_rate   | 0.00356  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 231000.
Eval num_timesteps=5600000, episode_reward=0.74 +/- 0.67
Episode length: 30.09 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.74     |
| time/              |          |
|    total_timesteps | 5600000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.74
SELFPLAY: new best model, bumping up generation to 98
Ep done - 232000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.618    |
|    return_std      | 5.53     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11631    |
|    total_timesteps | 5623677  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 625      |
|    learning_rate   | 0.00355  |
|    step_size       | 0.000107 |
---------------------------------
Ep done - 233000.
Eval num_timesteps=5650000, episode_reward=0.70 +/- 0.70
Episode length: 30.13 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.698    |
| time/              |          |
|    total_timesteps | 5650000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6975
SELFPLAY: new best model, bumping up generation to 99
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.624    |
|    return_std      | 4.32     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11710    |
|    total_timesteps | 5659749  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 626      |
|    learning_rate   | 0.00355  |
|    step_size       | 0.000137 |
---------------------------------
Ep done - 234000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.602    |
|    return_std      | 3.8      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11770    |
|    total_timesteps | 5695864  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 627      |
|    learning_rate   | 0.00355  |
|    step_size       | 0.000156 |
---------------------------------
Ep done - 235000.
Eval num_timesteps=5700000, episode_reward=0.74 +/- 0.65
Episode length: 30.14 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.745    |
| time/              |          |
|    total_timesteps | 5700000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.745
SELFPLAY: new best model, bumping up generation to 100
Ep done - 236000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.584    |
|    return_std      | 3.98     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11852    |
|    total_timesteps | 5731953  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 628      |
|    learning_rate   | 0.00354  |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 237000.
Eval num_timesteps=5750000, episode_reward=0.69 +/- 0.69
Episode length: 30.08 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.693    |
| time/              |          |
|    total_timesteps | 5750000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6925
SELFPLAY: new best model, bumping up generation to 101
Ep done - 238000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.605    |
|    return_std      | 5.16     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 11931    |
|    total_timesteps | 5768060  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 629      |
|    learning_rate   | 0.00354  |
|    step_size       | 0.000114 |
---------------------------------
Ep done - 239000.
Eval num_timesteps=5800000, episode_reward=0.70 +/- 0.69
Episode length: 30.09 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.698    |
| time/              |          |
|    total_timesteps | 5800000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6975
SELFPLAY: new best model, bumping up generation to 102
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.59     |
|    return_std      | 2.72     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12012    |
|    total_timesteps | 5804183  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 630      |
|    learning_rate   | 0.00354  |
|    step_size       | 0.000216 |
---------------------------------
Ep done - 240000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.633    |
|    return_std      | 3.58     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12072    |
|    total_timesteps | 5840319  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 631      |
|    learning_rate   | 0.00354  |
|    step_size       | 0.000165 |
---------------------------------
Ep done - 241000.
Eval num_timesteps=5850000, episode_reward=0.63 +/- 0.75
Episode length: 30.09 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.63     |
| time/              |          |
|    total_timesteps | 5850000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.63
SELFPLAY: new best model, bumping up generation to 103
Ep done - 242000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.626    |
|    return_std      | 2.67     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12151    |
|    total_timesteps | 5876459  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 632      |
|    learning_rate   | 0.00353  |
|    step_size       | 0.000221 |
---------------------------------
Ep done - 243000.
Eval num_timesteps=5900000, episode_reward=0.68 +/- 0.71
Episode length: 30.07 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.68     |
| time/              |          |
|    total_timesteps | 5900000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.68
SELFPLAY: new best model, bumping up generation to 104
Ep done - 244000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.648    |
|    return_std      | 3.37     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12233    |
|    total_timesteps | 5912567  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 633      |
|    learning_rate   | 0.00353  |
|    step_size       | 0.000175 |
---------------------------------
Ep done - 245000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.616    |
|    return_std      | 5.24     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12292    |
|    total_timesteps | 5948666  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 634      |
|    learning_rate   | 0.00353  |
|    step_size       | 0.000112 |
---------------------------------
Eval num_timesteps=5950000, episode_reward=0.69 +/- 0.70
Episode length: 30.11 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.688    |
| time/              |          |
|    total_timesteps | 5950000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.6875
SELFPLAY: new best model, bumping up generation to 105
Ep done - 246000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.638    |
|    return_std      | 3.73     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12371    |
|    total_timesteps | 5984805  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 635      |
|    learning_rate   | 0.00352  |
|    step_size       | 0.000158 |
---------------------------------
Ep done - 247000.
Eval num_timesteps=6000000, episode_reward=0.69 +/- 0.71
Episode length: 30.11 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 6000000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.69
SELFPLAY: new best model, bumping up generation to 106
Ep done - 248000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.666    |
|    return_std      | 4.1      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12453    |
|    total_timesteps | 6020943  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 636      |
|    learning_rate   | 0.00352  |
|    step_size       | 0.000143 |
---------------------------------
Ep done - 249000.
Eval num_timesteps=6050000, episode_reward=0.78 +/- 0.61
Episode length: 30.10 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.775    |
| time/              |          |
|    total_timesteps | 6050000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.775
SELFPLAY: new best model, bumping up generation to 107
Ep done - 250000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.608    |
|    return_std      | 4.06     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12532    |
|    total_timesteps | 6057050  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 637      |
|    learning_rate   | 0.00352  |
|    step_size       | 0.000145 |
---------------------------------
Ep done - 251000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.635    |
|    return_std      | 3.56     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12591    |
|    total_timesteps | 6093116  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 638      |
|    learning_rate   | 0.00352  |
|    step_size       | 0.000164 |
---------------------------------
Eval num_timesteps=6100000, episode_reward=0.65 +/- 0.74
Episode length: 30.11 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.65     |
| time/              |          |
|    total_timesteps | 6100000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.65
SELFPLAY: new best model, bumping up generation to 108
Ep done - 252000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.633    |
|    return_std      | 4.12     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12673    |
|    total_timesteps | 6129205  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 639      |
|    learning_rate   | 0.00351  |
|    step_size       | 0.000142 |
---------------------------------
Ep done - 253000.
Eval num_timesteps=6150000, episode_reward=0.73 +/- 0.66
Episode length: 30.09 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.735    |
| time/              |          |
|    total_timesteps | 6150000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.735
SELFPLAY: new best model, bumping up generation to 109
Ep done - 254000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.639    |
|    return_std      | 5.28     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12752    |
|    total_timesteps | 6165315  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 640      |
|    learning_rate   | 0.00351  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 255000.
Eval num_timesteps=6200000, episode_reward=0.71 +/- 0.68
Episode length: 30.07 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.715    |
| time/              |          |
|    total_timesteps | 6200000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.715
SELFPLAY: new best model, bumping up generation to 110
Ep done - 256000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.625    |
|    return_std      | 3.63     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12831    |
|    total_timesteps | 6201414  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 641      |
|    learning_rate   | 0.00351  |
|    step_size       | 0.000161 |
---------------------------------
Ep done - 257000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.632    |
|    return_std      | 4.19     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12893    |
|    total_timesteps | 6237534  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 642      |
|    learning_rate   | 0.0035   |
|    step_size       | 0.000139 |
---------------------------------
Ep done - 258000.
Eval num_timesteps=6250000, episode_reward=0.73 +/- 0.67
Episode length: 30.12 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.733    |
| time/              |          |
|    total_timesteps | 6250000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7325
SELFPLAY: new best model, bumping up generation to 111
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.599    |
|    return_std      | 4.17     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 12972    |
|    total_timesteps | 6273621  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 643      |
|    learning_rate   | 0.0035   |
|    step_size       | 0.00014  |
---------------------------------
Ep done - 259000.
Ep done - 260000.
Eval num_timesteps=6300000, episode_reward=0.73 +/- 0.67
Episode length: 30.05 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.735    |
| time/              |          |
|    total_timesteps | 6300000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.735
SELFPLAY: new best model, bumping up generation to 112
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.577    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13051    |
|    total_timesteps | 6309689  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 644      |
|    learning_rate   | 0.0035   |
|    step_size       | 0.000289 |
---------------------------------
Ep done - 261000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.643    |
|    return_std      | 4.05     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13113    |
|    total_timesteps | 6345807  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 645      |
|    learning_rate   | 0.0035   |
|    step_size       | 0.000144 |
---------------------------------
Ep done - 262000.
Eval num_timesteps=6350000, episode_reward=0.81 +/- 0.55
Episode length: 30.18 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.815    |
| time/              |          |
|    total_timesteps | 6350000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.815
SELFPLAY: new best model, bumping up generation to 113
Ep done - 263000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.659    |
|    return_std      | 3.97     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13192    |
|    total_timesteps | 6381918  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 646      |
|    learning_rate   | 0.00349  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 264000.
Eval num_timesteps=6400000, episode_reward=0.80 +/- 0.58
Episode length: 30.14 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.797    |
| time/              |          |
|    total_timesteps | 6400000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7975
SELFPLAY: new best model, bumping up generation to 114
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.644    |
|    return_std      | 3.52     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13271    |
|    total_timesteps | 6418059  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 647      |
|    learning_rate   | 0.00349  |
|    step_size       | 0.000165 |
---------------------------------
Ep done - 265000.
Ep done - 266000.
Eval num_timesteps=6450000, episode_reward=0.74 +/- 0.66
Episode length: 30.06 +/- 1.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.74     |
| time/              |          |
|    total_timesteps | 6450000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.74
SELFPLAY: new best model, bumping up generation to 115
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.611    |
|    return_std      | 3.09     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13354    |
|    total_timesteps | 6454214  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 648      |
|    learning_rate   | 0.00349  |
|    step_size       | 0.000188 |
---------------------------------
Ep done - 267000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.636    |
|    return_std      | 3.8      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13414    |
|    total_timesteps | 6490295  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 649      |
|    learning_rate   | 0.00348  |
|    step_size       | 0.000153 |
---------------------------------
Ep done - 268000.
Eval num_timesteps=6500000, episode_reward=0.76 +/- 0.63
Episode length: 30.10 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.762    |
| time/              |          |
|    total_timesteps | 6500000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7625
SELFPLAY: new best model, bumping up generation to 116
Ep done - 269000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.638    |
|    return_std      | 2.18     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13494    |
|    total_timesteps | 6526402  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 650      |
|    learning_rate   | 0.00348  |
|    step_size       | 0.000266 |
---------------------------------
Ep done - 270000.
Eval num_timesteps=6550000, episode_reward=0.76 +/- 0.65
Episode length: 30.14 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.755    |
| time/              |          |
|    total_timesteps | 6550000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.755
SELFPLAY: new best model, bumping up generation to 117
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.676    |
|    return_std      | 3.84     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13576    |
|    total_timesteps | 6562506  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 651      |
|    learning_rate   | 0.00348  |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 271000.
Ep done - 272000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.653    |
|    return_std      | 4.05     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13635    |
|    total_timesteps | 6598619  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 652      |
|    learning_rate   | 0.00347  |
|    step_size       | 0.000143 |
---------------------------------
Eval num_timesteps=6600000, episode_reward=0.73 +/- 0.66
Episode length: 30.14 +/- 0.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.735    |
| time/              |          |
|    total_timesteps | 6600000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.735
SELFPLAY: new best model, bumping up generation to 118
Ep done - 273000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.696    |
|    return_std      | 3.46     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13714    |
|    total_timesteps | 6634796  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 653      |
|    learning_rate   | 0.00347  |
|    step_size       | 0.000167 |
---------------------------------
Ep done - 274000.
Eval num_timesteps=6650000, episode_reward=0.78 +/- 0.61
Episode length: 30.13 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.777    |
| time/              |          |
|    total_timesteps | 6650000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7775
SELFPLAY: new best model, bumping up generation to 119
Ep done - 275000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.652    |
|    return_std      | 3.3      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13796    |
|    total_timesteps | 6670930  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 654      |
|    learning_rate   | 0.00347  |
|    step_size       | 0.000175 |
---------------------------------
Ep done - 276000.
Eval num_timesteps=6700000, episode_reward=0.82 +/- 0.56
Episode length: 30.12 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.818    |
| time/              |          |
|    total_timesteps | 6700000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8175
SELFPLAY: new best model, bumping up generation to 120
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.661    |
|    return_std      | 4.36     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13875    |
|    total_timesteps | 6707046  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 655      |
|    learning_rate   | 0.00347  |
|    step_size       | 0.000133 |
---------------------------------
Ep done - 277000.
Ep done - 278000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.713    |
|    return_std      | 2.87     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 13935    |
|    total_timesteps | 6743177  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 656      |
|    learning_rate   | 0.00346  |
|    step_size       | 0.000201 |
---------------------------------
Eval num_timesteps=6750000, episode_reward=0.72 +/- 0.66
Episode length: 30.10 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.725    |
| time/              |          |
|    total_timesteps | 6750000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.725
SELFPLAY: new best model, bumping up generation to 121
Ep done - 279000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.68     |
|    return_std      | 4.58     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14017    |
|    total_timesteps | 6779328  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 657      |
|    learning_rate   | 0.00346  |
|    step_size       | 0.000126 |
---------------------------------
Ep done - 280000.
Eval num_timesteps=6800000, episode_reward=0.78 +/- 0.61
Episode length: 30.14 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.782    |
| time/              |          |
|    total_timesteps | 6800000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7825
SELFPLAY: new best model, bumping up generation to 122
Ep done - 281000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.687    |
|    return_std      | 3.75     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14095    |
|    total_timesteps | 6815455  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 658      |
|    learning_rate   | 0.00346  |
|    step_size       | 0.000154 |
---------------------------------
Ep done - 282000.
Eval num_timesteps=6850000, episode_reward=0.69 +/- 0.71
Episode length: 30.09 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.685    |
| time/              |          |
|    total_timesteps | 6850000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.685
SELFPLAY: new best model, bumping up generation to 123
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.683    |
|    return_std      | 3.33     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14177    |
|    total_timesteps | 6851591  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 659      |
|    learning_rate   | 0.00345  |
|    step_size       | 0.000173 |
---------------------------------
Ep done - 283000.
Ep done - 284000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.688    |
|    return_std      | 3.72     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14237    |
|    total_timesteps | 6887725  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 660      |
|    learning_rate   | 0.00345  |
|    step_size       | 0.000155 |
---------------------------------
Eval num_timesteps=6900000, episode_reward=0.76 +/- 0.63
Episode length: 30.11 +/- 1.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.76     |
| time/              |          |
|    total_timesteps | 6900000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.76
SELFPLAY: new best model, bumping up generation to 124
Ep done - 285000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.642    |
|    return_std      | 4.4      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14317    |
|    total_timesteps | 6923873  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 661      |
|    learning_rate   | 0.00345  |
|    step_size       | 0.000131 |
---------------------------------
Ep done - 286000.
Eval num_timesteps=6950000, episode_reward=0.76 +/- 0.63
Episode length: 30.09 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.762    |
| time/              |          |
|    total_timesteps | 6950000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7625
SELFPLAY: new best model, bumping up generation to 125
Ep done - 287000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.657    |
|    return_std      | 3.46     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14400    |
|    total_timesteps | 6959993  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 662      |
|    learning_rate   | 0.00345  |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 288000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.667    |
|    return_std      | 4.59     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14459    |
|    total_timesteps | 6996077  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 663      |
|    learning_rate   | 0.00344  |
|    step_size       | 0.000125 |
---------------------------------
Eval num_timesteps=7000000, episode_reward=0.80 +/- 0.59
Episode length: 30.19 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.795    |
| time/              |          |
|    total_timesteps | 7000000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.795
SELFPLAY: new best model, bumping up generation to 126
Ep done - 289000.
Ep done - 290000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.677    |
|    return_std      | 3.72     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14538    |
|    total_timesteps | 7032228  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 664      |
|    learning_rate   | 0.00344  |
|    step_size       | 0.000154 |
---------------------------------
Eval num_timesteps=7050000, episode_reward=0.83 +/- 0.53
Episode length: 30.12 +/- 1.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.83     |
| time/              |          |
|    total_timesteps | 7050000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.83
SELFPLAY: new best model, bumping up generation to 127
Ep done - 291000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.697    |
|    return_std      | 3.79     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14620    |
|    total_timesteps | 7068347  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 665      |
|    learning_rate   | 0.00344  |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 292000.
Ep done - 293000.
Eval num_timesteps=7100000, episode_reward=0.79 +/- 0.60
Episode length: 30.08 +/- 1.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.79     |
| time/              |          |
|    total_timesteps | 7100000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.79
SELFPLAY: new best model, bumping up generation to 128
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.676    |
|    return_std      | 3.9      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14700    |
|    total_timesteps | 7104499  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 666      |
|    learning_rate   | 0.00343  |
|    step_size       | 0.000147 |
---------------------------------
Ep done - 294000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.686    |
|    return_std      | 4.2      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14760    |
|    total_timesteps | 7140657  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 667      |
|    learning_rate   | 0.00343  |
|    step_size       | 0.000136 |
---------------------------------
Ep done - 295000.
Eval num_timesteps=7150000, episode_reward=0.81 +/- 0.57
Episode length: 30.13 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.807    |
| time/              |          |
|    total_timesteps | 7150000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8075
SELFPLAY: new best model, bumping up generation to 129
Ep done - 296000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.644    |
|    return_std      | 4.54     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14842    |
|    total_timesteps | 7176736  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 668      |
|    learning_rate   | 0.00343  |
|    step_size       | 0.000126 |
---------------------------------
Ep done - 297000.
Eval num_timesteps=7200000, episode_reward=0.80 +/- 0.59
Episode length: 30.14 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 7200000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8
SELFPLAY: new best model, bumping up generation to 130
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.689    |
|    return_std      | 3.3      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14921    |
|    total_timesteps | 7212871  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 669      |
|    learning_rate   | 0.00343  |
|    step_size       | 0.000173 |
---------------------------------
Ep done - 298000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.677    |
|    return_std      | 3.04     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 14981    |
|    total_timesteps | 7249000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 670      |
|    learning_rate   | 0.00342  |
|    step_size       | 0.000188 |
---------------------------------
Ep done - 299000.
Eval num_timesteps=7250000, episode_reward=0.72 +/- 0.66
Episode length: 30.15 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.72     |
| time/              |          |
|    total_timesteps | 7250000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.72
SELFPLAY: new best model, bumping up generation to 131
Ep done - 300000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.689    |
|    return_std      | 3.51     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15062    |
|    total_timesteps | 7285128  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 671      |
|    learning_rate   | 0.00342  |
|    step_size       | 0.000162 |
---------------------------------
Ep done - 301000.
Eval num_timesteps=7300000, episode_reward=0.84 +/- 0.53
Episode length: 30.20 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.838    |
| time/              |          |
|    total_timesteps | 7300000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8375
SELFPLAY: new best model, bumping up generation to 132
Ep done - 302000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.672    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15141    |
|    total_timesteps | 7321243  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 672      |
|    learning_rate   | 0.00342  |
|    step_size       | 0.000239 |
---------------------------------
Ep done - 303000.
Eval num_timesteps=7350000, episode_reward=0.80 +/- 0.58
Episode length: 30.16 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.797    |
| time/              |          |
|    total_timesteps | 7350000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7975
SELFPLAY: new best model, bumping up generation to 133
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.682    |
|    return_std      | 4.19     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15220    |
|    total_timesteps | 7357347  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 673      |
|    learning_rate   | 0.00341  |
|    step_size       | 0.000136 |
---------------------------------
Ep done - 304000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.682    |
|    return_std      | 3.62     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15282    |
|    total_timesteps | 7393449  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 674      |
|    learning_rate   | 0.00341  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 305000.
Eval num_timesteps=7400000, episode_reward=0.79 +/- 0.58
Episode length: 30.17 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.79     |
| time/              |          |
|    total_timesteps | 7400000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.79
SELFPLAY: new best model, bumping up generation to 134
Ep done - 306000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.722    |
|    return_std      | 4.22     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15361    |
|    total_timesteps | 7429578  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 675      |
|    learning_rate   | 0.00341  |
|    step_size       | 0.000135 |
---------------------------------
Ep done - 307000.
Eval num_timesteps=7450000, episode_reward=0.76 +/- 0.63
Episode length: 30.13 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.762    |
| time/              |          |
|    total_timesteps | 7450000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7625
SELFPLAY: new best model, bumping up generation to 135
Ep done - 308000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.713    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15443    |
|    total_timesteps | 7465724  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 676      |
|    learning_rate   | 0.00341  |
|    step_size       | 0.000264 |
---------------------------------
Ep done - 309000.
Eval num_timesteps=7500000, episode_reward=0.81 +/- 0.58
Episode length: 30.18 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.81     |
| time/              |          |
|    total_timesteps | 7500000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.81
SELFPLAY: new best model, bumping up generation to 136
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.698    |
|    return_std      | 3.6      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15524    |
|    total_timesteps | 7501842  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 677      |
|    learning_rate   | 0.0034   |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 310000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.68     |
|    return_std      | 4.79     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15583    |
|    total_timesteps | 7537955  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 678      |
|    learning_rate   | 0.0034   |
|    step_size       | 0.000118 |
---------------------------------
Ep done - 311000.
Eval num_timesteps=7550000, episode_reward=0.82 +/- 0.56
Episode length: 30.14 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.818    |
| time/              |          |
|    total_timesteps | 7550000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8175
SELFPLAY: new best model, bumping up generation to 137
Ep done - 312000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.683    |
|    return_std      | 4.88     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15662    |
|    total_timesteps | 7574065  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 679      |
|    learning_rate   | 0.0034   |
|    step_size       | 0.000116 |
---------------------------------
Ep done - 313000.
Eval num_timesteps=7600000, episode_reward=0.77 +/- 0.63
Episode length: 30.08 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.77     |
| time/              |          |
|    total_timesteps | 7600000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.77
SELFPLAY: new best model, bumping up generation to 138
Ep done - 314000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.702    |
|    return_std      | 4.29     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15744    |
|    total_timesteps | 7610253  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 680      |
|    learning_rate   | 0.00339  |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 315000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.699    |
|    return_std      | 3.52     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15804    |
|    total_timesteps | 7646383  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 681      |
|    learning_rate   | 0.00339  |
|    step_size       | 0.000161 |
---------------------------------
Eval num_timesteps=7650000, episode_reward=0.82 +/- 0.56
Episode length: 30.16 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.818    |
| time/              |          |
|    total_timesteps | 7650000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8175
SELFPLAY: new best model, bumping up generation to 139
Ep done - 316000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.721    |
|    return_std      | 3.65     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15883    |
|    total_timesteps | 7682567  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 682      |
|    learning_rate   | 0.00339  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 317000.
Eval num_timesteps=7700000, episode_reward=0.82 +/- 0.56
Episode length: 30.16 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.823    |
| time/              |          |
|    total_timesteps | 7700000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8225
SELFPLAY: new best model, bumping up generation to 140
Ep done - 318000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.668    |
|    return_std      | 3.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 15965    |
|    total_timesteps | 7718693  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 683      |
|    learning_rate   | 0.00339  |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 319000.
Eval num_timesteps=7750000, episode_reward=0.78 +/- 0.61
Episode length: 30.16 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.777    |
| time/              |          |
|    total_timesteps | 7750000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7775
SELFPLAY: new best model, bumping up generation to 141
Ep done - 320000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.69     |
|    return_std      | 2.64     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16044    |
|    total_timesteps | 7754817  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 684      |
|    learning_rate   | 0.00338  |
|    step_size       | 0.000213 |
---------------------------------
Ep done - 321000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.699    |
|    return_std      | 3.55     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16104    |
|    total_timesteps | 7790934  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 685      |
|    learning_rate   | 0.00338  |
|    step_size       | 0.000159 |
---------------------------------
Eval num_timesteps=7800000, episode_reward=0.77 +/- 0.62
Episode length: 30.20 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.767    |
| time/              |          |
|    total_timesteps | 7800000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7675
SELFPLAY: new best model, bumping up generation to 142
Ep done - 322000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.698    |
|    return_std      | 2.77     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16186    |
|    total_timesteps | 7827074  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 686      |
|    learning_rate   | 0.00338  |
|    step_size       | 0.000203 |
---------------------------------
Ep done - 323000.
Eval num_timesteps=7850000, episode_reward=0.78 +/- 0.61
Episode length: 30.15 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.777    |
| time/              |          |
|    total_timesteps | 7850000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7775
SELFPLAY: new best model, bumping up generation to 143
Ep done - 324000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.703    |
|    return_std      | 4.17     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16265    |
|    total_timesteps | 7863191  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 687      |
|    learning_rate   | 0.00337  |
|    step_size       | 0.000135 |
---------------------------------
Ep done - 325000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.703    |
|    return_std      | 2.75     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16327    |
|    total_timesteps | 7899343  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 688      |
|    learning_rate   | 0.00337  |
|    step_size       | 0.000205 |
---------------------------------
Ep done - 326000.
Eval num_timesteps=7900000, episode_reward=0.82 +/- 0.56
Episode length: 30.14 +/- 0.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.818    |
| time/              |          |
|    total_timesteps | 7900000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8175
SELFPLAY: new best model, bumping up generation to 144
Ep done - 327000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.703    |
|    return_std      | 3.03     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16407    |
|    total_timesteps | 7935509  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 689      |
|    learning_rate   | 0.00337  |
|    step_size       | 0.000185 |
---------------------------------
Ep done - 328000.
Eval num_timesteps=7950000, episode_reward=0.79 +/- 0.60
Episode length: 30.18 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.792    |
| time/              |          |
|    total_timesteps | 7950000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7925
SELFPLAY: new best model, bumping up generation to 145
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.709    |
|    return_std      | 3.02     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16486    |
|    total_timesteps | 7971648  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 690      |
|    learning_rate   | 0.00337  |
|    step_size       | 0.000186 |
---------------------------------
Ep done - 329000.
Ep done - 330000.
Eval num_timesteps=8000000, episode_reward=0.81 +/- 0.57
Episode length: 30.20 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.807    |
| time/              |          |
|    total_timesteps | 8000000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8075
SELFPLAY: new best model, bumping up generation to 146
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.7      |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16568    |
|    total_timesteps | 8007811  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 691      |
|    learning_rate   | 0.00336  |
|    step_size       | 0.000207 |
---------------------------------
Ep done - 331000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.682    |
|    return_std      | 4.86     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16627    |
|    total_timesteps | 8043983  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 692      |
|    learning_rate   | 0.00336  |
|    step_size       | 0.000115 |
---------------------------------
Ep done - 332000.
Eval num_timesteps=8050000, episode_reward=0.78 +/- 0.62
Episode length: 30.17 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.775    |
| time/              |          |
|    total_timesteps | 8050000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.775
SELFPLAY: new best model, bumping up generation to 147
Ep done - 333000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.741    |
|    return_std      | 3        |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16706    |
|    total_timesteps | 8080168  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 693      |
|    learning_rate   | 0.00336  |
|    step_size       | 0.000187 |
---------------------------------
Ep done - 334000.
Eval num_timesteps=8100000, episode_reward=0.83 +/- 0.55
Episode length: 30.21 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.828    |
| time/              |          |
|    total_timesteps | 8100000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8275
SELFPLAY: new best model, bumping up generation to 148
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.726    |
|    return_std      | 4.31     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16788    |
|    total_timesteps | 8116311  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 694      |
|    learning_rate   | 0.00335  |
|    step_size       | 0.00013  |
---------------------------------
Ep done - 335000.
Ep done - 336000.
Eval num_timesteps=8150000, episode_reward=0.85 +/- 0.50
Episode length: 30.22 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.855    |
| time/              |          |
|    total_timesteps | 8150000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.855
SELFPLAY: new best model, bumping up generation to 149
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.727    |
|    return_std      | 3.03     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16867    |
|    total_timesteps | 8152501  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 695      |
|    learning_rate   | 0.00335  |
|    step_size       | 0.000185 |
---------------------------------
Ep done - 337000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.711    |
|    return_std      | 2.61     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 16926    |
|    total_timesteps | 8188633  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 696      |
|    learning_rate   | 0.00335  |
|    step_size       | 0.000214 |
---------------------------------
Ep done - 338000.
Eval num_timesteps=8200000, episode_reward=0.81 +/- 0.55
Episode length: 30.16 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.812    |
| time/              |          |
|    total_timesteps | 8200000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8125
SELFPLAY: new best model, bumping up generation to 150
Ep done - 339000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.678    |
|    return_std      | 2.73     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17008    |
|    total_timesteps | 8224768  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 697      |
|    learning_rate   | 0.00334  |
|    step_size       | 0.000204 |
---------------------------------
Ep done - 340000.
Eval num_timesteps=8250000, episode_reward=0.80 +/- 0.58
Episode length: 30.09 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 8250000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8
SELFPLAY: new best model, bumping up generation to 151
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.691    |
|    return_std      | 3.26     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17087    |
|    total_timesteps | 8260864  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 698      |
|    learning_rate   | 0.00334  |
|    step_size       | 0.000171 |
---------------------------------
Ep done - 341000.
Ep done - 342000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.746    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17147    |
|    total_timesteps | 8297029  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 699      |
|    learning_rate   | 0.00334  |
|    step_size       | 0.000259 |
---------------------------------
Eval num_timesteps=8300000, episode_reward=0.81 +/- 0.57
Episode length: 30.23 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.81     |
| time/              |          |
|    total_timesteps | 8300000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.81
SELFPLAY: new best model, bumping up generation to 152
Ep done - 343000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.746    |
|    return_std      | 4.03     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17228    |
|    total_timesteps | 8333188  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 700      |
|    learning_rate   | 0.00334  |
|    step_size       | 0.000138 |
---------------------------------
Ep done - 344000.
Eval num_timesteps=8350000, episode_reward=0.81 +/- 0.57
Episode length: 30.25 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.807    |
| time/              |          |
|    total_timesteps | 8350000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8075
SELFPLAY: new best model, bumping up generation to 153
Ep done - 345000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.735    |
|    return_std      | 3.49     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17308    |
|    total_timesteps | 8369339  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 701      |
|    learning_rate   | 0.00333  |
|    step_size       | 0.000159 |
---------------------------------
Ep done - 346000.
Eval num_timesteps=8400000, episode_reward=0.86 +/- 0.48
Episode length: 30.23 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.863    |
| time/              |          |
|    total_timesteps | 8400000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8625
SELFPLAY: new best model, bumping up generation to 154
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.704    |
|    return_std      | 3.11     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17390    |
|    total_timesteps | 8405520  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 702      |
|    learning_rate   | 0.00333  |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 347000.
Ep done - 348000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.757    |
|    return_std      | 4.89     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17449    |
|    total_timesteps | 8441693  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 703      |
|    learning_rate   | 0.00333  |
|    step_size       | 0.000113 |
---------------------------------
Eval num_timesteps=8450000, episode_reward=0.83 +/- 0.54
Episode length: 30.10 +/- 1.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.828    |
| time/              |          |
|    total_timesteps | 8450000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8275
SELFPLAY: new best model, bumping up generation to 155
Ep done - 349000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.707    |
|    return_std      | 3.39     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17528    |
|    total_timesteps | 8477819  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 704      |
|    learning_rate   | 0.00332  |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 350000.
Eval num_timesteps=8500000, episode_reward=0.81 +/- 0.57
Episode length: 30.14 +/- 1.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.807    |
| time/              |          |
|    total_timesteps | 8500000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8075
SELFPLAY: new best model, bumping up generation to 156
Ep done - 351000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.713    |
|    return_std      | 1.98     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17611    |
|    total_timesteps | 8513931  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 705      |
|    learning_rate   | 0.00332  |
|    step_size       | 0.00028  |
---------------------------------
Ep done - 352000.
Eval num_timesteps=8550000, episode_reward=0.83 +/- 0.55
Episode length: 30.23 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.828    |
| time/              |          |
|    total_timesteps | 8550000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8275
SELFPLAY: new best model, bumping up generation to 157
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.743    |
|    return_std      | 3.58     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17691    |
|    total_timesteps | 8550090  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 706      |
|    learning_rate   | 0.00332  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 353000.
Ep done - 354000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.726    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17751    |
|    total_timesteps | 8586182  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 707      |
|    learning_rate   | 0.00332  |
|    step_size       | 0.000273 |
---------------------------------
Eval num_timesteps=8600000, episode_reward=0.80 +/- 0.58
Episode length: 30.24 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 8600000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8
SELFPLAY: new best model, bumping up generation to 158
Ep done - 355000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.717    |
|    return_std      | 3.53     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17830    |
|    total_timesteps | 8622277  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 708      |
|    learning_rate   | 0.00331  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 356000.
Eval num_timesteps=8650000, episode_reward=0.91 +/- 0.40
Episode length: 30.20 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 8650000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 159
Ep done - 357000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.745    |
|    return_std      | 5.64     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17912    |
|    total_timesteps | 8658419  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 709      |
|    learning_rate   | 0.00331  |
|    step_size       | 9.79e-05 |
---------------------------------
Ep done - 358000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.737    |
|    return_std      | 2.54     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 17971    |
|    total_timesteps | 8694600  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 710      |
|    learning_rate   | 0.00331  |
|    step_size       | 0.000217 |
---------------------------------
Eval num_timesteps=8700000, episode_reward=0.84 +/- 0.53
Episode length: 30.18 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.84     |
| time/              |          |
|    total_timesteps | 8700000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.84
SELFPLAY: new best model, bumping up generation to 160
Ep done - 359000.
Ep done - 360000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.712    |
|    return_std      | 3.84     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18052    |
|    total_timesteps | 8730753  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 711      |
|    learning_rate   | 0.0033   |
|    step_size       | 0.000143 |
---------------------------------
Ep done - 361000.
Eval num_timesteps=8750000, episode_reward=0.82 +/- 0.55
Episode length: 30.15 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.825    |
| time/              |          |
|    total_timesteps | 8750000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.825
SELFPLAY: new best model, bumping up generation to 161
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.699    |
|    return_std      | 3.19     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18132    |
|    total_timesteps | 8766902  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 712      |
|    learning_rate   | 0.0033   |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 362000.
Ep done - 363000.
Eval num_timesteps=8800000, episode_reward=0.91 +/- 0.40
Episode length: 30.22 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 8800000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 162
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.734    |
|    return_std      | 3.51     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18212    |
|    total_timesteps | 8803067  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 713      |
|    learning_rate   | 0.0033   |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 364000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.71     |
|    return_std      | 3.65     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18274    |
|    total_timesteps | 8839238  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 714      |
|    learning_rate   | 0.0033   |
|    step_size       | 0.00015  |
---------------------------------
Ep done - 365000.
Eval num_timesteps=8850000, episode_reward=0.84 +/- 0.53
Episode length: 30.17 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.838    |
| time/              |          |
|    total_timesteps | 8850000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8375
SELFPLAY: new best model, bumping up generation to 163
Ep done - 366000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.718    |
|    return_std      | 3.12     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18354    |
|    total_timesteps | 8875424  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 715      |
|    learning_rate   | 0.00329  |
|    step_size       | 0.000176 |
---------------------------------
Ep done - 367000.
Eval num_timesteps=8900000, episode_reward=0.79 +/- 0.60
Episode length: 30.18 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.792    |
| time/              |          |
|    total_timesteps | 8900000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7925
SELFPLAY: new best model, bumping up generation to 164
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.74     |
|    return_std      | 2.17     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18433    |
|    total_timesteps | 8911590  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 716      |
|    learning_rate   | 0.00329  |
|    step_size       | 0.000253 |
---------------------------------
Ep done - 368000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.744    |
|    return_std      | 2.64     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18495    |
|    total_timesteps | 8947712  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 717      |
|    learning_rate   | 0.00329  |
|    step_size       | 0.000207 |
---------------------------------
Ep done - 369000.
Eval num_timesteps=8950000, episode_reward=0.84 +/- 0.51
Episode length: 30.23 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.843    |
| time/              |          |
|    total_timesteps | 8950000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8425
SELFPLAY: new best model, bumping up generation to 165
Ep done - 370000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.736    |
|    return_std      | 4.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18574    |
|    total_timesteps | 8983857  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 718      |
|    learning_rate   | 0.00328  |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 371000.
Eval num_timesteps=9000000, episode_reward=0.84 +/- 0.51
Episode length: 30.23 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.845    |
| time/              |          |
|    total_timesteps | 9000000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.845
SELFPLAY: new best model, bumping up generation to 166
Ep done - 372000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.728    |
|    return_std      | 2.25     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18654    |
|    total_timesteps | 9020041  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 719      |
|    learning_rate   | 0.00328  |
|    step_size       | 0.000243 |
---------------------------------
Ep done - 373000.
Eval num_timesteps=9050000, episode_reward=0.82 +/- 0.56
Episode length: 30.21 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.82     |
| time/              |          |
|    total_timesteps | 9050000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.82
SELFPLAY: new best model, bumping up generation to 167
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.71     |
|    return_std      | 4.85     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18736    |
|    total_timesteps | 9056184  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 720      |
|    learning_rate   | 0.00328  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 374000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.69     |
|    return_std      | 3.84     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18795    |
|    total_timesteps | 9092311  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 721      |
|    learning_rate   | 0.00328  |
|    step_size       | 0.000142 |
---------------------------------
Ep done - 375000.
Eval num_timesteps=9100000, episode_reward=0.83 +/- 0.53
Episode length: 30.14 +/- 1.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.83     |
| time/              |          |
|    total_timesteps | 9100000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.83
SELFPLAY: new best model, bumping up generation to 168
Ep done - 376000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.748    |
|    return_std      | 4.65     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18874    |
|    total_timesteps | 9128527  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 722      |
|    learning_rate   | 0.00327  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 377000.
Eval num_timesteps=9150000, episode_reward=0.80 +/- 0.59
Episode length: 30.17 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.802    |
| time/              |          |
|    total_timesteps | 9150000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8025
SELFPLAY: new best model, bumping up generation to 169
Ep done - 378000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.756    |
|    return_std      | 3.37     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 18956    |
|    total_timesteps | 9164697  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 723      |
|    learning_rate   | 0.00327  |
|    step_size       | 0.000162 |
---------------------------------
Ep done - 379000.
Eval num_timesteps=9200000, episode_reward=0.88 +/- 0.46
Episode length: 30.12 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 9200000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 170
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.772    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19036    |
|    total_timesteps | 9200908  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 724      |
|    learning_rate   | 0.00327  |
|    step_size       | 0.000224 |
---------------------------------
Ep done - 380000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.762    |
|    return_std      | 0.9      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19096    |
|    total_timesteps | 9237049  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 725      |
|    learning_rate   | 0.00326  |
|    step_size       | 0.000604 |
---------------------------------
Ep done - 381000.
Eval num_timesteps=9250000, episode_reward=0.80 +/- 0.59
Episode length: 30.21 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.795    |
| time/              |          |
|    total_timesteps | 9250000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.795
SELFPLAY: new best model, bumping up generation to 171
Ep done - 382000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.741    |
|    return_std      | 3.68     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19177    |
|    total_timesteps | 9273239  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 726      |
|    learning_rate   | 0.00326  |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 383000.
Eval num_timesteps=9300000, episode_reward=0.86 +/- 0.48
Episode length: 30.18 +/- 1.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.863    |
| time/              |          |
|    total_timesteps | 9300000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8625
SELFPLAY: new best model, bumping up generation to 172
Ep done - 384000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.726    |
|    return_std      | 3.03     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19256    |
|    total_timesteps | 9309420  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 727      |
|    learning_rate   | 0.00326  |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 385000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.721    |
|    return_std      | 3.28     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19316    |
|    total_timesteps | 9345582  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 728      |
|    learning_rate   | 0.00326  |
|    step_size       | 0.000165 |
---------------------------------
Eval num_timesteps=9350000, episode_reward=0.80 +/- 0.59
Episode length: 30.20 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.802    |
| time/              |          |
|    total_timesteps | 9350000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8025
SELFPLAY: new best model, bumping up generation to 173
Ep done - 386000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.744    |
|    return_std      | 4.87     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19398    |
|    total_timesteps | 9381773  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 729      |
|    learning_rate   | 0.00325  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 387000.
Eval num_timesteps=9400000, episode_reward=0.81 +/- 0.56
Episode length: 30.15 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.815    |
| time/              |          |
|    total_timesteps | 9400000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.815
SELFPLAY: new best model, bumping up generation to 174
Ep done - 388000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.744    |
|    return_std      | 2.14     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19477    |
|    total_timesteps | 9417911  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 730      |
|    learning_rate   | 0.00325  |
|    step_size       | 0.000253 |
---------------------------------
Ep done - 389000.
Eval num_timesteps=9450000, episode_reward=0.87 +/- 0.48
Episode length: 30.23 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.87     |
| time/              |          |
|    total_timesteps | 9450000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.87
SELFPLAY: new best model, bumping up generation to 175
Ep done - 390000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.708    |
|    return_std      | 4.1      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19559    |
|    total_timesteps | 9454070  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 731      |
|    learning_rate   | 0.00325  |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 391000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.757    |
|    return_std      | 2.79     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19618    |
|    total_timesteps | 9490255  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 732      |
|    learning_rate   | 0.00324  |
|    step_size       | 0.000194 |
---------------------------------
Eval num_timesteps=9500000, episode_reward=0.85 +/- 0.49
Episode length: 30.19 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.85     |
| time/              |          |
|    total_timesteps | 9500000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.85
SELFPLAY: new best model, bumping up generation to 176
Ep done - 392000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.757    |
|    return_std      | 3.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19697    |
|    total_timesteps | 9526393  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 733      |
|    learning_rate   | 0.00324  |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 393000.
Eval num_timesteps=9550000, episode_reward=0.84 +/- 0.53
Episode length: 30.18 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.84     |
| time/              |          |
|    total_timesteps | 9550000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.84
SELFPLAY: new best model, bumping up generation to 177
Ep done - 394000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.747    |
|    return_std      | 4.42     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19779    |
|    total_timesteps | 9562505  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 734      |
|    learning_rate   | 0.00324  |
|    step_size       | 0.000122 |
---------------------------------
Ep done - 395000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.761    |
|    return_std      | 3.28     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19840    |
|    total_timesteps | 9598694  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 735      |
|    learning_rate   | 0.00323  |
|    step_size       | 0.000164 |
---------------------------------
Ep done - 396000.
Eval num_timesteps=9600000, episode_reward=0.84 +/- 0.53
Episode length: 30.23 +/- 0.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.838    |
| time/              |          |
|    total_timesteps | 9600000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8375
SELFPLAY: new best model, bumping up generation to 178
Ep done - 397000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.756    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 19919    |
|    total_timesteps | 9634863  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 736      |
|    learning_rate   | 0.00323  |
|    step_size       | 0.000279 |
---------------------------------
Ep done - 398000.
Eval num_timesteps=9650000, episode_reward=0.86 +/- 0.49
Episode length: 30.21 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.858    |
| time/              |          |
|    total_timesteps | 9650000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8575
SELFPLAY: new best model, bumping up generation to 179
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.752    |
|    return_std      | 3.08     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20000    |
|    total_timesteps | 9671029  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 737      |
|    learning_rate   | 0.00323  |
|    step_size       | 0.000174 |
---------------------------------
Ep done - 399000.
Ep done - 400000.
Eval num_timesteps=9700000, episode_reward=0.82 +/- 0.56
Episode length: 30.19 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.818    |
| time/              |          |
|    total_timesteps | 9700000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8175
SELFPLAY: new best model, bumping up generation to 180
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.702    |
|    return_std      | 3.75     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20080    |
|    total_timesteps | 9707200  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 738      |
|    learning_rate   | 0.00323  |
|    step_size       | 0.000143 |
---------------------------------
Ep done - 401000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.73     |
|    return_std      | 3.13     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20140    |
|    total_timesteps | 9743342  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 739      |
|    learning_rate   | 0.00322  |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 402000.
Eval num_timesteps=9750000, episode_reward=0.79 +/- 0.59
Episode length: 30.21 +/- 0.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.787    |
| time/              |          |
|    total_timesteps | 9750000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.7875
SELFPLAY: new best model, bumping up generation to 181
Ep done - 403000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.751    |
|    return_std      | 3.52     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20222    |
|    total_timesteps | 9779497  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 740      |
|    learning_rate   | 0.00322  |
|    step_size       | 0.000152 |
---------------------------------
Ep done - 404000.
Eval num_timesteps=9800000, episode_reward=0.82 +/- 0.56
Episode length: 30.21 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.82     |
| time/              |          |
|    total_timesteps | 9800000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.82
SELFPLAY: new best model, bumping up generation to 182
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.713    |
|    return_std      | 3.67     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20301    |
|    total_timesteps | 9815655  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 741      |
|    learning_rate   | 0.00322  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 405000.
Ep done - 406000.
Eval num_timesteps=9850000, episode_reward=0.81 +/- 0.57
Episode length: 30.18 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.81     |
| time/              |          |
|    total_timesteps | 9850000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.81
SELFPLAY: new best model, bumping up generation to 183
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.743    |
|    return_std      | 3.03     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20380    |
|    total_timesteps | 9851804  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 742      |
|    learning_rate   | 0.00321  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 407000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.731    |
|    return_std      | 1.95     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20443    |
|    total_timesteps | 9887935  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 743      |
|    learning_rate   | 0.00321  |
|    step_size       | 0.000275 |
---------------------------------
Ep done - 408000.
Eval num_timesteps=9900000, episode_reward=0.82 +/- 0.55
Episode length: 30.18 +/- 0.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.82     |
| time/              |          |
|    total_timesteps | 9900000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.82
SELFPLAY: new best model, bumping up generation to 184
Ep done - 409000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.738    |
|    return_std      | 3.62     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20522    |
|    total_timesteps | 9924092  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 744      |
|    learning_rate   | 0.00321  |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 410000.
Eval num_timesteps=9950000, episode_reward=0.81 +/- 0.57
Episode length: 30.10 +/- 1.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.807    |
| time/              |          |
|    total_timesteps | 9950000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8075
SELFPLAY: new best model, bumping up generation to 185
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.738    |
|    return_std      | 3.66     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20601    |
|    total_timesteps | 9960201  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 745      |
|    learning_rate   | 0.00321  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 411000.
Ep done - 412000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.749    |
|    return_std      | 3.39     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20663    |
|    total_timesteps | 9996391  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 746      |
|    learning_rate   | 0.0032   |
|    step_size       | 0.000158 |
---------------------------------
Eval num_timesteps=10000000, episode_reward=0.86 +/- 0.49
Episode length: 30.10 +/- 1.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.86     |
| time/              |          |
|    total_timesteps | 10000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.86
SELFPLAY: new best model, bumping up generation to 186
Ep done - 413000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.739    |
|    return_std      | 2.53     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20742    |
|    total_timesteps | 10032574 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 747      |
|    learning_rate   | 0.0032   |
|    step_size       | 0.000211 |
---------------------------------
Ep done - 414000.
Eval num_timesteps=10050000, episode_reward=0.80 +/- 0.59
Episode length: 30.20 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 10050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8
SELFPLAY: new best model, bumping up generation to 187
Ep done - 415000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.762    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20821    |
|    total_timesteps | 10068753 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 748      |
|    learning_rate   | 0.0032   |
|    step_size       | 0.000248 |
---------------------------------
Ep done - 416000.
Eval num_timesteps=10100000, episode_reward=0.89 +/- 0.43
Episode length: 30.22 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 10100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 188
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.752    |
|    return_std      | 3.12     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20903    |
|    total_timesteps | 10104912 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 749      |
|    learning_rate   | 0.00319  |
|    step_size       | 0.000171 |
---------------------------------
Ep done - 417000.
Ep done - 418000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.752    |
|    return_std      | 3.66     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 20963    |
|    total_timesteps | 10141113 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 750      |
|    learning_rate   | 0.00319  |
|    step_size       | 0.000145 |
---------------------------------
Eval num_timesteps=10150000, episode_reward=0.84 +/- 0.52
Episode length: 30.16 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.84     |
| time/              |          |
|    total_timesteps | 10150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.84
SELFPLAY: new best model, bumping up generation to 189
Ep done - 419000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.757    |
|    return_std      | 2.58     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21042    |
|    total_timesteps | 10177282 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 751      |
|    learning_rate   | 0.00319  |
|    step_size       | 0.000206 |
---------------------------------
Ep done - 420000.
Eval num_timesteps=10200000, episode_reward=0.80 +/- 0.59
Episode length: 30.16 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 10200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8
SELFPLAY: new best model, bumping up generation to 190
Ep done - 421000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.755    |
|    return_std      | 3.42     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21124    |
|    total_timesteps | 10213459 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 752      |
|    learning_rate   | 0.00319  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 422000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.757    |
|    return_std      | 2.64     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21183    |
|    total_timesteps | 10249660 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 753      |
|    learning_rate   | 0.00318  |
|    step_size       | 0.000201 |
---------------------------------
Eval num_timesteps=10250000, episode_reward=0.82 +/- 0.54
Episode length: 30.14 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.823    |
| time/              |          |
|    total_timesteps | 10250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8225
SELFPLAY: new best model, bumping up generation to 191
Ep done - 423000.
Ep done - 424000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.799    |
|    return_std      | 2.48     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21262    |
|    total_timesteps | 10285848 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 754      |
|    learning_rate   | 0.00318  |
|    step_size       | 0.000214 |
---------------------------------
Eval num_timesteps=10300000, episode_reward=0.87 +/- 0.48
Episode length: 30.19 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.873    |
| time/              |          |
|    total_timesteps | 10300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8725
SELFPLAY: new best model, bumping up generation to 192
Ep done - 425000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.769    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21344    |
|    total_timesteps | 10322074 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 755      |
|    learning_rate   | 0.00318  |
|    step_size       | 0.000251 |
---------------------------------
Ep done - 426000.
Eval num_timesteps=10350000, episode_reward=0.79 +/- 0.60
Episode length: 30.09 +/- 1.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.79     |
| time/              |          |
|    total_timesteps | 10350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.79
SELFPLAY: new best model, bumping up generation to 193
Ep done - 427000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.777    |
|    return_std      | 5.12     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21423    |
|    total_timesteps | 10358226 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 756      |
|    learning_rate   | 0.00317  |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 428000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.771    |
|    return_std      | 1.86     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21485    |
|    total_timesteps | 10394423 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 757      |
|    learning_rate   | 0.00317  |
|    step_size       | 0.000283 |
---------------------------------
Eval num_timesteps=10400000, episode_reward=0.83 +/- 0.53
Episode length: 30.18 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.835    |
| time/              |          |
|    total_timesteps | 10400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.835
SELFPLAY: new best model, bumping up generation to 194
Ep done - 429000.
Ep done - 430000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.763    |
|    return_std      | 2.83     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21564    |
|    total_timesteps | 10430615 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 758      |
|    learning_rate   | 0.00317  |
|    step_size       | 0.000186 |
---------------------------------
Ep done - 431000.
Eval num_timesteps=10450000, episode_reward=0.83 +/- 0.53
Episode length: 30.18 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.835    |
| time/              |          |
|    total_timesteps | 10450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.835
SELFPLAY: new best model, bumping up generation to 195
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.776    |
|    return_std      | 2.99     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21644    |
|    total_timesteps | 10466765 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 759      |
|    learning_rate   | 0.00317  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 432000.
Ep done - 433000.
Eval num_timesteps=10500000, episode_reward=0.88 +/- 0.47
Episode length: 30.23 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.875    |
| time/              |          |
|    total_timesteps | 10500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.875
SELFPLAY: new best model, bumping up generation to 196
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.752    |
|    return_std      | 2.34     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21726    |
|    total_timesteps | 10502966 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 760      |
|    learning_rate   | 0.00316  |
|    step_size       | 0.000225 |
---------------------------------
Ep done - 434000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.765    |
|    return_std      | 2.95     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21785    |
|    total_timesteps | 10539113 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 761      |
|    learning_rate   | 0.00316  |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 435000.
Eval num_timesteps=10550000, episode_reward=0.85 +/- 0.51
Episode length: 30.15 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.85     |
| time/              |          |
|    total_timesteps | 10550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.85
SELFPLAY: new best model, bumping up generation to 197
Ep done - 436000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.75     |
|    return_std      | 3.13     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21864    |
|    total_timesteps | 10575295 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 762      |
|    learning_rate   | 0.00316  |
|    step_size       | 0.000168 |
---------------------------------
Ep done - 437000.
Eval num_timesteps=10600000, episode_reward=0.80 +/- 0.59
Episode length: 30.14 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 10600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8
SELFPLAY: new best model, bumping up generation to 198
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.777    |
|    return_std      | 3.23     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 21946    |
|    total_timesteps | 10611451 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 763      |
|    learning_rate   | 0.00315  |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 438000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.78     |
|    return_std      | 2.53     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22007    |
|    total_timesteps | 10647630 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 764      |
|    learning_rate   | 0.00315  |
|    step_size       | 0.000207 |
---------------------------------
Ep done - 439000.
Eval num_timesteps=10650000, episode_reward=0.80 +/- 0.59
Episode length: 30.18 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.795    |
| time/              |          |
|    total_timesteps | 10650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.795
SELFPLAY: new best model, bumping up generation to 199
Ep done - 440000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.762    |
|    return_std      | 2.61     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22086    |
|    total_timesteps | 10683845 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 765      |
|    learning_rate   | 0.00315  |
|    step_size       | 0.000201 |
---------------------------------
Ep done - 441000.
Eval num_timesteps=10700000, episode_reward=0.81 +/- 0.57
Episode length: 30.12 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.81     |
| time/              |          |
|    total_timesteps | 10700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.81
SELFPLAY: new best model, bumping up generation to 200
Ep done - 442000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.765    |
|    return_std      | 3.65     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22167    |
|    total_timesteps | 10720033 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 766      |
|    learning_rate   | 0.00315  |
|    step_size       | 0.000143 |
---------------------------------
Ep done - 443000.
Eval num_timesteps=10750000, episode_reward=0.86 +/- 0.49
Episode length: 30.21 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.858    |
| time/              |          |
|    total_timesteps | 10750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8575
SELFPLAY: new best model, bumping up generation to 201
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.76     |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22247    |
|    total_timesteps | 10756228 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 767      |
|    learning_rate   | 0.00314  |
|    step_size       | 0.000193 |
---------------------------------
Ep done - 444000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.772    |
|    return_std      | 2.42     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22306    |
|    total_timesteps | 10792411 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 768      |
|    learning_rate   | 0.00314  |
|    step_size       | 0.000216 |
---------------------------------
Ep done - 445000.
Eval num_timesteps=10800000, episode_reward=0.85 +/- 0.51
Episode length: 30.19 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.853    |
| time/              |          |
|    total_timesteps | 10800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8525
SELFPLAY: new best model, bumping up generation to 202
Ep done - 446000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.776    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22388    |
|    total_timesteps | 10828593 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 769      |
|    learning_rate   | 0.00314  |
|    step_size       | 0.00023  |
---------------------------------
Ep done - 447000.
Eval num_timesteps=10850000, episode_reward=0.80 +/- 0.58
Episode length: 30.16 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.802    |
| time/              |          |
|    total_timesteps | 10850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8025
SELFPLAY: new best model, bumping up generation to 203
Ep done - 448000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.796    |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22467    |
|    total_timesteps | 10864833 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 770      |
|    learning_rate   | 0.00313  |
|    step_size       | 0.000302 |
---------------------------------
Ep done - 449000.
Eval num_timesteps=10900000, episode_reward=0.82 +/- 0.55
Episode length: 30.17 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.82     |
| time/              |          |
|    total_timesteps | 10900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.82
SELFPLAY: new best model, bumping up generation to 204
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.768    |
|    return_std      | 3.25     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22546    |
|    total_timesteps | 10901057 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 771      |
|    learning_rate   | 0.00313  |
|    step_size       | 0.000161 |
---------------------------------
Ep done - 450000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.805    |
|    return_std      | 2.73     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22608    |
|    total_timesteps | 10937248 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 772      |
|    learning_rate   | 0.00313  |
|    step_size       | 0.000191 |
---------------------------------
Ep done - 451000.
Eval num_timesteps=10950000, episode_reward=0.83 +/- 0.53
Episode length: 30.14 +/- 0.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.83     |
| time/              |          |
|    total_timesteps | 10950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.83
SELFPLAY: new best model, bumping up generation to 205
Ep done - 452000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.784    |
|    return_std      | 2.56     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22687    |
|    total_timesteps | 10973437 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 773      |
|    learning_rate   | 0.00313  |
|    step_size       | 0.000204 |
---------------------------------
Ep done - 453000.
Eval num_timesteps=11000000, episode_reward=0.84 +/- 0.51
Episode length: 30.14 +/- 0.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.845    |
| time/              |          |
|    total_timesteps | 11000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.845
SELFPLAY: new best model, bumping up generation to 206
Ep done - 454000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.743    |
|    return_std      | 2.72     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22766    |
|    total_timesteps | 11009599 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 774      |
|    learning_rate   | 0.00312  |
|    step_size       | 0.000191 |
---------------------------------
Ep done - 455000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.775    |
|    return_std      | 1.66     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22828    |
|    total_timesteps | 11045760 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 775      |
|    learning_rate   | 0.00312  |
|    step_size       | 0.000313 |
---------------------------------
Eval num_timesteps=11050000, episode_reward=0.84 +/- 0.52
Episode length: 30.23 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.843    |
| time/              |          |
|    total_timesteps | 11050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8425
SELFPLAY: new best model, bumping up generation to 207
Ep done - 456000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.773    |
|    return_std      | 3.19     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22907    |
|    total_timesteps | 11081995 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 776      |
|    learning_rate   | 0.00312  |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 457000.
Eval num_timesteps=11100000, episode_reward=0.84 +/- 0.53
Episode length: 30.17 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.838    |
| time/              |          |
|    total_timesteps | 11100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8375
SELFPLAY: new best model, bumping up generation to 208
Ep done - 458000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.744    |
|    return_std      | 1.75     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 22986    |
|    total_timesteps | 11118204 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 777      |
|    learning_rate   | 0.00311  |
|    step_size       | 0.000297 |
---------------------------------
Ep done - 459000.
Eval num_timesteps=11150000, episode_reward=0.89 +/- 0.46
Episode length: 30.20 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 11150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 209
Ep done - 460000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.754    |
|    return_std      | 2.84     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23068    |
|    total_timesteps | 11154400 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 778      |
|    learning_rate   | 0.00311  |
|    step_size       | 0.000182 |
---------------------------------
Ep done - 461000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.793    |
|    return_std      | 2.49     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23127    |
|    total_timesteps | 11190604 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 779      |
|    learning_rate   | 0.00311  |
|    step_size       | 0.000208 |
---------------------------------
Eval num_timesteps=11200000, episode_reward=0.81 +/- 0.58
Episode length: 30.19 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.807    |
| time/              |          |
|    total_timesteps | 11200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8075
SELFPLAY: new best model, bumping up generation to 210
Ep done - 462000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.743    |
|    return_std      | 3.17     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23206    |
|    total_timesteps | 11226770 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 780      |
|    learning_rate   | 0.0031   |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 463000.
Eval num_timesteps=11250000, episode_reward=0.85 +/- 0.51
Episode length: 30.21 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.853    |
| time/              |          |
|    total_timesteps | 11250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8525
SELFPLAY: new best model, bumping up generation to 211
Ep done - 464000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.787    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23288    |
|    total_timesteps | 11262960 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 781      |
|    learning_rate   | 0.0031   |
|    step_size       | 0.000245 |
---------------------------------
Ep done - 465000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.803    |
|    return_std      | 3.46     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23347    |
|    total_timesteps | 11299181 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 782      |
|    learning_rate   | 0.0031   |
|    step_size       | 0.000149 |
---------------------------------
Ep done - 466000.
Eval num_timesteps=11300000, episode_reward=0.89 +/- 0.45
Episode length: 30.15 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 11300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 212
Ep done - 467000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.774    |
|    return_std      | 3.86     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23429    |
|    total_timesteps | 11335350 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 783      |
|    learning_rate   | 0.0031   |
|    step_size       | 0.000134 |
---------------------------------
Ep done - 468000.
Eval num_timesteps=11350000, episode_reward=0.83 +/- 0.54
Episode length: 30.24 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.833    |
| time/              |          |
|    total_timesteps | 11350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8325
SELFPLAY: new best model, bumping up generation to 213
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.778    |
|    return_std      | 3        |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23509    |
|    total_timesteps | 11371526 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 784      |
|    learning_rate   | 0.00309  |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 469000.
Ep done - 470000.
Eval num_timesteps=11400000, episode_reward=0.87 +/- 0.47
Episode length: 30.21 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.873    |
| time/              |          |
|    total_timesteps | 11400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8725
SELFPLAY: new best model, bumping up generation to 214
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.791    |
|    return_std      | 2.92     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23588    |
|    total_timesteps | 11407746 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 785      |
|    learning_rate   | 0.00309  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 471000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.792    |
|    return_std      | 2.5      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23650    |
|    total_timesteps | 11443941 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 786      |
|    learning_rate   | 0.00309  |
|    step_size       | 0.000206 |
---------------------------------
Ep done - 472000.
Eval num_timesteps=11450000, episode_reward=0.87 +/- 0.48
Episode length: 30.26 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.873    |
| time/              |          |
|    total_timesteps | 11450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8725
SELFPLAY: new best model, bumping up generation to 215
Ep done - 473000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.751    |
|    return_std      | 2.95     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23729    |
|    total_timesteps | 11480094 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 787      |
|    learning_rate   | 0.00308  |
|    step_size       | 0.000174 |
---------------------------------
Ep done - 474000.
Eval num_timesteps=11500000, episode_reward=0.89 +/- 0.45
Episode length: 30.25 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 11500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 216
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.788    |
|    return_std      | 2.14     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23808    |
|    total_timesteps | 11516333 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 788      |
|    learning_rate   | 0.00308  |
|    step_size       | 0.00024  |
---------------------------------
Ep done - 475000.
Ep done - 476000.
Eval num_timesteps=11550000, episode_reward=0.87 +/- 0.48
Episode length: 30.23 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.87     |
| time/              |          |
|    total_timesteps | 11550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.87
SELFPLAY: new best model, bumping up generation to 217
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.808    |
|    return_std      | 2.3      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23890    |
|    total_timesteps | 11552566 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 789      |
|    learning_rate   | 0.00308  |
|    step_size       | 0.000223 |
---------------------------------
Ep done - 477000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.787    |
|    return_std      | 2.9      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 23949    |
|    total_timesteps | 11588769 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 790      |
|    learning_rate   | 0.00308  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 478000.
Eval num_timesteps=11600000, episode_reward=0.88 +/- 0.46
Episode length: 30.25 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 11600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 218
Ep done - 479000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.792    |
|    return_std      | 2.26     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24029    |
|    total_timesteps | 11624995 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 791      |
|    learning_rate   | 0.00307  |
|    step_size       | 0.000227 |
---------------------------------
Ep done - 480000.
Eval num_timesteps=11650000, episode_reward=0.86 +/- 0.51
Episode length: 30.25 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.858    |
| time/              |          |
|    total_timesteps | 11650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8575
SELFPLAY: new best model, bumping up generation to 219
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.789    |
|    return_std      | 2.79     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24111    |
|    total_timesteps | 11661187 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 792      |
|    learning_rate   | 0.00307  |
|    step_size       | 0.000183 |
---------------------------------
Ep done - 481000.
Ep done - 482000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.795    |
|    return_std      | 2.31     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24172    |
|    total_timesteps | 11697352 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 793      |
|    learning_rate   | 0.00307  |
|    step_size       | 0.000221 |
---------------------------------
Eval num_timesteps=11700000, episode_reward=0.85 +/- 0.52
Episode length: 30.20 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.848    |
| time/              |          |
|    total_timesteps | 11700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8475
SELFPLAY: new best model, bumping up generation to 220
Ep done - 483000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.772    |
|    return_std      | 3.04     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24251    |
|    total_timesteps | 11733540 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 794      |
|    learning_rate   | 0.00306  |
|    step_size       | 0.000168 |
---------------------------------
Ep done - 484000.
Eval num_timesteps=11750000, episode_reward=0.90 +/- 0.42
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 11750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 221
Ep done - 485000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.75     |
|    return_std      | 4.94     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24333    |
|    total_timesteps | 11769710 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 795      |
|    learning_rate   | 0.00306  |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 486000.
Eval num_timesteps=11800000, episode_reward=0.85 +/- 0.53
Episode length: 30.29 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.848    |
| time/              |          |
|    total_timesteps | 11800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8475
SELFPLAY: new best model, bumping up generation to 222
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.788    |
|    return_std      | 3.37     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24412    |
|    total_timesteps | 11805974 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 796      |
|    learning_rate   | 0.00306  |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 487000.
Ep done - 488000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.82     |
|    return_std      | 2.47     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24471    |
|    total_timesteps | 11842172 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 797      |
|    learning_rate   | 0.00306  |
|    step_size       | 0.000206 |
---------------------------------
Eval num_timesteps=11850000, episode_reward=0.88 +/- 0.46
Episode length: 30.25 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.88     |
| time/              |          |
|    total_timesteps | 11850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.88
SELFPLAY: new best model, bumping up generation to 223
Ep done - 489000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.82     |
|    return_std      | 2.61     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24553    |
|    total_timesteps | 11878389 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 798      |
|    learning_rate   | 0.00305  |
|    step_size       | 0.000195 |
---------------------------------
Ep done - 490000.
Eval num_timesteps=11900000, episode_reward=0.90 +/- 0.41
Episode length: 30.28 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 11900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9025
SELFPLAY: new best model, bumping up generation to 224
Ep done - 491000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.823    |
|    return_std      | 4.23     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24632    |
|    total_timesteps | 11914594 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 799      |
|    learning_rate   | 0.00305  |
|    step_size       | 0.00012  |
---------------------------------
Ep done - 492000.
Eval num_timesteps=11950000, episode_reward=0.86 +/- 0.49
Episode length: 30.20 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.865    |
| time/              |          |
|    total_timesteps | 11950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.865
SELFPLAY: new best model, bumping up generation to 225
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.772    |
|    return_std      | 3.55     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24711    |
|    total_timesteps | 11950755 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 800      |
|    learning_rate   | 0.00305  |
|    step_size       | 0.000143 |
---------------------------------
Ep done - 493000.
Ep done - 494000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.792    |
|    return_std      | 2.63     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24773    |
|    total_timesteps | 11986936 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 801      |
|    learning_rate   | 0.00304  |
|    step_size       | 0.000193 |
---------------------------------
Eval num_timesteps=12000000, episode_reward=0.88 +/- 0.47
Episode length: 30.25 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.875    |
| time/              |          |
|    total_timesteps | 12000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.875
SELFPLAY: new best model, bumping up generation to 226
Ep done - 495000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.795    |
|    return_std      | 3.33     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24852    |
|    total_timesteps | 12023146 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 802      |
|    learning_rate   | 0.00304  |
|    step_size       | 0.000152 |
---------------------------------
Ep done - 496000.
Eval num_timesteps=12050000, episode_reward=0.85 +/- 0.52
Episode length: 30.24 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.85     |
| time/              |          |
|    total_timesteps | 12050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.85
SELFPLAY: new best model, bumping up generation to 227
Ep done - 497000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.794    |
|    return_std      | 3.03     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24931    |
|    total_timesteps | 12059385 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 803      |
|    learning_rate   | 0.00304  |
|    step_size       | 0.000167 |
---------------------------------
Ep done - 498000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.784    |
|    return_std      | 3.23     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 24993    |
|    total_timesteps | 12095556 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 804      |
|    learning_rate   | 0.00304  |
|    step_size       | 0.000157 |
---------------------------------
Eval num_timesteps=12100000, episode_reward=0.85 +/- 0.52
Episode length: 30.23 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.848    |
| time/              |          |
|    total_timesteps | 12100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8475
SELFPLAY: new best model, bumping up generation to 228
Ep done - 499000.
Ep done - 500000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.788    |
|    return_std      | 2.87     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25072    |
|    total_timesteps | 12131751 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 805      |
|    learning_rate   | 0.00303  |
|    step_size       | 0.000176 |
---------------------------------
Ep done - 501000.
Eval num_timesteps=12150000, episode_reward=0.85 +/- 0.51
Episode length: 30.23 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.855    |
| time/              |          |
|    total_timesteps | 12150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.855
SELFPLAY: new best model, bumping up generation to 229
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.803    |
|    return_std      | 2.48     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25151    |
|    total_timesteps | 12167928 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 806      |
|    learning_rate   | 0.00303  |
|    step_size       | 0.000204 |
---------------------------------
Ep done - 502000.
Ep done - 503000.
Eval num_timesteps=12200000, episode_reward=0.85 +/- 0.51
Episode length: 30.20 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.855    |
| time/              |          |
|    total_timesteps | 12200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.855
SELFPLAY: new best model, bumping up generation to 230
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.8      |
|    return_std      | 2.54     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25233    |
|    total_timesteps | 12204170 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 807      |
|    learning_rate   | 0.00303  |
|    step_size       | 0.000199 |
---------------------------------
Ep done - 504000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.786    |
|    return_std      | 2.5      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25292    |
|    total_timesteps | 12240318 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 808      |
|    learning_rate   | 0.00302  |
|    step_size       | 0.000201 |
---------------------------------
Ep done - 505000.
Eval num_timesteps=12250000, episode_reward=0.90 +/- 0.44
Episode length: 30.29 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 12250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 231
Ep done - 506000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.814    |
|    return_std      | 3.77     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25374    |
|    total_timesteps | 12276563 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 809      |
|    learning_rate   | 0.00302  |
|    step_size       | 0.000134 |
---------------------------------
Ep done - 507000.
Eval num_timesteps=12300000, episode_reward=0.91 +/- 0.40
Episode length: 30.17 +/- 1.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 12300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 232
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.796    |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25453    |
|    total_timesteps | 12312782 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 810      |
|    learning_rate   | 0.00302  |
|    step_size       | 0.000291 |
---------------------------------
Ep done - 508000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.784    |
|    return_std      | 2.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 25512    |
|    total_timesteps | 12349001 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 811      |
|    learning_rate   | 0.00301  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 509000.
Eval num_timesteps=12350000, episode_reward=0.89 +/- 0.45
Episode length: 30.28 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 12350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 233
Ep done - 510000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.813    |
|    return_std      | 3.17     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25594    |
|    total_timesteps | 12385239 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 812      |
|    learning_rate   | 0.00301  |
|    step_size       | 0.000159 |
---------------------------------
Ep done - 511000.
Eval num_timesteps=12400000, episode_reward=0.86 +/- 0.50
Episode length: 30.20 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.858    |
| time/              |          |
|    total_timesteps | 12400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8575
SELFPLAY: new best model, bumping up generation to 234
Ep done - 512000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.773    |
|    return_std      | 2.91     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25673    |
|    total_timesteps | 12421403 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 813      |
|    learning_rate   | 0.00301  |
|    step_size       | 0.000173 |
---------------------------------
Ep done - 513000.
Eval num_timesteps=12450000, episode_reward=0.86 +/- 0.50
Episode length: 30.25 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.858    |
| time/              |          |
|    total_timesteps | 12450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8575
SELFPLAY: new best model, bumping up generation to 235
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.798    |
|    return_std      | 3.44     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25752    |
|    total_timesteps | 12457609 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 814      |
|    learning_rate   | 0.00301  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 514000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.796    |
|    return_std      | 1.82     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25815    |
|    total_timesteps | 12493765 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 815      |
|    learning_rate   | 0.003    |
|    step_size       | 0.000276 |
---------------------------------
Ep done - 515000.
Eval num_timesteps=12500000, episode_reward=0.87 +/- 0.48
Episode length: 30.22 +/- 1.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.873    |
| time/              |          |
|    total_timesteps | 12500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8725
SELFPLAY: new best model, bumping up generation to 236
Ep done - 516000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.812    |
|    return_std      | 3.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25894    |
|    total_timesteps | 12529987 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 816      |
|    learning_rate   | 0.003    |
|    step_size       | 0.000159 |
---------------------------------
Ep done - 517000.
Eval num_timesteps=12550000, episode_reward=0.92 +/- 0.38
Episode length: 30.23 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 12550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 237
Ep done - 518000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.788    |
|    return_std      | 3.2      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 25973    |
|    total_timesteps | 12566239 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 817      |
|    learning_rate   | 0.003    |
|    step_size       | 0.000156 |
---------------------------------
Ep done - 519000.
Eval num_timesteps=12600000, episode_reward=0.89 +/- 0.44
Episode length: 30.29 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 12600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 238
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.782    |
|    return_std      | 4.3      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26055    |
|    total_timesteps | 12602412 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 818      |
|    learning_rate   | 0.00299  |
|    step_size       | 0.000116 |
---------------------------------
Ep done - 520000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.798    |
|    return_std      | 3.01     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26114    |
|    total_timesteps | 12638609 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 819      |
|    learning_rate   | 0.00299  |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 521000.
Eval num_timesteps=12650000, episode_reward=0.88 +/- 0.46
Episode length: 30.25 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 12650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 239
Ep done - 522000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.837    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26193    |
|    total_timesteps | 12674843 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 820      |
|    learning_rate   | 0.00299  |
|    step_size       | 0.000208 |
---------------------------------
Ep done - 523000.
Eval num_timesteps=12700000, episode_reward=0.89 +/- 0.45
Episode length: 30.18 +/- 1.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 12700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 240
Ep done - 524000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.792    |
|    return_std      | 1.91     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26275    |
|    total_timesteps | 12711075 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 821      |
|    learning_rate   | 0.00299  |
|    step_size       | 0.00026  |
---------------------------------
Ep done - 525000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.776    |
|    return_std      | 2.18     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 26336    |
|    total_timesteps | 12747284 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 822      |
|    learning_rate   | 0.00298  |
|    step_size       | 0.000228 |
---------------------------------
Eval num_timesteps=12750000, episode_reward=0.89 +/- 0.42
Episode length: 30.29 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 12750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 241
Ep done - 526000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.787    |
|    return_std      | 2.93     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26416    |
|    total_timesteps | 12783503 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 823      |
|    learning_rate   | 0.00298  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 527000.
Eval num_timesteps=12800000, episode_reward=0.85 +/- 0.51
Episode length: 30.16 +/- 1.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.85     |
| time/              |          |
|    total_timesteps | 12800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.85
SELFPLAY: new best model, bumping up generation to 242
Ep done - 528000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.791    |
|    return_std      | 4.04     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26498    |
|    total_timesteps | 12819673 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 824      |
|    learning_rate   | 0.00298  |
|    step_size       | 0.000123 |
---------------------------------
Ep done - 529000.
Eval num_timesteps=12850000, episode_reward=0.91 +/- 0.41
Episode length: 30.23 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 12850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 243
Ep done - 530000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.797    |
|    return_std      | 4.83     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26577    |
|    total_timesteps | 12855862 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 825      |
|    learning_rate   | 0.00297  |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 531000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.807    |
|    return_std      | 4.27     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26637    |
|    total_timesteps | 12892080 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 826      |
|    learning_rate   | 0.00297  |
|    step_size       | 0.000116 |
---------------------------------
Eval num_timesteps=12900000, episode_reward=0.88 +/- 0.45
Episode length: 30.23 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 12900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 244
Ep done - 532000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.839    |
|    return_std      | 2.45     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26719    |
|    total_timesteps | 12928282 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 827      |
|    learning_rate   | 0.00297  |
|    step_size       | 0.000202 |
---------------------------------
Ep done - 533000.
Eval num_timesteps=12950000, episode_reward=0.86 +/- 0.50
Episode length: 30.25 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.858    |
| time/              |          |
|    total_timesteps | 12950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8575
SELFPLAY: new best model, bumping up generation to 245
Ep done - 534000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.804    |
|    return_std      | 3.19     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26798    |
|    total_timesteps | 12964515 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 828      |
|    learning_rate   | 0.00297  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 535000.
Eval num_timesteps=13000000, episode_reward=0.87 +/- 0.47
Episode length: 30.22 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.868    |
| time/              |          |
|    total_timesteps | 13000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8675
SELFPLAY: new best model, bumping up generation to 246
Ep done - 536000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.8      |
|    return_std      | 3.75     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26877    |
|    total_timesteps | 13000700 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 829      |
|    learning_rate   | 0.00296  |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 537000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.81     |
|    return_std      | 2.35     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 26939    |
|    total_timesteps | 13036914 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 830      |
|    learning_rate   | 0.00296  |
|    step_size       | 0.00021  |
---------------------------------
Ep done - 538000.
Eval num_timesteps=13050000, episode_reward=0.84 +/- 0.53
Episode length: 30.23 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.843    |
| time/              |          |
|    total_timesteps | 13050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8425
SELFPLAY: new best model, bumping up generation to 247
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.807    |
|    return_std      | 2.79     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27019    |
|    total_timesteps | 13073111 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 831      |
|    learning_rate   | 0.00296  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 539000.
Ep done - 540000.
Eval num_timesteps=13100000, episode_reward=0.88 +/- 0.47
Episode length: 30.22 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.877    |
| time/              |          |
|    total_timesteps | 13100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8775
SELFPLAY: new best model, bumping up generation to 248
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.818    |
|    return_std      | 2.64     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27100    |
|    total_timesteps | 13109329 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 832      |
|    learning_rate   | 0.00295  |
|    step_size       | 0.000186 |
---------------------------------
Ep done - 541000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.803    |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 27160    |
|    total_timesteps | 13145530 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 833      |
|    learning_rate   | 0.00295  |
|    step_size       | 0.000182 |
---------------------------------
Ep done - 542000.
Eval num_timesteps=13150000, episode_reward=0.91 +/- 0.41
Episode length: 30.29 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 13150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 249
Ep done - 543000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.26     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27239    |
|    total_timesteps | 13181748 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 834      |
|    learning_rate   | 0.00295  |
|    step_size       | 0.000217 |
---------------------------------
Ep done - 544000.
Eval num_timesteps=13200000, episode_reward=0.93 +/- 0.35
Episode length: 30.15 +/- 1.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 13200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 250
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.797    |
|    return_std      | 1.48     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27320    |
|    total_timesteps | 13217987 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 835      |
|    learning_rate   | 0.00295  |
|    step_size       | 0.000331 |
---------------------------------
Ep done - 545000.
Ep done - 546000.
Eval num_timesteps=13250000, episode_reward=0.88 +/- 0.47
Episode length: 30.23 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.875    |
| time/              |          |
|    total_timesteps | 13250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.875
SELFPLAY: new best model, bumping up generation to 251
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.814    |
|    return_std      | 4.33     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27399    |
|    total_timesteps | 13254185 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 836      |
|    learning_rate   | 0.00294  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 547000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.829    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 27458    |
|    total_timesteps | 13290430 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 837      |
|    learning_rate   | 0.00294  |
|    step_size       | 0.000255 |
---------------------------------
Ep done - 548000.
Eval num_timesteps=13300000, episode_reward=0.89 +/- 0.45
Episode length: 30.23 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 13300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 252
Ep done - 549000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.812    |
|    return_std      | 2.81     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27540    |
|    total_timesteps | 13326659 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 838      |
|    learning_rate   | 0.00294  |
|    step_size       | 0.000174 |
---------------------------------
Ep done - 550000.
Eval num_timesteps=13350000, episode_reward=0.86 +/- 0.49
Episode length: 30.23 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.863    |
| time/              |          |
|    total_timesteps | 13350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8625
SELFPLAY: new best model, bumping up generation to 253
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.8      |
|    return_std      | 3.2      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27619    |
|    total_timesteps | 13362886 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 839      |
|    learning_rate   | 0.00293  |
|    step_size       | 0.000153 |
---------------------------------
Ep done - 551000.
Ep done - 552000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.797    |
|    return_std      | 2.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 27679    |
|    total_timesteps | 13399090 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 840      |
|    learning_rate   | 0.00293  |
|    step_size       | 0.000172 |
---------------------------------
Eval num_timesteps=13400000, episode_reward=0.88 +/- 0.45
Episode length: 30.14 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 13400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 254
Ep done - 553000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.76     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27760    |
|    total_timesteps | 13435266 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 841      |
|    learning_rate   | 0.00293  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 554000.
Eval num_timesteps=13450000, episode_reward=0.91 +/- 0.41
Episode length: 30.16 +/- 1.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 13450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 255
Ep done - 555000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.803    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27839    |
|    total_timesteps | 13471488 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 842      |
|    learning_rate   | 0.00293  |
|    step_size       | 0.000274 |
---------------------------------
Ep done - 556000.
Eval num_timesteps=13500000, episode_reward=0.88 +/- 0.47
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.875    |
| time/              |          |
|    total_timesteps | 13500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.875
SELFPLAY: new best model, bumping up generation to 256
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 1.68     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 27918    |
|    total_timesteps | 13507668 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 843      |
|    learning_rate   | 0.00292  |
|    step_size       | 0.000291 |
---------------------------------
Ep done - 557000.
Ep done - 558000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.09     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 27980    |
|    total_timesteps | 13543890 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 844      |
|    learning_rate   | 0.00292  |
|    step_size       | 0.000232 |
---------------------------------
Eval num_timesteps=13550000, episode_reward=0.92 +/- 0.38
Episode length: 30.27 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 13550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 257
Ep done - 559000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.807    |
|    return_std      | 2.66     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 28059    |
|    total_timesteps | 13580124 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 845      |
|    learning_rate   | 0.00292  |
|    step_size       | 0.000183 |
---------------------------------
Ep done - 560000.
Eval num_timesteps=13600000, episode_reward=0.91 +/- 0.39
Episode length: 30.26 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 13600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 258
Ep done - 561000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.818    |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 28138    |
|    total_timesteps | 13616319 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 846      |
|    learning_rate   | 0.00291  |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 562000.
Eval num_timesteps=13650000, episode_reward=0.89 +/- 0.44
Episode length: 30.21 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 13650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 259
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.812    |
|    return_std      | 2.37     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 28220    |
|    total_timesteps | 13652526 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 847      |
|    learning_rate   | 0.00291  |
|    step_size       | 0.000204 |
---------------------------------
Ep done - 563000.
Ep done - 564000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.814    |
|    return_std      | 2.79     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 28280    |
|    total_timesteps | 13688713 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 848      |
|    learning_rate   | 0.00291  |
|    step_size       | 0.000174 |
---------------------------------
Eval num_timesteps=13700000, episode_reward=0.92 +/- 0.37
Episode length: 30.25 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 13700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 260
Ep done - 565000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.86     |
|    return_std      | 1.97     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 28359    |
|    total_timesteps | 13724913 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 849      |
|    learning_rate   | 0.0029   |
|    step_size       | 0.000246 |
---------------------------------
Ep done - 566000.
Eval num_timesteps=13750000, episode_reward=0.87 +/- 0.46
Episode length: 30.19 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.868    |
| time/              |          |
|    total_timesteps | 13750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8675
SELFPLAY: new best model, bumping up generation to 261
Ep done - 567000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.777    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 28441    |
|    total_timesteps | 13761133 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 850      |
|    learning_rate   | 0.0029   |
|    step_size       | 0.000213 |
---------------------------------
Ep done - 568000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.799    |
|    return_std      | 1.4      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 28501    |
|    total_timesteps | 13797372 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 851      |
|    learning_rate   | 0.0029   |
|    step_size       | 0.000344 |
---------------------------------
Eval num_timesteps=13800000, episode_reward=0.84 +/- 0.52
Episode length: 30.31 +/- 0.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.843    |
| time/              |          |
|    total_timesteps | 13800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8425
SELFPLAY: new best model, bumping up generation to 262
Ep done - 569000.
Ep done - 570000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.825    |
|    return_std      | 2.42     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 28580    |
|    total_timesteps | 13833583 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 852      |
|    learning_rate   | 0.0029   |
|    step_size       | 0.0002   |
---------------------------------
Eval num_timesteps=13850000, episode_reward=0.90 +/- 0.43
Episode length: 30.23 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 13850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 263
Ep done - 571000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.821    |
|    return_std      | 2.91     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 28663    |
|    total_timesteps | 13869811 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 853      |
|    learning_rate   | 0.00289  |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 572000.
Eval num_timesteps=13900000, episode_reward=0.86 +/- 0.49
Episode length: 30.23 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.86     |
| time/              |          |
|    total_timesteps | 13900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.86
SELFPLAY: new best model, bumping up generation to 264
Ep done - 573000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 2.1      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 28743    |
|    total_timesteps | 13906090 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 854      |
|    learning_rate   | 0.00289  |
|    step_size       | 0.000229 |
---------------------------------
Ep done - 574000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 2.68     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 28803    |
|    total_timesteps | 13942284 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 855      |
|    learning_rate   | 0.00289  |
|    step_size       | 0.00018  |
---------------------------------
Ep done - 575000.
Eval num_timesteps=13950000, episode_reward=0.88 +/- 0.46
Episode length: 30.30 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.88     |
| time/              |          |
|    total_timesteps | 13950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.88
SELFPLAY: new best model, bumping up generation to 265
Ep done - 576000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.81     |
|    return_std      | 2.26     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 28885    |
|    total_timesteps | 13978532 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 856      |
|    learning_rate   | 0.00288  |
|    step_size       | 0.000213 |
---------------------------------
Ep done - 577000.
Eval num_timesteps=14000000, episode_reward=0.92 +/- 0.37
Episode length: 30.32 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 14000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 266
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.818    |
|    return_std      | 2.9      |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 28965    |
|    total_timesteps | 14014746 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 857      |
|    learning_rate   | 0.00288  |
|    step_size       | 0.000165 |
---------------------------------
Ep done - 578000.
Ep done - 579000.
Eval num_timesteps=14050000, episode_reward=0.90 +/- 0.42
Episode length: 30.19 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 14050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 267
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.798    |
|    return_std      | 2.57     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 29047    |
|    total_timesteps | 14050994 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 858      |
|    learning_rate   | 0.00288  |
|    step_size       | 0.000186 |
---------------------------------
Ep done - 580000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.801    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 29107    |
|    total_timesteps | 14087180 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 859      |
|    learning_rate   | 0.00288  |
|    step_size       | 0.000237 |
---------------------------------
Ep done - 581000.
Eval num_timesteps=14100000, episode_reward=0.94 +/- 0.34
Episode length: 30.29 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 14100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 268
Ep done - 582000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.801    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 29186    |
|    total_timesteps | 14123398 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 860      |
|    learning_rate   | 0.00287  |
|    step_size       | 0.000197 |
---------------------------------
Ep done - 583000.
Eval num_timesteps=14150000, episode_reward=0.89 +/- 0.45
Episode length: 30.30 +/- 0.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 14150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 269
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.835    |
|    return_std      | 2.06     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 29268    |
|    total_timesteps | 14159691 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 861      |
|    learning_rate   | 0.00287  |
|    step_size       | 0.000232 |
---------------------------------
Ep done - 584000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 3.45     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 29327    |
|    total_timesteps | 14195940 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 862      |
|    learning_rate   | 0.00287  |
|    step_size       | 0.000139 |
---------------------------------
Ep done - 585000.
Eval num_timesteps=14200000, episode_reward=0.89 +/- 0.43
Episode length: 30.26 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 14200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 270
Ep done - 586000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.825    |
|    return_std      | 2.14     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 29406    |
|    total_timesteps | 14232159 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 863      |
|    learning_rate   | 0.00286  |
|    step_size       | 0.000223 |
---------------------------------
Ep done - 587000.
Eval num_timesteps=14250000, episode_reward=0.90 +/- 0.44
Episode length: 30.24 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 14250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 271
Ep done - 588000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.804    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 29487    |
|    total_timesteps | 14268419 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 864      |
|    learning_rate   | 0.00286  |
|    step_size       | 0.000196 |
---------------------------------
Ep done - 589000.
Eval num_timesteps=14300000, episode_reward=0.89 +/- 0.46
Episode length: 30.27 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 14300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 272
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 3.31     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 29566    |
|    total_timesteps | 14304678 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 865      |
|    learning_rate   | 0.00286  |
|    step_size       | 0.000144 |
---------------------------------
Ep done - 590000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 3.21     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 29626    |
|    total_timesteps | 14340893 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 866      |
|    learning_rate   | 0.00286  |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 591000.
Eval num_timesteps=14350000, episode_reward=0.90 +/- 0.43
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 14350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 273
Ep done - 592000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.813    |
|    return_std      | 4.09     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 29707    |
|    total_timesteps | 14377083 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 867      |
|    learning_rate   | 0.00285  |
|    step_size       | 0.000116 |
---------------------------------
Ep done - 593000.
Eval num_timesteps=14400000, episode_reward=0.92 +/- 0.37
Episode length: 30.23 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 14400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 274
Ep done - 594000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.783    |
|    return_std      | 3.56     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 29786    |
|    total_timesteps | 14413281 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 868      |
|    learning_rate   | 0.00285  |
|    step_size       | 0.000133 |
---------------------------------
Ep done - 595000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.806    |
|    return_std      | 2.05     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 29845    |
|    total_timesteps | 14449489 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 869      |
|    learning_rate   | 0.00285  |
|    step_size       | 0.000231 |
---------------------------------
Eval num_timesteps=14450000, episode_reward=0.86 +/- 0.50
Episode length: 30.20 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.858    |
| time/              |          |
|    total_timesteps | 14450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8575
SELFPLAY: new best model, bumping up generation to 275
Ep done - 596000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.817    |
|    return_std      | 4.39     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 29927    |
|    total_timesteps | 14485712 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 870      |
|    learning_rate   | 0.00284  |
|    step_size       | 0.000108 |
---------------------------------
Ep done - 597000.
Eval num_timesteps=14500000, episode_reward=0.91 +/- 0.41
Episode length: 30.28 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 14500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 276
Ep done - 598000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.797    |
|    return_std      | 2.45     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 30006    |
|    total_timesteps | 14521923 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 871      |
|    learning_rate   | 0.00284  |
|    step_size       | 0.000193 |
---------------------------------
Ep done - 599000.
Eval num_timesteps=14550000, episode_reward=0.87 +/- 0.48
Episode length: 30.19 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.868    |
| time/              |          |
|    total_timesteps | 14550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8675
SELFPLAY: new best model, bumping up generation to 277
Ep done - 600000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.844    |
|    return_std      | 1.59     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 30085    |
|    total_timesteps | 14558169 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 872      |
|    learning_rate   | 0.00284  |
|    step_size       | 0.000298 |
---------------------------------
Ep done - 601000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 30147    |
|    total_timesteps | 14594410 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 873      |
|    learning_rate   | 0.00284  |
|    step_size       | 0.000235 |
---------------------------------
Eval num_timesteps=14600000, episode_reward=0.92 +/- 0.39
Episode length: 30.21 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 14600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 278
Ep done - 602000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.813    |
|    return_std      | 2.84     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 30225    |
|    total_timesteps | 14630658 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 874      |
|    learning_rate   | 0.00283  |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 603000.
Eval num_timesteps=14650000, episode_reward=0.91 +/- 0.41
Episode length: 30.15 +/- 1.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 14650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 279
Ep done - 604000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.29     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 30304    |
|    total_timesteps | 14666922 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 875      |
|    learning_rate   | 0.00283  |
|    step_size       | 0.000206 |
---------------------------------
Ep done - 605000.
Eval num_timesteps=14700000, episode_reward=0.92 +/- 0.38
Episode length: 30.18 +/- 1.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 14700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 280
Ep done - 606000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 30386    |
|    total_timesteps | 14703130 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 876      |
|    learning_rate   | 0.00283  |
|    step_size       | 0.000197 |
---------------------------------
Ep done - 607000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.807    |
|    return_std      | 2.64     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 30445    |
|    total_timesteps | 14739367 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 877      |
|    learning_rate   | 0.00282  |
|    step_size       | 0.000178 |
---------------------------------
Eval num_timesteps=14750000, episode_reward=0.88 +/- 0.46
Episode length: 30.22 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 14750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 281
Ep done - 608000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.831    |
|    return_std      | 2.05     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 30526    |
|    total_timesteps | 14775640 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 878      |
|    learning_rate   | 0.00282  |
|    step_size       | 0.000229 |
---------------------------------
Ep done - 609000.
Ep done - 610000.
Eval num_timesteps=14800000, episode_reward=0.87 +/- 0.47
Episode length: 30.19 +/- 1.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.873    |
| time/              |          |
|    total_timesteps | 14800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8725
SELFPLAY: new best model, bumping up generation to 282
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.829    |
|    return_std      | 2.78     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 30607    |
|    total_timesteps | 14811882 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 879      |
|    learning_rate   | 0.00282  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 611000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.802    |
|    return_std      | 2.96     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 30667    |
|    total_timesteps | 14848139 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 880      |
|    learning_rate   | 0.00282  |
|    step_size       | 0.000159 |
---------------------------------
Ep done - 612000.
Eval num_timesteps=14850000, episode_reward=0.91 +/- 0.39
Episode length: 30.25 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 14850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 283
Ep done - 613000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.813    |
|    return_std      | 3.82     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 30747    |
|    total_timesteps | 14884378 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 881      |
|    learning_rate   | 0.00281  |
|    step_size       | 0.000123 |
---------------------------------
Ep done - 614000.
Eval num_timesteps=14900000, episode_reward=0.92 +/- 0.39
Episode length: 30.16 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 14900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 284
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 30829    |
|    total_timesteps | 14920618 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 882      |
|    learning_rate   | 0.00281  |
|    step_size       | 0.000217 |
---------------------------------
Ep done - 615000.
Ep done - 616000.
Eval num_timesteps=14950000, episode_reward=0.86 +/- 0.50
Episode length: 30.14 +/- 1.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.86     |
| time/              |          |
|    total_timesteps | 14950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.86
SELFPLAY: new best model, bumping up generation to 285
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 30908    |
|    total_timesteps | 14956825 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 883      |
|    learning_rate   | 0.00281  |
|    step_size       | 0.000249 |
---------------------------------
Ep done - 617000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.805    |
|    return_std      | 3.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 30968    |
|    total_timesteps | 14993034 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 884      |
|    learning_rate   | 0.0028   |
|    step_size       | 0.000124 |
---------------------------------
Ep done - 618000.
Eval num_timesteps=15000000, episode_reward=0.92 +/- 0.38
Episode length: 30.25 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 15000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 286
Ep done - 619000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.829    |
|    return_std      | 1.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 31050    |
|    total_timesteps | 15029326 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 885      |
|    learning_rate   | 0.0028   |
|    step_size       | 0.000298 |
---------------------------------
Ep done - 620000.
Eval num_timesteps=15050000, episode_reward=0.87 +/- 0.48
Episode length: 30.20 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.873    |
| time/              |          |
|    total_timesteps | 15050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8725
SELFPLAY: new best model, bumping up generation to 287
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 31129    |
|    total_timesteps | 15065569 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 886      |
|    learning_rate   | 0.0028   |
|    step_size       | 0.00027  |
---------------------------------
Ep done - 621000.
Ep done - 622000.
Eval num_timesteps=15100000, episode_reward=0.89 +/- 0.45
Episode length: 30.25 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 15100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 288
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.821    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 31211    |
|    total_timesteps | 15101743 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 887      |
|    learning_rate   | 0.00279  |
|    step_size       | 0.000195 |
---------------------------------
Ep done - 623000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 2.86     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 31270    |
|    total_timesteps | 15137994 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 888      |
|    learning_rate   | 0.00279  |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 624000.
Eval num_timesteps=15150000, episode_reward=0.90 +/- 0.41
Episode length: 30.26 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 15150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 289
Ep done - 625000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.827    |
|    return_std      | 1.61     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 31349    |
|    total_timesteps | 15174260 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 889      |
|    learning_rate   | 0.00279  |
|    step_size       | 0.000288 |
---------------------------------
Ep done - 626000.
Eval num_timesteps=15200000, episode_reward=0.93 +/- 0.35
Episode length: 30.27 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 15200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 290
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 1.54     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 31431    |
|    total_timesteps | 15210567 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 890      |
|    learning_rate   | 0.00279  |
|    step_size       | 0.000301 |
---------------------------------
Ep done - 627000.
Ep done - 628000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 2.68     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 31490    |
|    total_timesteps | 15246752 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 891      |
|    learning_rate   | 0.00278  |
|    step_size       | 0.000173 |
---------------------------------
Eval num_timesteps=15250000, episode_reward=0.95 +/- 0.31
Episode length: 30.34 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.948    |
| time/              |          |
|    total_timesteps | 15250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9475
SELFPLAY: new best model, bumping up generation to 291
Ep done - 629000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.842    |
|    return_std      | 2.54     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 31570    |
|    total_timesteps | 15283045 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 892      |
|    learning_rate   | 0.00278  |
|    step_size       | 0.000182 |
---------------------------------
Ep done - 630000.
Eval num_timesteps=15300000, episode_reward=0.90 +/- 0.43
Episode length: 30.28 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 15300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 292
Ep done - 631000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.809    |
|    return_std      | 2.79     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 31652    |
|    total_timesteps | 15319287 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 893      |
|    learning_rate   | 0.00278  |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 632000.
Eval num_timesteps=15350000, episode_reward=0.89 +/- 0.44
Episode length: 30.18 +/- 1.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 15350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 293
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 1.85     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 31731    |
|    total_timesteps | 15355527 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 894      |
|    learning_rate   | 0.00277  |
|    step_size       | 0.00025  |
---------------------------------
Ep done - 633000.
Ep done - 634000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 2.42     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 31790    |
|    total_timesteps | 15391797 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 895      |
|    learning_rate   | 0.00277  |
|    step_size       | 0.000191 |
---------------------------------
Eval num_timesteps=15400000, episode_reward=0.90 +/- 0.42
Episode length: 30.28 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 15400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 294
Ep done - 635000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 31872    |
|    total_timesteps | 15428032 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 896      |
|    learning_rate   | 0.00277  |
|    step_size       | 0.000219 |
---------------------------------
Ep done - 636000.
Eval num_timesteps=15450000, episode_reward=0.92 +/- 0.36
Episode length: 30.24 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 15450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 295
Ep done - 637000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.823    |
|    return_std      | 2.09     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 31951    |
|    total_timesteps | 15464256 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 897      |
|    learning_rate   | 0.00277  |
|    step_size       | 0.00022  |
---------------------------------
Ep done - 638000.
Eval num_timesteps=15500000, episode_reward=0.93 +/- 0.35
Episode length: 30.26 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 15500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 296
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 32030    |
|    total_timesteps | 15500510 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 898      |
|    learning_rate   | 0.00276  |
|    step_size       | 0.000258 |
---------------------------------
Ep done - 639000.
Ep done - 640000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32092    |
|    total_timesteps | 15536746 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 899      |
|    learning_rate   | 0.00276  |
|    step_size       | 0.000245 |
---------------------------------
Eval num_timesteps=15550000, episode_reward=0.94 +/- 0.33
Episode length: 30.27 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 15550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.94
SELFPLAY: new best model, bumping up generation to 297
Ep done - 641000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32172    |
|    total_timesteps | 15573001 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 900      |
|    learning_rate   | 0.00276  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 642000.
Eval num_timesteps=15600000, episode_reward=0.92 +/- 0.37
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 15600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 298
Ep done - 643000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 32253    |
|    total_timesteps | 15609231 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 901      |
|    learning_rate   | 0.00275  |
|    step_size       | 0.000228 |
---------------------------------
Ep done - 644000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.797    |
|    return_std      | 2.54     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32313    |
|    total_timesteps | 15645482 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 902      |
|    learning_rate   | 0.00275  |
|    step_size       | 0.000181 |
---------------------------------
Eval num_timesteps=15650000, episode_reward=0.94 +/- 0.32
Episode length: 30.29 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 15650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 299
Ep done - 645000.
Ep done - 646000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 1.87     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32392    |
|    total_timesteps | 15681733 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 903      |
|    learning_rate   | 0.00275  |
|    step_size       | 0.000244 |
---------------------------------
Ep done - 647000.
Eval num_timesteps=15700000, episode_reward=0.90 +/- 0.42
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 15700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 300
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.867    |
|    return_std      | 2.47     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32474    |
|    total_timesteps | 15717971 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 904      |
|    learning_rate   | 0.00275  |
|    step_size       | 0.000186 |
---------------------------------
Ep done - 648000.
Ep done - 649000.
Eval num_timesteps=15750000, episode_reward=0.85 +/- 0.51
Episode length: 30.25 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.853    |
| time/              |          |
|    total_timesteps | 15750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8525
SELFPLAY: new best model, bumping up generation to 301
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 32553    |
|    total_timesteps | 15754128 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 905      |
|    learning_rate   | 0.00274  |
|    step_size       | 0.000212 |
---------------------------------
Ep done - 650000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.08     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32612    |
|    total_timesteps | 15790373 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 906      |
|    learning_rate   | 0.00274  |
|    step_size       | 0.000219 |
---------------------------------
Ep done - 651000.
Eval num_timesteps=15800000, episode_reward=0.91 +/- 0.41
Episode length: 30.27 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 15800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 302
Ep done - 652000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32694    |
|    total_timesteps | 15826609 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 907      |
|    learning_rate   | 0.00274  |
|    step_size       | 0.000281 |
---------------------------------
Ep done - 653000.
Eval num_timesteps=15850000, episode_reward=0.92 +/- 0.38
Episode length: 30.27 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 15850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 303
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 1.53     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 32775    |
|    total_timesteps | 15862846 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 908      |
|    learning_rate   | 0.00273  |
|    step_size       | 0.000298 |
---------------------------------
Ep done - 654000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.841    |
|    return_std      | 2.52     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32834    |
|    total_timesteps | 15899092 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 909      |
|    learning_rate   | 0.00273  |
|    step_size       | 0.000181 |
---------------------------------
Ep done - 655000.
Eval num_timesteps=15900000, episode_reward=0.91 +/- 0.39
Episode length: 30.19 +/- 1.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 15900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 304
Ep done - 656000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.847    |
|    return_std      | 2.7      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32913    |
|    total_timesteps | 15935354 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 910      |
|    learning_rate   | 0.00273  |
|    step_size       | 0.000168 |
---------------------------------
Ep done - 657000.
Eval num_timesteps=15950000, episode_reward=0.88 +/- 0.46
Episode length: 30.18 +/- 1.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.877    |
| time/              |          |
|    total_timesteps | 15950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8775
SELFPLAY: new best model, bumping up generation to 305
Ep done - 658000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.816    |
|    return_std      | 2.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 32995    |
|    total_timesteps | 15971608 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 911      |
|    learning_rate   | 0.00273  |
|    step_size       | 0.000173 |
---------------------------------
Ep done - 659000.
Eval num_timesteps=16000000, episode_reward=0.92 +/- 0.37
Episode length: 30.21 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 16000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 306
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.823    |
|    return_std      | 2.79     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 33074    |
|    total_timesteps | 16007847 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 912      |
|    learning_rate   | 0.00272  |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 660000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 3.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33133    |
|    total_timesteps | 16044069 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 913      |
|    learning_rate   | 0.00272  |
|    step_size       | 0.000137 |
---------------------------------
Ep done - 661000.
Eval num_timesteps=16050000, episode_reward=0.90 +/- 0.42
Episode length: 30.26 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 16050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 307
Ep done - 662000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33215    |
|    total_timesteps | 16080328 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 914      |
|    learning_rate   | 0.00272  |
|    step_size       | 0.000219 |
---------------------------------
Ep done - 663000.
Eval num_timesteps=16100000, episode_reward=0.90 +/- 0.43
Episode length: 30.25 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 16100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 308
Ep done - 664000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 2.33     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33294    |
|    total_timesteps | 16116556 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 915      |
|    learning_rate   | 0.00271  |
|    step_size       | 0.000194 |
---------------------------------
Ep done - 665000.
Eval num_timesteps=16150000, episode_reward=0.90 +/- 0.42
Episode length: 30.18 +/- 1.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 16150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 309
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.818    |
|    return_std      | 2.53     |
| time/              |          |
|    fps             | 483      |
|    time_elapsed    | 33376    |
|    total_timesteps | 16152797 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 916      |
|    learning_rate   | 0.00271  |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 666000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33435    |
|    total_timesteps | 16189003 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 917      |
|    learning_rate   | 0.00271  |
|    step_size       | 0.000235 |
---------------------------------
Ep done - 667000.
Eval num_timesteps=16200000, episode_reward=0.89 +/- 0.45
Episode length: 30.30 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 16200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 310
Ep done - 668000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.819    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33514    |
|    total_timesteps | 16225227 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 918      |
|    learning_rate   | 0.0027   |
|    step_size       | 0.000254 |
---------------------------------
Ep done - 669000.
Eval num_timesteps=16250000, episode_reward=0.89 +/- 0.45
Episode length: 30.24 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 16250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 311
Ep done - 670000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.823    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33595    |
|    total_timesteps | 16261472 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 919      |
|    learning_rate   | 0.0027   |
|    step_size       | 0.000252 |
---------------------------------
Ep done - 671000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 2.94     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33655    |
|    total_timesteps | 16297700 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 920      |
|    learning_rate   | 0.0027   |
|    step_size       | 0.000153 |
---------------------------------
Eval num_timesteps=16300000, episode_reward=0.91 +/- 0.40
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 16300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 312
Ep done - 672000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.82     |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33734    |
|    total_timesteps | 16333994 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 921      |
|    learning_rate   | 0.0027   |
|    step_size       | 0.000185 |
---------------------------------
Ep done - 673000.
Eval num_timesteps=16350000, episode_reward=0.93 +/- 0.36
Episode length: 30.28 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 16350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 313
Ep done - 674000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33816    |
|    total_timesteps | 16370198 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 922      |
|    learning_rate   | 0.00269  |
|    step_size       | 0.000213 |
---------------------------------
Ep done - 675000.
Eval num_timesteps=16400000, episode_reward=0.91 +/- 0.40
Episode length: 30.22 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 16400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 314
Ep done - 676000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.818    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33895    |
|    total_timesteps | 16406478 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 923      |
|    learning_rate   | 0.00269  |
|    step_size       | 0.000232 |
---------------------------------
Ep done - 677000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 1.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 33954    |
|    total_timesteps | 16442679 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 924      |
|    learning_rate   | 0.00269  |
|    step_size       | 0.000245 |
---------------------------------
Eval num_timesteps=16450000, episode_reward=0.89 +/- 0.44
Episode length: 30.26 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 16450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 315
Ep done - 678000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34036    |
|    total_timesteps | 16478970 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 925      |
|    learning_rate   | 0.00268  |
|    step_size       | 0.000251 |
---------------------------------
Ep done - 679000.
Eval num_timesteps=16500000, episode_reward=0.94 +/- 0.32
Episode length: 30.26 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.943    |
| time/              |          |
|    total_timesteps | 16500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9425
SELFPLAY: new best model, bumping up generation to 316
Ep done - 680000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.38     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34115    |
|    total_timesteps | 16515280 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 926      |
|    learning_rate   | 0.00268  |
|    step_size       | 0.000324 |
---------------------------------
Ep done - 681000.
Eval num_timesteps=16550000, episode_reward=0.91 +/- 0.40
Episode length: 30.29 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 16550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 317
Ep done - 682000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.826    |
|    return_std      | 3.37     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34197    |
|    total_timesteps | 16551537 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 927      |
|    learning_rate   | 0.00268  |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 683000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.823    |
|    return_std      | 2.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34256    |
|    total_timesteps | 16587759 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 928      |
|    learning_rate   | 0.00268  |
|    step_size       | 0.000149 |
---------------------------------
Ep done - 684000.
Eval num_timesteps=16600000, episode_reward=0.92 +/- 0.39
Episode length: 30.31 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 16600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 318
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.854    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34335    |
|    total_timesteps | 16624069 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 929      |
|    learning_rate   | 0.00267  |
|    step_size       | 0.000232 |
---------------------------------
Ep done - 685000.
Ep done - 686000.
Eval num_timesteps=16650000, episode_reward=0.90 +/- 0.43
Episode length: 30.26 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 16650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 319
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.84     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34417    |
|    total_timesteps | 16660362 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 930      |
|    learning_rate   | 0.00267  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 687000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 2.23     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34476    |
|    total_timesteps | 16696558 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 931      |
|    learning_rate   | 0.00267  |
|    step_size       | 0.000199 |
---------------------------------
Ep done - 688000.
Eval num_timesteps=16700000, episode_reward=0.92 +/- 0.40
Episode length: 30.26 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 16700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 320
Ep done - 689000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.842    |
|    return_std      | 2.47     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34555    |
|    total_timesteps | 16732809 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 932      |
|    learning_rate   | 0.00266  |
|    step_size       | 0.00018  |
---------------------------------
Ep done - 690000.
Eval num_timesteps=16750000, episode_reward=0.95 +/- 0.30
Episode length: 30.30 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.95     |
| time/              |          |
|    total_timesteps | 16750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.95
SELFPLAY: new best model, bumping up generation to 321
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.835    |
|    return_std      | 2.22     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34636    |
|    total_timesteps | 16769007 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 933      |
|    learning_rate   | 0.00266  |
|    step_size       | 0.0002   |
---------------------------------
Ep done - 691000.
Ep done - 692000.
Eval num_timesteps=16800000, episode_reward=0.93 +/- 0.34
Episode length: 30.26 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 16800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 322
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.819    |
|    return_std      | 2.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34715    |
|    total_timesteps | 16805254 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 934      |
|    learning_rate   | 0.00266  |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 693000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.854    |
|    return_std      | 2.09     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34775    |
|    total_timesteps | 16841501 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 935      |
|    learning_rate   | 0.00266  |
|    step_size       | 0.000211 |
---------------------------------
Ep done - 694000.
Eval num_timesteps=16850000, episode_reward=0.90 +/- 0.43
Episode length: 30.20 +/- 1.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 16850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9025
SELFPLAY: new best model, bumping up generation to 323
Ep done - 695000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.832    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34856    |
|    total_timesteps | 16877782 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 936      |
|    learning_rate   | 0.00265  |
|    step_size       | 0.000273 |
---------------------------------
Ep done - 696000.
Eval num_timesteps=16900000, episode_reward=0.92 +/- 0.40
Episode length: 30.26 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 16900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 324
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 3.58     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 34938    |
|    total_timesteps | 16914039 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 937      |
|    learning_rate   | 0.00265  |
|    step_size       | 0.000123 |
---------------------------------
Ep done - 697000.
Ep done - 698000.
Eval num_timesteps=16950000, episode_reward=0.90 +/- 0.44
Episode length: 30.24 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 16950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 325
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.839    |
|    return_std      | 1.91     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35017    |
|    total_timesteps | 16950273 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 938      |
|    learning_rate   | 0.00265  |
|    step_size       | 0.000231 |
---------------------------------
Ep done - 699000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.827    |
|    return_std      | 2.26     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35079    |
|    total_timesteps | 16986501 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 939      |
|    learning_rate   | 0.00264  |
|    step_size       | 0.000195 |
---------------------------------
Ep done - 700000.
Eval num_timesteps=17000000, episode_reward=0.89 +/- 0.45
Episode length: 30.27 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 17000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 326
Ep done - 701000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.01     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35158    |
|    total_timesteps | 17022793 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 940      |
|    learning_rate   | 0.00264  |
|    step_size       | 0.000219 |
---------------------------------
Ep done - 702000.
Eval num_timesteps=17050000, episode_reward=0.91 +/- 0.40
Episode length: 30.29 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 17050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 327
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.823    |
|    return_std      | 3.55     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35237    |
|    total_timesteps | 17059003 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 941      |
|    learning_rate   | 0.00264  |
|    step_size       | 0.000124 |
---------------------------------
Ep done - 703000.
Ep done - 704000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.97     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35299    |
|    total_timesteps | 17095284 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 942      |
|    learning_rate   | 0.00264  |
|    step_size       | 0.000223 |
---------------------------------
Eval num_timesteps=17100000, episode_reward=0.91 +/- 0.41
Episode length: 30.25 +/- 0.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 17100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 328
Ep done - 705000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.818    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35378    |
|    total_timesteps | 17131507 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 943      |
|    learning_rate   | 0.00263  |
|    step_size       | 0.000193 |
---------------------------------
Ep done - 706000.
Eval num_timesteps=17150000, episode_reward=0.94 +/- 0.33
Episode length: 30.28 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 17150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.94
SELFPLAY: new best model, bumping up generation to 329
Ep done - 707000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.871    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35457    |
|    total_timesteps | 17167808 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 944      |
|    learning_rate   | 0.00263  |
|    step_size       | 0.000228 |
---------------------------------
Ep done - 708000.
Eval num_timesteps=17200000, episode_reward=0.94 +/- 0.35
Episode length: 30.33 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 17200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 330
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.821    |
|    return_std      | 1.72     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35539    |
|    total_timesteps | 17204070 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 945      |
|    learning_rate   | 0.00263  |
|    step_size       | 0.000254 |
---------------------------------
Ep done - 709000.
Ep done - 710000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.832    |
|    return_std      | 3.3      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35598    |
|    total_timesteps | 17240304 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 946      |
|    learning_rate   | 0.00262  |
|    step_size       | 0.000132 |
---------------------------------
Eval num_timesteps=17250000, episode_reward=0.88 +/- 0.45
Episode length: 30.25 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 17250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 331
Ep done - 711000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.816    |
|    return_std      | 3.96     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35676    |
|    total_timesteps | 17276529 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 947      |
|    learning_rate   | 0.00262  |
|    step_size       | 0.00011  |
---------------------------------
Ep done - 712000.
Eval num_timesteps=17300000, episode_reward=0.90 +/- 0.44
Episode length: 30.28 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 17300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 332
Ep done - 713000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.842    |
|    return_std      | 2.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35758    |
|    total_timesteps | 17312805 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 948      |
|    learning_rate   | 0.00262  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 714000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 3.2      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35817    |
|    total_timesteps | 17349091 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 949      |
|    learning_rate   | 0.00261  |
|    step_size       | 0.000136 |
---------------------------------
Eval num_timesteps=17350000, episode_reward=0.95 +/- 0.27
Episode length: 30.27 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.955    |
| time/              |          |
|    total_timesteps | 17350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.955
SELFPLAY: new best model, bumping up generation to 333
Ep done - 715000.
Ep done - 716000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 2.57     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35896    |
|    total_timesteps | 17385322 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 950      |
|    learning_rate   | 0.00261  |
|    step_size       | 0.000169 |
---------------------------------
Eval num_timesteps=17400000, episode_reward=0.86 +/- 0.50
Episode length: 30.22 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.863    |
| time/              |          |
|    total_timesteps | 17400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8625
SELFPLAY: new best model, bumping up generation to 334
Ep done - 717000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.801    |
|    return_std      | 2.09     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 35978    |
|    total_timesteps | 17421551 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 951      |
|    learning_rate   | 0.00261  |
|    step_size       | 0.000208 |
---------------------------------
Ep done - 718000.
Eval num_timesteps=17450000, episode_reward=0.91 +/- 0.39
Episode length: 30.28 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 17450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 335
Ep done - 719000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.52     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36058    |
|    total_timesteps | 17457855 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 952      |
|    learning_rate   | 0.00261  |
|    step_size       | 0.000173 |
---------------------------------
Ep done - 720000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 2.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36118    |
|    total_timesteps | 17494140 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 953      |
|    learning_rate   | 0.0026   |
|    step_size       | 0.000169 |
---------------------------------
Eval num_timesteps=17500000, episode_reward=0.92 +/- 0.39
Episode length: 30.23 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 17500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 336
Ep done - 721000.
Ep done - 722000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.8      |
|    return_std      | 2.68     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36200    |
|    total_timesteps | 17530386 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 954      |
|    learning_rate   | 0.0026   |
|    step_size       | 0.000162 |
---------------------------------
Ep done - 723000.
Eval num_timesteps=17550000, episode_reward=0.90 +/- 0.43
Episode length: 30.26 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 17550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 337
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 2.52     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36279    |
|    total_timesteps | 17566642 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 955      |
|    learning_rate   | 0.0026   |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 724000.
Ep done - 725000.
Eval num_timesteps=17600000, episode_reward=0.89 +/- 0.44
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 17600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 338
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 3.47     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36361    |
|    total_timesteps | 17602897 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 956      |
|    learning_rate   | 0.00259  |
|    step_size       | 0.000125 |
---------------------------------
Ep done - 726000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 1.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36420    |
|    total_timesteps | 17639138 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 957      |
|    learning_rate   | 0.00259  |
|    step_size       | 0.000219 |
---------------------------------
Ep done - 727000.
Eval num_timesteps=17650000, episode_reward=0.90 +/- 0.43
Episode length: 30.24 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 17650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 339
Ep done - 728000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 2.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36499    |
|    total_timesteps | 17675366 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 958      |
|    learning_rate   | 0.00259  |
|    step_size       | 0.000168 |
---------------------------------
Ep done - 729000.
Eval num_timesteps=17700000, episode_reward=0.92 +/- 0.36
Episode length: 30.27 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 17700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 340
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36581    |
|    total_timesteps | 17711631 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 959      |
|    learning_rate   | 0.00259  |
|    step_size       | 0.000249 |
---------------------------------
Ep done - 730000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 2.26     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36640    |
|    total_timesteps | 17747881 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 960      |
|    learning_rate   | 0.00258  |
|    step_size       | 0.00019  |
---------------------------------
Ep done - 731000.
Eval num_timesteps=17750000, episode_reward=0.89 +/- 0.45
Episode length: 30.31 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 17750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 341
Ep done - 732000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 1.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36719    |
|    total_timesteps | 17784156 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 961      |
|    learning_rate   | 0.00258  |
|    step_size       | 0.000235 |
---------------------------------
Ep done - 733000.
Eval num_timesteps=17800000, episode_reward=0.89 +/- 0.44
Episode length: 30.22 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 17800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 342
Ep done - 734000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.61     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36801    |
|    total_timesteps | 17820436 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 962      |
|    learning_rate   | 0.00258  |
|    step_size       | 0.000165 |
---------------------------------
Ep done - 735000.
Eval num_timesteps=17850000, episode_reward=0.90 +/- 0.42
Episode length: 30.28 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 17850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9025
SELFPLAY: new best model, bumping up generation to 343
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.819    |
|    return_std      | 2.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36880    |
|    total_timesteps | 17856714 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 963      |
|    learning_rate   | 0.00257  |
|    step_size       | 0.000154 |
---------------------------------
Ep done - 736000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.84     |
|    return_std      | 3.19     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 36939    |
|    total_timesteps | 17892926 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 964      |
|    learning_rate   | 0.00257  |
|    step_size       | 0.000134 |
---------------------------------
Ep done - 737000.
Eval num_timesteps=17900000, episode_reward=0.88 +/- 0.45
Episode length: 30.23 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 17900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 344
Ep done - 738000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.83     |
|    return_std      | 1.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37021    |
|    total_timesteps | 17929144 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 965      |
|    learning_rate   | 0.00257  |
|    step_size       | 0.000217 |
---------------------------------
Ep done - 739000.
Eval num_timesteps=17950000, episode_reward=0.89 +/- 0.45
Episode length: 30.29 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 17950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 345
Ep done - 740000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.839    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37102    |
|    total_timesteps | 17965430 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 966      |
|    learning_rate   | 0.00257  |
|    step_size       | 0.000227 |
---------------------------------
Ep done - 741000.
Eval num_timesteps=18000000, episode_reward=0.85 +/- 0.52
Episode length: 30.20 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.85     |
| time/              |          |
|    total_timesteps | 18000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.85
SELFPLAY: new best model, bumping up generation to 346
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 3        |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37182    |
|    total_timesteps | 18001695 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 967      |
|    learning_rate   | 0.00256  |
|    step_size       | 0.000142 |
---------------------------------
Ep done - 742000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.83     |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37244    |
|    total_timesteps | 18037919 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 968      |
|    learning_rate   | 0.00256  |
|    step_size       | 0.000176 |
---------------------------------
Ep done - 743000.
Eval num_timesteps=18050000, episode_reward=0.91 +/- 0.40
Episode length: 30.23 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 18050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 347
Ep done - 744000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.817    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37323    |
|    total_timesteps | 18074175 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 969      |
|    learning_rate   | 0.00256  |
|    step_size       | 0.000221 |
---------------------------------
Ep done - 745000.
Eval num_timesteps=18100000, episode_reward=0.89 +/- 0.45
Episode length: 30.26 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 18100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 348
Ep done - 746000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 3.17     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37402    |
|    total_timesteps | 18110399 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 970      |
|    learning_rate   | 0.00255  |
|    step_size       | 0.000134 |
---------------------------------
Ep done - 747000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37465    |
|    total_timesteps | 18146661 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 971      |
|    learning_rate   | 0.00255  |
|    step_size       | 0.000221 |
---------------------------------
Eval num_timesteps=18150000, episode_reward=0.93 +/- 0.35
Episode length: 30.29 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 18150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 349
Ep done - 748000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.85     |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37544    |
|    total_timesteps | 18182925 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 972      |
|    learning_rate   | 0.00255  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 749000.
Eval num_timesteps=18200000, episode_reward=0.89 +/- 0.45
Episode length: 30.30 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 18200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 350
Ep done - 750000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.85     |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37623    |
|    total_timesteps | 18219175 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 973      |
|    learning_rate   | 0.00255  |
|    step_size       | 0.000187 |
---------------------------------
Ep done - 751000.
Eval num_timesteps=18250000, episode_reward=0.90 +/- 0.42
Episode length: 30.30 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 18250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 351
Ep done - 752000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37705    |
|    total_timesteps | 18255411 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 974      |
|    learning_rate   | 0.00254  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 753000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 1.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37764    |
|    total_timesteps | 18291721 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 975      |
|    learning_rate   | 0.00254  |
|    step_size       | 0.000214 |
---------------------------------
Eval num_timesteps=18300000, episode_reward=0.90 +/- 0.43
Episode length: 30.30 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 18300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 352
Ep done - 754000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.856    |
|    return_std      | 2.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37844    |
|    total_timesteps | 18327992 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 976      |
|    learning_rate   | 0.00254  |
|    step_size       | 0.000183 |
---------------------------------
Ep done - 755000.
Eval num_timesteps=18350000, episode_reward=0.91 +/- 0.41
Episode length: 30.25 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 18350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 353
Ep done - 756000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.852    |
|    return_std      | 2.5      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 37926    |
|    total_timesteps | 18364241 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 977      |
|    learning_rate   | 0.00253  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 757000.
Eval num_timesteps=18400000, episode_reward=0.91 +/- 0.42
Episode length: 30.26 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 18400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 354
Ep done - 758000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.812    |
|    return_std      | 2.77     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38005    |
|    total_timesteps | 18400427 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 978      |
|    learning_rate   | 0.00253  |
|    step_size       | 0.000152 |
---------------------------------
Ep done - 759000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.846    |
|    return_std      | 2.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38065    |
|    total_timesteps | 18436671 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 979      |
|    learning_rate   | 0.00253  |
|    step_size       | 0.000144 |
---------------------------------
Ep done - 760000.
Eval num_timesteps=18450000, episode_reward=0.87 +/- 0.48
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.87     |
| time/              |          |
|    total_timesteps | 18450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.87
SELFPLAY: new best model, bumping up generation to 355
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38147    |
|    total_timesteps | 18472941 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 980      |
|    learning_rate   | 0.00253  |
|    step_size       | 0.000176 |
---------------------------------
Ep done - 761000.
Ep done - 762000.
Eval num_timesteps=18500000, episode_reward=0.90 +/- 0.41
Episode length: 30.25 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 18500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 356
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 2.53     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38226    |
|    total_timesteps | 18509205 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 981      |
|    learning_rate   | 0.00252  |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 763000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.855    |
|    return_std      | 2.49     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38285    |
|    total_timesteps | 18545497 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 982      |
|    learning_rate   | 0.00252  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 764000.
Eval num_timesteps=18550000, episode_reward=0.87 +/- 0.48
Episode length: 30.29 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.868    |
| time/              |          |
|    total_timesteps | 18550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8675
SELFPLAY: new best model, bumping up generation to 357
Ep done - 765000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.829    |
|    return_std      | 4.06     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38367    |
|    total_timesteps | 18581750 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 983      |
|    learning_rate   | 0.00252  |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 766000.
Eval num_timesteps=18600000, episode_reward=0.90 +/- 0.42
Episode length: 30.28 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 18600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 358
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.826    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38446    |
|    total_timesteps | 18617994 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 984      |
|    learning_rate   | 0.00251  |
|    step_size       | 0.000222 |
---------------------------------
Ep done - 767000.
Ep done - 768000.
Eval num_timesteps=18650000, episode_reward=0.91 +/- 0.39
Episode length: 30.26 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 18650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 359
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.86     |
|    return_std      | 2.5      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38527    |
|    total_timesteps | 18654251 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 985      |
|    learning_rate   | 0.00251  |
|    step_size       | 0.000167 |
---------------------------------
Ep done - 769000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.89     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38586    |
|    total_timesteps | 18690493 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 986      |
|    learning_rate   | 0.00251  |
|    step_size       | 0.000145 |
---------------------------------
Ep done - 770000.
Eval num_timesteps=18700000, episode_reward=0.90 +/- 0.40
Episode length: 30.30 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 18700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9025
SELFPLAY: new best model, bumping up generation to 360
Ep done - 771000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.867    |
|    return_std      | 2.96     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38665    |
|    total_timesteps | 18726726 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 987      |
|    learning_rate   | 0.0025   |
|    step_size       | 0.000141 |
---------------------------------
Ep done - 772000.
Eval num_timesteps=18750000, episode_reward=0.93 +/- 0.34
Episode length: 30.37 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 18750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 361
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.822    |
|    return_std      | 2.97     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38746    |
|    total_timesteps | 18762936 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 988      |
|    learning_rate   | 0.0025   |
|    step_size       | 0.00014  |
---------------------------------
Ep done - 773000.
Ep done - 774000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 1.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38806    |
|    total_timesteps | 18799196 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 989      |
|    learning_rate   | 0.0025   |
|    step_size       | 0.000266 |
---------------------------------
Eval num_timesteps=18800000, episode_reward=0.93 +/- 0.36
Episode length: 30.28 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 18800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 362
Ep done - 775000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 1.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38885    |
|    total_timesteps | 18835429 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 990      |
|    learning_rate   | 0.0025   |
|    step_size       | 0.000317 |
---------------------------------
Ep done - 776000.
Eval num_timesteps=18850000, episode_reward=0.90 +/- 0.43
Episode length: 30.26 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 18850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 363
Ep done - 777000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 38967    |
|    total_timesteps | 18871646 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 991      |
|    learning_rate   | 0.00249  |
|    step_size       | 0.000193 |
---------------------------------
Ep done - 778000.
Eval num_timesteps=18900000, episode_reward=0.89 +/- 0.44
Episode length: 30.30 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 18900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 364
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.859    |
|    return_std      | 3.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39046    |
|    total_timesteps | 18907920 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 992      |
|    learning_rate   | 0.00249  |
|    step_size       | 0.000135 |
---------------------------------
Ep done - 779000.
Ep done - 780000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.831    |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39106    |
|    total_timesteps | 18944188 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 993      |
|    learning_rate   | 0.00249  |
|    step_size       | 0.000201 |
---------------------------------
Eval num_timesteps=18950000, episode_reward=0.91 +/- 0.39
Episode length: 30.23 +/- 1.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 18950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 365
Ep done - 781000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 2.17     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39188    |
|    total_timesteps | 18980474 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 994      |
|    learning_rate   | 0.00248  |
|    step_size       | 0.000191 |
---------------------------------
Ep done - 782000.
Eval num_timesteps=19000000, episode_reward=0.90 +/- 0.42
Episode length: 30.30 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 19000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 366
Ep done - 783000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 1.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39269    |
|    total_timesteps | 19016740 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 995      |
|    learning_rate   | 0.00248  |
|    step_size       | 0.000226 |
---------------------------------
Ep done - 784000.
Eval num_timesteps=19050000, episode_reward=0.91 +/- 0.40
Episode length: 30.29 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 19050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 367
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 2.73     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39348    |
|    total_timesteps | 19053033 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 996      |
|    learning_rate   | 0.00248  |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 785000.
Ep done - 786000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.805    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39409    |
|    total_timesteps | 19089244 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 997      |
|    learning_rate   | 0.00248  |
|    step_size       | 0.00017  |
---------------------------------
Eval num_timesteps=19100000, episode_reward=0.94 +/- 0.33
Episode length: 30.29 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 19100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 368
Ep done - 787000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.72     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39488    |
|    total_timesteps | 19125467 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 998      |
|    learning_rate   | 0.00247  |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 788000.
Eval num_timesteps=19150000, episode_reward=0.91 +/- 0.41
Episode length: 30.27 +/- 1.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 19150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 369
Ep done - 789000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.818    |
|    return_std      | 3.9      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39567    |
|    total_timesteps | 19161719 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 999      |
|    learning_rate   | 0.00247  |
|    step_size       | 0.000106 |
---------------------------------
Ep done - 790000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.842    |
|    return_std      | 2.73     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39629    |
|    total_timesteps | 19197980 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1000     |
|    learning_rate   | 0.00247  |
|    step_size       | 0.000151 |
---------------------------------
Eval num_timesteps=19200000, episode_reward=0.89 +/- 0.45
Episode length: 30.28 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 19200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 370
Ep done - 791000.
Ep done - 792000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.852    |
|    return_std      | 2.17     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39708    |
|    total_timesteps | 19234262 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1001     |
|    learning_rate   | 0.00246  |
|    step_size       | 0.000189 |
---------------------------------
Eval num_timesteps=19250000, episode_reward=0.95 +/- 0.29
Episode length: 30.27 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.95     |
| time/              |          |
|    total_timesteps | 19250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.95
SELFPLAY: new best model, bumping up generation to 371
Ep done - 793000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | 0.86     |
|    return_std      | 2.17     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39787    |
|    total_timesteps | 19270584 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1002     |
|    learning_rate   | 0.00246  |
|    step_size       | 0.000189 |
---------------------------------
Ep done - 794000.
Eval num_timesteps=19300000, episode_reward=0.88 +/- 0.46
Episode length: 30.30 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.88     |
| time/              |          |
|    total_timesteps | 19300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.88
SELFPLAY: new best model, bumping up generation to 372
Ep done - 795000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 3.09     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39869    |
|    total_timesteps | 19306902 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1003     |
|    learning_rate   | 0.00246  |
|    step_size       | 0.000133 |
---------------------------------
Ep done - 796000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.827    |
|    return_std      | 2.61     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 39928    |
|    total_timesteps | 19343134 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1004     |
|    learning_rate   | 0.00246  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 797000.
Eval num_timesteps=19350000, episode_reward=0.92 +/- 0.39
Episode length: 30.26 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 19350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 373
Ep done - 798000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.846    |
|    return_std      | 2.23     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40007    |
|    total_timesteps | 19379406 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1005     |
|    learning_rate   | 0.00245  |
|    step_size       | 0.000183 |
---------------------------------
Ep done - 799000.
Eval num_timesteps=19400000, episode_reward=0.90 +/- 0.42
Episode length: 30.28 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 19400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 374
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.847    |
|    return_std      | 2.1      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40089    |
|    total_timesteps | 19415725 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1006     |
|    learning_rate   | 0.00245  |
|    step_size       | 0.000194 |
---------------------------------
Ep done - 800000.
Ep done - 801000.
Eval num_timesteps=19450000, episode_reward=0.92 +/- 0.38
Episode length: 30.34 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 19450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 375
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40168    |
|    total_timesteps | 19452000 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1007     |
|    learning_rate   | 0.00245  |
|    step_size       | 0.000211 |
---------------------------------
Ep done - 802000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 3.03     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40227    |
|    total_timesteps | 19488236 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1008     |
|    learning_rate   | 0.00244  |
|    step_size       | 0.000135 |
---------------------------------
Ep done - 803000.
Eval num_timesteps=19500000, episode_reward=0.91 +/- 0.42
Episode length: 30.28 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 19500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 376
Ep done - 804000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 1.29     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40309    |
|    total_timesteps | 19524509 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1009     |
|    learning_rate   | 0.00244  |
|    step_size       | 0.000316 |
---------------------------------
Ep done - 805000.
Eval num_timesteps=19550000, episode_reward=0.88 +/- 0.47
Episode length: 30.34 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.877    |
| time/              |          |
|    total_timesteps | 19550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8775
SELFPLAY: new best model, bumping up generation to 377
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.837    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40388    |
|    total_timesteps | 19560709 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1010     |
|    learning_rate   | 0.00244  |
|    step_size       | 0.000228 |
---------------------------------
Ep done - 806000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.871    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40449    |
|    total_timesteps | 19596972 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1011     |
|    learning_rate   | 0.00244  |
|    step_size       | 0.00025  |
---------------------------------
Ep done - 807000.
Eval num_timesteps=19600000, episode_reward=0.92 +/- 0.39
Episode length: 30.30 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 19600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 378
Ep done - 808000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40530    |
|    total_timesteps | 19633234 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1012     |
|    learning_rate   | 0.00243  |
|    step_size       | 0.000216 |
---------------------------------
Ep done - 809000.
Eval num_timesteps=19650000, episode_reward=0.91 +/- 0.41
Episode length: 30.32 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 19650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 379
Ep done - 810000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.852    |
|    return_std      | 1.54     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40609    |
|    total_timesteps | 19669520 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1013     |
|    learning_rate   | 0.00243  |
|    step_size       | 0.000263 |
---------------------------------
Ep done - 811000.
Eval num_timesteps=19700000, episode_reward=0.89 +/- 0.45
Episode length: 30.32 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 19700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 380
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.855    |
|    return_std      | 1.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40691    |
|    total_timesteps | 19705794 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1014     |
|    learning_rate   | 0.00243  |
|    step_size       | 0.00026  |
---------------------------------
Ep done - 812000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 2.66     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40750    |
|    total_timesteps | 19742024 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1015     |
|    learning_rate   | 0.00242  |
|    step_size       | 0.000152 |
---------------------------------
Ep done - 813000.
Eval num_timesteps=19750000, episode_reward=0.92 +/- 0.39
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 19750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 381
Ep done - 814000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 1.41     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40829    |
|    total_timesteps | 19778285 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1016     |
|    learning_rate   | 0.00242  |
|    step_size       | 0.000285 |
---------------------------------
Ep done - 815000.
Eval num_timesteps=19800000, episode_reward=0.92 +/- 0.38
Episode length: 30.29 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 19800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 382
Ep done - 816000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.854    |
|    return_std      | 3.37     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40911    |
|    total_timesteps | 19814567 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1017     |
|    learning_rate   | 0.00242  |
|    step_size       | 0.00012  |
---------------------------------
Ep done - 817000.
Eval num_timesteps=19850000, episode_reward=0.93 +/- 0.36
Episode length: 30.34 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 19850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 383
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 0.965    |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 40990    |
|    total_timesteps | 19850845 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1018     |
|    learning_rate   | 0.00241  |
|    step_size       | 0.000417 |
---------------------------------
Ep done - 818000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.856    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41049    |
|    total_timesteps | 19887130 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1019     |
|    learning_rate   | 0.00241  |
|    step_size       | 0.000191 |
---------------------------------
Ep done - 819000.
Eval num_timesteps=19900000, episode_reward=0.91 +/- 0.40
Episode length: 30.23 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 19900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 384
Ep done - 820000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 2.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41131    |
|    total_timesteps | 19923403 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1020     |
|    learning_rate   | 0.00241  |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 821000.
Eval num_timesteps=19950000, episode_reward=0.91 +/- 0.41
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 19950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 385
Ep done - 822000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.819    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41210    |
|    total_timesteps | 19959673 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1021     |
|    learning_rate   | 0.00241  |
|    step_size       | 0.000187 |
---------------------------------
Ep done - 823000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.823    |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41269    |
|    total_timesteps | 19995946 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1022     |
|    learning_rate   | 0.0024   |
|    step_size       | 0.000148 |
---------------------------------
Eval num_timesteps=20000000, episode_reward=0.87 +/- 0.48
Episode length: 30.23 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.873    |
| time/              |          |
|    total_timesteps | 20000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8725
SELFPLAY: new best model, bumping up generation to 386
Ep done - 824000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.837    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41351    |
|    total_timesteps | 20032212 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1023     |
|    learning_rate   | 0.0024   |
|    step_size       | 0.000213 |
---------------------------------
Ep done - 825000.
Eval num_timesteps=20050000, episode_reward=0.92 +/- 0.38
Episode length: 30.25 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 20050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 387
Ep done - 826000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.84     |
|    return_std      | 3.5      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41431    |
|    total_timesteps | 20068469 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1024     |
|    learning_rate   | 0.0024   |
|    step_size       | 0.000114 |
---------------------------------
Ep done - 827000.
Eval num_timesteps=20100000, episode_reward=0.89 +/- 0.45
Episode length: 30.25 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 20100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 388
Ep done - 828000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 2.91     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41510    |
|    total_timesteps | 20104743 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1025     |
|    learning_rate   | 0.00239  |
|    step_size       | 0.000137 |
---------------------------------
Ep done - 829000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.825    |
|    return_std      | 3.37     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41572    |
|    total_timesteps | 20140982 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1026     |
|    learning_rate   | 0.00239  |
|    step_size       | 0.000118 |
---------------------------------
Eval num_timesteps=20150000, episode_reward=0.90 +/- 0.44
Episode length: 30.23 +/- 1.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 20150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 389
Ep done - 830000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.824    |
|    return_std      | 3.6      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41651    |
|    total_timesteps | 20177256 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1027     |
|    learning_rate   | 0.00239  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 831000.
Eval num_timesteps=20200000, episode_reward=0.88 +/- 0.46
Episode length: 30.25 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.88     |
| time/              |          |
|    total_timesteps | 20200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.88
SELFPLAY: new best model, bumping up generation to 390
Ep done - 832000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.886    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41730    |
|    total_timesteps | 20213551 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1028     |
|    learning_rate   | 0.00239  |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 833000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41793    |
|    total_timesteps | 20249855 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1029     |
|    learning_rate   | 0.00238  |
|    step_size       | 0.000192 |
---------------------------------
Ep done - 834000.
Eval num_timesteps=20250000, episode_reward=0.91 +/- 0.41
Episode length: 30.30 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 20250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 391
Ep done - 835000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.83     |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41872    |
|    total_timesteps | 20286112 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1030     |
|    learning_rate   | 0.00238  |
|    step_size       | 0.000229 |
---------------------------------
Ep done - 836000.
Eval num_timesteps=20300000, episode_reward=0.88 +/- 0.47
Episode length: 30.26 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.875    |
| time/              |          |
|    total_timesteps | 20300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.875
SELFPLAY: new best model, bumping up generation to 392
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 41951    |
|    total_timesteps | 20322404 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1031     |
|    learning_rate   | 0.00238  |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 837000.
Ep done - 838000.
Eval num_timesteps=20350000, episode_reward=0.89 +/- 0.44
Episode length: 30.32 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 20350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 393
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 2.18     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42033    |
|    total_timesteps | 20358654 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1032     |
|    learning_rate   | 0.00237  |
|    step_size       | 0.000182 |
---------------------------------
Ep done - 839000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42092    |
|    total_timesteps | 20394941 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1033     |
|    learning_rate   | 0.00237  |
|    step_size       | 0.000228 |
---------------------------------
Ep done - 840000.
Eval num_timesteps=20400000, episode_reward=0.91 +/- 0.40
Episode length: 30.32 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 20400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 394
Ep done - 841000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42171    |
|    total_timesteps | 20431212 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1034     |
|    learning_rate   | 0.00237  |
|    step_size       | 0.000152 |
---------------------------------
Ep done - 842000.
Eval num_timesteps=20450000, episode_reward=0.90 +/- 0.43
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 20450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 395
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 2.37     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42253    |
|    total_timesteps | 20467516 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1035     |
|    learning_rate   | 0.00237  |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 843000.
Ep done - 844000.
Eval num_timesteps=20500000, episode_reward=0.93 +/- 0.36
Episode length: 30.30 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 20500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 396
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.864    |
|    return_std      | 1.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42332    |
|    total_timesteps | 20503771 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1036     |
|    learning_rate   | 0.00236  |
|    step_size       | 0.000248 |
---------------------------------
Ep done - 845000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.846    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42395    |
|    total_timesteps | 20540012 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1037     |
|    learning_rate   | 0.00236  |
|    step_size       | 0.000195 |
---------------------------------
Ep done - 846000.
Eval num_timesteps=20550000, episode_reward=0.92 +/- 0.37
Episode length: 30.29 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 20550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 397
Ep done - 847000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.847    |
|    return_std      | 2.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42474    |
|    total_timesteps | 20576279 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1038     |
|    learning_rate   | 0.00236  |
|    step_size       | 0.00017  |
---------------------------------
Ep done - 848000.
Eval num_timesteps=20600000, episode_reward=0.89 +/- 0.46
Episode length: 30.25 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 20600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 398
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.856    |
|    return_std      | 2.67     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42553    |
|    total_timesteps | 20612595 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1039     |
|    learning_rate   | 0.00235  |
|    step_size       | 0.000147 |
---------------------------------
Ep done - 849000.
Ep done - 850000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 2.54     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42615    |
|    total_timesteps | 20648830 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1040     |
|    learning_rate   | 0.00235  |
|    step_size       | 0.000154 |
---------------------------------
Eval num_timesteps=20650000, episode_reward=0.91 +/- 0.39
Episode length: 30.22 +/- 1.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 20650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 399
Ep done - 851000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.822    |
|    return_std      | 2.29     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42694    |
|    total_timesteps | 20685075 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1041     |
|    learning_rate   | 0.00235  |
|    step_size       | 0.000171 |
---------------------------------
Ep done - 852000.
Eval num_timesteps=20700000, episode_reward=0.91 +/- 0.41
Episode length: 30.30 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 20700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 400
Ep done - 853000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.867    |
|    return_std      | 2.67     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42774    |
|    total_timesteps | 20721367 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1042     |
|    learning_rate   | 0.00235  |
|    step_size       | 0.000147 |
---------------------------------
Ep done - 854000.
Eval num_timesteps=20750000, episode_reward=0.92 +/- 0.38
Episode length: 30.29 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 20750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 401
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.837    |
|    return_std      | 1.57     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42856    |
|    total_timesteps | 20757586 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1043     |
|    learning_rate   | 0.00234  |
|    step_size       | 0.000249 |
---------------------------------
Ep done - 855000.
Ep done - 856000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.86     |
|    return_std      | 2.24     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42915    |
|    total_timesteps | 20793822 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1044     |
|    learning_rate   | 0.00234  |
|    step_size       | 0.000174 |
---------------------------------
Eval num_timesteps=20800000, episode_reward=0.88 +/- 0.46
Episode length: 30.22 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 20800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 402
Ep done - 857000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 42994    |
|    total_timesteps | 20830030 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1045     |
|    learning_rate   | 0.00234  |
|    step_size       | 0.000225 |
---------------------------------
Ep done - 858000.
Eval num_timesteps=20850000, episode_reward=0.92 +/- 0.37
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 20850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 403
Ep done - 859000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.877    |
|    return_std      | 1.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43076    |
|    total_timesteps | 20866306 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1046     |
|    learning_rate   | 0.00233  |
|    step_size       | 0.000245 |
---------------------------------
Ep done - 860000.
Eval num_timesteps=20900000, episode_reward=0.91 +/- 0.41
Episode length: 30.32 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 20900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 404
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.856    |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43155    |
|    total_timesteps | 20902531 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1047     |
|    learning_rate   | 0.00233  |
|    step_size       | 0.000225 |
---------------------------------
Ep done - 861000.
Ep done - 862000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43214    |
|    total_timesteps | 20938822 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1048     |
|    learning_rate   | 0.00233  |
|    step_size       | 0.000188 |
---------------------------------
Eval num_timesteps=20950000, episode_reward=0.89 +/- 0.44
Episode length: 30.29 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 20950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 405
Ep done - 863000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 2.52     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43296    |
|    total_timesteps | 20975118 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1049     |
|    learning_rate   | 0.00232  |
|    step_size       | 0.000154 |
---------------------------------
Ep done - 864000.
Eval num_timesteps=21000000, episode_reward=0.89 +/- 0.45
Episode length: 30.30 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 21000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 406
Ep done - 865000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 3.2      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43374    |
|    total_timesteps | 21011398 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1050     |
|    learning_rate   | 0.00232  |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 866000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.841    |
|    return_std      | 2.57     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43434    |
|    total_timesteps | 21047655 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1051     |
|    learning_rate   | 0.00232  |
|    step_size       | 0.00015  |
---------------------------------
Eval num_timesteps=21050000, episode_reward=0.91 +/- 0.41
Episode length: 30.30 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 21050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 407
Ep done - 867000.
Ep done - 868000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43516    |
|    total_timesteps | 21083937 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1052     |
|    learning_rate   | 0.00232  |
|    step_size       | 0.00017  |
---------------------------------
Eval num_timesteps=21100000, episode_reward=0.90 +/- 0.43
Episode length: 30.30 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 21100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 408
Ep done - 869000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.823    |
|    return_std      | 3.85     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43596    |
|    total_timesteps | 21120151 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1053     |
|    learning_rate   | 0.00231  |
|    step_size       | 0.0001   |
---------------------------------
Ep done - 870000.
Eval num_timesteps=21150000, episode_reward=0.91 +/- 0.39
Episode length: 30.21 +/- 0.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 21150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 409
Ep done - 871000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.28     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43675    |
|    total_timesteps | 21156429 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1054     |
|    learning_rate   | 0.00231  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 872000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43737    |
|    total_timesteps | 21192679 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1055     |
|    learning_rate   | 0.00231  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 873000.
Eval num_timesteps=21200000, episode_reward=0.85 +/- 0.51
Episode length: 30.24 +/- 1.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.853    |
| time/              |          |
|    total_timesteps | 21200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8525
SELFPLAY: new best model, bumping up generation to 410
Ep done - 874000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 1.75     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43816    |
|    total_timesteps | 21228946 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1056     |
|    learning_rate   | 0.0023   |
|    step_size       | 0.00022  |
---------------------------------
Ep done - 875000.
Eval num_timesteps=21250000, episode_reward=0.92 +/- 0.36
Episode length: 30.28 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 21250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 411
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43895    |
|    total_timesteps | 21265183 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1057     |
|    learning_rate   | 0.0023   |
|    step_size       | 0.000215 |
---------------------------------
Ep done - 876000.
Ep done - 877000.
Eval num_timesteps=21300000, episode_reward=0.93 +/- 0.35
Episode length: 30.30 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 21300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 412
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 2.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 43977    |
|    total_timesteps | 21301424 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1058     |
|    learning_rate   | 0.0023   |
|    step_size       | 0.000164 |
---------------------------------
Ep done - 878000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.857    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44036    |
|    total_timesteps | 21337662 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1059     |
|    learning_rate   | 0.0023   |
|    step_size       | 0.000198 |
---------------------------------
Ep done - 879000.
Eval num_timesteps=21350000, episode_reward=0.95 +/- 0.32
Episode length: 30.25 +/- 0.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.948    |
| time/              |          |
|    total_timesteps | 21350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9475
SELFPLAY: new best model, bumping up generation to 413
Ep done - 880000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.24     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44115    |
|    total_timesteps | 21373967 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1060     |
|    learning_rate   | 0.00229  |
|    step_size       | 0.000307 |
---------------------------------
Ep done - 881000.
Eval num_timesteps=21400000, episode_reward=0.93 +/- 0.34
Episode length: 30.30 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 21400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 414
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.864    |
|    return_std      | 1.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44197    |
|    total_timesteps | 21410184 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1061     |
|    learning_rate   | 0.00229  |
|    step_size       | 0.000264 |
---------------------------------
Ep done - 882000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.835    |
|    return_std      | 2.66     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44256    |
|    total_timesteps | 21446434 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1062     |
|    learning_rate   | 0.00229  |
|    step_size       | 0.000143 |
---------------------------------
Ep done - 883000.
Eval num_timesteps=21450000, episode_reward=0.92 +/- 0.36
Episode length: 30.30 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 21450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 415
Ep done - 884000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.84     |
|    return_std      | 3.26     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44338    |
|    total_timesteps | 21482646 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1063     |
|    learning_rate   | 0.00228  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 885000.
Eval num_timesteps=21500000, episode_reward=0.93 +/- 0.36
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 21500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 416
Ep done - 886000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.832    |
|    return_std      | 2.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44417    |
|    total_timesteps | 21518887 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1064     |
|    learning_rate   | 0.00228  |
|    step_size       | 0.000149 |
---------------------------------
Ep done - 887000.
Eval num_timesteps=21550000, episode_reward=0.90 +/- 0.41
Episode length: 30.28 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 21550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 417
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 0.888    |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44496    |
|    total_timesteps | 21555176 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1065     |
|    learning_rate   | 0.00228  |
|    step_size       | 0.000428 |
---------------------------------
Ep done - 888000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.45     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44558    |
|    total_timesteps | 21591418 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1066     |
|    learning_rate   | 0.00228  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 889000.
Eval num_timesteps=21600000, episode_reward=0.91 +/- 0.41
Episode length: 30.31 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 21600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 418
Ep done - 890000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.874    |
|    return_std      | 1.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44637    |
|    total_timesteps | 21627674 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1067     |
|    learning_rate   | 0.00227  |
|    step_size       | 0.000299 |
---------------------------------
Ep done - 891000.
Eval num_timesteps=21650000, episode_reward=0.95 +/- 0.29
Episode length: 30.30 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.955    |
| time/              |          |
|    total_timesteps | 21650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.955
SELFPLAY: new best model, bumping up generation to 419
Ep done - 892000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44716    |
|    total_timesteps | 21663969 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1068     |
|    learning_rate   | 0.00227  |
|    step_size       | 0.000156 |
---------------------------------
Ep done - 893000.
Eval num_timesteps=21700000, episode_reward=0.92 +/- 0.38
Episode length: 30.30 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 21700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 420
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.875    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44798    |
|    total_timesteps | 21700274 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1069     |
|    learning_rate   | 0.00227  |
|    step_size       | 0.000201 |
---------------------------------
Ep done - 894000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.835    |
|    return_std      | 2.52     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44857    |
|    total_timesteps | 21736532 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1070     |
|    learning_rate   | 0.00226  |
|    step_size       | 0.00015  |
---------------------------------
Ep done - 895000.
Eval num_timesteps=21750000, episode_reward=0.92 +/- 0.39
Episode length: 30.25 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 21750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 421
Ep done - 896000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.869    |
|    return_std      | 2.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 44936    |
|    total_timesteps | 21772779 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1071     |
|    learning_rate   | 0.00226  |
|    step_size       | 0.000161 |
---------------------------------
Ep done - 897000.
Eval num_timesteps=21800000, episode_reward=0.89 +/- 0.44
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 21800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 422
Ep done - 898000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 2.41     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45018    |
|    total_timesteps | 21809024 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1072     |
|    learning_rate   | 0.00226  |
|    step_size       | 0.000156 |
---------------------------------
Ep done - 899000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.831    |
|    return_std      | 2.87     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45077    |
|    total_timesteps | 21845267 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1073     |
|    learning_rate   | 0.00226  |
|    step_size       | 0.000131 |
---------------------------------
Eval num_timesteps=21850000, episode_reward=0.91 +/- 0.40
Episode length: 30.32 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 21850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 423
Ep done - 900000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.841    |
|    return_std      | 3.36     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45156    |
|    total_timesteps | 21881531 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1074     |
|    learning_rate   | 0.00225  |
|    step_size       | 0.000112 |
---------------------------------
Ep done - 901000.
Eval num_timesteps=21900000, episode_reward=0.94 +/- 0.35
Episode length: 30.31 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 21900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 424
Ep done - 902000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.889    |
|    return_std      | 0.996    |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45238    |
|    total_timesteps | 21917847 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1075     |
|    learning_rate   | 0.00225  |
|    step_size       | 0.000376 |
---------------------------------
Ep done - 903000.
Eval num_timesteps=21950000, episode_reward=0.91 +/- 0.40
Episode length: 30.23 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 21950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 425
Ep done - 904000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 2.14     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45317    |
|    total_timesteps | 21954112 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1076     |
|    learning_rate   | 0.00225  |
|    step_size       | 0.000175 |
---------------------------------
Ep done - 905000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 3.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45376    |
|    total_timesteps | 21990361 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1077     |
|    learning_rate   | 0.00224  |
|    step_size       | 0.000112 |
---------------------------------
Eval num_timesteps=22000000, episode_reward=0.94 +/- 0.32
Episode length: 30.29 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.943    |
| time/              |          |
|    total_timesteps | 22000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9425
SELFPLAY: new best model, bumping up generation to 426
Ep done - 906000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.41     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45458    |
|    total_timesteps | 22026645 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1078     |
|    learning_rate   | 0.00224  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 907000.
Eval num_timesteps=22050000, episode_reward=0.91 +/- 0.39
Episode length: 30.32 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 22050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 427
Ep done - 908000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.888    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45537    |
|    total_timesteps | 22062925 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1079     |
|    learning_rate   | 0.00224  |
|    step_size       | 0.000164 |
---------------------------------
Ep done - 909000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 1.86     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45596    |
|    total_timesteps | 22099123 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1080     |
|    learning_rate   | 0.00223  |
|    step_size       | 0.0002   |
---------------------------------
Ep done - 910000.
Eval num_timesteps=22100000, episode_reward=0.92 +/- 0.39
Episode length: 30.28 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 22100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 428
Ep done - 911000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.855    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45678    |
|    total_timesteps | 22135426 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1081     |
|    learning_rate   | 0.00223  |
|    step_size       | 0.000198 |
---------------------------------
Ep done - 912000.
Eval num_timesteps=22150000, episode_reward=0.93 +/- 0.36
Episode length: 30.29 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 22150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 429
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.861    |
|    return_std      | 1.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45759    |
|    total_timesteps | 22171719 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1082     |
|    learning_rate   | 0.00223  |
|    step_size       | 0.000293 |
---------------------------------
Ep done - 913000.
Ep done - 914000.
Eval num_timesteps=22200000, episode_reward=0.92 +/- 0.38
Episode length: 30.29 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 22200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 430
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.841    |
|    return_std      | 2.89     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45838    |
|    total_timesteps | 22207974 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1083     |
|    learning_rate   | 0.00223  |
|    step_size       | 0.000129 |
---------------------------------
Ep done - 915000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.814    |
|    return_std      | 2.64     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45900    |
|    total_timesteps | 22244183 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1084     |
|    learning_rate   | 0.00222  |
|    step_size       | 0.00014  |
---------------------------------
Ep done - 916000.
Eval num_timesteps=22250000, episode_reward=0.93 +/- 0.36
Episode length: 30.31 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 22250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 431
Ep done - 917000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.875    |
|    return_std      | 1.81     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 45979    |
|    total_timesteps | 22280455 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1085     |
|    learning_rate   | 0.00222  |
|    step_size       | 0.000205 |
---------------------------------
Ep done - 918000.
Eval num_timesteps=22300000, episode_reward=0.91 +/- 0.39
Episode length: 30.31 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 22300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 432
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.856    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46058    |
|    total_timesteps | 22316731 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1086     |
|    learning_rate   | 0.00222  |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 919000.
Ep done - 920000.
Eval num_timesteps=22350000, episode_reward=0.88 +/- 0.45
Episode length: 30.26 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 22350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 433
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 1.76     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46140    |
|    total_timesteps | 22352994 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1087     |
|    learning_rate   | 0.00221  |
|    step_size       | 0.000209 |
---------------------------------
Ep done - 921000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.841    |
|    return_std      | 1.95     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46199    |
|    total_timesteps | 22389290 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1088     |
|    learning_rate   | 0.00221  |
|    step_size       | 0.000189 |
---------------------------------
Ep done - 922000.
Eval num_timesteps=22400000, episode_reward=0.89 +/- 0.46
Episode length: 30.31 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 22400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 434
Ep done - 923000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.841    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46280    |
|    total_timesteps | 22425565 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1089     |
|    learning_rate   | 0.00221  |
|    step_size       | 0.000183 |
---------------------------------
Ep done - 924000.
Eval num_timesteps=22450000, episode_reward=0.93 +/- 0.34
Episode length: 30.31 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 22450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 435
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 1.38     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46359    |
|    total_timesteps | 22461810 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1090     |
|    learning_rate   | 0.00221  |
|    step_size       | 0.000267 |
---------------------------------
Ep done - 925000.
Ep done - 926000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 1.96     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46419    |
|    total_timesteps | 22498098 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1091     |
|    learning_rate   | 0.0022   |
|    step_size       | 0.000187 |
---------------------------------
Eval num_timesteps=22500000, episode_reward=0.93 +/- 0.35
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 22500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 436
Ep done - 927000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.841    |
|    return_std      | 2.05     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46501    |
|    total_timesteps | 22534389 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1092     |
|    learning_rate   | 0.0022   |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 928000.
Eval num_timesteps=22550000, episode_reward=0.91 +/- 0.41
Episode length: 30.31 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 22550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 437
Ep done - 929000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.859    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46580    |
|    total_timesteps | 22570679 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1093     |
|    learning_rate   | 0.0022   |
|    step_size       | 0.000182 |
---------------------------------
Ep done - 930000.
Eval num_timesteps=22600000, episode_reward=0.91 +/- 0.41
Episode length: 30.20 +/- 1.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 22600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 438
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.844    |
|    return_std      | 2.52     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46659    |
|    total_timesteps | 22606967 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1094     |
|    learning_rate   | 0.00219  |
|    step_size       | 0.000145 |
---------------------------------
Ep done - 931000.
Ep done - 932000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.819    |
|    return_std      | 3.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46721    |
|    total_timesteps | 22643196 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1095     |
|    learning_rate   | 0.00219  |
|    step_size       | 0.000101 |
---------------------------------
Eval num_timesteps=22650000, episode_reward=0.91 +/- 0.41
Episode length: 30.23 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 22650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 439
Ep done - 933000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.823    |
|    return_std      | 3.1      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46800    |
|    total_timesteps | 22679489 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1096     |
|    learning_rate   | 0.00219  |
|    step_size       | 0.000118 |
---------------------------------
Ep done - 934000.
Eval num_timesteps=22700000, episode_reward=0.96 +/- 0.23
Episode length: 30.28 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.965    |
| time/              |          |
|    total_timesteps | 22700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.965
SELFPLAY: new best model, bumping up generation to 440
Ep done - 935000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 1.99     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46879    |
|    total_timesteps | 22715748 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1097     |
|    learning_rate   | 0.00219  |
|    step_size       | 0.000183 |
---------------------------------
Ep done - 936000.
Eval num_timesteps=22750000, episode_reward=0.93 +/- 0.36
Episode length: 30.29 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 22750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 441
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.5      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 46961    |
|    total_timesteps | 22751999 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1098     |
|    learning_rate   | 0.00218  |
|    step_size       | 0.000145 |
---------------------------------
Ep done - 937000.
Ep done - 938000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 2.5      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47020    |
|    total_timesteps | 22788257 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1099     |
|    learning_rate   | 0.00218  |
|    step_size       | 0.000145 |
---------------------------------
Eval num_timesteps=22800000, episode_reward=0.92 +/- 0.39
Episode length: 30.27 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 22800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 442
Ep done - 939000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 1.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47099    |
|    total_timesteps | 22824551 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1100     |
|    learning_rate   | 0.00218  |
|    step_size       | 0.000199 |
---------------------------------
Ep done - 940000.
Eval num_timesteps=22850000, episode_reward=0.93 +/- 0.37
Episode length: 30.25 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 22850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 443
Ep done - 941000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47181    |
|    total_timesteps | 22860813 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1101     |
|    learning_rate   | 0.00217  |
|    step_size       | 0.000128 |
---------------------------------
Ep done - 942000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.847    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47240    |
|    total_timesteps | 22897046 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1102     |
|    learning_rate   | 0.00217  |
|    step_size       | 0.000179 |
---------------------------------
Eval num_timesteps=22900000, episode_reward=0.93 +/- 0.34
Episode length: 30.28 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 22900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 444
Ep done - 943000.
Ep done - 944000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.875    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47319    |
|    total_timesteps | 22933357 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1103     |
|    learning_rate   | 0.00217  |
|    step_size       | 0.000151 |
---------------------------------
Eval num_timesteps=22950000, episode_reward=0.94 +/- 0.34
Episode length: 30.32 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 22950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 445
Ep done - 945000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.867    |
|    return_std      | 1.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47400    |
|    total_timesteps | 22969632 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1104     |
|    learning_rate   | 0.00217  |
|    step_size       | 0.000211 |
---------------------------------
Ep done - 946000.
Ep done - 947000.
Eval num_timesteps=23000000, episode_reward=0.94 +/- 0.34
Episode length: 30.31 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 23000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 446
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47479    |
|    total_timesteps | 23005937 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1105     |
|    learning_rate   | 0.00216  |
|    step_size       | 0.000168 |
---------------------------------
Ep done - 948000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 2.7      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47539    |
|    total_timesteps | 23042169 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1106     |
|    learning_rate   | 0.00216  |
|    step_size       | 0.000133 |
---------------------------------
Ep done - 949000.
Eval num_timesteps=23050000, episode_reward=0.92 +/- 0.38
Episode length: 30.26 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 23050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 447
Ep done - 950000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.894    |
|    return_std      | 1.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47620    |
|    total_timesteps | 23078465 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1107     |
|    learning_rate   | 0.00216  |
|    step_size       | 0.000284 |
---------------------------------
Ep done - 951000.
Eval num_timesteps=23100000, episode_reward=0.93 +/- 0.35
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 23100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 448
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.847    |
|    return_std      | 2.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47699    |
|    total_timesteps | 23114756 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1108     |
|    learning_rate   | 0.00215  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 952000.
Ep done - 953000.
Eval num_timesteps=23150000, episode_reward=0.91 +/- 0.41
Episode length: 30.27 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 23150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 449
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 3.13     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47781    |
|    total_timesteps | 23151058 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1109     |
|    learning_rate   | 0.00215  |
|    step_size       | 0.000115 |
---------------------------------
Ep done - 954000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.84     |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47840    |
|    total_timesteps | 23187299 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1110     |
|    learning_rate   | 0.00215  |
|    step_size       | 0.000201 |
---------------------------------
Ep done - 955000.
Eval num_timesteps=23200000, episode_reward=0.93 +/- 0.35
Episode length: 30.29 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 23200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 450
Ep done - 956000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.85     |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47921    |
|    total_timesteps | 23223581 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1111     |
|    learning_rate   | 0.00215  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 957000.
Eval num_timesteps=23250000, episode_reward=0.90 +/- 0.43
Episode length: 30.30 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 23250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 451
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.09     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 47999    |
|    total_timesteps | 23259853 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1112     |
|    learning_rate   | 0.00214  |
|    step_size       | 0.00017  |
---------------------------------
Ep done - 958000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.872    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48061    |
|    total_timesteps | 23296082 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1113     |
|    learning_rate   | 0.00214  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 959000.
Eval num_timesteps=23300000, episode_reward=0.92 +/- 0.39
Episode length: 30.25 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 23300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 452
Ep done - 960000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.827    |
|    return_std      | 2.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48140    |
|    total_timesteps | 23332327 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1114     |
|    learning_rate   | 0.00214  |
|    step_size       | 0.000136 |
---------------------------------
Ep done - 961000.
Eval num_timesteps=23350000, episode_reward=0.95 +/- 0.30
Episode length: 30.34 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.95     |
| time/              |          |
|    total_timesteps | 23350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.95
SELFPLAY: new best model, bumping up generation to 453
Ep done - 962000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48222    |
|    total_timesteps | 23368567 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1115     |
|    learning_rate   | 0.00213  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 963000.
Eval num_timesteps=23400000, episode_reward=0.94 +/- 0.33
Episode length: 30.26 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 23400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.94
SELFPLAY: new best model, bumping up generation to 454
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.861    |
|    return_std      | 2.04     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48302    |
|    total_timesteps | 23404825 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1116     |
|    learning_rate   | 0.00213  |
|    step_size       | 0.000174 |
---------------------------------
Ep done - 964000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.84     |
|    return_std      | 2.75     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48361    |
|    total_timesteps | 23441063 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1117     |
|    learning_rate   | 0.00213  |
|    step_size       | 0.000129 |
---------------------------------
Ep done - 965000.
Eval num_timesteps=23450000, episode_reward=0.93 +/- 0.35
Episode length: 30.30 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 23450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 455
Ep done - 966000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 1.68     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48443    |
|    total_timesteps | 23477353 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1118     |
|    learning_rate   | 0.00212  |
|    step_size       | 0.000211 |
---------------------------------
Ep done - 967000.
Eval num_timesteps=23500000, episode_reward=0.93 +/- 0.36
Episode length: 30.31 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 23500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 456
Ep done - 968000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48522    |
|    total_timesteps | 23513577 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1119     |
|    learning_rate   | 0.00212  |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 969000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48581    |
|    total_timesteps | 23549771 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1120     |
|    learning_rate   | 0.00212  |
|    step_size       | 0.000167 |
---------------------------------
Eval num_timesteps=23550000, episode_reward=0.92 +/- 0.38
Episode length: 30.25 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 23550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 457
Ep done - 970000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.818    |
|    return_std      | 3.03     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48663    |
|    total_timesteps | 23586013 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1121     |
|    learning_rate   | 0.00212  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 971000.
Eval num_timesteps=23600000, episode_reward=0.94 +/- 0.33
Episode length: 30.32 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 23600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 458
Ep done - 972000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48743    |
|    total_timesteps | 23622293 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1122     |
|    learning_rate   | 0.00211  |
|    step_size       | 0.000225 |
---------------------------------
Ep done - 973000.
Eval num_timesteps=23650000, episode_reward=0.94 +/- 0.35
Episode length: 30.26 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 23650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 459
Ep done - 974000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.867    |
|    return_std      | 1.66     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48822    |
|    total_timesteps | 23658600 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1123     |
|    learning_rate   | 0.00211  |
|    step_size       | 0.000212 |
---------------------------------
Ep done - 975000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.873    |
|    return_std      | 2.49     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48884    |
|    total_timesteps | 23694831 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1124     |
|    learning_rate   | 0.00211  |
|    step_size       | 0.000141 |
---------------------------------
Eval num_timesteps=23700000, episode_reward=0.91 +/- 0.39
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 23700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 460
Ep done - 976000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 2.05     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 48964    |
|    total_timesteps | 23731108 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1125     |
|    learning_rate   | 0.0021   |
|    step_size       | 0.000171 |
---------------------------------
Ep done - 977000.
Eval num_timesteps=23750000, episode_reward=0.92 +/- 0.38
Episode length: 30.29 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 23750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 461
Ep done - 978000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.829    |
|    return_std      | 1.82     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49043    |
|    total_timesteps | 23767311 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1126     |
|    learning_rate   | 0.0021   |
|    step_size       | 0.000193 |
---------------------------------
Ep done - 979000.
Eval num_timesteps=23800000, episode_reward=0.95 +/- 0.30
Episode length: 30.29 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.948    |
| time/              |          |
|    total_timesteps | 23800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9475
SELFPLAY: new best model, bumping up generation to 462
Ep done - 980000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.841    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49125    |
|    total_timesteps | 23803582 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1127     |
|    learning_rate   | 0.0021   |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 981000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.821    |
|    return_std      | 2.84     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49184    |
|    total_timesteps | 23839759 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1128     |
|    learning_rate   | 0.0021   |
|    step_size       | 0.000123 |
---------------------------------
Eval num_timesteps=23850000, episode_reward=0.92 +/- 0.40
Episode length: 30.27 +/- 1.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 23850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 463
Ep done - 982000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.861    |
|    return_std      | 2.47     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49264    |
|    total_timesteps | 23876027 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1129     |
|    learning_rate   | 0.00209  |
|    step_size       | 0.000141 |
---------------------------------
Ep done - 983000.
Eval num_timesteps=23900000, episode_reward=0.92 +/- 0.37
Episode length: 30.24 +/- 1.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 23900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 464
Ep done - 984000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.857    |
|    return_std      | 2.37     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49345    |
|    total_timesteps | 23912282 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1130     |
|    learning_rate   | 0.00209  |
|    step_size       | 0.000147 |
---------------------------------
Ep done - 985000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.841    |
|    return_std      | 1.53     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49404    |
|    total_timesteps | 23948505 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1131     |
|    learning_rate   | 0.00209  |
|    step_size       | 0.000228 |
---------------------------------
Ep done - 986000.
Eval num_timesteps=23950000, episode_reward=0.94 +/- 0.31
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.943    |
| time/              |          |
|    total_timesteps | 23950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9425
SELFPLAY: new best model, bumping up generation to 465
Ep done - 987000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 1.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49483    |
|    total_timesteps | 23984792 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1132     |
|    learning_rate   | 0.00208  |
|    step_size       | 0.000241 |
---------------------------------
Ep done - 988000.
Eval num_timesteps=24000000, episode_reward=0.92 +/- 0.39
Episode length: 30.29 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 24000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 466
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.875    |
|    return_std      | 1.35     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49565    |
|    total_timesteps | 24021057 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1133     |
|    learning_rate   | 0.00208  |
|    step_size       | 0.000257 |
---------------------------------
Ep done - 989000.
Ep done - 990000.
Eval num_timesteps=24050000, episode_reward=0.92 +/- 0.38
Episode length: 30.27 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 24050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 467
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.83     |
|    return_std      | 2.05     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49644    |
|    total_timesteps | 24057314 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1134     |
|    learning_rate   | 0.00208  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 991000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.856    |
|    return_std      | 2.32     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49706    |
|    total_timesteps | 24093574 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1135     |
|    learning_rate   | 0.00208  |
|    step_size       | 0.000149 |
---------------------------------
Ep done - 992000.
Eval num_timesteps=24100000, episode_reward=0.88 +/- 0.47
Episode length: 30.29 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.88     |
| time/              |          |
|    total_timesteps | 24100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.88
SELFPLAY: new best model, bumping up generation to 468
Ep done - 993000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.846    |
|    return_std      | 3.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49785    |
|    total_timesteps | 24129871 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1136     |
|    learning_rate   | 0.00207  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 994000.
Eval num_timesteps=24150000, episode_reward=0.92 +/- 0.39
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 24150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 469
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.827    |
|    return_std      | 1.61     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49864    |
|    total_timesteps | 24166139 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1137     |
|    learning_rate   | 0.00207  |
|    step_size       | 0.000214 |
---------------------------------
Ep done - 995000.
Ep done - 996000.
Eval num_timesteps=24200000, episode_reward=0.93 +/- 0.37
Episode length: 30.36 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 24200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 470
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 49946    |
|    total_timesteps | 24202417 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1138     |
|    learning_rate   | 0.00207  |
|    step_size       | 0.000122 |
---------------------------------
Ep done - 997000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50006    |
|    total_timesteps | 24238672 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1139     |
|    learning_rate   | 0.00206  |
|    step_size       | 0.000152 |
---------------------------------
Ep done - 998000.
Eval num_timesteps=24250000, episode_reward=0.90 +/- 0.42
Episode length: 30.30 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 24250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 471
Ep done - 999000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 2.53     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50086    |
|    total_timesteps | 24274948 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1140     |
|    learning_rate   | 0.00206  |
|    step_size       | 0.000136 |
---------------------------------
Ep done - 1000000.
Eval num_timesteps=24300000, episode_reward=0.92 +/- 0.38
Episode length: 30.23 +/- 1.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 24300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 472
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 1.04     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50168    |
|    total_timesteps | 24311192 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1141     |
|    learning_rate   | 0.00206  |
|    step_size       | 0.000328 |
---------------------------------
Ep done - 1001000.
Ep done - 1002000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.85     |
|    return_std      | 3.12     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50227    |
|    total_timesteps | 24347399 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1142     |
|    learning_rate   | 0.00206  |
|    step_size       | 0.00011  |
---------------------------------
Eval num_timesteps=24350000, episode_reward=0.87 +/- 0.48
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.868    |
| time/              |          |
|    total_timesteps | 24350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8675
SELFPLAY: new best model, bumping up generation to 473
Ep done - 1003000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.847    |
|    return_std      | 3.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50306    |
|    total_timesteps | 24383651 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1143     |
|    learning_rate   | 0.00205  |
|    step_size       | 0.000102 |
---------------------------------
Ep done - 1004000.
Eval num_timesteps=24400000, episode_reward=0.92 +/- 0.36
Episode length: 30.26 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 24400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 474
Ep done - 1005000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.837    |
|    return_std      | 3.28     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50388    |
|    total_timesteps | 24419915 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1144     |
|    learning_rate   | 0.00205  |
|    step_size       | 0.000104 |
---------------------------------
Ep done - 1006000.
Eval num_timesteps=24450000, episode_reward=0.92 +/- 0.38
Episode length: 30.25 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 24450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 475
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.86     |
|    return_std      | 1.08     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50467    |
|    total_timesteps | 24456207 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1145     |
|    learning_rate   | 0.00205  |
|    step_size       | 0.000315 |
---------------------------------
Ep done - 1007000.
Ep done - 1008000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 2.01     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50526    |
|    total_timesteps | 24492472 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1146     |
|    learning_rate   | 0.00204  |
|    step_size       | 0.00017  |
---------------------------------
Eval num_timesteps=24500000, episode_reward=0.89 +/- 0.45
Episode length: 30.26 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 24500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 476
Ep done - 1009000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 2.6      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50608    |
|    total_timesteps | 24528744 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1147     |
|    learning_rate   | 0.00204  |
|    step_size       | 0.000131 |
---------------------------------
Ep done - 1010000.
Eval num_timesteps=24550000, episode_reward=0.88 +/- 0.47
Episode length: 30.29 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.877    |
| time/              |          |
|    total_timesteps | 24550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8775
SELFPLAY: new best model, bumping up generation to 477
Ep done - 1011000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.805    |
|    return_std      | 2.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50687    |
|    total_timesteps | 24564946 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1148     |
|    learning_rate   | 0.00204  |
|    step_size       | 0.000131 |
---------------------------------
Ep done - 1012000.
Eval num_timesteps=24600000, episode_reward=0.94 +/- 0.33
Episode length: 30.32 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 24600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.94
SELFPLAY: new best model, bumping up generation to 478
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.872    |
|    return_std      | 2.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50766    |
|    total_timesteps | 24601209 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1149     |
|    learning_rate   | 0.00203  |
|    step_size       | 0.000129 |
---------------------------------
Ep done - 1013000.
Ep done - 1014000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.831    |
|    return_std      | 1.6      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50828    |
|    total_timesteps | 24637433 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1150     |
|    learning_rate   | 0.00203  |
|    step_size       | 0.000211 |
---------------------------------
Eval num_timesteps=24650000, episode_reward=0.93 +/- 0.37
Episode length: 30.28 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 24650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 479
Ep done - 1015000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50907    |
|    total_timesteps | 24673714 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1151     |
|    learning_rate   | 0.00203  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 1016000.
Eval num_timesteps=24700000, episode_reward=0.92 +/- 0.39
Episode length: 30.22 +/- 1.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 24700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 480
Ep done - 1017000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.839    |
|    return_std      | 2.37     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 50986    |
|    total_timesteps | 24709962 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1152     |
|    learning_rate   | 0.00203  |
|    step_size       | 0.000143 |
---------------------------------
Ep done - 1018000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.844    |
|    return_std      | 1.53     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51048    |
|    total_timesteps | 24746178 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1153     |
|    learning_rate   | 0.00202  |
|    step_size       | 0.000221 |
---------------------------------
Eval num_timesteps=24750000, episode_reward=0.92 +/- 0.40
Episode length: 30.26 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 24750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 481
Ep done - 1019000.
Ep done - 1020000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.854    |
|    return_std      | 3.6      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51127    |
|    total_timesteps | 24782472 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1154     |
|    learning_rate   | 0.00202  |
|    step_size       | 9.35e-05 |
---------------------------------
Eval num_timesteps=24800000, episode_reward=0.94 +/- 0.33
Episode length: 30.20 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 24800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.94
SELFPLAY: new best model, bumping up generation to 482
Ep done - 1021000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 2.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51206    |
|    total_timesteps | 24818786 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1155     |
|    learning_rate   | 0.00202  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 1022000.
Ep done - 1023000.
Eval num_timesteps=24850000, episode_reward=0.91 +/- 0.40
Episode length: 30.28 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 24850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 483
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.852    |
|    return_std      | 1.86     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51288    |
|    total_timesteps | 24855052 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1156     |
|    learning_rate   | 0.00201  |
|    step_size       | 0.00018  |
---------------------------------
Ep done - 1024000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.859    |
|    return_std      | 2.19     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51347    |
|    total_timesteps | 24891293 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1157     |
|    learning_rate   | 0.00201  |
|    step_size       | 0.000153 |
---------------------------------
Ep done - 1025000.
Eval num_timesteps=24900000, episode_reward=0.95 +/- 0.32
Episode length: 30.23 +/- 1.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.948    |
| time/              |          |
|    total_timesteps | 24900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9475
SELFPLAY: new best model, bumping up generation to 484
Ep done - 1026000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51427    |
|    total_timesteps | 24927593 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1158     |
|    learning_rate   | 0.00201  |
|    step_size       | 0.000138 |
---------------------------------
Ep done - 1027000.
Eval num_timesteps=24950000, episode_reward=0.93 +/- 0.34
Episode length: 30.29 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 24950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 485
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.837    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51508    |
|    total_timesteps | 24963870 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1159     |
|    learning_rate   | 0.00201  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 1028000.
Ep done - 1029000.
Eval num_timesteps=25000000, episode_reward=0.91 +/- 0.40
Episode length: 30.28 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 25000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 486
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.864    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51587    |
|    total_timesteps | 25000152 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1160     |
|    learning_rate   | 0.002    |
|    step_size       | 0.000174 |
---------------------------------
Ep done - 1030000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.855    |
|    return_std      | 2.76     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51649    |
|    total_timesteps | 25036415 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1161     |
|    learning_rate   | 0.002    |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 1031000.
Eval num_timesteps=25050000, episode_reward=0.89 +/- 0.44
Episode length: 30.26 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 25050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 487
Ep done - 1032000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.861    |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51729    |
|    total_timesteps | 25072604 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1162     |
|    learning_rate   | 0.002    |
|    step_size       | 0.000161 |
---------------------------------
Ep done - 1033000.
Eval num_timesteps=25100000, episode_reward=0.90 +/- 0.42
Episode length: 30.22 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 25100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 488
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.864    |
|    return_std      | 2.19     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51808    |
|    total_timesteps | 25108881 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1163     |
|    learning_rate   | 0.00199  |
|    step_size       | 0.000152 |
---------------------------------
Ep done - 1034000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.861    |
|    return_std      | 1.08     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51870    |
|    total_timesteps | 25145120 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1164     |
|    learning_rate   | 0.00199  |
|    step_size       | 0.000306 |
---------------------------------
Ep done - 1035000.
Eval num_timesteps=25150000, episode_reward=0.94 +/- 0.34
Episode length: 30.32 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 25150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 489
Ep done - 1036000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 51948    |
|    total_timesteps | 25181422 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1165     |
|    learning_rate   | 0.00199  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 1037000.
Eval num_timesteps=25200000, episode_reward=0.92 +/- 0.37
Episode length: 30.27 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 25200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 490
Ep done - 1038000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.861    |
|    return_std      | 2.47     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52027    |
|    total_timesteps | 25217710 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1166     |
|    learning_rate   | 0.00199  |
|    step_size       | 0.000134 |
---------------------------------
Ep done - 1039000.
Eval num_timesteps=25250000, episode_reward=0.92 +/- 0.37
Episode length: 30.23 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 25250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 491
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.837    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52110    |
|    total_timesteps | 25253969 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1167     |
|    learning_rate   | 0.00198  |
|    step_size       | 0.000164 |
---------------------------------
Ep done - 1040000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.51     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52170    |
|    total_timesteps | 25290241 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1168     |
|    learning_rate   | 0.00198  |
|    step_size       | 0.000219 |
---------------------------------
Ep done - 1041000.
Eval num_timesteps=25300000, episode_reward=0.89 +/- 0.44
Episode length: 30.28 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 25300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 492
Ep done - 1042000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 1.91     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52250    |
|    total_timesteps | 25326512 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1169     |
|    learning_rate   | 0.00198  |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 1043000.
Eval num_timesteps=25350000, episode_reward=0.92 +/- 0.38
Episode length: 30.29 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 25350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 493
Ep done - 1044000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.885    |
|    return_std      | 2.23     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52332    |
|    total_timesteps | 25362791 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1170     |
|    learning_rate   | 0.00197  |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 1045000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.869    |
|    return_std      | 1.6      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52392    |
|    total_timesteps | 25399084 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1171     |
|    learning_rate   | 0.00197  |
|    step_size       | 0.000205 |
---------------------------------
Eval num_timesteps=25400000, episode_reward=0.93 +/- 0.35
Episode length: 30.21 +/- 1.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 25400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 494
Ep done - 1046000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 2.64     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52471    |
|    total_timesteps | 25435349 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1172     |
|    learning_rate   | 0.00197  |
|    step_size       | 0.000124 |
---------------------------------
Ep done - 1047000.
Eval num_timesteps=25450000, episode_reward=0.91 +/- 0.42
Episode length: 30.21 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 25450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 495
Ep done - 1048000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.839    |
|    return_std      | 2.6      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52553    |
|    total_timesteps | 25471599 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1173     |
|    learning_rate   | 0.00197  |
|    step_size       | 0.000126 |
---------------------------------
Ep done - 1049000.
Eval num_timesteps=25500000, episode_reward=0.93 +/- 0.38
Episode length: 30.27 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 25500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 496
Ep done - 1050000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.881    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52632    |
|    total_timesteps | 25507842 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1174     |
|    learning_rate   | 0.00196  |
|    step_size       | 0.000184 |
---------------------------------
Ep done - 1051000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.827    |
|    return_std      | 2.33     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52691    |
|    total_timesteps | 25544101 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1175     |
|    learning_rate   | 0.00196  |
|    step_size       | 0.00014  |
---------------------------------
Eval num_timesteps=25550000, episode_reward=0.90 +/- 0.43
Episode length: 30.24 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 25550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 497
Ep done - 1052000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.872    |
|    return_std      | 2.86     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52773    |
|    total_timesteps | 25580381 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1176     |
|    learning_rate   | 0.00196  |
|    step_size       | 0.000114 |
---------------------------------
Ep done - 1053000.
Eval num_timesteps=25600000, episode_reward=0.94 +/- 0.33
Episode length: 30.31 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 25600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.94
SELFPLAY: new best model, bumping up generation to 498
Ep done - 1054000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.799    |
|    return_std      | 2.45     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52852    |
|    total_timesteps | 25616652 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1177     |
|    learning_rate   | 0.00195  |
|    step_size       | 0.000133 |
---------------------------------
Ep done - 1055000.
Eval num_timesteps=25650000, episode_reward=0.89 +/- 0.44
Episode length: 30.28 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 25650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 499
Ep done - 1056000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.846    |
|    return_std      | 2.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52931    |
|    total_timesteps | 25652901 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1178     |
|    learning_rate   | 0.00195  |
|    step_size       | 0.000133 |
---------------------------------
Ep done - 1057000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 52993    |
|    total_timesteps | 25689189 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1179     |
|    learning_rate   | 0.00195  |
|    step_size       | 0.00012  |
---------------------------------
Eval num_timesteps=25700000, episode_reward=0.89 +/- 0.44
Episode length: 30.30 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 25700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 500
Ep done - 1058000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53072    |
|    total_timesteps | 25725462 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1180     |
|    learning_rate   | 0.00194  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 1059000.
Ep done - 1060000.
Eval num_timesteps=25750000, episode_reward=0.91 +/- 0.40
Episode length: 30.23 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 25750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 501
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.846    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53152    |
|    total_timesteps | 25761727 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1181     |
|    learning_rate   | 0.00194  |
|    step_size       | 0.0002   |
---------------------------------
Ep done - 1061000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.836    |
|    return_std      | 2.29     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53214    |
|    total_timesteps | 25797990 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1182     |
|    learning_rate   | 0.00194  |
|    step_size       | 0.000141 |
---------------------------------
Ep done - 1062000.
Eval num_timesteps=25800000, episode_reward=0.93 +/- 0.35
Episode length: 30.26 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 25800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 502
Ep done - 1063000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.873    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53293    |
|    total_timesteps | 25834228 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1183     |
|    learning_rate   | 0.00194  |
|    step_size       | 0.000199 |
---------------------------------
Ep done - 1064000.
Eval num_timesteps=25850000, episode_reward=0.93 +/- 0.37
Episode length: 30.26 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 25850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 503
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.874    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53374    |
|    total_timesteps | 25870441 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1184     |
|    learning_rate   | 0.00193  |
|    step_size       | 0.00015  |
---------------------------------
Ep done - 1065000.
Ep done - 1066000.
Eval num_timesteps=25900000, episode_reward=0.92 +/- 0.39
Episode length: 30.28 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 25900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 504
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.859    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53453    |
|    total_timesteps | 25906705 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1185     |
|    learning_rate   | 0.00193  |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 1067000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 2.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53512    |
|    total_timesteps | 25942924 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1186     |
|    learning_rate   | 0.00193  |
|    step_size       | 0.000139 |
---------------------------------
Ep done - 1068000.
Eval num_timesteps=25950000, episode_reward=0.91 +/- 0.40
Episode length: 30.29 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 25950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 505
Ep done - 1069000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.861    |
|    return_std      | 2.04     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53594    |
|    total_timesteps | 25979183 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1187     |
|    learning_rate   | 0.00192  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 1070000.
Eval num_timesteps=26000000, episode_reward=0.90 +/- 0.42
Episode length: 30.26 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 26000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 506
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.856    |
|    return_std      | 2.49     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53674    |
|    total_timesteps | 26015443 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1188     |
|    learning_rate   | 0.00192  |
|    step_size       | 0.000129 |
---------------------------------
Ep done - 1071000.
Ep done - 1072000.
Eval num_timesteps=26050000, episode_reward=0.90 +/- 0.43
Episode length: 30.33 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 26050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 507
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.837    |
|    return_std      | 3.23     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53753    |
|    total_timesteps | 26051720 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1189     |
|    learning_rate   | 0.00192  |
|    step_size       | 9.89e-05 |
---------------------------------
Ep done - 1073000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53815    |
|    total_timesteps | 26087965 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1190     |
|    learning_rate   | 0.00192  |
|    step_size       | 0.000134 |
---------------------------------
Ep done - 1074000.
Eval num_timesteps=26100000, episode_reward=0.91 +/- 0.40
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 26100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 508
Ep done - 1075000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.827    |
|    return_std      | 2.35     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53894    |
|    total_timesteps | 26124222 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1191     |
|    learning_rate   | 0.00191  |
|    step_size       | 0.000136 |
---------------------------------
Ep done - 1076000.
Eval num_timesteps=26150000, episode_reward=0.90 +/- 0.42
Episode length: 30.27 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 26150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 509
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.96     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 53973    |
|    total_timesteps | 26160489 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1192     |
|    learning_rate   | 0.00191  |
|    step_size       | 0.000162 |
---------------------------------
Ep done - 1077000.
Ep done - 1078000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.837    |
|    return_std      | 1.53     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54035    |
|    total_timesteps | 26196746 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1193     |
|    learning_rate   | 0.00191  |
|    step_size       | 0.000208 |
---------------------------------
Eval num_timesteps=26200000, episode_reward=0.92 +/- 0.38
Episode length: 30.27 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 26200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 510
Ep done - 1079000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | 0.878    |
|    return_std      | 2.1      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54113    |
|    total_timesteps | 26233077 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1194     |
|    learning_rate   | 0.0019   |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 1080000.
Eval num_timesteps=26250000, episode_reward=0.92 +/- 0.38
Episode length: 30.26 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 26250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 511
Ep done - 1081000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 2.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54192    |
|    total_timesteps | 26269317 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1195     |
|    learning_rate   | 0.0019   |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 1082000.
Eval num_timesteps=26300000, episode_reward=0.91 +/- 0.40
Episode length: 30.25 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 26300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 512
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.867    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54274    |
|    total_timesteps | 26305565 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1196     |
|    learning_rate   | 0.0019   |
|    step_size       | 0.000139 |
---------------------------------
Ep done - 1083000.
Ep done - 1084000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 2.81     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54335    |
|    total_timesteps | 26341833 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1197     |
|    learning_rate   | 0.0019   |
|    step_size       | 0.000112 |
---------------------------------
Eval num_timesteps=26350000, episode_reward=0.94 +/- 0.35
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 26350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 513
Ep done - 1085000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.828    |
|    return_std      | 3.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54414    |
|    total_timesteps | 26378063 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1198     |
|    learning_rate   | 0.00189  |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 1086000.
Eval num_timesteps=26400000, episode_reward=0.93 +/- 0.36
Episode length: 30.20 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 26400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 514
Ep done - 1087000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 3.2      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54495    |
|    total_timesteps | 26414322 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1199     |
|    learning_rate   | 0.00189  |
|    step_size       | 9.83e-05 |
---------------------------------
Ep done - 1088000.
Eval num_timesteps=26450000, episode_reward=0.96 +/- 0.25
Episode length: 30.26 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.96     |
| time/              |          |
|    total_timesteps | 26450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.96
SELFPLAY: new best model, bumping up generation to 515
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 2.94     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54574    |
|    total_timesteps | 26450573 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1200     |
|    learning_rate   | 0.00189  |
|    step_size       | 0.000107 |
---------------------------------
Ep done - 1089000.
Ep done - 1090000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.5      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54634    |
|    total_timesteps | 26486852 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1201     |
|    learning_rate   | 0.00188  |
|    step_size       | 0.00021  |
---------------------------------
Eval num_timesteps=26500000, episode_reward=0.94 +/- 0.33
Episode length: 30.27 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 26500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 516
Ep done - 1091000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | 0.886    |
|    return_std      | 2.84     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54716    |
|    total_timesteps | 26523168 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1202     |
|    learning_rate   | 0.00188  |
|    step_size       | 0.00011  |
---------------------------------
Ep done - 1092000.
Eval num_timesteps=26550000, episode_reward=0.94 +/- 0.34
Episode length: 30.22 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 26550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 517
Ep done - 1093000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54795    |
|    total_timesteps | 26559412 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1203     |
|    learning_rate   | 0.00188  |
|    step_size       | 0.000239 |
---------------------------------
Ep done - 1094000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54855    |
|    total_timesteps | 26595686 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1204     |
|    learning_rate   | 0.00188  |
|    step_size       | 0.000138 |
---------------------------------
Eval num_timesteps=26600000, episode_reward=0.95 +/- 0.29
Episode length: 30.32 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.953    |
| time/              |          |
|    total_timesteps | 26600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9525
SELFPLAY: new best model, bumping up generation to 518
Ep done - 1095000.
Ep done - 1096000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.871    |
|    return_std      | 2.72     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 54937    |
|    total_timesteps | 26631955 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1205     |
|    learning_rate   | 0.00187  |
|    step_size       | 0.000115 |
---------------------------------
Eval num_timesteps=26650000, episode_reward=0.88 +/- 0.47
Episode length: 30.26 +/- 0.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.875    |
| time/              |          |
|    total_timesteps | 26650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.875
SELFPLAY: new best model, bumping up generation to 519
Ep done - 1097000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.842    |
|    return_std      | 1.4      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55016    |
|    total_timesteps | 26668210 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1206     |
|    learning_rate   | 0.00187  |
|    step_size       | 0.000222 |
---------------------------------
Ep done - 1098000.
Ep done - 1099000.
Eval num_timesteps=26700000, episode_reward=0.93 +/- 0.35
Episode length: 30.29 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 26700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 520
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55095    |
|    total_timesteps | 26704466 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1207     |
|    learning_rate   | 0.00187  |
|    step_size       | 0.000162 |
---------------------------------
Ep done - 1100000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55158    |
|    total_timesteps | 26740718 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1208     |
|    learning_rate   | 0.00186  |
|    step_size       | 0.000154 |
---------------------------------
Ep done - 1101000.
Eval num_timesteps=26750000, episode_reward=0.86 +/- 0.50
Episode length: 30.23 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.86     |
| time/              |          |
|    total_timesteps | 26750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.86
SELFPLAY: new best model, bumping up generation to 521
Ep done - 1102000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.88     |
|    return_std      | 1.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55237    |
|    total_timesteps | 26776992 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1209     |
|    learning_rate   | 0.00186  |
|    step_size       | 0.000245 |
---------------------------------
Ep done - 1103000.
Eval num_timesteps=26800000, episode_reward=0.94 +/- 0.32
Episode length: 30.23 +/- 1.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.943    |
| time/              |          |
|    total_timesteps | 26800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9425
SELFPLAY: new best model, bumping up generation to 522
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.83     |
|    return_std      | 3.06     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55318    |
|    total_timesteps | 26813286 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1210     |
|    learning_rate   | 0.00186  |
|    step_size       | 0.000101 |
---------------------------------
Ep done - 1104000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.842    |
|    return_std      | 2.75     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55378    |
|    total_timesteps | 26849581 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1211     |
|    learning_rate   | 0.00185  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 1105000.
Eval num_timesteps=26850000, episode_reward=0.92 +/- 0.39
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 26850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 523
Ep done - 1106000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 2.94     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55457    |
|    total_timesteps | 26885843 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1212     |
|    learning_rate   | 0.00185  |
|    step_size       | 0.000105 |
---------------------------------
Ep done - 1107000.
Eval num_timesteps=26900000, episode_reward=0.88 +/- 0.46
Episode length: 30.27 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.88     |
| time/              |          |
|    total_timesteps | 26900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.88
SELFPLAY: new best model, bumping up generation to 524
Ep done - 1108000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.859    |
|    return_std      | 2.77     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55539    |
|    total_timesteps | 26922146 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1213     |
|    learning_rate   | 0.00185  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 1109000.
Eval num_timesteps=26950000, episode_reward=0.93 +/- 0.36
Episode length: 30.33 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 26950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 525
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.877    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55618    |
|    total_timesteps | 26958432 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1214     |
|    learning_rate   | 0.00185  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 1110000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.867    |
|    return_std      | 2.23     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55677    |
|    total_timesteps | 26994724 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1215     |
|    learning_rate   | 0.00184  |
|    step_size       | 0.000138 |
---------------------------------
Ep done - 1111000.
Eval num_timesteps=27000000, episode_reward=0.91 +/- 0.39
Episode length: 30.27 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 27000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 526
Ep done - 1112000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 1.97     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55759    |
|    total_timesteps | 27030997 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1216     |
|    learning_rate   | 0.00184  |
|    step_size       | 0.000156 |
---------------------------------
Ep done - 1113000.
Eval num_timesteps=27050000, episode_reward=0.92 +/- 0.39
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 27050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 527
Ep done - 1114000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 1.82     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55838    |
|    total_timesteps | 27067181 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1217     |
|    learning_rate   | 0.00184  |
|    step_size       | 0.000169 |
---------------------------------
Ep done - 1115000.
Eval num_timesteps=27100000, episode_reward=0.91 +/- 0.40
Episode length: 30.25 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 27100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 528
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.877    |
|    return_std      | 1.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55917    |
|    total_timesteps | 27103439 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1218     |
|    learning_rate   | 0.00183  |
|    step_size       | 0.000233 |
---------------------------------
Ep done - 1116000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.813    |
|    return_std      | 3.18     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 55980    |
|    total_timesteps | 27139667 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1219     |
|    learning_rate   | 0.00183  |
|    step_size       | 9.61e-05 |
---------------------------------
Ep done - 1117000.
Eval num_timesteps=27150000, episode_reward=0.92 +/- 0.37
Episode length: 30.27 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 27150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 529
Ep done - 1118000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 2.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56058    |
|    total_timesteps | 27175949 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1220     |
|    learning_rate   | 0.00183  |
|    step_size       | 0.000125 |
---------------------------------
Ep done - 1119000.
Eval num_timesteps=27200000, episode_reward=0.92 +/- 0.39
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 27200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 530
Ep done - 1120000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56138    |
|    total_timesteps | 27212205 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1221     |
|    learning_rate   | 0.00183  |
|    step_size       | 0.000142 |
---------------------------------
Ep done - 1121000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.827    |
|    return_std      | 3.58     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56200    |
|    total_timesteps | 27248448 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1222     |
|    learning_rate   | 0.00182  |
|    step_size       | 8.49e-05 |
---------------------------------
Eval num_timesteps=27250000, episode_reward=0.90 +/- 0.43
Episode length: 30.25 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 27250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 531
Ep done - 1122000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.887    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56279    |
|    total_timesteps | 27284755 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1223     |
|    learning_rate   | 0.00182  |
|    step_size       | 0.000157 |
---------------------------------
Ep done - 1123000.
Eval num_timesteps=27300000, episode_reward=0.92 +/- 0.38
Episode length: 30.29 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 27300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 532
Ep done - 1124000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 0.985    |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56359    |
|    total_timesteps | 27320993 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1224     |
|    learning_rate   | 0.00182  |
|    step_size       | 0.000308 |
---------------------------------
Ep done - 1125000.
Eval num_timesteps=27350000, episode_reward=0.94 +/- 0.33
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 27350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 533
Ep done - 1126000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 1.61     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56440    |
|    total_timesteps | 27357286 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1225     |
|    learning_rate   | 0.00181  |
|    step_size       | 0.000187 |
---------------------------------
Ep done - 1127000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.42     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56501    |
|    total_timesteps | 27393525 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1226     |
|    learning_rate   | 0.00181  |
|    step_size       | 0.000212 |
---------------------------------
Eval num_timesteps=27400000, episode_reward=0.94 +/- 0.33
Episode length: 30.23 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 27400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.94
SELFPLAY: new best model, bumping up generation to 534
Ep done - 1128000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.52     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56580    |
|    total_timesteps | 27429796 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1227     |
|    learning_rate   | 0.00181  |
|    step_size       | 0.000119 |
---------------------------------
Ep done - 1129000.
Eval num_timesteps=27450000, episode_reward=0.93 +/- 0.36
Episode length: 30.25 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 27450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 535
Ep done - 1130000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.881    |
|    return_std      | 1.99     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56662    |
|    total_timesteps | 27466092 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1228     |
|    learning_rate   | 0.00181  |
|    step_size       | 0.000151 |
---------------------------------
Ep done - 1131000.
Eval num_timesteps=27500000, episode_reward=0.85 +/- 0.51
Episode length: 30.30 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.848    |
| time/              |          |
|    total_timesteps | 27500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8475
SELFPLAY: new best model, bumping up generation to 536
Ep done - 1132000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.86     |
|    return_std      | 2.3      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56741    |
|    total_timesteps | 27502362 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1229     |
|    learning_rate   | 0.0018   |
|    step_size       | 0.000131 |
---------------------------------
Ep done - 1133000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56800    |
|    total_timesteps | 27538525 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1230     |
|    learning_rate   | 0.0018   |
|    step_size       | 0.000126 |
---------------------------------
Eval num_timesteps=27550000, episode_reward=0.89 +/- 0.46
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 27550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.885
SELFPLAY: new best model, bumping up generation to 537
Ep done - 1134000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 1.68     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56882    |
|    total_timesteps | 27574782 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1231     |
|    learning_rate   | 0.0018   |
|    step_size       | 0.000179 |
---------------------------------
Ep done - 1135000.
Ep done - 1136000.
Eval num_timesteps=27600000, episode_reward=0.93 +/- 0.37
Episode length: 30.27 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 27600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 538
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.45     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 56961    |
|    total_timesteps | 27611051 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1232     |
|    learning_rate   | 0.00179  |
|    step_size       | 0.000122 |
---------------------------------
Ep done - 1137000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.879    |
|    return_std      | 2.52     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57020    |
|    total_timesteps | 27647299 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1233     |
|    learning_rate   | 0.00179  |
|    step_size       | 0.000119 |
---------------------------------
Ep done - 1138000.
Eval num_timesteps=27650000, episode_reward=0.89 +/- 0.46
Episode length: 30.32 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 27650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8875
SELFPLAY: new best model, bumping up generation to 539
Ep done - 1139000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.859    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57102    |
|    total_timesteps | 27683566 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1234     |
|    learning_rate   | 0.00179  |
|    step_size       | 0.000158 |
---------------------------------
Ep done - 1140000.
Eval num_timesteps=27700000, episode_reward=0.92 +/- 0.36
Episode length: 30.30 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 27700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 540
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57181    |
|    total_timesteps | 27719823 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1235     |
|    learning_rate   | 0.00179  |
|    step_size       | 0.000184 |
---------------------------------
Ep done - 1141000.
Ep done - 1142000.
Eval num_timesteps=27750000, episode_reward=0.96 +/- 0.27
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.96     |
| time/              |          |
|    total_timesteps | 27750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.96
SELFPLAY: new best model, bumping up generation to 541
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.882    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57262    |
|    total_timesteps | 27756103 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1236     |
|    learning_rate   | 0.00178  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 1143000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57322    |
|    total_timesteps | 27792360 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1237     |
|    learning_rate   | 0.00178  |
|    step_size       | 0.000154 |
---------------------------------
Ep done - 1144000.
Eval num_timesteps=27800000, episode_reward=0.91 +/- 0.39
Episode length: 30.24 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 27800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 542
Ep done - 1145000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.877    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57401    |
|    total_timesteps | 27828650 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1238     |
|    learning_rate   | 0.00178  |
|    step_size       | 0.000154 |
---------------------------------
Ep done - 1146000.
Eval num_timesteps=27850000, episode_reward=0.94 +/- 0.33
Episode length: 30.31 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.945    |
| time/              |          |
|    total_timesteps | 27850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.945
SELFPLAY: new best model, bumping up generation to 543
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57483    |
|    total_timesteps | 27864897 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1239     |
|    learning_rate   | 0.00177  |
|    step_size       | 0.000109 |
---------------------------------
Ep done - 1147000.
Ep done - 1148000.
Eval num_timesteps=27900000, episode_reward=0.90 +/- 0.42
Episode length: 30.28 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 27900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 544
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.882    |
|    return_std      | 1.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57562    |
|    total_timesteps | 27901212 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1240     |
|    learning_rate   | 0.00177  |
|    step_size       | 0.000204 |
---------------------------------
Ep done - 1149000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 2.9      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57622    |
|    total_timesteps | 27937451 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1241     |
|    learning_rate   | 0.00177  |
|    step_size       | 0.000102 |
---------------------------------
Ep done - 1150000.
Eval num_timesteps=27950000, episode_reward=0.91 +/- 0.43
Episode length: 30.34 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 27950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 545
Ep done - 1151000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 2.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57704    |
|    total_timesteps | 27973740 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1242     |
|    learning_rate   | 0.00177  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 1152000.
Eval num_timesteps=28000000, episode_reward=0.93 +/- 0.37
Episode length: 30.30 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 28000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 546
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57784    |
|    total_timesteps | 28010037 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1243     |
|    learning_rate   | 0.00176  |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 1153000.
Ep done - 1154000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.48     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57843    |
|    total_timesteps | 28046302 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1244     |
|    learning_rate   | 0.00176  |
|    step_size       | 0.000197 |
---------------------------------
Eval num_timesteps=28050000, episode_reward=0.93 +/- 0.36
Episode length: 30.28 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 28050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 547
Ep done - 1155000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.885    |
|    return_std      | 1.85     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 57925    |
|    total_timesteps | 28082560 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1245     |
|    learning_rate   | 0.00176  |
|    step_size       | 0.000158 |
---------------------------------
Ep done - 1156000.
Eval num_timesteps=28100000, episode_reward=0.93 +/- 0.38
Episode length: 30.32 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 28100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 548
Ep done - 1157000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 3.45     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58004    |
|    total_timesteps | 28118779 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1246     |
|    learning_rate   | 0.00175  |
|    step_size       | 8.48e-05 |
---------------------------------
Ep done - 1158000.
Eval num_timesteps=28150000, episode_reward=0.93 +/- 0.37
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 28150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 549
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.77     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58082    |
|    total_timesteps | 28155008 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1247     |
|    learning_rate   | 0.00175  |
|    step_size       | 0.000105 |
---------------------------------
Ep done - 1159000.
Ep done - 1160000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.847    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58144    |
|    total_timesteps | 28191214 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1248     |
|    learning_rate   | 0.00175  |
|    step_size       | 0.00018  |
---------------------------------
Eval num_timesteps=28200000, episode_reward=0.92 +/- 0.39
Episode length: 30.28 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 28200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 550
Ep done - 1161000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 1.96     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58223    |
|    total_timesteps | 28227503 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1249     |
|    learning_rate   | 0.00174  |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 1162000.
Eval num_timesteps=28250000, episode_reward=0.93 +/- 0.37
Episode length: 30.29 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 28250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 551
Ep done - 1163000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.871    |
|    return_std      | 2.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58302    |
|    total_timesteps | 28263770 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1250     |
|    learning_rate   | 0.00174  |
|    step_size       | 0.000138 |
---------------------------------
Ep done - 1164000.
Eval num_timesteps=28300000, episode_reward=0.93 +/- 0.37
Episode length: 30.33 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 28300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 552
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.879    |
|    return_std      | 2.17     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58383    |
|    total_timesteps | 28300030 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1251     |
|    learning_rate   | 0.00174  |
|    step_size       | 0.000134 |
---------------------------------
Ep done - 1165000.
Ep done - 1166000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.68     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58443    |
|    total_timesteps | 28336284 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1252     |
|    learning_rate   | 0.00174  |
|    step_size       | 0.000173 |
---------------------------------
Eval num_timesteps=28350000, episode_reward=0.89 +/- 0.44
Episode length: 30.32 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 28350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 553
Ep done - 1167000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.856    |
|    return_std      | 1.68     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58522    |
|    total_timesteps | 28372527 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1253     |
|    learning_rate   | 0.00173  |
|    step_size       | 0.000172 |
---------------------------------
Ep done - 1168000.
Eval num_timesteps=28400000, episode_reward=0.93 +/- 0.37
Episode length: 30.23 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 28400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 554
Ep done - 1169000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.41     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58603    |
|    total_timesteps | 28408777 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1254     |
|    learning_rate   | 0.00173  |
|    step_size       | 0.00012  |
---------------------------------
Ep done - 1170000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.846    |
|    return_std      | 2.21     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58664    |
|    total_timesteps | 28445014 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1255     |
|    learning_rate   | 0.00173  |
|    step_size       | 0.00013  |
---------------------------------
Eval num_timesteps=28450000, episode_reward=0.93 +/- 0.36
Episode length: 30.24 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 28450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 555
Ep done - 1171000.
Ep done - 1172000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.875    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58743    |
|    total_timesteps | 28481304 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1256     |
|    learning_rate   | 0.00172  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 1173000.
Eval num_timesteps=28500000, episode_reward=0.88 +/- 0.47
Episode length: 30.27 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.877    |
| time/              |          |
|    total_timesteps | 28500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8775
SELFPLAY: new best model, bumping up generation to 556
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.861    |
|    return_std      | 1.6      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58825    |
|    total_timesteps | 28517515 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1257     |
|    learning_rate   | 0.00172  |
|    step_size       | 0.00018  |
---------------------------------
Ep done - 1174000.
Ep done - 1175000.
Eval num_timesteps=28550000, episode_reward=0.85 +/- 0.51
Episode length: 30.24 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.853    |
| time/              |          |
|    total_timesteps | 28550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8525
SELFPLAY: new best model, bumping up generation to 557
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58904    |
|    total_timesteps | 28553753 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1258     |
|    learning_rate   | 0.00172  |
|    step_size       | 0.000181 |
---------------------------------
Ep done - 1176000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 2.57     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 58964    |
|    total_timesteps | 28589990 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1259     |
|    learning_rate   | 0.00172  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 1177000.
Eval num_timesteps=28600000, episode_reward=0.92 +/- 0.40
Episode length: 30.30 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 28600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 558
Ep done - 1178000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.869    |
|    return_std      | 1.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59045    |
|    total_timesteps | 28626236 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1260     |
|    learning_rate   | 0.00171  |
|    step_size       | 0.000145 |
---------------------------------
Ep done - 1179000.
Eval num_timesteps=28650000, episode_reward=0.90 +/- 0.42
Episode length: 30.27 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 28650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 559
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.833    |
|    return_std      | 2.94     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59125    |
|    total_timesteps | 28662521 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1261     |
|    learning_rate   | 0.00171  |
|    step_size       | 9.7e-05  |
---------------------------------
Ep done - 1180000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.877    |
|    return_std      | 1.75     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59184    |
|    total_timesteps | 28698802 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1262     |
|    learning_rate   | 0.00171  |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 1181000.
Eval num_timesteps=28700000, episode_reward=0.91 +/- 0.40
Episode length: 30.23 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 28700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 560
Ep done - 1182000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | 0.88     |
|    return_std      | 1.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59265    |
|    total_timesteps | 28735117 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1263     |
|    learning_rate   | 0.0017   |
|    step_size       | 0.000166 |
---------------------------------
Ep done - 1183000.
Eval num_timesteps=28750000, episode_reward=0.94 +/- 0.33
Episode length: 30.26 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 28750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 561
Ep done - 1184000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | 0.873    |
|    return_std      | 2.1      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59344    |
|    total_timesteps | 28771444 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1264     |
|    learning_rate   | 0.0017   |
|    step_size       | 0.000135 |
---------------------------------
Ep done - 1185000.
Eval num_timesteps=28800000, episode_reward=0.96 +/- 0.27
Episode length: 30.21 +/- 1.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.96     |
| time/              |          |
|    total_timesteps | 28800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.96
SELFPLAY: new best model, bumping up generation to 562
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 2.76     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59426    |
|    total_timesteps | 28807681 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1265     |
|    learning_rate   | 0.0017   |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 1186000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.864    |
|    return_std      | 2.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59485    |
|    total_timesteps | 28843941 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1266     |
|    learning_rate   | 0.0017   |
|    step_size       | 0.000116 |
---------------------------------
Ep done - 1187000.
Eval num_timesteps=28850000, episode_reward=0.92 +/- 0.39
Episode length: 30.31 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 28850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 563
Ep done - 1188000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 1.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59564    |
|    total_timesteps | 28880206 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1267     |
|    learning_rate   | 0.00169  |
|    step_size       | 0.000215 |
---------------------------------
Ep done - 1189000.
Eval num_timesteps=28900000, episode_reward=0.93 +/- 0.36
Episode length: 30.25 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 28900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 564
Ep done - 1190000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.884    |
|    return_std      | 1.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59646    |
|    total_timesteps | 28916493 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1268     |
|    learning_rate   | 0.00169  |
|    step_size       | 0.000195 |
---------------------------------
Ep done - 1191000.
Eval num_timesteps=28950000, episode_reward=0.91 +/- 0.40
Episode length: 30.23 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 28950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 565
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.902    |
|    return_std      | 1.4      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59725    |
|    total_timesteps | 28952780 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1269     |
|    learning_rate   | 0.00169  |
|    step_size       | 0.0002   |
---------------------------------
Ep done - 1192000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.835    |
|    return_std      | 0.985    |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59785    |
|    total_timesteps | 28989012 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1270     |
|    learning_rate   | 0.00168  |
|    step_size       | 0.000285 |
---------------------------------
Ep done - 1193000.
Eval num_timesteps=29000000, episode_reward=0.91 +/- 0.41
Episode length: 30.29 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 29000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 566
Ep done - 1194000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.879    |
|    return_std      | 2.14     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59867    |
|    total_timesteps | 29025308 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1271     |
|    learning_rate   | 0.00168  |
|    step_size       | 0.000131 |
---------------------------------
Ep done - 1195000.
Eval num_timesteps=29050000, episode_reward=0.92 +/- 0.38
Episode length: 30.25 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 29050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 567
Ep done - 1196000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.87     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 59945    |
|    total_timesteps | 29061610 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1272     |
|    learning_rate   | 0.00168  |
|    step_size       | 9.73e-05 |
---------------------------------
Ep done - 1197000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 2.45     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60005    |
|    total_timesteps | 29097855 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1273     |
|    learning_rate   | 0.00168  |
|    step_size       | 0.000114 |
---------------------------------
Eval num_timesteps=29100000, episode_reward=0.90 +/- 0.44
Episode length: 30.27 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 29100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 568
Ep done - 1198000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 3.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60087    |
|    total_timesteps | 29134131 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1274     |
|    learning_rate   | 0.00167  |
|    step_size       | 7.38e-05 |
---------------------------------
Ep done - 1199000.
Eval num_timesteps=29150000, episode_reward=0.95 +/- 0.30
Episode length: 30.22 +/- 1.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.955    |
| time/              |          |
|    total_timesteps | 29150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.955
SELFPLAY: new best model, bumping up generation to 569
Ep done - 1200000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.866    |
|    return_std      | 0.778    |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60165    |
|    total_timesteps | 29170419 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1275     |
|    learning_rate   | 0.00167  |
|    step_size       | 0.000357 |
---------------------------------
Ep done - 1201000.
Eval num_timesteps=29200000, episode_reward=0.88 +/- 0.47
Episode length: 30.27 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.877    |
| time/              |          |
|    total_timesteps | 29200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8775
SELFPLAY: new best model, bumping up generation to 570
Ep done - 1202000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.866    |
|    return_std      | 2.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60244    |
|    total_timesteps | 29206672 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1276     |
|    learning_rate   | 0.00167  |
|    step_size       | 0.000119 |
---------------------------------
Ep done - 1203000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.49     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60306    |
|    total_timesteps | 29242878 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1277     |
|    learning_rate   | 0.00166  |
|    step_size       | 0.000111 |
---------------------------------
Eval num_timesteps=29250000, episode_reward=0.93 +/- 0.35
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 29250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 571
Ep done - 1204000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 1.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60385    |
|    total_timesteps | 29279191 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1278     |
|    learning_rate   | 0.00166  |
|    step_size       | 0.000248 |
---------------------------------
Ep done - 1205000.
Eval num_timesteps=29300000, episode_reward=0.88 +/- 0.47
Episode length: 30.23 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.875    |
| time/              |          |
|    total_timesteps | 29300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.875
SELFPLAY: new best model, bumping up generation to 572
Ep done - 1206000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 1.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60464    |
|    total_timesteps | 29315477 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1279     |
|    learning_rate   | 0.00166  |
|    step_size       | 0.000191 |
---------------------------------
Ep done - 1207000.
Eval num_timesteps=29350000, episode_reward=0.91 +/- 0.41
Episode length: 30.24 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 29350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 573
Ep done - 1208000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.87     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60546    |
|    total_timesteps | 29351723 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1280     |
|    learning_rate   | 0.00165  |
|    step_size       | 0.000147 |
---------------------------------
Ep done - 1209000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.86     |
|    return_std      | 2.66     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60605    |
|    total_timesteps | 29387982 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1281     |
|    learning_rate   | 0.00165  |
|    step_size       | 0.000104 |
---------------------------------
Eval num_timesteps=29400000, episode_reward=0.91 +/- 0.40
Episode length: 30.18 +/- 1.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 29400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 574
Ep done - 1210000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 1.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60687    |
|    total_timesteps | 29424230 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1282     |
|    learning_rate   | 0.00165  |
|    step_size       | 0.000206 |
---------------------------------
Ep done - 1211000.
Ep done - 1212000.
Eval num_timesteps=29450000, episode_reward=0.93 +/- 0.35
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 29450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 575
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.896    |
|    return_std      | 2.06     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60766    |
|    total_timesteps | 29460540 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1283     |
|    learning_rate   | 0.00165  |
|    step_size       | 0.000133 |
---------------------------------
Ep done - 1213000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 2.41     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60827    |
|    total_timesteps | 29496703 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1284     |
|    learning_rate   | 0.00164  |
|    step_size       | 0.000114 |
---------------------------------
Ep done - 1214000.
Eval num_timesteps=29500000, episode_reward=0.93 +/- 0.34
Episode length: 30.30 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 29500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 576
Ep done - 1215000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 1.3      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60906    |
|    total_timesteps | 29532958 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1285     |
|    learning_rate   | 0.00164  |
|    step_size       | 0.00021  |
---------------------------------
Ep done - 1216000.
Eval num_timesteps=29550000, episode_reward=0.94 +/- 0.33
Episode length: 30.28 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 29550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 577
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 3.79     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 60988    |
|    total_timesteps | 29569203 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1286     |
|    learning_rate   | 0.00164  |
|    step_size       | 7.2e-05  |
---------------------------------
Ep done - 1217000.
Ep done - 1218000.
Eval num_timesteps=29600000, episode_reward=0.92 +/- 0.39
Episode length: 30.23 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 29600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 578
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 2.45     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61066    |
|    total_timesteps | 29605443 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1287     |
|    learning_rate   | 0.00163  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 1219000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.876    |
|    return_std      | 2.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61126    |
|    total_timesteps | 29641712 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1288     |
|    learning_rate   | 0.00163  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 1220000.
Eval num_timesteps=29650000, episode_reward=0.94 +/- 0.34
Episode length: 30.27 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 29650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 579
Ep done - 1221000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.871    |
|    return_std      | 2.24     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61208    |
|    total_timesteps | 29678003 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1289     |
|    learning_rate   | 0.00163  |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 1222000.
Eval num_timesteps=29700000, episode_reward=0.92 +/- 0.39
Episode length: 30.28 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 29700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 580
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 2.57     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61287    |
|    total_timesteps | 29714232 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1290     |
|    learning_rate   | 0.00163  |
|    step_size       | 0.000105 |
---------------------------------
Ep done - 1223000.
Ep done - 1224000.
Eval num_timesteps=29750000, episode_reward=0.93 +/- 0.35
Episode length: 30.27 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 29750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 581
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 2.42     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61369    |
|    total_timesteps | 29750513 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1291     |
|    learning_rate   | 0.00162  |
|    step_size       | 0.000112 |
---------------------------------
Ep done - 1225000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.27     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61428    |
|    total_timesteps | 29786782 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1292     |
|    learning_rate   | 0.00162  |
|    step_size       | 0.000119 |
---------------------------------
Ep done - 1226000.
Eval num_timesteps=29800000, episode_reward=0.88 +/- 0.45
Episode length: 30.20 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 29800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 582
Ep done - 1227000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.879    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61507    |
|    total_timesteps | 29823037 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1293     |
|    learning_rate   | 0.00162  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 1228000.
Eval num_timesteps=29850000, episode_reward=0.92 +/- 0.38
Episode length: 30.25 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 29850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 583
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 2.01     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61590    |
|    total_timesteps | 29859301 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1294     |
|    learning_rate   | 0.00161  |
|    step_size       | 0.000134 |
---------------------------------
Ep done - 1229000.
Ep done - 1230000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.879    |
|    return_std      | 2.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61649    |
|    total_timesteps | 29895574 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1295     |
|    learning_rate   | 0.00161  |
|    step_size       | 0.000115 |
---------------------------------
Eval num_timesteps=29900000, episode_reward=0.92 +/- 0.39
Episode length: 30.27 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 29900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 584
Ep done - 1231000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.84     |
|    return_std      | 2.76     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61729    |
|    total_timesteps | 29931839 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1296     |
|    learning_rate   | 0.00161  |
|    step_size       | 9.72e-05 |
---------------------------------
Ep done - 1232000.
Eval num_timesteps=29950000, episode_reward=0.95 +/- 0.29
Episode length: 30.27 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.953    |
| time/              |          |
|    total_timesteps | 29950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9525
SELFPLAY: new best model, bumping up generation to 585
Ep done - 1233000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.72     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61811    |
|    total_timesteps | 29968103 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1297     |
|    learning_rate   | 0.00161  |
|    step_size       | 0.000155 |
---------------------------------
Ep done - 1234000.
Eval num_timesteps=30000000, episode_reward=0.93 +/- 0.35
Episode length: 30.30 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 30000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 586
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.857    |
|    return_std      | 1.42     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61890    |
|    total_timesteps | 30004389 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1298     |
|    learning_rate   | 0.0016   |
|    step_size       | 0.000188 |
---------------------------------
Ep done - 1235000.
Ep done - 1236000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 1.16     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 61949    |
|    total_timesteps | 30040635 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1299     |
|    learning_rate   | 0.0016   |
|    step_size       | 0.000229 |
---------------------------------
Eval num_timesteps=30050000, episode_reward=0.95 +/- 0.31
Episode length: 30.34 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.948    |
| time/              |          |
|    total_timesteps | 30050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9475
SELFPLAY: new best model, bumping up generation to 587
Ep done - 1237000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.844    |
|    return_std      | 2.84     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62031    |
|    total_timesteps | 30076925 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1300     |
|    learning_rate   | 0.0016   |
|    step_size       | 9.36e-05 |
---------------------------------
Ep done - 1238000.
Eval num_timesteps=30100000, episode_reward=0.90 +/- 0.41
Episode length: 30.29 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 30100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 588
Ep done - 1239000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.876    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62111    |
|    total_timesteps | 30113208 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1301     |
|    learning_rate   | 0.00159  |
|    step_size       | 0.000138 |
---------------------------------
Ep done - 1240000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 2.77     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62170    |
|    total_timesteps | 30149444 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1302     |
|    learning_rate   | 0.00159  |
|    step_size       | 9.56e-05 |
---------------------------------
Eval num_timesteps=30150000, episode_reward=0.88 +/- 0.47
Episode length: 30.25 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.877    |
| time/              |          |
|    total_timesteps | 30150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8775
SELFPLAY: new best model, bumping up generation to 589
Ep done - 1241000.
Ep done - 1242000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.883    |
|    return_std      | 2.68     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62252    |
|    total_timesteps | 30185734 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1303     |
|    learning_rate   | 0.00159  |
|    step_size       | 9.88e-05 |
---------------------------------
Eval num_timesteps=30200000, episode_reward=0.90 +/- 0.43
Episode length: 30.21 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 30200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8975
SELFPLAY: new best model, bumping up generation to 590
Ep done - 1243000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.872    |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62331    |
|    total_timesteps | 30222009 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1304     |
|    learning_rate   | 0.00159  |
|    step_size       | 0.000128 |
---------------------------------
Ep done - 1244000.
Eval num_timesteps=30250000, episode_reward=0.93 +/- 0.36
Episode length: 30.25 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 30250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 591
Ep done - 1245000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.856    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62413    |
|    total_timesteps | 30258279 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1305     |
|    learning_rate   | 0.00158  |
|    step_size       | 0.000163 |
---------------------------------
Ep done - 1246000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.873    |
|    return_std      | 2.26     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62473    |
|    total_timesteps | 30294524 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1306     |
|    learning_rate   | 0.00158  |
|    step_size       | 0.000116 |
---------------------------------
Eval num_timesteps=30300000, episode_reward=0.93 +/- 0.35
Episode length: 30.27 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 30300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 592
Ep done - 1247000.
Ep done - 1248000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.866    |
|    return_std      | 1.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62552    |
|    total_timesteps | 30330783 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1307     |
|    learning_rate   | 0.00158  |
|    step_size       | 0.0002   |
---------------------------------
Ep done - 1249000.
Eval num_timesteps=30350000, episode_reward=0.90 +/- 0.42
Episode length: 30.26 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 30350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9025
SELFPLAY: new best model, bumping up generation to 593
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.832    |
|    return_std      | 1.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62634    |
|    total_timesteps | 30367031 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1308     |
|    learning_rate   | 0.00157  |
|    step_size       | 0.000165 |
---------------------------------
Ep done - 1250000.
Ep done - 1251000.
Eval num_timesteps=30400000, episode_reward=0.91 +/- 0.39
Episode length: 30.24 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 30400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 594
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 2.29     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62713    |
|    total_timesteps | 30403290 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1309     |
|    learning_rate   | 0.00157  |
|    step_size       | 0.000114 |
---------------------------------
Ep done - 1252000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.866    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62772    |
|    total_timesteps | 30439571 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1310     |
|    learning_rate   | 0.00157  |
|    step_size       | 0.000147 |
---------------------------------
Ep done - 1253000.
Eval num_timesteps=30450000, episode_reward=0.89 +/- 0.44
Episode length: 30.32 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 30450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 595
Ep done - 1254000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62854    |
|    total_timesteps | 30475843 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1311     |
|    learning_rate   | 0.00156  |
|    step_size       | 0.000126 |
---------------------------------
Ep done - 1255000.
Eval num_timesteps=30500000, episode_reward=0.94 +/- 0.34
Episode length: 30.25 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 30500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 596
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.872    |
|    return_std      | 3.54     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62934    |
|    total_timesteps | 30512107 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1312     |
|    learning_rate   | 0.00156  |
|    step_size       | 7.35e-05 |
---------------------------------
Ep done - 1256000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.846    |
|    return_std      | 3.58     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 62994    |
|    total_timesteps | 30548343 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1313     |
|    learning_rate   | 0.00156  |
|    step_size       | 7.26e-05 |
---------------------------------
Ep done - 1257000.
Eval num_timesteps=30550000, episode_reward=0.92 +/- 0.38
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 30550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 597
Ep done - 1258000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.857    |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63073    |
|    total_timesteps | 30584650 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1314     |
|    learning_rate   | 0.00156  |
|    step_size       | 0.000108 |
---------------------------------
Ep done - 1259000.
Eval num_timesteps=30600000, episode_reward=0.91 +/- 0.39
Episode length: 30.27 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 30600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 598
Ep done - 1260000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.873    |
|    return_std      | 1.37     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63155    |
|    total_timesteps | 30620931 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1315     |
|    learning_rate   | 0.00155  |
|    step_size       | 0.000189 |
---------------------------------
Ep done - 1261000.
Eval num_timesteps=30650000, episode_reward=0.95 +/- 0.29
Episode length: 30.29 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.948    |
| time/              |          |
|    total_timesteps | 30650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9475
SELFPLAY: new best model, bumping up generation to 599
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 3.19     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63234    |
|    total_timesteps | 30657221 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1316     |
|    learning_rate   | 0.00155  |
|    step_size       | 8.09e-05 |
---------------------------------
Ep done - 1262000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.877    |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63296    |
|    total_timesteps | 30693504 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1317     |
|    learning_rate   | 0.00155  |
|    step_size       | 0.00012  |
---------------------------------
Ep done - 1263000.
Eval num_timesteps=30700000, episode_reward=0.93 +/- 0.36
Episode length: 30.23 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 30700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 600
Ep done - 1264000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.869    |
|    return_std      | 1.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63376    |
|    total_timesteps | 30729770 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1318     |
|    learning_rate   | 0.00154  |
|    step_size       | 0.000231 |
---------------------------------
Ep done - 1265000.
Eval num_timesteps=30750000, episode_reward=0.90 +/- 0.43
Episode length: 30.29 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 30750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 601
Ep done - 1266000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 1.76     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63455    |
|    total_timesteps | 30766063 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1319     |
|    learning_rate   | 0.00154  |
|    step_size       | 0.000146 |
---------------------------------
Ep done - 1267000.
Eval num_timesteps=30800000, episode_reward=0.91 +/- 0.39
Episode length: 30.24 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 30800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 602
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.869    |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63537    |
|    total_timesteps | 30802295 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1320     |
|    learning_rate   | 0.00154  |
|    step_size       | 0.000124 |
---------------------------------
Ep done - 1268000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 3.22     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63597    |
|    total_timesteps | 30838557 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1321     |
|    learning_rate   | 0.00154  |
|    step_size       | 7.94e-05 |
---------------------------------
Ep done - 1269000.
Eval num_timesteps=30850000, episode_reward=0.91 +/- 0.41
Episode length: 30.27 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 30850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 603
Ep done - 1270000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 3.2      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63676    |
|    total_timesteps | 30874826 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1322     |
|    learning_rate   | 0.00153  |
|    step_size       | 7.97e-05 |
---------------------------------
Ep done - 1271000.
Eval num_timesteps=30900000, episode_reward=0.91 +/- 0.41
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 30900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 604
Ep done - 1272000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 1.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63758    |
|    total_timesteps | 30911057 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1323     |
|    learning_rate   | 0.00153  |
|    step_size       | 0.000191 |
---------------------------------
Ep done - 1273000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.867    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63818    |
|    total_timesteps | 30947350 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1324     |
|    learning_rate   | 0.00153  |
|    step_size       | 0.000105 |
---------------------------------
Eval num_timesteps=30950000, episode_reward=0.93 +/- 0.36
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 30950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 605
Ep done - 1274000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.879    |
|    return_std      | 1.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63897    |
|    total_timesteps | 30983623 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1325     |
|    learning_rate   | 0.00152  |
|    step_size       | 0.000162 |
---------------------------------
Ep done - 1275000.
Eval num_timesteps=31000000, episode_reward=0.91 +/- 0.41
Episode length: 30.26 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 31000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 606
Ep done - 1276000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.855    |
|    return_std      | 1.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 63979    |
|    total_timesteps | 31019844 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1326     |
|    learning_rate   | 0.00152  |
|    step_size       | 0.00016  |
---------------------------------
Ep done - 1277000.
Eval num_timesteps=31050000, episode_reward=0.93 +/- 0.34
Episode length: 30.34 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 31050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 607
Ep done - 1278000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.88     |
|    return_std      | 3.04     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64058    |
|    total_timesteps | 31056099 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1327     |
|    learning_rate   | 0.00152  |
|    step_size       | 8.32e-05 |
---------------------------------
Ep done - 1279000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.881    |
|    return_std      | 1.99     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64118    |
|    total_timesteps | 31092386 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1328     |
|    learning_rate   | 0.00152  |
|    step_size       | 0.000127 |
---------------------------------
Eval num_timesteps=31100000, episode_reward=0.92 +/- 0.35
Episode length: 30.19 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 31100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 608
Ep done - 1280000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.882    |
|    return_std      | 1.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64200    |
|    total_timesteps | 31128648 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1329     |
|    learning_rate   | 0.00151  |
|    step_size       | 0.000161 |
---------------------------------
Ep done - 1281000.
Eval num_timesteps=31150000, episode_reward=0.92 +/- 0.37
Episode length: 30.25 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 31150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 609
Ep done - 1282000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.864    |
|    return_std      | 1.41     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64279    |
|    total_timesteps | 31164926 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1330     |
|    learning_rate   | 0.00151  |
|    step_size       | 0.000178 |
---------------------------------
Ep done - 1283000.
Eval num_timesteps=31200000, episode_reward=0.90 +/- 0.43
Episode length: 30.31 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 31200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 610
Ep done - 1284000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.887    |
|    return_std      | 2.05     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64361    |
|    total_timesteps | 31201202 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1331     |
|    learning_rate   | 0.00151  |
|    step_size       | 0.000122 |
---------------------------------
Ep done - 1285000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.18     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64420    |
|    total_timesteps | 31237393 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1332     |
|    learning_rate   | 0.0015   |
|    step_size       | 0.000115 |
---------------------------------
Ep done - 1286000.
Eval num_timesteps=31250000, episode_reward=0.90 +/- 0.43
Episode length: 30.36 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 31250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9025
SELFPLAY: new best model, bumping up generation to 611
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.871    |
|    return_std      | 1.24     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64499    |
|    total_timesteps | 31273684 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1333     |
|    learning_rate   | 0.0015   |
|    step_size       | 0.000202 |
---------------------------------
Ep done - 1287000.
Ep done - 1288000.
Eval num_timesteps=31300000, episode_reward=0.93 +/- 0.36
Episode length: 30.32 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 31300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 612
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 1.72     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64581    |
|    total_timesteps | 31309905 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1334     |
|    learning_rate   | 0.0015   |
|    step_size       | 0.000145 |
---------------------------------
Ep done - 1289000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.85     |
|    return_std      | 1.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64641    |
|    total_timesteps | 31346139 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1335     |
|    learning_rate   | 0.0015   |
|    step_size       | 0.000126 |
---------------------------------
Ep done - 1290000.
Eval num_timesteps=31350000, episode_reward=0.93 +/- 0.37
Episode length: 30.31 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 31350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 613
Ep done - 1291000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.857    |
|    return_std      | 2.23     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64720    |
|    total_timesteps | 31382370 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1336     |
|    learning_rate   | 0.00149  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 1292000.
Eval num_timesteps=31400000, episode_reward=0.91 +/- 0.39
Episode length: 30.21 +/- 1.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 31400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 614
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 2.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64802    |
|    total_timesteps | 31418603 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1337     |
|    learning_rate   | 0.00149  |
|    step_size       | 0.000107 |
---------------------------------
Ep done - 1293000.
Ep done - 1294000.
Eval num_timesteps=31450000, episode_reward=0.93 +/- 0.36
Episode length: 30.26 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 31450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 615
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.842    |
|    return_std      | 3.42     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64881    |
|    total_timesteps | 31454866 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1338     |
|    learning_rate   | 0.00149  |
|    step_size       | 7.24e-05 |
---------------------------------
Ep done - 1295000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 64941    |
|    total_timesteps | 31491133 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1339     |
|    learning_rate   | 0.00148  |
|    step_size       | 9.12e-05 |
---------------------------------
Ep done - 1296000.
Eval num_timesteps=31500000, episode_reward=0.94 +/- 0.33
Episode length: 30.23 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.943    |
| time/              |          |
|    total_timesteps | 31500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9425
SELFPLAY: new best model, bumping up generation to 616
Ep done - 1297000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.874    |
|    return_std      | 2.77     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65023    |
|    total_timesteps | 31527406 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1340     |
|    learning_rate   | 0.00148  |
|    step_size       | 8.92e-05 |
---------------------------------
Ep done - 1298000.
Eval num_timesteps=31550000, episode_reward=0.92 +/- 0.38
Episode length: 30.27 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 31550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 617
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 2.1      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65103    |
|    total_timesteps | 31563641 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1341     |
|    learning_rate   | 0.00148  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 1299000.
Ep done - 1300000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.876    |
|    return_std      | 2.23     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65163    |
|    total_timesteps | 31599921 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1342     |
|    learning_rate   | 0.00147  |
|    step_size       | 0.00011  |
---------------------------------
Eval num_timesteps=31600000, episode_reward=0.91 +/- 0.41
Episode length: 30.25 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 31600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 618
Ep done - 1301000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.869    |
|    return_std      | 2.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65245    |
|    total_timesteps | 31636177 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1343     |
|    learning_rate   | 0.00147  |
|    step_size       | 0.000106 |
---------------------------------
Ep done - 1302000.
Eval num_timesteps=31650000, episode_reward=0.92 +/- 0.39
Episode length: 30.26 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 31650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 619
Ep done - 1303000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 3.21     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65324    |
|    total_timesteps | 31672451 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1344     |
|    learning_rate   | 0.00147  |
|    step_size       | 7.62e-05 |
---------------------------------
Ep done - 1304000.
Eval num_timesteps=31700000, episode_reward=0.94 +/- 0.33
Episode length: 30.28 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.938    |
| time/              |          |
|    total_timesteps | 31700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9375
SELFPLAY: new best model, bumping up generation to 620
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.871    |
|    return_std      | 2.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65403    |
|    total_timesteps | 31708723 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1345     |
|    learning_rate   | 0.00147  |
|    step_size       | 8.8e-05  |
---------------------------------
Ep done - 1305000.
Ep done - 1306000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.847    |
|    return_std      | 2.26     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65465    |
|    total_timesteps | 31744989 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1346     |
|    learning_rate   | 0.00146  |
|    step_size       | 0.000108 |
---------------------------------
Eval num_timesteps=31750000, episode_reward=0.91 +/- 0.39
Episode length: 30.31 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.91     |
| time/              |          |
|    total_timesteps | 31750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.91
SELFPLAY: new best model, bumping up generation to 621
Ep done - 1307000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.872    |
|    return_std      | 1.86     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65544    |
|    total_timesteps | 31781251 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1347     |
|    learning_rate   | 0.00146  |
|    step_size       | 0.000131 |
---------------------------------
Ep done - 1308000.
Eval num_timesteps=31800000, episode_reward=0.93 +/- 0.34
Episode length: 30.18 +/- 1.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 31800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 622
Ep done - 1309000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.869    |
|    return_std      | 1.48     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65623    |
|    total_timesteps | 31817505 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1348     |
|    learning_rate   | 0.00146  |
|    step_size       | 0.000164 |
---------------------------------
Ep done - 1310000.
Eval num_timesteps=31850000, episode_reward=0.88 +/- 0.46
Episode length: 30.25 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.882    |
| time/              |          |
|    total_timesteps | 31850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8825
SELFPLAY: new best model, bumping up generation to 623
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 2.06     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65705    |
|    total_timesteps | 31853772 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1349     |
|    learning_rate   | 0.00145  |
|    step_size       | 0.000118 |
---------------------------------
Ep done - 1311000.
Ep done - 1312000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.886    |
|    return_std      | 1.75     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65765    |
|    total_timesteps | 31889991 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1350     |
|    learning_rate   | 0.00145  |
|    step_size       | 0.000138 |
---------------------------------
Eval num_timesteps=31900000, episode_reward=0.89 +/- 0.44
Episode length: 30.25 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 31900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 624
Ep done - 1313000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.875    |
|    return_std      | 1.76     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65844    |
|    total_timesteps | 31926256 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1351     |
|    learning_rate   | 0.00145  |
|    step_size       | 0.000137 |
---------------------------------
Ep done - 1314000.
Eval num_timesteps=31950000, episode_reward=0.93 +/- 0.37
Episode length: 30.26 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 31950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 625
Ep done - 1315000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 2.49     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65925    |
|    total_timesteps | 31962529 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1352     |
|    learning_rate   | 0.00145  |
|    step_size       | 9.67e-05 |
---------------------------------
Ep done - 1316000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.848    |
|    return_std      | 4.23     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 65985    |
|    total_timesteps | 31998751 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1353     |
|    learning_rate   | 0.00144  |
|    step_size       | 5.69e-05 |
---------------------------------
Eval num_timesteps=32000000, episode_reward=0.92 +/- 0.39
Episode length: 30.25 +/- 0.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 32000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 626
Ep done - 1317000.
Ep done - 1318000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 1.16     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66064    |
|    total_timesteps | 32035023 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1354     |
|    learning_rate   | 0.00144  |
|    step_size       | 0.000206 |
---------------------------------
Eval num_timesteps=32050000, episode_reward=0.90 +/- 0.43
Episode length: 30.28 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 32050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9025
SELFPLAY: new best model, bumping up generation to 627
Ep done - 1319000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 3.18     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66146    |
|    total_timesteps | 32071291 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1355     |
|    learning_rate   | 0.00144  |
|    step_size       | 7.54e-05 |
---------------------------------
Ep done - 1320000.
Eval num_timesteps=32100000, episode_reward=0.93 +/- 0.36
Episode length: 30.27 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 32100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 628
Ep done - 1321000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.879    |
|    return_std      | 1.31     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66225    |
|    total_timesteps | 32107559 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1356     |
|    learning_rate   | 0.00143  |
|    step_size       | 0.000182 |
---------------------------------
Ep done - 1322000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.86     |
|    return_std      | 2.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66285    |
|    total_timesteps | 32143820 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1357     |
|    learning_rate   | 0.00143  |
|    step_size       | 9.22e-05 |
---------------------------------
Ep done - 1323000.
Eval num_timesteps=32150000, episode_reward=0.94 +/- 0.34
Episode length: 30.26 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 32150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 629
Ep done - 1324000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.83     |
|    return_std      | 2.96     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66367    |
|    total_timesteps | 32180068 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1358     |
|    learning_rate   | 0.00143  |
|    step_size       | 8.03e-05 |
---------------------------------
Ep done - 1325000.
Eval num_timesteps=32200000, episode_reward=0.91 +/- 0.41
Episode length: 30.27 +/- 0.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.907    |
| time/              |          |
|    total_timesteps | 32200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9075
SELFPLAY: new best model, bumping up generation to 630
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.85     |
|    return_std      | 0.888    |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66445    |
|    total_timesteps | 32216289 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1359     |
|    learning_rate   | 0.00143  |
|    step_size       | 0.000268 |
---------------------------------
Ep done - 1326000.
Ep done - 1327000.
Eval num_timesteps=32250000, episode_reward=0.92 +/- 0.36
Episode length: 30.29 +/- 0.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 32250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 631
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.882    |
|    return_std      | 1.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66527    |
|    total_timesteps | 32252570 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1360     |
|    learning_rate   | 0.00142  |
|    step_size       | 0.000177 |
---------------------------------
Ep done - 1328000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.874    |
|    return_std      | 2.14     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66587    |
|    total_timesteps | 32288868 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1361     |
|    learning_rate   | 0.00142  |
|    step_size       | 0.000111 |
---------------------------------
Ep done - 1329000.
Eval num_timesteps=32300000, episode_reward=0.95 +/- 0.30
Episode length: 30.22 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.948    |
| time/              |          |
|    total_timesteps | 32300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9475
SELFPLAY: new best model, bumping up generation to 632
Ep done - 1330000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.886    |
|    return_std      | 1.03     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66666    |
|    total_timesteps | 32325131 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1362     |
|    learning_rate   | 0.00142  |
|    step_size       | 0.000229 |
---------------------------------
Ep done - 1331000.
Eval num_timesteps=32350000, episode_reward=0.93 +/- 0.36
Episode length: 30.28 +/- 0.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 32350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 633
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.877    |
|    return_std      | 2.24     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66748    |
|    total_timesteps | 32361427 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1363     |
|    learning_rate   | 0.00141  |
|    step_size       | 0.000105 |
---------------------------------
Ep done - 1332000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 2.94     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66807    |
|    total_timesteps | 32397712 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1364     |
|    learning_rate   | 0.00141  |
|    step_size       | 8.01e-05 |
---------------------------------
Ep done - 1333000.
Eval num_timesteps=32400000, episode_reward=0.90 +/- 0.41
Episode length: 30.32 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 32400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9025
SELFPLAY: new best model, bumping up generation to 634
Ep done - 1334000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.872    |
|    return_std      | 1.87     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66886    |
|    total_timesteps | 32433959 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1365     |
|    learning_rate   | 0.00141  |
|    step_size       | 0.000125 |
---------------------------------
Ep done - 1335000.
Eval num_timesteps=32450000, episode_reward=0.90 +/- 0.42
Episode length: 30.28 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 32450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 635
Ep done - 1336000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.859    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 66968    |
|    total_timesteps | 32470178 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1366     |
|    learning_rate   | 0.00141  |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 1337000.
Eval num_timesteps=32500000, episode_reward=0.93 +/- 0.35
Episode length: 30.29 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 32500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 636
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 2.84     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67048    |
|    total_timesteps | 32506434 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1367     |
|    learning_rate   | 0.0014   |
|    step_size       | 8.23e-05 |
---------------------------------
Ep done - 1338000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.875    |
|    return_std      | 1.92     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67107    |
|    total_timesteps | 32542663 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1368     |
|    learning_rate   | 0.0014   |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 1339000.
Eval num_timesteps=32550000, episode_reward=0.91 +/- 0.42
Episode length: 30.25 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 32550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 637
Ep done - 1340000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.35     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67189    |
|    total_timesteps | 32578936 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1369     |
|    learning_rate   | 0.0014   |
|    step_size       | 9.91e-05 |
---------------------------------
Ep done - 1341000.
Eval num_timesteps=32600000, episode_reward=0.91 +/- 0.40
Episode length: 30.31 +/- 0.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 32600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 638
Ep done - 1342000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.851    |
|    return_std      | 1.87     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67270    |
|    total_timesteps | 32615201 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1370     |
|    learning_rate   | 0.00139  |
|    step_size       | 0.000124 |
---------------------------------
Ep done - 1343000.
Eval num_timesteps=32650000, episode_reward=0.93 +/- 0.37
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 32650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 639
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67349    |
|    total_timesteps | 32651478 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1371     |
|    learning_rate   | 0.00139  |
|    step_size       | 0.000148 |
---------------------------------
Ep done - 1344000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.843    |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67411    |
|    total_timesteps | 32687663 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1372     |
|    learning_rate   | 0.00139  |
|    step_size       | 8.53e-05 |
---------------------------------
Ep done - 1345000.
Eval num_timesteps=32700000, episode_reward=0.92 +/- 0.39
Episode length: 30.25 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 32700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 640
Ep done - 1346000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.881    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67490    |
|    total_timesteps | 32723911 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1373     |
|    learning_rate   | 0.00138  |
|    step_size       | 0.000123 |
---------------------------------
Ep done - 1347000.
Eval num_timesteps=32750000, episode_reward=0.89 +/- 0.44
Episode length: 30.25 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.89     |
| time/              |          |
|    total_timesteps | 32750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.89
SELFPLAY: new best model, bumping up generation to 641
Ep done - 1348000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.845    |
|    return_std      | 1.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67565    |
|    total_timesteps | 32760192 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1374     |
|    learning_rate   | 0.00138  |
|    step_size       | 0.000117 |
---------------------------------
Ep done - 1349000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.861    |
|    return_std      | 2.3      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67623    |
|    total_timesteps | 32796457 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1375     |
|    learning_rate   | 0.00138  |
|    step_size       | 9.99e-05 |
---------------------------------
Eval num_timesteps=32800000, episode_reward=0.93 +/- 0.34
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 32800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 642
Ep done - 1350000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.884    |
|    return_std      | 1.23     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67702    |
|    total_timesteps | 32832720 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1376     |
|    learning_rate   | 0.00138  |
|    step_size       | 0.000186 |
---------------------------------
Ep done - 1351000.
Eval num_timesteps=32850000, episode_reward=0.94 +/- 0.33
Episode length: 30.28 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 32850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.94
SELFPLAY: new best model, bumping up generation to 643
Ep done - 1352000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67784    |
|    total_timesteps | 32869003 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1377     |
|    learning_rate   | 0.00137  |
|    step_size       | 0.000132 |
---------------------------------
Ep done - 1353000.
Eval num_timesteps=32900000, episode_reward=0.95 +/- 0.27
Episode length: 30.21 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.953    |
| time/              |          |
|    total_timesteps | 32900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9525
SELFPLAY: new best model, bumping up generation to 644
Ep done - 1354000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.866    |
|    return_std      | 2.09     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67862    |
|    total_timesteps | 32905249 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1378     |
|    learning_rate   | 0.00137  |
|    step_size       | 0.000109 |
---------------------------------
Ep done - 1355000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 1.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 67922    |
|    total_timesteps | 32941501 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1379     |
|    learning_rate   | 0.00137  |
|    step_size       | 0.000124 |
---------------------------------
Eval num_timesteps=32950000, episode_reward=0.89 +/- 0.44
Episode length: 30.24 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 32950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.8925
SELFPLAY: new best model, bumping up generation to 645
Ep done - 1356000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 1.4      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68004    |
|    total_timesteps | 32977818 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1380     |
|    learning_rate   | 0.00136  |
|    step_size       | 0.000162 |
---------------------------------
Ep done - 1357000.
Eval num_timesteps=33000000, episode_reward=0.92 +/- 0.38
Episode length: 30.27 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 33000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 646
Ep done - 1358000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.857    |
|    return_std      | 1.83     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68082    |
|    total_timesteps | 33014050 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1381     |
|    learning_rate   | 0.00136  |
|    step_size       | 0.000124 |
---------------------------------
Ep done - 1359000.
Eval num_timesteps=33050000, episode_reward=0.93 +/- 0.35
Episode length: 30.26 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 33050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 647
Ep done - 1360000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.86     |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68165    |
|    total_timesteps | 33050302 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1382     |
|    learning_rate   | 0.00136  |
|    step_size       | 0.00011  |
---------------------------------
Ep done - 1361000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | 0.854    |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68224    |
|    total_timesteps | 33086605 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1383     |
|    learning_rate   | 0.00136  |
|    step_size       | 0.000109 |
---------------------------------
Ep done - 1362000.
Eval num_timesteps=33100000, episode_reward=0.95 +/- 0.31
Episode length: 30.32 +/- 0.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.95     |
| time/              |          |
|    total_timesteps | 33100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.95
SELFPLAY: new best model, bumping up generation to 648
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.01     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68302    |
|    total_timesteps | 33122842 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1384     |
|    learning_rate   | 0.00135  |
|    step_size       | 0.000112 |
---------------------------------
Ep done - 1363000.
Ep done - 1364000.
Eval num_timesteps=33150000, episode_reward=0.93 +/- 0.36
Episode length: 30.21 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 33150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 649
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.864    |
|    return_std      | 2.87     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68384    |
|    total_timesteps | 33159097 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1385     |
|    learning_rate   | 0.00135  |
|    step_size       | 7.83e-05 |
---------------------------------
Ep done - 1365000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.872    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 485      |
|    time_elapsed    | 68443    |
|    total_timesteps | 33195332 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1386     |
|    learning_rate   | 0.00135  |
|    step_size       | 0.000126 |
---------------------------------
Ep done - 1366000.
Eval num_timesteps=33200000, episode_reward=0.90 +/- 0.42
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 33200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9
SELFPLAY: new best model, bumping up generation to 650
Ep done - 1367000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.834    |
|    return_std      | 1.98     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68523    |
|    total_timesteps | 33231574 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1387     |
|    learning_rate   | 0.00134  |
|    step_size       | 0.000113 |
---------------------------------
Ep done - 1368000.
Eval num_timesteps=33250000, episode_reward=0.93 +/- 0.37
Episode length: 30.28 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 33250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 651
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 1.76     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68606    |
|    total_timesteps | 33267828 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1388     |
|    learning_rate   | 0.00134  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 1369000.
Ep done - 1370000.
Eval num_timesteps=33300000, episode_reward=0.90 +/- 0.43
Episode length: 30.24 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 33300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 652
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.76     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68685    |
|    total_timesteps | 33304131 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1389     |
|    learning_rate   | 0.00134  |
|    step_size       | 8.09e-05 |
---------------------------------
Ep done - 1371000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.858    |
|    return_std      | 2.17     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68745    |
|    total_timesteps | 33340396 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1390     |
|    learning_rate   | 0.00134  |
|    step_size       | 0.000103 |
---------------------------------
Ep done - 1372000.
Eval num_timesteps=33350000, episode_reward=0.93 +/- 0.36
Episode length: 30.26 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 33350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 653
Ep done - 1373000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.904    |
|    return_std      | 1.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68828    |
|    total_timesteps | 33376688 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1391     |
|    learning_rate   | 0.00133  |
|    step_size       | 0.000142 |
---------------------------------
Ep done - 1374000.
Eval num_timesteps=33400000, episode_reward=0.90 +/- 0.44
Episode length: 30.20 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 33400000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 654
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.885    |
|    return_std      | 2.5      |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 68906    |
|    total_timesteps | 33412986 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1392     |
|    learning_rate   | 0.00133  |
|    step_size       | 8.87e-05 |
---------------------------------
Ep done - 1375000.
Ep done - 1376000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 2.68     |
| time/              |          |
|    fps             | 485      |
|    time_elapsed    | 68966    |
|    total_timesteps | 33449230 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1393     |
|    learning_rate   | 0.00133  |
|    step_size       | 8.26e-05 |
---------------------------------
Eval num_timesteps=33450000, episode_reward=0.94 +/- 0.33
Episode length: 30.29 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 33450000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 655
Ep done - 1377000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.876    |
|    return_std      | 2.71     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69049    |
|    total_timesteps | 33485462 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1394     |
|    learning_rate   | 0.00132  |
|    step_size       | 8.14e-05 |
---------------------------------
Ep done - 1378000.
Eval num_timesteps=33500000, episode_reward=0.91 +/- 0.43
Episode length: 30.30 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 33500000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 656
Ep done - 1379000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 1.56     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69127    |
|    total_timesteps | 33521740 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1395     |
|    learning_rate   | 0.00132  |
|    step_size       | 0.000141 |
---------------------------------
Ep done - 1380000.
Eval num_timesteps=33550000, episode_reward=0.91 +/- 0.40
Episode length: 30.28 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 33550000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9125
SELFPLAY: new best model, bumping up generation to 657
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.882    |
|    return_std      | 1.62     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69207    |
|    total_timesteps | 33558011 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1396     |
|    learning_rate   | 0.00132  |
|    step_size       | 0.000136 |
---------------------------------
Ep done - 1381000.
Ep done - 1382000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.882    |
|    return_std      | 2.44     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69270    |
|    total_timesteps | 33594253 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1397     |
|    learning_rate   | 0.00132  |
|    step_size       | 8.97e-05 |
---------------------------------
Eval num_timesteps=33600000, episode_reward=0.90 +/- 0.43
Episode length: 30.26 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 33600000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.895
SELFPLAY: new best model, bumping up generation to 658
Ep done - 1383000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.878    |
|    return_std      | 2.57     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69349    |
|    total_timesteps | 33630518 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1398     |
|    learning_rate   | 0.00131  |
|    step_size       | 8.5e-05  |
---------------------------------
Ep done - 1384000.
Eval num_timesteps=33650000, episode_reward=0.92 +/- 0.38
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 33650000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 659
Ep done - 1385000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69431    |
|    total_timesteps | 33666805 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1399     |
|    learning_rate   | 0.00131  |
|    step_size       | 0.000106 |
---------------------------------
Ep done - 1386000.
Eval num_timesteps=33700000, episode_reward=0.93 +/- 0.35
Episode length: 30.30 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.927    |
| time/              |          |
|    total_timesteps | 33700000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9275
SELFPLAY: new best model, bumping up generation to 660
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.882    |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69511    |
|    total_timesteps | 33703093 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1400     |
|    learning_rate   | 0.00131  |
|    step_size       | 0.000116 |
---------------------------------
Ep done - 1387000.
Ep done - 1388000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 3.07     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69570    |
|    total_timesteps | 33739362 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1401     |
|    learning_rate   | 0.0013   |
|    step_size       | 7.07e-05 |
---------------------------------
Eval num_timesteps=33750000, episode_reward=0.93 +/- 0.35
Episode length: 30.30 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 33750000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 661
Ep done - 1389000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.849    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69653    |
|    total_timesteps | 33775652 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1402     |
|    learning_rate   | 0.0013   |
|    step_size       | 0.000122 |
---------------------------------
Ep done - 1390000.
Eval num_timesteps=33800000, episode_reward=0.92 +/- 0.39
Episode length: 30.27 +/- 0.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 33800000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 662
Ep done - 1391000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.853    |
|    return_std      | 1.78     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69734    |
|    total_timesteps | 33811943 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1403     |
|    learning_rate   | 0.0013   |
|    step_size       | 0.000121 |
---------------------------------
Ep done - 1392000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.881    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69793    |
|    total_timesteps | 33848218 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1404     |
|    learning_rate   | 0.0013   |
|    step_size       | 0.000112 |
---------------------------------
Eval num_timesteps=33850000, episode_reward=0.93 +/- 0.37
Episode length: 30.24 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 33850000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 663
Ep done - 1393000.
Ep done - 1394000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.87     |
|    return_std      | 1.38     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69872    |
|    total_timesteps | 33884518 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1405     |
|    learning_rate   | 0.00129  |
|    step_size       | 0.000156 |
---------------------------------
Eval num_timesteps=33900000, episode_reward=0.92 +/- 0.38
Episode length: 30.22 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.917    |
| time/              |          |
|    total_timesteps | 33900000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9175
SELFPLAY: new best model, bumping up generation to 664
Ep done - 1395000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.838    |
|    return_std      | 2.43     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 69954    |
|    total_timesteps | 33920761 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1406     |
|    learning_rate   | 0.00129  |
|    step_size       | 8.85e-05 |
---------------------------------
Ep done - 1396000.
Eval num_timesteps=33950000, episode_reward=0.91 +/- 0.41
Episode length: 30.25 +/- 0.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.905    |
| time/              |          |
|    total_timesteps | 33950000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.905
SELFPLAY: new best model, bumping up generation to 665
Ep done - 1397000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.882    |
|    return_std      | 1.34     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70034    |
|    total_timesteps | 33957040 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1407     |
|    learning_rate   | 0.00129  |
|    step_size       | 0.00016  |
---------------------------------
Ep done - 1398000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.863    |
|    return_std      | 2.48     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70093    |
|    total_timesteps | 33993287 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1408     |
|    learning_rate   | 0.00128  |
|    step_size       | 8.62e-05 |
---------------------------------
Ep done - 1399000.
Eval num_timesteps=34000000, episode_reward=0.92 +/- 0.39
Episode length: 30.27 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.915    |
| time/              |          |
|    total_timesteps | 34000000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.915
SELFPLAY: new best model, bumping up generation to 666
Ep done - 1400000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.877    |
|    return_std      | 2.61     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70176    |
|    total_timesteps | 34029589 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1409     |
|    learning_rate   | 0.00128  |
|    step_size       | 8.18e-05 |
---------------------------------
Ep done - 1401000.
Eval num_timesteps=34050000, episode_reward=0.92 +/- 0.38
Episode length: 30.32 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 34050000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.92
SELFPLAY: new best model, bumping up generation to 667
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.826    |
|    return_std      | 2.49     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70254    |
|    total_timesteps | 34065834 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1410     |
|    learning_rate   | 0.00128  |
|    step_size       | 8.55e-05 |
---------------------------------
Ep done - 1402000.
Ep done - 1403000.
Eval num_timesteps=34100000, episode_reward=0.94 +/- 0.33
Episode length: 30.24 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | 0.935    |
| time/              |          |
|    total_timesteps | 34100000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.935
SELFPLAY: new best model, bumping up generation to 668
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.68     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70337    |
|    total_timesteps | 34102124 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1411     |
|    learning_rate   | 0.00127  |
|    step_size       | 0.000127 |
---------------------------------
Ep done - 1404000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.865    |
|    return_std      | 1.54     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70396    |
|    total_timesteps | 34138352 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1412     |
|    learning_rate   | 0.00127  |
|    step_size       | 0.000137 |
---------------------------------
Ep done - 1405000.
Eval num_timesteps=34150000, episode_reward=0.90 +/- 0.41
Episode length: 30.14 +/- 1.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 34150000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9025
SELFPLAY: new best model, bumping up generation to 669
Ep done - 1406000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 2.59     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70475    |
|    total_timesteps | 34174572 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1413     |
|    learning_rate   | 0.00127  |
|    step_size       | 8.15e-05 |
---------------------------------
Ep done - 1407000.
Eval num_timesteps=34200000, episode_reward=0.92 +/- 0.37
Episode length: 30.26 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.922    |
| time/              |          |
|    total_timesteps | 34200000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9225
SELFPLAY: new best model, bumping up generation to 670
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.88     |
|    return_std      | 1.51     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70558    |
|    total_timesteps | 34210842 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1414     |
|    learning_rate   | 0.00127  |
|    step_size       | 0.00014  |
---------------------------------
Ep done - 1408000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 1.38     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70617    |
|    total_timesteps | 34247122 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1415     |
|    learning_rate   | 0.00126  |
|    step_size       | 0.000153 |
---------------------------------
Ep done - 1409000.
Eval num_timesteps=34250000, episode_reward=0.93 +/- 0.36
Episode length: 30.29 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 34250000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.925
SELFPLAY: new best model, bumping up generation to 671
Ep done - 1410000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.868    |
|    return_std      | 2.64     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70696    |
|    total_timesteps | 34283360 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1416     |
|    learning_rate   | 0.00126  |
|    step_size       | 7.96e-05 |
---------------------------------
Ep done - 1411000.
Eval num_timesteps=34300000, episode_reward=0.93 +/- 0.34
Episode length: 30.28 +/- 0.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.932    |
| time/              |          |
|    total_timesteps | 34300000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.9325
SELFPLAY: new best model, bumping up generation to 672
Ep done - 1412000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.877    |
|    return_std      | 2.91     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70779    |
|    total_timesteps | 34319641 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1417     |
|    learning_rate   | 0.00126  |
|    step_size       | 7.21e-05 |
---------------------------------
Ep done - 1413000.
Eval num_timesteps=34350000, episode_reward=0.93 +/- 0.36
Episode length: 30.26 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.3     |
|    mean_reward     | 0.93     |
| time/              |          |
|    total_timesteps | 34350000 |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.93
SELFPLAY: new best model, bumping up generation to 673
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.862    |
|    return_std      | 1.11     |
| time/              |          |
|    fps             | 484      |
|    time_elapsed    | 70858    |
|    total_timesteps | 34355861 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1418     |
|    learning_rate   | 0.00125  |
|    step_size       | 0.000188 |
---------------------------------
slurmstepd-n16: error: *** JOB 833 ON n16 CANCELLED AT 2024-06-23T15:40:45 ***
