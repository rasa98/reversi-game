CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
CUDA available: True
seed: 12 
num_timesteps: 30000000 
eval_freq: 20000 
eval_episoded: 200 
best_threshold: 0.25 
logdir: scripts/rl/output/phase2/ppo/cnn/base-v2/ 
cnn_policy: True 
continueFrom_model: scripts/rl/output/phase2/ppo/cnn/base/history_0005

params: {'learning_rate': 0.0001, 'n_steps': 20480, 'n_epochs': 5, 'clip_range': 0.2, 'batch_size': 128, 'ent_coef': 0.012, 'verbose': 100, 'seed': 12}

Eval num_timesteps=20000, episode_reward=0.38 +/- 0.91
Episode length: 29.99 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.38     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.38
SELFPLAY: new best model, bumping up generation to 1
Ep done - 1000.
Eval num_timesteps=40000, episode_reward=0.39 +/- 0.91
Episode length: 30.07 +/- 0.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.385       |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.091668054 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | -0.0413     |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0913     |
|    n_updates            | 275         |
|    policy_gradient_loss | -0.0693     |
|    value_loss           | 0.198       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.385
SELFPLAY: new best model, bumping up generation to 2
Ep done - 2000.
Eval num_timesteps=60000, episode_reward=0.28 +/- 0.94
Episode length: 30.06 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.285      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.09775309 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.0414     |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0964    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.073     |
|    value_loss           | 0.189      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 3
Ep done - 3000.
Eval num_timesteps=80000, episode_reward=0.36 +/- 0.91
Episode length: 30.03 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.365      |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.10995625 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.123      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.101     |
|    n_updates            | 285        |
|    policy_gradient_loss | -0.0718    |
|    value_loss           | 0.174      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.365
SELFPLAY: new best model, bumping up generation to 4
Ep done - 4000.
Eval num_timesteps=100000, episode_reward=0.15 +/- 0.96
Episode length: 30.01 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.11731503 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.12       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0945    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0707    |
|    value_loss           | 0.175      |
----------------------------------------
Ep done - 5000.
Eval num_timesteps=120000, episode_reward=0.24 +/- 0.96
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.24       |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.12400772 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.167      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0705    |
|    n_updates            | 295        |
|    policy_gradient_loss | -0.0696    |
|    value_loss           | 0.167      |
----------------------------------------
Ep done - 6000.
Eval num_timesteps=140000, episode_reward=0.39 +/- 0.90
Episode length: 30.09 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.385      |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.12783709 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.0804     |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0755    |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0682    |
|    value_loss           | 0.189      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.385
SELFPLAY: new best model, bumping up generation to 5
Eval num_timesteps=160000, episode_reward=0.18 +/- 0.97
Episode length: 30.04 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.18       |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.13085851 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.163      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0831    |
|    n_updates            | 305        |
|    policy_gradient_loss | -0.0694    |
|    value_loss           | 0.171      |
----------------------------------------
Ep done - 7000.
Eval num_timesteps=180000, episode_reward=0.44 +/- 0.88
Episode length: 30.07 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.44       |
| time/                   |            |
|    total_timesteps      | 180000     |
| train/                  |            |
|    approx_kl            | 0.13852164 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.154      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0686    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0694    |
|    value_loss           | 0.176      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.44
SELFPLAY: new best model, bumping up generation to 6
Ep done - 8000.
Eval num_timesteps=200000, episode_reward=0.20 +/- 0.97
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.195      |
| time/                   |            |
|    total_timesteps      | 200000     |
| train/                  |            |
|    approx_kl            | 0.15807232 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0779    |
|    n_updates            | 315        |
|    policy_gradient_loss | -0.068     |
|    value_loss           | 0.169      |
----------------------------------------
Ep done - 9000.
Eval num_timesteps=220000, episode_reward=0.23 +/- 0.95
Episode length: 30.03 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.235      |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.14911057 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.146      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0717    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0679    |
|    value_loss           | 0.172      |
----------------------------------------
Ep done - 10000.
Eval num_timesteps=240000, episode_reward=0.42 +/- 0.89
Episode length: 30.12 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.42       |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.15156493 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.134      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0659    |
|    n_updates            | 325        |
|    policy_gradient_loss | -0.0683    |
|    value_loss           | 0.173      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.42
SELFPLAY: new best model, bumping up generation to 7
Ep done - 11000.
Eval num_timesteps=260000, episode_reward=0.27 +/- 0.95
Episode length: 30.05 +/- 0.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.265     |
| time/                   |           |
|    total_timesteps      | 260000    |
| train/                  |           |
|    approx_kl            | 0.1586812 |
|    clip_fraction        | 0.387     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.17     |
|    explained_variance   | 0.17      |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0647   |
|    n_updates            | 330       |
|    policy_gradient_loss | -0.0668   |
|    value_loss           | 0.172     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 8
Ep done - 12000.
Eval num_timesteps=280000, episode_reward=0.23 +/- 0.95
Episode length: 29.94 +/- 1.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.235      |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.14567995 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.118      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0828    |
|    n_updates            | 335        |
|    policy_gradient_loss | -0.0666    |
|    value_loss           | 0.173      |
----------------------------------------
Eval num_timesteps=300000, episode_reward=0.32 +/- 0.94
Episode length: 30.05 +/- 0.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.315      |
| time/                   |            |
|    total_timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.16689166 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.124      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0616    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0645    |
|    value_loss           | 0.175      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.315
SELFPLAY: new best model, bumping up generation to 9
Ep done - 13000.
Eval num_timesteps=320000, episode_reward=0.28 +/- 0.94
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.275      |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.16283007 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.129      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0748    |
|    n_updates            | 345        |
|    policy_gradient_loss | -0.0673    |
|    value_loss           | 0.171      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 10
Ep done - 14000.
Eval num_timesteps=340000, episode_reward=0.20 +/- 0.96
Episode length: 30.11 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.19502823 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.108      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.104     |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0678    |
|    value_loss           | 0.17       |
----------------------------------------
Ep done - 15000.
Eval num_timesteps=360000, episode_reward=0.23 +/- 0.95
Episode length: 30.04 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.235      |
| time/                   |            |
|    total_timesteps      | 360000     |
| train/                  |            |
|    approx_kl            | 0.17414033 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.103      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0759    |
|    n_updates            | 355        |
|    policy_gradient_loss | -0.0677    |
|    value_loss           | 0.17       |
----------------------------------------
Ep done - 16000.
Eval num_timesteps=380000, episode_reward=0.18 +/- 0.97
Episode length: 30.06 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.17582396 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.143      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0708    |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0668    |
|    value_loss           | 0.167      |
----------------------------------------
Ep done - 17000.
Eval num_timesteps=400000, episode_reward=0.24 +/- 0.96
Episode length: 30.01 +/- 0.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.245     |
| time/                   |           |
|    total_timesteps      | 400000    |
| train/                  |           |
|    approx_kl            | 0.1846855 |
|    clip_fraction        | 0.4       |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.12     |
|    explained_variance   | 0.0926    |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0732   |
|    n_updates            | 365       |
|    policy_gradient_loss | -0.0655   |
|    value_loss           | 0.169     |
---------------------------------------
Ep done - 18000.
Eval num_timesteps=420000, episode_reward=0.26 +/- 0.94
Episode length: 30.04 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.255      |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.18635811 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.159      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.081     |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0666    |
|    value_loss           | 0.165      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 11
Ep done - 19000.
Eval num_timesteps=440000, episode_reward=0.20 +/- 0.97
Episode length: 29.98 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.20396683 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.0915     |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0825    |
|    n_updates            | 375        |
|    policy_gradient_loss | -0.0657    |
|    value_loss           | 0.176      |
----------------------------------------
Eval num_timesteps=460000, episode_reward=0.23 +/- 0.96
Episode length: 30.05 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.17325255 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.123      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0594    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0658    |
|    value_loss           | 0.168      |
----------------------------------------
Ep done - 20000.
Eval num_timesteps=480000, episode_reward=0.12 +/- 0.98
Episode length: 29.93 +/- 1.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.12      |
| time/                   |           |
|    total_timesteps      | 480000    |
| train/                  |           |
|    approx_kl            | 0.2068069 |
|    clip_fraction        | 0.404     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.1      |
|    explained_variance   | 0.128     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0741   |
|    n_updates            | 385       |
|    policy_gradient_loss | -0.0662   |
|    value_loss           | 0.168     |
---------------------------------------
Ep done - 21000.
Eval num_timesteps=500000, episode_reward=0.12 +/- 0.97
Episode length: 29.96 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 500000     |
| train/                  |            |
|    approx_kl            | 0.18822293 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.152      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0868    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0662    |
|    value_loss           | 0.16       |
----------------------------------------
Ep done - 22000.
Eval num_timesteps=520000, episode_reward=0.18 +/- 0.96
Episode length: 30.07 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.18226187 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.185      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0751    |
|    n_updates            | 395        |
|    policy_gradient_loss | -0.0649    |
|    value_loss           | 0.163      |
----------------------------------------
Ep done - 23000.
Eval num_timesteps=540000, episode_reward=0.24 +/- 0.95
Episode length: 30.04 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.24      |
| time/                   |           |
|    total_timesteps      | 540000    |
| train/                  |           |
|    approx_kl            | 0.1831681 |
|    clip_fraction        | 0.405     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.11     |
|    explained_variance   | 0.196     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0731   |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.0649   |
|    value_loss           | 0.163     |
---------------------------------------
Ep done - 24000.
Eval num_timesteps=560000, episode_reward=0.38 +/- 0.91
Episode length: 30.06 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.375     |
| time/                   |           |
|    total_timesteps      | 560000    |
| train/                  |           |
|    approx_kl            | 0.1840362 |
|    clip_fraction        | 0.402     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.11     |
|    explained_variance   | 0.145     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.078    |
|    n_updates            | 405       |
|    policy_gradient_loss | -0.0654   |
|    value_loss           | 0.168     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.375
SELFPLAY: new best model, bumping up generation to 12
Ep done - 25000.
Eval num_timesteps=580000, episode_reward=0.21 +/- 0.97
Episode length: 30.02 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 580000     |
| train/                  |            |
|    approx_kl            | 0.18340802 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.065      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0785    |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0644    |
|    value_loss           | 0.176      |
----------------------------------------
Eval num_timesteps=600000, episode_reward=0.13 +/- 0.97
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.20326194 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.189      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0836    |
|    n_updates            | 415        |
|    policy_gradient_loss | -0.0654    |
|    value_loss           | 0.159      |
----------------------------------------
Ep done - 26000.
Eval num_timesteps=620000, episode_reward=0.32 +/- 0.93
Episode length: 30.06 +/- 0.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.315      |
| time/                   |            |
|    total_timesteps      | 620000     |
| train/                  |            |
|    approx_kl            | 0.19146006 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.12       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0738    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0654    |
|    value_loss           | 0.17       |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.315
SELFPLAY: new best model, bumping up generation to 13
Ep done - 27000.
Eval num_timesteps=640000, episode_reward=0.21 +/- 0.97
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 640000     |
| train/                  |            |
|    approx_kl            | 0.20238593 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.144      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0752    |
|    n_updates            | 425        |
|    policy_gradient_loss | -0.0649    |
|    value_loss           | 0.164      |
----------------------------------------
Ep done - 28000.
Eval num_timesteps=660000, episode_reward=0.21 +/- 0.97
Episode length: 30.07 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.215     |
| time/                   |           |
|    total_timesteps      | 660000    |
| train/                  |           |
|    approx_kl            | 0.2123015 |
|    clip_fraction        | 0.408     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08     |
|    explained_variance   | 0.16      |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0746   |
|    n_updates            | 430       |
|    policy_gradient_loss | -0.0642   |
|    value_loss           | 0.167     |
---------------------------------------
Ep done - 29000.
Eval num_timesteps=680000, episode_reward=0.24 +/- 0.96
Episode length: 30.04 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.24       |
| time/                   |            |
|    total_timesteps      | 680000     |
| train/                  |            |
|    approx_kl            | 0.21118128 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.189      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0576    |
|    n_updates            | 435        |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.164      |
----------------------------------------
Ep done - 30000.
Eval num_timesteps=700000, episode_reward=0.23 +/- 0.95
Episode length: 29.89 +/- 1.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 700000     |
| train/                  |            |
|    approx_kl            | 0.21668033 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.201      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0605    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.063     |
|    value_loss           | 0.167      |
----------------------------------------
Ep done - 31000.
Eval num_timesteps=720000, episode_reward=0.24 +/- 0.96
Episode length: 29.90 +/- 1.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.245      |
| time/                   |            |
|    total_timesteps      | 720000     |
| train/                  |            |
|    approx_kl            | 0.20386164 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.0992     |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0864    |
|    n_updates            | 445        |
|    policy_gradient_loss | -0.062     |
|    value_loss           | 0.176      |
----------------------------------------
Ep done - 32000.
Eval num_timesteps=740000, episode_reward=0.31 +/- 0.93
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.31       |
| time/                   |            |
|    total_timesteps      | 740000     |
| train/                  |            |
|    approx_kl            | 0.19564173 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.129      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0467    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0644    |
|    value_loss           | 0.173      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 14
Eval num_timesteps=760000, episode_reward=0.16 +/- 0.98
Episode length: 30.02 +/- 0.57
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.16     |
| time/                   |          |
|    total_timesteps      | 760000   |
| train/                  |          |
|    approx_kl            | 0.191685 |
|    clip_fraction        | 0.409    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.09    |
|    explained_variance   | 0.225    |
|    learning_rate        | 0.0001   |
|    loss                 | -0.0813  |
|    n_updates            | 455      |
|    policy_gradient_loss | -0.0647  |
|    value_loss           | 0.161    |
--------------------------------------
Ep done - 33000.
Eval num_timesteps=780000, episode_reward=0.23 +/- 0.96
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 780000     |
| train/                  |            |
|    approx_kl            | 0.21179147 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.198      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0885    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0646    |
|    value_loss           | 0.167      |
----------------------------------------
Ep done - 34000.
Eval num_timesteps=800000, episode_reward=0.28 +/- 0.95
Episode length: 30.07 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.275      |
| time/                   |            |
|    total_timesteps      | 800000     |
| train/                  |            |
|    approx_kl            | 0.21412775 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.244      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0788    |
|    n_updates            | 465        |
|    policy_gradient_loss | -0.0646    |
|    value_loss           | 0.16       |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 15
Ep done - 35000.
Eval num_timesteps=820000, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 820000     |
| train/                  |            |
|    approx_kl            | 0.19615918 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.186      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0771    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.064     |
|    value_loss           | 0.17       |
----------------------------------------
Ep done - 36000.
Eval num_timesteps=840000, episode_reward=0.15 +/- 0.98
Episode length: 30.03 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 840000     |
| train/                  |            |
|    approx_kl            | 0.20336309 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.149      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.061     |
|    n_updates            | 475        |
|    policy_gradient_loss | -0.0637    |
|    value_loss           | 0.174      |
----------------------------------------
Ep done - 37000.
Eval num_timesteps=860000, episode_reward=0.20 +/- 0.95
Episode length: 30.08 +/- 0.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
Ep done - 38000.
Eval num_timesteps=880000, episode_reward=0.27 +/- 0.95
Episode length: 30.07 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.265     |
| time/                   |           |
|    total_timesteps      | 880000    |
| train/                  |           |
|    approx_kl            | 0.2010628 |
|    clip_fraction        | 0.412     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08     |
|    explained_variance   | 0.205     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.061    |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.064    |
|    value_loss           | 0.17      |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 16
Eval num_timesteps=900000, episode_reward=0.20 +/- 0.97
Episode length: 29.95 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 900000     |
| train/                  |            |
|    approx_kl            | 0.19808665 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0492    |
|    n_updates            | 485        |
|    policy_gradient_loss | -0.065     |
|    value_loss           | 0.169      |
----------------------------------------
Ep done - 39000.
Eval num_timesteps=920000, episode_reward=0.23 +/- 0.96
Episode length: 30.05 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 920000     |
| train/                  |            |
|    approx_kl            | 0.19811948 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.2        |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0697    |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0642    |
|    value_loss           | 0.176      |
----------------------------------------
Ep done - 40000.
Eval num_timesteps=940000, episode_reward=0.34 +/- 0.91
Episode length: 29.96 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.34       |
| time/                   |            |
|    total_timesteps      | 940000     |
| train/                  |            |
|    approx_kl            | 0.20301561 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.135      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0622    |
|    n_updates            | 495        |
|    policy_gradient_loss | -0.0636    |
|    value_loss           | 0.178      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.34
SELFPLAY: new best model, bumping up generation to 17
Ep done - 41000.
Eval num_timesteps=960000, episode_reward=0.09 +/- 0.98
Episode length: 29.95 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.085      |
| time/                   |            |
|    total_timesteps      | 960000     |
| train/                  |            |
|    approx_kl            | 0.21123275 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.185      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0795    |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0647    |
|    value_loss           | 0.171      |
----------------------------------------
Ep done - 42000.
Eval num_timesteps=980000, episode_reward=0.29 +/- 0.93
Episode length: 30.05 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.295      |
| time/                   |            |
|    total_timesteps      | 980000     |
| train/                  |            |
|    approx_kl            | 0.20481522 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.125      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0607    |
|    n_updates            | 505        |
|    policy_gradient_loss | -0.0631    |
|    value_loss           | 0.184      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.295
SELFPLAY: new best model, bumping up generation to 18
Ep done - 43000.
Eval num_timesteps=1000000, episode_reward=0.24 +/- 0.96
Episode length: 30.01 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.245      |
| time/                   |            |
|    total_timesteps      | 1000000    |
| train/                  |            |
|    approx_kl            | 0.20906644 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.175      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.088     |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0632    |
|    value_loss           | 0.174      |
----------------------------------------
Ep done - 44000.
Eval num_timesteps=1020000, episode_reward=0.31 +/- 0.94
Episode length: 30.07 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.31       |
| time/                   |            |
|    total_timesteps      | 1020000    |
| train/                  |            |
|    approx_kl            | 0.21111795 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.157      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0457    |
|    n_updates            | 515        |
|    policy_gradient_loss | -0.0647    |
|    value_loss           | 0.181      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 19
Ep done - 45000.
Eval num_timesteps=1040000, episode_reward=0.20 +/- 0.95
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 1040000    |
| train/                  |            |
|    approx_kl            | 0.20392156 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.129      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0797    |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0617    |
|    value_loss           | 0.183      |
----------------------------------------
Eval num_timesteps=1060000, episode_reward=0.12 +/- 0.97
Episode length: 30.05 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.12      |
| time/                   |           |
|    total_timesteps      | 1060000   |
| train/                  |           |
|    approx_kl            | 0.2004921 |
|    clip_fraction        | 0.409     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.04     |
|    explained_variance   | 0.203     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.071    |
|    n_updates            | 525       |
|    policy_gradient_loss | -0.0631   |
|    value_loss           | 0.172     |
---------------------------------------
Ep done - 46000.
Eval num_timesteps=1080000, episode_reward=0.31 +/- 0.95
Episode length: 30.08 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.31       |
| time/                   |            |
|    total_timesteps      | 1080000    |
| train/                  |            |
|    approx_kl            | 0.20925331 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.103      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0645    |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0636    |
|    value_loss           | 0.18       |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 20
Ep done - 47000.
Eval num_timesteps=1100000, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 1100000    |
| train/                  |            |
|    approx_kl            | 0.20772298 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.193      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0706    |
|    n_updates            | 535        |
|    policy_gradient_loss | -0.0634    |
|    value_loss           | 0.184      |
----------------------------------------
Ep done - 48000.
Eval num_timesteps=1120000, episode_reward=0.33 +/- 0.93
Episode length: 30.04 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.325      |
| time/                   |            |
|    total_timesteps      | 1120000    |
| train/                  |            |
|    approx_kl            | 0.20859909 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.161      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0611    |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0631    |
|    value_loss           | 0.183      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.325
SELFPLAY: new best model, bumping up generation to 21
Ep done - 49000.
Eval num_timesteps=1140000, episode_reward=0.28 +/- 0.94
Episode length: 30.03 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.275     |
| time/                   |           |
|    total_timesteps      | 1140000   |
| train/                  |           |
|    approx_kl            | 0.2058138 |
|    clip_fraction        | 0.402     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.01     |
|    explained_variance   | 0.156     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0648   |
|    n_updates            | 545       |
|    policy_gradient_loss | -0.0628   |
|    value_loss           | 0.186     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 22
Ep done - 50000.
Eval num_timesteps=1160000, episode_reward=0.21 +/- 0.95
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.215      |
| time/                   |            |
|    total_timesteps      | 1160000    |
| train/                  |            |
|    approx_kl            | 0.20144379 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | 0.146      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0695    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.183      |
----------------------------------------
Ep done - 51000.
Eval num_timesteps=1180000, episode_reward=0.28 +/- 0.94
Episode length: 30.04 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.28       |
| time/                   |            |
|    total_timesteps      | 1180000    |
| train/                  |            |
|    approx_kl            | 0.20988016 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | 0.169      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0732    |
|    n_updates            | 555        |
|    policy_gradient_loss | -0.0618    |
|    value_loss           | 0.188      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 23
Eval num_timesteps=1200000, episode_reward=0.12 +/- 0.97
Episode length: 30.05 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 1200000    |
| train/                  |            |
|    approx_kl            | 0.22265014 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.995     |
|    explained_variance   | 0.221      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.056     |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0612    |
|    value_loss           | 0.179      |
----------------------------------------
Ep done - 52000.
Eval num_timesteps=1220000, episode_reward=0.34 +/- 0.94
Episode length: 30.10 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.345      |
| time/                   |            |
|    total_timesteps      | 1220000    |
| train/                  |            |
|    approx_kl            | 0.21590237 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.995     |
|    explained_variance   | 0.123      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0592    |
|    n_updates            | 565        |
|    policy_gradient_loss | -0.0596    |
|    value_loss           | 0.199      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.345
SELFPLAY: new best model, bumping up generation to 24
Ep done - 53000.
Eval num_timesteps=1240000, episode_reward=0.27 +/- 0.95
Episode length: 30.05 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 1240000    |
| train/                  |            |
|    approx_kl            | 0.20711963 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.995     |
|    explained_variance   | 0.187      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0679    |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.0613    |
|    value_loss           | 0.181      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 25
Ep done - 54000.
Eval num_timesteps=1260000, episode_reward=0.25 +/- 0.96
Episode length: 30.10 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.25       |
| time/                   |            |
|    total_timesteps      | 1260000    |
| train/                  |            |
|    approx_kl            | 0.21468544 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.987     |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0478    |
|    n_updates            | 575        |
|    policy_gradient_loss | -0.0615    |
|    value_loss           | 0.192      |
----------------------------------------
Ep done - 55000.
Eval num_timesteps=1280000, episode_reward=0.34 +/- 0.93
Episode length: 30.10 +/- 0.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.345     |
| time/                   |           |
|    total_timesteps      | 1280000   |
| train/                  |           |
|    approx_kl            | 0.2097139 |
|    clip_fraction        | 0.399     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.981    |
|    explained_variance   | 0.163     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0245   |
|    n_updates            | 580       |
|    policy_gradient_loss | -0.0603   |
|    value_loss           | 0.197     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.345
SELFPLAY: new best model, bumping up generation to 26
Ep done - 56000.
Eval num_timesteps=1300000, episode_reward=0.22 +/- 0.95
Episode length: 30.07 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.22      |
| time/                   |           |
|    total_timesteps      | 1300000   |
| train/                  |           |
|    approx_kl            | 0.2033349 |
|    clip_fraction        | 0.395     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.979    |
|    explained_variance   | 0.188     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0785   |
|    n_updates            | 585       |
|    policy_gradient_loss | -0.0608   |
|    value_loss           | 0.182     |
---------------------------------------
Ep done - 57000.
Eval num_timesteps=1320000, episode_reward=0.34 +/- 0.92
Episode length: 30.07 +/- 0.56
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30.1     |
|    mean_reward          | 0.34     |
| time/                   |          |
|    total_timesteps      | 1320000  |
| train/                  |          |
|    approx_kl            | 0.211382 |
|    clip_fraction        | 0.393    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.989   |
|    explained_variance   | 0.186    |
|    learning_rate        | 0.0001   |
|    loss                 | -0.0531  |
|    n_updates            | 590      |
|    policy_gradient_loss | -0.0601  |
|    value_loss           | 0.186    |
--------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.34
SELFPLAY: new best model, bumping up generation to 27
Ep done - 58000.
Eval num_timesteps=1340000, episode_reward=0.20 +/- 0.96
Episode length: 30.02 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 1340000    |
| train/                  |            |
|    approx_kl            | 0.20665646 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.972     |
|    explained_variance   | 0.169      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0644    |
|    n_updates            | 595        |
|    policy_gradient_loss | -0.0608    |
|    value_loss           | 0.187      |
----------------------------------------
Eval num_timesteps=1360000, episode_reward=0.18 +/- 0.97
Episode length: 30.05 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.18       |
| time/                   |            |
|    total_timesteps      | 1360000    |
| train/                  |            |
|    approx_kl            | 0.21421091 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.971     |
|    explained_variance   | 0.162      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0656    |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0611    |
|    value_loss           | 0.199      |
----------------------------------------
Ep done - 59000.
Eval num_timesteps=1380000, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.085      |
| time/                   |            |
|    total_timesteps      | 1380000    |
| train/                  |            |
|    approx_kl            | 0.22191997 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.977     |
|    explained_variance   | 0.217      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0679    |
|    n_updates            | 605        |
|    policy_gradient_loss | -0.0618    |
|    value_loss           | 0.192      |
----------------------------------------
Ep done - 60000.
Eval num_timesteps=1400000, episode_reward=0.23 +/- 0.96
Episode length: 30.05 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 1400000    |
| train/                  |            |
|    approx_kl            | 0.22472095 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.981     |
|    explained_variance   | 0.22       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0545    |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.195      |
----------------------------------------
Ep done - 61000.
Eval num_timesteps=1420000, episode_reward=0.26 +/- 0.95
Episode length: 29.88 +/- 2.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.255      |
| time/                   |            |
|    total_timesteps      | 1420000    |
| train/                  |            |
|    approx_kl            | 0.22634406 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.967     |
|    explained_variance   | 0.19       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0631    |
|    n_updates            | 615        |
|    policy_gradient_loss | -0.0612    |
|    value_loss           | 0.194      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 28
Ep done - 62000.
Eval num_timesteps=1440000, episode_reward=0.17 +/- 0.96
Episode length: 29.72 +/- 3.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.7       |
|    mean_reward          | 0.17       |
| time/                   |            |
|    total_timesteps      | 1440000    |
| train/                  |            |
|    approx_kl            | 0.20298824 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.989     |
|    explained_variance   | 0.2        |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0454    |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0602    |
|    value_loss           | 0.196      |
----------------------------------------
Ep done - 63000.
Eval num_timesteps=1460000, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.145      |
| time/                   |            |
|    total_timesteps      | 1460000    |
| train/                  |            |
|    approx_kl            | 0.21056008 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.988     |
|    explained_variance   | 0.195      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0579    |
|    n_updates            | 625        |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.201      |
----------------------------------------
Ep done - 64000.
Eval num_timesteps=1480000, episode_reward=0.12 +/- 0.96
Episode length: 30.05 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 1480000    |
| train/                  |            |
|    approx_kl            | 0.23391719 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.965     |
|    explained_variance   | 0.179      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0385    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0599    |
|    value_loss           | 0.196      |
----------------------------------------
Eval num_timesteps=1500000, episode_reward=0.30 +/- 0.92
Episode length: 30.05 +/- 0.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.305     |
| time/                   |           |
|    total_timesteps      | 1500000   |
| train/                  |           |
|    approx_kl            | 0.2650482 |
|    clip_fraction        | 0.403     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.958    |
|    explained_variance   | 0.214     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0543   |
|    n_updates            | 635       |
|    policy_gradient_loss | -0.0611   |
|    value_loss           | 0.197     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.305
SELFPLAY: new best model, bumping up generation to 29
Ep done - 65000.
Eval num_timesteps=1520000, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 1520000    |
| train/                  |            |
|    approx_kl            | 0.21987733 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.96      |
|    explained_variance   | 0.156      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0579    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0607    |
|    value_loss           | 0.204      |
----------------------------------------
Ep done - 66000.
Eval num_timesteps=1540000, episode_reward=0.19 +/- 0.97
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.19       |
| time/                   |            |
|    total_timesteps      | 1540000    |
| train/                  |            |
|    approx_kl            | 0.22208242 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.969     |
|    explained_variance   | 0.168      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0481    |
|    n_updates            | 645        |
|    policy_gradient_loss | -0.0613    |
|    value_loss           | 0.193      |
----------------------------------------
Ep done - 67000.
Eval num_timesteps=1560000, episode_reward=0.34 +/- 0.92
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.345      |
| time/                   |            |
|    total_timesteps      | 1560000    |
| train/                  |            |
|    approx_kl            | 0.22099057 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.971     |
|    explained_variance   | 0.174      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.045     |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0618    |
|    value_loss           | 0.203      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.345
SELFPLAY: new best model, bumping up generation to 30
Ep done - 68000.
Eval num_timesteps=1580000, episode_reward=0.18 +/- 0.98
Episode length: 29.98 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 1580000    |
| train/                  |            |
|    approx_kl            | 0.22026345 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.94      |
|    explained_variance   | 0.19       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0459    |
|    n_updates            | 655        |
|    policy_gradient_loss | -0.0623    |
|    value_loss           | 0.196      |
----------------------------------------
Ep done - 69000.
Eval num_timesteps=1600000, episode_reward=0.23 +/- 0.95
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 1600000    |
| train/                  |            |
|    approx_kl            | 0.30164438 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.929     |
|    explained_variance   | 0.195      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0597    |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0579    |
|    value_loss           | 0.192      |
----------------------------------------
Ep done - 70000.
Eval num_timesteps=1620000, episode_reward=0.28 +/- 0.95
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.285      |
| time/                   |            |
|    total_timesteps      | 1620000    |
| train/                  |            |
|    approx_kl            | 0.22747163 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.938     |
|    explained_variance   | 0.203      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0191    |
|    n_updates            | 665        |
|    policy_gradient_loss | -0.0571    |
|    value_loss           | 0.193      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 31
Ep done - 71000.
Eval num_timesteps=1640000, episode_reward=0.20 +/- 0.96
Episode length: 29.95 +/- 1.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.195     |
| time/                   |           |
|    total_timesteps      | 1640000   |
| train/                  |           |
|    approx_kl            | 0.2300607 |
|    clip_fraction        | 0.383     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.909    |
|    explained_variance   | 0.15      |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0459   |
|    n_updates            | 670       |
|    policy_gradient_loss | -0.0573   |
|    value_loss           | 0.191     |
---------------------------------------
Eval num_timesteps=1660000, episode_reward=0.15 +/- 0.98
Episode length: 30.05 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.15      |
| time/                   |           |
|    total_timesteps      | 1660000   |
| train/                  |           |
|    approx_kl            | 0.2311544 |
|    clip_fraction        | 0.39      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.929    |
|    explained_variance   | 0.121     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0333   |
|    n_updates            | 675       |
|    policy_gradient_loss | -0.0579   |
|    value_loss           | 0.197     |
---------------------------------------
Ep done - 72000.
Eval num_timesteps=1680000, episode_reward=0.04 +/- 0.97
Episode length: 29.97 +/- 0.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.04      |
| time/                   |           |
|    total_timesteps      | 1680000   |
| train/                  |           |
|    approx_kl            | 0.2355401 |
|    clip_fraction        | 0.391     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.914    |
|    explained_variance   | 0.145     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0229   |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.0586   |
|    value_loss           | 0.193     |
---------------------------------------
Ep done - 73000.
Eval num_timesteps=1700000, episode_reward=0.28 +/- 0.94
Episode length: 30.04 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.28       |
| time/                   |            |
|    total_timesteps      | 1700000    |
| train/                  |            |
|    approx_kl            | 0.23987103 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.918     |
|    explained_variance   | 0.172      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0417    |
|    n_updates            | 685        |
|    policy_gradient_loss | -0.0585    |
|    value_loss           | 0.202      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 32
Ep done - 74000.
Eval num_timesteps=1720000, episode_reward=0.17 +/- 0.98
Episode length: 30.07 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | 0.165    |
| time/              |          |
|    total_timesteps | 1720000  |
---------------------------------
Ep done - 75000.
Eval num_timesteps=1740000, episode_reward=0.28 +/- 0.95
Episode length: 30.08 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.285      |
| time/                   |            |
|    total_timesteps      | 1740000    |
| train/                  |            |
|    approx_kl            | 0.21639018 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.921     |
|    explained_variance   | 0.181      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0133    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0578    |
|    value_loss           | 0.196      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 33
Ep done - 76000.
Eval num_timesteps=1760000, episode_reward=0.15 +/- 0.97
Episode length: 30.03 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 1760000    |
| train/                  |            |
|    approx_kl            | 0.23047042 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.915     |
|    explained_variance   | 0.168      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0512    |
|    n_updates            | 695        |
|    policy_gradient_loss | -0.0586    |
|    value_loss           | 0.2        |
----------------------------------------
Ep done - 77000.
Eval num_timesteps=1780000, episode_reward=0.26 +/- 0.96
Episode length: 30.04 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.26       |
| time/                   |            |
|    total_timesteps      | 1780000    |
| train/                  |            |
|    approx_kl            | 0.22830573 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.916     |
|    explained_variance   | 0.194      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0203    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0589    |
|    value_loss           | 0.188      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 34
Eval num_timesteps=1800000, episode_reward=0.34 +/- 0.91
Episode length: 30.07 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.345      |
| time/                   |            |
|    total_timesteps      | 1800000    |
| train/                  |            |
|    approx_kl            | 0.22282481 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.908     |
|    explained_variance   | 0.158      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0444    |
|    n_updates            | 705        |
|    policy_gradient_loss | -0.0583    |
|    value_loss           | 0.199      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.345
SELFPLAY: new best model, bumping up generation to 35
Ep done - 78000.
Eval num_timesteps=1820000, episode_reward=0.25 +/- 0.95
Episode length: 30.08 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.25       |
| time/                   |            |
|    total_timesteps      | 1820000    |
| train/                  |            |
|    approx_kl            | 0.21956272 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.901     |
|    explained_variance   | 0.108      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0276    |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.0577    |
|    value_loss           | 0.201      |
----------------------------------------
Ep done - 79000.
Eval num_timesteps=1840000, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.095     |
| time/                   |           |
|    total_timesteps      | 1840000   |
| train/                  |           |
|    approx_kl            | 0.2240611 |
|    clip_fraction        | 0.392     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.916    |
|    explained_variance   | 0.179     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0381   |
|    n_updates            | 715       |
|    policy_gradient_loss | -0.0578   |
|    value_loss           | 0.195     |
---------------------------------------
Ep done - 80000.
Eval num_timesteps=1860000, episode_reward=0.17 +/- 0.98
Episode length: 30.00 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 1860000    |
| train/                  |            |
|    approx_kl            | 0.23541543 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.914     |
|    explained_variance   | 0.143      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.023     |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0581    |
|    value_loss           | 0.212      |
----------------------------------------
Ep done - 81000.
Eval num_timesteps=1880000, episode_reward=0.23 +/- 0.95
Episode length: 30.07 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.225     |
| time/                   |           |
|    total_timesteps      | 1880000   |
| train/                  |           |
|    approx_kl            | 0.2399631 |
|    clip_fraction        | 0.385     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.907    |
|    explained_variance   | 0.196     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0542   |
|    n_updates            | 725       |
|    policy_gradient_loss | -0.0573   |
|    value_loss           | 0.201     |
---------------------------------------
Ep done - 82000.
Eval num_timesteps=1900000, episode_reward=0.16 +/- 0.96
Episode length: 30.04 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.16       |
| time/                   |            |
|    total_timesteps      | 1900000    |
| train/                  |            |
|    approx_kl            | 0.24788523 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.896     |
|    explained_variance   | 0.208      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0177    |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.196      |
----------------------------------------
Ep done - 83000.
Eval num_timesteps=1920000, episode_reward=0.17 +/- 0.97
Episode length: 30.03 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 1920000    |
| train/                  |            |
|    approx_kl            | 0.23553595 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.892     |
|    explained_variance   | 0.239      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.04      |
|    n_updates            | 735        |
|    policy_gradient_loss | -0.058     |
|    value_loss           | 0.198      |
----------------------------------------
Ep done - 84000.
Eval num_timesteps=1940000, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 1.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 1940000    |
| train/                  |            |
|    approx_kl            | 0.24912688 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.885     |
|    explained_variance   | 0.194      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.065     |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0588    |
|    value_loss           | 0.211      |
----------------------------------------
Eval num_timesteps=1960000, episode_reward=0.17 +/- 0.97
Episode length: 29.98 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.175     |
| time/                   |           |
|    total_timesteps      | 1960000   |
| train/                  |           |
|    approx_kl            | 0.2457666 |
|    clip_fraction        | 0.386     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.89     |
|    explained_variance   | 0.207     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0329   |
|    n_updates            | 745       |
|    policy_gradient_loss | -0.0586   |
|    value_loss           | 0.206     |
---------------------------------------
Ep done - 85000.
Eval num_timesteps=1980000, episode_reward=0.08 +/- 0.99
Episode length: 29.95 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 1980000    |
| train/                  |            |
|    approx_kl            | 0.23066211 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.883     |
|    explained_variance   | 0.223      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0379    |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0585    |
|    value_loss           | 0.205      |
----------------------------------------
Ep done - 86000.
Eval num_timesteps=2000000, episode_reward=0.21 +/- 0.96
Episode length: 30.07 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 2000000    |
| train/                  |            |
|    approx_kl            | 0.24073124 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.883     |
|    explained_variance   | 0.168      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0267    |
|    n_updates            | 755        |
|    policy_gradient_loss | -0.0584    |
|    value_loss           | 0.212      |
----------------------------------------
Ep done - 87000.
Eval num_timesteps=2020000, episode_reward=0.12 +/- 0.97
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 2020000    |
| train/                  |            |
|    approx_kl            | 0.23034784 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.886     |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0125    |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0582    |
|    value_loss           | 0.201      |
----------------------------------------
Ep done - 88000.
Eval num_timesteps=2040000, episode_reward=0.32 +/- 0.94
Episode length: 30.05 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.315      |
| time/                   |            |
|    total_timesteps      | 2040000    |
| train/                  |            |
|    approx_kl            | 0.27150822 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.878     |
|    explained_variance   | 0.211      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0049     |
|    n_updates            | 765        |
|    policy_gradient_loss | -0.0578    |
|    value_loss           | 0.197      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.315
SELFPLAY: new best model, bumping up generation to 36
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 184      |
|    iterations      | 100      |
|    time_elapsed    | 11124    |
|    total_timesteps | 2048000  |
---------------------------------
Ep done - 89000.
Eval num_timesteps=2060000, episode_reward=0.06 +/- 0.99
Episode length: 29.85 +/- 1.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.055     |
| time/                   |           |
|    total_timesteps      | 2060000   |
| train/                  |           |
|    approx_kl            | 0.2447731 |
|    clip_fraction        | 0.385     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.883    |
|    explained_variance   | 0.199     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.045    |
|    n_updates            | 770       |
|    policy_gradient_loss | -0.0577   |
|    value_loss           | 0.2       |
---------------------------------------
Ep done - 90000.
Eval num_timesteps=2080000, episode_reward=0.17 +/- 0.97
Episode length: 30.07 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.175      |
| time/                   |            |
|    total_timesteps      | 2080000    |
| train/                  |            |
|    approx_kl            | 0.24297452 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.891     |
|    explained_variance   | 0.209      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.043     |
|    n_updates            | 775        |
|    policy_gradient_loss | -0.0573    |
|    value_loss           | 0.201      |
----------------------------------------
Eval num_timesteps=2100000, episode_reward=0.27 +/- 0.95
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 2100000    |
| train/                  |            |
|    approx_kl            | 0.25154442 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.876     |
|    explained_variance   | 0.165      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0164    |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.203      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 37
Ep done - 91000.
Eval num_timesteps=2120000, episode_reward=0.23 +/- 0.96
Episode length: 30.08 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.235      |
| time/                   |            |
|    total_timesteps      | 2120000    |
| train/                  |            |
|    approx_kl            | 0.24174848 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.878     |
|    explained_variance   | 0.238      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0417    |
|    n_updates            | 785        |
|    policy_gradient_loss | -0.0557    |
|    value_loss           | 0.197      |
----------------------------------------
Ep done - 92000.
Eval num_timesteps=2140000, episode_reward=0.33 +/- 0.93
Episode length: 30.06 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.33       |
| time/                   |            |
|    total_timesteps      | 2140000    |
| train/                  |            |
|    approx_kl            | 0.26622766 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.873     |
|    explained_variance   | 0.194      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0621    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0584    |
|    value_loss           | 0.201      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.33
SELFPLAY: new best model, bumping up generation to 38
Ep done - 93000.
Eval num_timesteps=2160000, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 2160000    |
| train/                  |            |
|    approx_kl            | 0.23391576 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.88      |
|    explained_variance   | 0.171      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0389    |
|    n_updates            | 795        |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.198      |
----------------------------------------
Ep done - 94000.
Eval num_timesteps=2180000, episode_reward=0.20 +/- 0.97
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.195      |
| time/                   |            |
|    total_timesteps      | 2180000    |
| train/                  |            |
|    approx_kl            | 0.24115796 |
|    clip_fraction        | 0.389      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.88      |
|    explained_variance   | 0.155      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0253    |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0575    |
|    value_loss           | 0.206      |
----------------------------------------
Ep done - 95000.
Eval num_timesteps=2200000, episode_reward=0.16 +/- 0.98
Episode length: 30.02 +/- 0.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.16      |
| time/                   |           |
|    total_timesteps      | 2200000   |
| train/                  |           |
|    approx_kl            | 0.2558369 |
|    clip_fraction        | 0.388     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.875    |
|    explained_variance   | 0.187     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0447   |
|    n_updates            | 805       |
|    policy_gradient_loss | -0.0569   |
|    value_loss           | 0.195     |
---------------------------------------
Ep done - 96000.
Eval num_timesteps=2220000, episode_reward=0.25 +/- 0.96
Episode length: 30.12 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.25       |
| time/                   |            |
|    total_timesteps      | 2220000    |
| train/                  |            |
|    approx_kl            | 0.24053545 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.875     |
|    explained_variance   | 0.139      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0489    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.204      |
----------------------------------------
Ep done - 97000.
Eval num_timesteps=2240000, episode_reward=0.23 +/- 0.96
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 2240000    |
| train/                  |            |
|    approx_kl            | 0.25771704 |
|    clip_fraction        | 0.392      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.864     |
|    explained_variance   | 0.168      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0202    |
|    n_updates            | 815        |
|    policy_gradient_loss | -0.0569    |
|    value_loss           | 0.2        |
----------------------------------------
Eval num_timesteps=2260000, episode_reward=0.11 +/- 0.97
Episode length: 29.96 +/- 1.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 2260000    |
| train/                  |            |
|    approx_kl            | 0.24208307 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.866     |
|    explained_variance   | 0.197      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0465    |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.196      |
----------------------------------------
Ep done - 98000.
Eval num_timesteps=2280000, episode_reward=0.14 +/- 0.97
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 2280000    |
| train/                  |            |
|    approx_kl            | 0.25637612 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.856     |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0564    |
|    n_updates            | 825        |
|    policy_gradient_loss | -0.0558    |
|    value_loss           | 0.203      |
----------------------------------------
Ep done - 99000.
Eval num_timesteps=2300000, episode_reward=0.22 +/- 0.95
Episode length: 30.05 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.22       |
| time/                   |            |
|    total_timesteps      | 2300000    |
| train/                  |            |
|    approx_kl            | 0.30557084 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.838     |
|    explained_variance   | 0.15       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0243    |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.207      |
----------------------------------------
Ep done - 100000.
Eval num_timesteps=2320000, episode_reward=0.20 +/- 0.95
Episode length: 29.99 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 2320000    |
| train/                  |            |
|    approx_kl            | 0.26530194 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.831     |
|    explained_variance   | 0.182      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0393    |
|    n_updates            | 835        |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.193      |
----------------------------------------
Ep done - 101000.
Eval num_timesteps=2340000, episode_reward=0.23 +/- 0.96
Episode length: 30.08 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 2340000    |
| train/                  |            |
|    approx_kl            | 0.26762813 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.0873     |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0444    |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.209      |
----------------------------------------
Ep done - 102000.
Eval num_timesteps=2360000, episode_reward=0.25 +/- 0.96
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.25       |
| time/                   |            |
|    total_timesteps      | 2360000    |
| train/                  |            |
|    approx_kl            | 0.25588712 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.15       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0541    |
|    n_updates            | 845        |
|    policy_gradient_loss | -0.0553    |
|    value_loss           | 0.197      |
----------------------------------------
Ep done - 103000.
Eval num_timesteps=2380000, episode_reward=0.39 +/- 0.91
Episode length: 30.10 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.39       |
| time/                   |            |
|    total_timesteps      | 2380000    |
| train/                  |            |
|    approx_kl            | 0.24846967 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.129      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0429    |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0542    |
|    value_loss           | 0.192      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.39
SELFPLAY: new best model, bumping up generation to 39
Eval num_timesteps=2400000, episode_reward=0.30 +/- 0.94
Episode length: 30.05 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 2400000    |
| train/                  |            |
|    approx_kl            | 0.24519834 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.834     |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0244    |
|    n_updates            | 855        |
|    policy_gradient_loss | -0.0559    |
|    value_loss           | 0.195      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 40
Ep done - 104000.
Eval num_timesteps=2420000, episode_reward=0.09 +/- 0.99
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.085      |
| time/                   |            |
|    total_timesteps      | 2420000    |
| train/                  |            |
|    approx_kl            | 0.22047862 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.839     |
|    explained_variance   | 0.13       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0294    |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.206      |
----------------------------------------
Ep done - 105000.
Eval num_timesteps=2440000, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 2440000    |
| train/                  |            |
|    approx_kl            | 0.25009173 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.852     |
|    explained_variance   | 0.138      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0428    |
|    n_updates            | 865        |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.207      |
----------------------------------------
Ep done - 106000.
Eval num_timesteps=2460000, episode_reward=0.23 +/- 0.97
Episode length: 30.04 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.23      |
| time/                   |           |
|    total_timesteps      | 2460000   |
| train/                  |           |
|    approx_kl            | 0.2360971 |
|    clip_fraction        | 0.38      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.862    |
|    explained_variance   | 0.174     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0388   |
|    n_updates            | 870       |
|    policy_gradient_loss | -0.056    |
|    value_loss           | 0.201     |
---------------------------------------
Ep done - 107000.
Eval num_timesteps=2480000, episode_reward=0.33 +/- 0.93
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.325      |
| time/                   |            |
|    total_timesteps      | 2480000    |
| train/                  |            |
|    approx_kl            | 0.24236086 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.856     |
|    explained_variance   | 0.202      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0481    |
|    n_updates            | 875        |
|    policy_gradient_loss | -0.0577    |
|    value_loss           | 0.192      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.325
SELFPLAY: new best model, bumping up generation to 41
Ep done - 108000.
Eval num_timesteps=2500000, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 2500000    |
| train/                  |            |
|    approx_kl            | 0.22527528 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.858     |
|    explained_variance   | 0.135      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0448    |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.056     |
|    value_loss           | 0.204      |
----------------------------------------
Ep done - 109000.
Eval num_timesteps=2520000, episode_reward=0.13 +/- 0.97
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 2520000    |
| train/                  |            |
|    approx_kl            | 0.22996835 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.863     |
|    explained_variance   | 0.214      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0459    |
|    n_updates            | 885        |
|    policy_gradient_loss | -0.0583    |
|    value_loss           | 0.195      |
----------------------------------------
Ep done - 110000.
Eval num_timesteps=2540000, episode_reward=0.28 +/- 0.94
Episode length: 29.96 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.285      |
| time/                   |            |
|    total_timesteps      | 2540000    |
| train/                  |            |
|    approx_kl            | 0.23073287 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.844     |
|    explained_variance   | 0.168      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0458    |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0557    |
|    value_loss           | 0.201      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 42
Eval num_timesteps=2560000, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.165    |
| time/              |          |
|    total_timesteps | 2560000  |
---------------------------------
Ep done - 111000.
Eval num_timesteps=2580000, episode_reward=0.14 +/- 0.97
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.135      |
| time/                   |            |
|    total_timesteps      | 2580000    |
| train/                  |            |
|    approx_kl            | 0.23910496 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.853     |
|    explained_variance   | 0.138      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0377    |
|    n_updates            | 895        |
|    policy_gradient_loss | -0.0568    |
|    value_loss           | 0.205      |
----------------------------------------
Ep done - 112000.
Eval num_timesteps=2600000, episode_reward=0.23 +/- 0.96
Episode length: 30.03 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.225     |
| time/                   |           |
|    total_timesteps      | 2600000   |
| train/                  |           |
|    approx_kl            | 0.2451952 |
|    clip_fraction        | 0.372     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.842    |
|    explained_variance   | 0.145     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0489   |
|    n_updates            | 900       |
|    policy_gradient_loss | -0.0558   |
|    value_loss           | 0.209     |
---------------------------------------
Ep done - 113000.
Eval num_timesteps=2620000, episode_reward=0.20 +/- 0.97
Episode length: 30.07 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 2620000    |
| train/                  |            |
|    approx_kl            | 0.24346718 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.187      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0353    |
|    n_updates            | 905        |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.205      |
----------------------------------------
Ep done - 114000.
Eval num_timesteps=2640000, episode_reward=0.23 +/- 0.96
Episode length: 30.09 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 2640000    |
| train/                  |            |
|    approx_kl            | 0.23939213 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.827     |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0605    |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.207      |
----------------------------------------
Ep done - 115000.
Eval num_timesteps=2660000, episode_reward=0.27 +/- 0.96
Episode length: 30.05 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.265      |
| time/                   |            |
|    total_timesteps      | 2660000    |
| train/                  |            |
|    approx_kl            | 0.23756821 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.822     |
|    explained_variance   | 0.172      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0317    |
|    n_updates            | 915        |
|    policy_gradient_loss | -0.0563    |
|    value_loss           | 0.203      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 43
Ep done - 116000.
Eval num_timesteps=2680000, episode_reward=0.16 +/- 0.97
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.16       |
| time/                   |            |
|    total_timesteps      | 2680000    |
| train/                  |            |
|    approx_kl            | 0.24691424 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.174      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00297   |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0544    |
|    value_loss           | 0.213      |
----------------------------------------
Eval num_timesteps=2700000, episode_reward=0.36 +/- 0.92
Episode length: 30.07 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.36       |
| time/                   |            |
|    total_timesteps      | 2700000    |
| train/                  |            |
|    approx_kl            | 0.23343341 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.826     |
|    explained_variance   | 0.128      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0402    |
|    n_updates            | 925        |
|    policy_gradient_loss | -0.056     |
|    value_loss           | 0.206      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.36
SELFPLAY: new best model, bumping up generation to 44
Ep done - 117000.
Eval num_timesteps=2720000, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 2720000    |
| train/                  |            |
|    approx_kl            | 0.24316645 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.834     |
|    explained_variance   | 0.14       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0206    |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.204      |
----------------------------------------
Ep done - 118000.
Eval num_timesteps=2740000, episode_reward=0.18 +/- 0.96
Episode length: 30.04 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.18       |
| time/                   |            |
|    total_timesteps      | 2740000    |
| train/                  |            |
|    approx_kl            | 0.23979712 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.827     |
|    explained_variance   | 0.158      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0463    |
|    n_updates            | 935        |
|    policy_gradient_loss | -0.0558    |
|    value_loss           | 0.199      |
----------------------------------------
Ep done - 119000.
Eval num_timesteps=2760000, episode_reward=0.28 +/- 0.94
Episode length: 29.98 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.28       |
| time/                   |            |
|    total_timesteps      | 2760000    |
| train/                  |            |
|    approx_kl            | 0.23717733 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.821     |
|    explained_variance   | 0.173      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0552    |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0567    |
|    value_loss           | 0.201      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 45
Ep done - 120000.
Eval num_timesteps=2780000, episode_reward=0.18 +/- 0.97
Episode length: 30.00 +/- 0.56
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.185    |
| time/                   |          |
|    total_timesteps      | 2780000  |
| train/                  |          |
|    approx_kl            | 0.243982 |
|    clip_fraction        | 0.369    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.811   |
|    explained_variance   | 0.116    |
|    learning_rate        | 0.0001   |
|    loss                 | -0.0523  |
|    n_updates            | 945      |
|    policy_gradient_loss | -0.0554  |
|    value_loss           | 0.2      |
--------------------------------------
Ep done - 121000.
Eval num_timesteps=2800000, episode_reward=0.34 +/- 0.93
Episode length: 30.08 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.335      |
| time/                   |            |
|    total_timesteps      | 2800000    |
| train/                  |            |
|    approx_kl            | 0.23305467 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.806     |
|    explained_variance   | 0.178      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0488    |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.201      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.335
SELFPLAY: new best model, bumping up generation to 46
Ep done - 122000.
Eval num_timesteps=2820000, episode_reward=0.14 +/- 0.98
Episode length: 30.05 +/- 0.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.14      |
| time/                   |           |
|    total_timesteps      | 2820000   |
| train/                  |           |
|    approx_kl            | 0.2446731 |
|    clip_fraction        | 0.366     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.795    |
|    explained_variance   | 0.198     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0431   |
|    n_updates            | 955       |
|    policy_gradient_loss | -0.0556   |
|    value_loss           | 0.201     |
---------------------------------------
Ep done - 123000.
Eval num_timesteps=2840000, episode_reward=0.28 +/- 0.95
Episode length: 30.08 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.285     |
| time/                   |           |
|    total_timesteps      | 2840000   |
| train/                  |           |
|    approx_kl            | 0.2534743 |
|    clip_fraction        | 0.374     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.81     |
|    explained_variance   | 0.168     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0331   |
|    n_updates            | 960       |
|    policy_gradient_loss | -0.0553   |
|    value_loss           | 0.206     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 47
Eval num_timesteps=2860000, episode_reward=0.14 +/- 0.99
Episode length: 29.99 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.145     |
| time/                   |           |
|    total_timesteps      | 2860000   |
| train/                  |           |
|    approx_kl            | 0.2574759 |
|    clip_fraction        | 0.373     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.821    |
|    explained_variance   | 0.152     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0198   |
|    n_updates            | 965       |
|    policy_gradient_loss | -0.0546   |
|    value_loss           | 0.205     |
---------------------------------------
Ep done - 124000.
Eval num_timesteps=2880000, episode_reward=0.20 +/- 0.96
Episode length: 30.05 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.195      |
| time/                   |            |
|    total_timesteps      | 2880000    |
| train/                  |            |
|    approx_kl            | 0.23195195 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.822     |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00506   |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0563    |
|    value_loss           | 0.195      |
----------------------------------------
Ep done - 125000.
Eval num_timesteps=2900000, episode_reward=0.15 +/- 0.98
Episode length: 29.95 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 2900000    |
| train/                  |            |
|    approx_kl            | 0.23641357 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.823     |
|    explained_variance   | 0.149      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0505    |
|    n_updates            | 975        |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.201      |
----------------------------------------
Ep done - 126000.
Eval num_timesteps=2920000, episode_reward=0.17 +/- 0.96
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 2920000    |
| train/                  |            |
|    approx_kl            | 0.22953872 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.813     |
|    explained_variance   | 0.187      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0473    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.199      |
----------------------------------------
Ep done - 127000.
Eval num_timesteps=2940000, episode_reward=0.18 +/- 0.97
Episode length: 29.99 +/- 0.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.18      |
| time/                   |           |
|    total_timesteps      | 2940000   |
| train/                  |           |
|    approx_kl            | 0.2650162 |
|    clip_fraction        | 0.371     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.821    |
|    explained_variance   | 0.18      |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0469   |
|    n_updates            | 985       |
|    policy_gradient_loss | -0.0571   |
|    value_loss           | 0.208     |
---------------------------------------
Ep done - 128000.
Eval num_timesteps=2960000, episode_reward=0.12 +/- 0.97
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 2960000    |
| train/                  |            |
|    approx_kl            | 0.26999086 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.806     |
|    explained_variance   | 0.146      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0322    |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.211      |
----------------------------------------
Ep done - 129000.
Eval num_timesteps=2980000, episode_reward=0.21 +/- 0.96
Episode length: 30.03 +/- 0.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.21      |
| time/                   |           |
|    total_timesteps      | 2980000   |
| train/                  |           |
|    approx_kl            | 0.2550231 |
|    clip_fraction        | 0.374     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.808    |
|    explained_variance   | 0.186     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0412   |
|    n_updates            | 995       |
|    policy_gradient_loss | -0.0553   |
|    value_loss           | 0.2       |
---------------------------------------
Eval num_timesteps=3000000, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.065      |
| time/                   |            |
|    total_timesteps      | 3000000    |
| train/                  |            |
|    approx_kl            | 0.24445906 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.796     |
|    explained_variance   | 0.134      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0446    |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.209      |
----------------------------------------
Ep done - 130000.
Eval num_timesteps=3020000, episode_reward=0.27 +/- 0.95
Episode length: 30.08 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 3020000    |
| train/                  |            |
|    approx_kl            | 0.23664021 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.789     |
|    explained_variance   | 0.175      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.04      |
|    n_updates            | 1005       |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.198      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 48
Ep done - 131000.
Eval num_timesteps=3040000, episode_reward=0.21 +/- 0.97
Episode length: 30.04 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.21      |
| time/                   |           |
|    total_timesteps      | 3040000   |
| train/                  |           |
|    approx_kl            | 0.2393587 |
|    clip_fraction        | 0.366     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.803    |
|    explained_variance   | 0.177     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.028    |
|    n_updates            | 1010      |
|    policy_gradient_loss | -0.054    |
|    value_loss           | 0.204     |
---------------------------------------
Ep done - 132000.
Eval num_timesteps=3060000, episode_reward=0.19 +/- 0.96
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.19       |
| time/                   |            |
|    total_timesteps      | 3060000    |
| train/                  |            |
|    approx_kl            | 0.24915421 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.127      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0418    |
|    n_updates            | 1015       |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.213      |
----------------------------------------
Ep done - 133000.
Eval num_timesteps=3080000, episode_reward=0.13 +/- 0.98
Episode length: 30.00 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 3080000    |
| train/                  |            |
|    approx_kl            | 0.25391755 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.145      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0283    |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.216      |
----------------------------------------
Ep done - 134000.
Eval num_timesteps=3100000, episode_reward=0.32 +/- 0.94
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.32       |
| time/                   |            |
|    total_timesteps      | 3100000    |
| train/                  |            |
|    approx_kl            | 0.26003486 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.763     |
|    explained_variance   | 0.192      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0172    |
|    n_updates            | 1025       |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.21       |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.32
SELFPLAY: new best model, bumping up generation to 49
Ep done - 135000.
Eval num_timesteps=3120000, episode_reward=0.26 +/- 0.95
Episode length: 29.96 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.255      |
| time/                   |            |
|    total_timesteps      | 3120000    |
| train/                  |            |
|    approx_kl            | 0.24749728 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.21       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0495    |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.209      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 50
Ep done - 136000.
Eval num_timesteps=3140000, episode_reward=0.15 +/- 0.97
Episode length: 30.02 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 3140000    |
| train/                  |            |
|    approx_kl            | 0.25675303 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.774     |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0037     |
|    n_updates            | 1035       |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.214      |
----------------------------------------
Eval num_timesteps=3160000, episode_reward=0.14 +/- 0.96
Episode length: 30.05 +/- 0.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.14      |
| time/                   |           |
|    total_timesteps      | 3160000   |
| train/                  |           |
|    approx_kl            | 0.2466782 |
|    clip_fraction        | 0.362     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.771    |
|    explained_variance   | 0.178     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0225   |
|    n_updates            | 1040      |
|    policy_gradient_loss | -0.0546   |
|    value_loss           | 0.205     |
---------------------------------------
Ep done - 137000.
Eval num_timesteps=3180000, episode_reward=0.27 +/- 0.95
Episode length: 30.06 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 3180000    |
| train/                  |            |
|    approx_kl            | 0.26152024 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.764     |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0141    |
|    n_updates            | 1045       |
|    policy_gradient_loss | -0.0527    |
|    value_loss           | 0.209      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 51
Ep done - 138000.
Eval num_timesteps=3200000, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 3200000    |
| train/                  |            |
|    approx_kl            | 0.25326806 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.782     |
|    explained_variance   | 0.0964     |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00993   |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0545    |
|    value_loss           | 0.217      |
----------------------------------------
Ep done - 139000.
Eval num_timesteps=3220000, episode_reward=0.21 +/- 0.96
Episode length: 29.97 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 3220000    |
| train/                  |            |
|    approx_kl            | 0.24654849 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.175      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0322    |
|    n_updates            | 1055       |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.206      |
----------------------------------------
Ep done - 140000.
Eval num_timesteps=3240000, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 3240000    |
| train/                  |            |
|    approx_kl            | 0.24668424 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.143      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0169    |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0526    |
|    value_loss           | 0.208      |
----------------------------------------
Ep done - 141000.
Eval num_timesteps=3260000, episode_reward=0.18 +/- 0.96
Episode length: 29.98 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.185     |
| time/                   |           |
|    total_timesteps      | 3260000   |
| train/                  |           |
|    approx_kl            | 0.2776738 |
|    clip_fraction        | 0.367     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.763    |
|    explained_variance   | 0.218     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0205   |
|    n_updates            | 1065      |
|    policy_gradient_loss | -0.0556   |
|    value_loss           | 0.206     |
---------------------------------------
Ep done - 142000.
Eval num_timesteps=3280000, episode_reward=0.11 +/- 0.98
Episode length: 29.96 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 3280000    |
| train/                  |            |
|    approx_kl            | 0.24859205 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.754     |
|    explained_variance   | 0.146      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00263   |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.213      |
----------------------------------------
Eval num_timesteps=3300000, episode_reward=0.15 +/- 0.98
Episode length: 30.05 +/- 0.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.155     |
| time/                   |           |
|    total_timesteps      | 3300000   |
| train/                  |           |
|    approx_kl            | 0.2571044 |
|    clip_fraction        | 0.364     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.762    |
|    explained_variance   | 0.164     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0348   |
|    n_updates            | 1075      |
|    policy_gradient_loss | -0.0536   |
|    value_loss           | 0.21      |
---------------------------------------
Ep done - 143000.
Eval num_timesteps=3320000, episode_reward=0.14 +/- 0.98
Episode length: 30.07 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 3320000    |
| train/                  |            |
|    approx_kl            | 0.25861096 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.768     |
|    explained_variance   | 0.155      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0168    |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.205      |
----------------------------------------
Ep done - 144000.
Eval num_timesteps=3340000, episode_reward=0.27 +/- 0.95
Episode length: 30.09 +/- 0.52
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30.1     |
|    mean_reward          | 0.27     |
| time/                   |          |
|    total_timesteps      | 3340000  |
| train/                  |          |
|    approx_kl            | 0.257619 |
|    clip_fraction        | 0.368    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.769   |
|    explained_variance   | 0.198    |
|    learning_rate        | 0.0001   |
|    loss                 | -0.0111  |
|    n_updates            | 1085     |
|    policy_gradient_loss | -0.0542  |
|    value_loss           | 0.204    |
--------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 52
Ep done - 145000.
Eval num_timesteps=3360000, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.145      |
| time/                   |            |
|    total_timesteps      | 3360000    |
| train/                  |            |
|    approx_kl            | 0.25071645 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.776     |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.056     |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.208      |
----------------------------------------
Ep done - 146000.
Eval num_timesteps=3380000, episode_reward=0.25 +/- 0.96
Episode length: 30.01 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.25       |
| time/                   |            |
|    total_timesteps      | 3380000    |
| train/                  |            |
|    approx_kl            | 0.26809782 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.775     |
|    explained_variance   | 0.193      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00437   |
|    n_updates            | 1095       |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.207      |
----------------------------------------
Ep done - 147000.
Eval num_timesteps=3400000, episode_reward=0.16 +/- 0.96
Episode length: 29.98 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.16      |
| time/                   |           |
|    total_timesteps      | 3400000   |
| train/                  |           |
|    approx_kl            | 0.2523025 |
|    clip_fraction        | 0.364     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.777    |
|    explained_variance   | 0.192     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0448   |
|    n_updates            | 1100      |
|    policy_gradient_loss | -0.0544   |
|    value_loss           | 0.206     |
---------------------------------------
Ep done - 148000.
Eval num_timesteps=3420000, episode_reward=0.26 +/- 0.95
Episode length: 30.01 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.26     |
| time/              |          |
|    total_timesteps | 3420000  |
---------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 53
Ep done - 149000.
Eval num_timesteps=3440000, episode_reward=0.15 +/- 0.97
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 3440000    |
| train/                  |            |
|    approx_kl            | 0.27128214 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.771     |
|    explained_variance   | 0.223      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0462    |
|    n_updates            | 1105       |
|    policy_gradient_loss | -0.0545    |
|    value_loss           | 0.204      |
----------------------------------------
Eval num_timesteps=3460000, episode_reward=0.08 +/- 0.99
Episode length: 30.03 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 3460000    |
| train/                  |            |
|    approx_kl            | 0.24141705 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.773     |
|    explained_variance   | 0.149      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.033     |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.215      |
----------------------------------------
Ep done - 150000.
Eval num_timesteps=3480000, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 3480000    |
| train/                  |            |
|    approx_kl            | 0.27055985 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.774     |
|    explained_variance   | 0.17       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0459    |
|    n_updates            | 1115       |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.22       |
----------------------------------------
Ep done - 151000.
Eval num_timesteps=3500000, episode_reward=0.20 +/- 0.97
Episode length: 30.07 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 3500000    |
| train/                  |            |
|    approx_kl            | 0.25980496 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.782     |
|    explained_variance   | 0.172      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0204    |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0549    |
|    value_loss           | 0.215      |
----------------------------------------
Ep done - 152000.
Eval num_timesteps=3520000, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.095      |
| time/                   |            |
|    total_timesteps      | 3520000    |
| train/                  |            |
|    approx_kl            | 0.26219115 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.775     |
|    explained_variance   | 0.181      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.025     |
|    n_updates            | 1125       |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.209      |
----------------------------------------
Ep done - 153000.
Eval num_timesteps=3540000, episode_reward=0.17 +/- 0.98
Episode length: 30.00 +/- 0.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.165     |
| time/                   |           |
|    total_timesteps      | 3540000   |
| train/                  |           |
|    approx_kl            | 0.2758278 |
|    clip_fraction        | 0.372     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.787    |
|    explained_variance   | 0.163     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0533   |
|    n_updates            | 1130      |
|    policy_gradient_loss | -0.055    |
|    value_loss           | 0.205     |
---------------------------------------
Ep done - 154000.
Eval num_timesteps=3560000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.115     |
| time/                   |           |
|    total_timesteps      | 3560000   |
| train/                  |           |
|    approx_kl            | 0.2609559 |
|    clip_fraction        | 0.373     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.773    |
|    explained_variance   | 0.13      |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0198   |
|    n_updates            | 1135      |
|    policy_gradient_loss | -0.0536   |
|    value_loss           | 0.214     |
---------------------------------------
Ep done - 155000.
Eval num_timesteps=3580000, episode_reward=0.15 +/- 0.97
Episode length: 30.01 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 3580000    |
| train/                  |            |
|    approx_kl            | 0.29999933 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.771     |
|    explained_variance   | 0.209      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.00402    |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.205      |
----------------------------------------
Eval num_timesteps=3600000, episode_reward=0.12 +/- 0.97
Episode length: 30.03 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 3600000    |
| train/                  |            |
|    approx_kl            | 0.28296927 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.036     |
|    n_updates            | 1145       |
|    policy_gradient_loss | -0.0563    |
|    value_loss           | 0.207      |
----------------------------------------
Ep done - 156000.
Eval num_timesteps=3620000, episode_reward=0.41 +/- 0.90
Episode length: 30.11 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.405      |
| time/                   |            |
|    total_timesteps      | 3620000    |
| train/                  |            |
|    approx_kl            | 0.27972317 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.152      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0204    |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.207      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.405
SELFPLAY: new best model, bumping up generation to 54
Ep done - 157000.
Eval num_timesteps=3640000, episode_reward=0.17 +/- 0.97
Episode length: 30.01 +/- 0.50
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.17     |
| time/                   |          |
|    total_timesteps      | 3640000  |
| train/                  |          |
|    approx_kl            | 0.291394 |
|    clip_fraction        | 0.372    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.772   |
|    explained_variance   | 0.166    |
|    learning_rate        | 0.0001   |
|    loss                 | -0.047   |
|    n_updates            | 1155     |
|    policy_gradient_loss | -0.0544  |
|    value_loss           | 0.211    |
--------------------------------------
Ep done - 158000.
Eval num_timesteps=3660000, episode_reward=0.20 +/- 0.96
Episode length: 30.03 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 3660000    |
| train/                  |            |
|    approx_kl            | 0.27588537 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.779     |
|    explained_variance   | 0.216      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0354    |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.202      |
----------------------------------------
Ep done - 159000.
Eval num_timesteps=3680000, episode_reward=0.33 +/- 0.92
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.325      |
| time/                   |            |
|    total_timesteps      | 3680000    |
| train/                  |            |
|    approx_kl            | 0.25699127 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.128      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.027     |
|    n_updates            | 1165       |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.217      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.325
SELFPLAY: new best model, bumping up generation to 55
Ep done - 160000.
Eval num_timesteps=3700000, episode_reward=0.17 +/- 0.97
Episode length: 30.03 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.17      |
| time/                   |           |
|    total_timesteps      | 3700000   |
| train/                  |           |
|    approx_kl            | 0.2866446 |
|    clip_fraction        | 0.373     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.771    |
|    explained_variance   | 0.149     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0567   |
|    n_updates            | 1170      |
|    policy_gradient_loss | -0.0561   |
|    value_loss           | 0.206     |
---------------------------------------
Ep done - 161000.
Eval num_timesteps=3720000, episode_reward=0.20 +/- 0.97
Episode length: 30.04 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 3720000    |
| train/                  |            |
|    approx_kl            | 0.25975573 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.158      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0272    |
|    n_updates            | 1175       |
|    policy_gradient_loss | -0.0549    |
|    value_loss           | 0.207      |
----------------------------------------
Ep done - 162000.
Eval num_timesteps=3740000, episode_reward=0.24 +/- 0.96
Episode length: 30.06 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.245      |
| time/                   |            |
|    total_timesteps      | 3740000    |
| train/                  |            |
|    approx_kl            | 0.25848794 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.773     |
|    explained_variance   | 0.144      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.041     |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.0555    |
|    value_loss           | 0.214      |
----------------------------------------
Eval num_timesteps=3760000, episode_reward=0.20 +/- 0.97
Episode length: 30.09 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 3760000    |
| train/                  |            |
|    approx_kl            | 0.25434795 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.162      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0199    |
|    n_updates            | 1185       |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.212      |
----------------------------------------
Ep done - 163000.
Eval num_timesteps=3780000, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.14      |
| time/                   |           |
|    total_timesteps      | 3780000   |
| train/                  |           |
|    approx_kl            | 0.2542441 |
|    clip_fraction        | 0.365     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.768    |
|    explained_variance   | 0.165     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0382   |
|    n_updates            | 1190      |
|    policy_gradient_loss | -0.055    |
|    value_loss           | 0.212     |
---------------------------------------
Ep done - 164000.
Eval num_timesteps=3800000, episode_reward=0.25 +/- 0.95
Episode length: 30.10 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.25       |
| time/                   |            |
|    total_timesteps      | 3800000    |
| train/                  |            |
|    approx_kl            | 0.27025062 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.762     |
|    explained_variance   | 0.192      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0317    |
|    n_updates            | 1195       |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.209      |
----------------------------------------
Ep done - 165000.
Eval num_timesteps=3820000, episode_reward=0.21 +/- 0.96
Episode length: 30.09 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.215      |
| time/                   |            |
|    total_timesteps      | 3820000    |
| train/                  |            |
|    approx_kl            | 0.24984105 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.172      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0125    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0542    |
|    value_loss           | 0.21       |
----------------------------------------
Ep done - 166000.
Eval num_timesteps=3840000, episode_reward=0.23 +/- 0.96
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 3840000    |
| train/                  |            |
|    approx_kl            | 0.25883493 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.199      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0167    |
|    n_updates            | 1205       |
|    policy_gradient_loss | -0.0545    |
|    value_loss           | 0.206      |
----------------------------------------
Ep done - 167000.
Eval num_timesteps=3860000, episode_reward=0.14 +/- 0.99
Episode length: 29.95 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 3860000    |
| train/                  |            |
|    approx_kl            | 0.24801393 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.129      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00318   |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.222      |
----------------------------------------
Ep done - 168000.
Eval num_timesteps=3880000, episode_reward=0.23 +/- 0.96
Episode length: 30.09 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.23      |
| time/                   |           |
|    total_timesteps      | 3880000   |
| train/                  |           |
|    approx_kl            | 0.2786721 |
|    clip_fraction        | 0.368     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.748    |
|    explained_variance   | 0.162     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.021    |
|    n_updates            | 1215      |
|    policy_gradient_loss | -0.0561   |
|    value_loss           | 0.215     |
---------------------------------------
Eval num_timesteps=3900000, episode_reward=0.24 +/- 0.96
Episode length: 30.14 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.245      |
| time/                   |            |
|    total_timesteps      | 3900000    |
| train/                  |            |
|    approx_kl            | 0.26722804 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.742     |
|    explained_variance   | 0.242      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0321    |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0555    |
|    value_loss           | 0.216      |
----------------------------------------
Ep done - 169000.
Eval num_timesteps=3920000, episode_reward=0.17 +/- 0.98
Episode length: 29.99 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.17      |
| time/                   |           |
|    total_timesteps      | 3920000   |
| train/                  |           |
|    approx_kl            | 0.2541925 |
|    clip_fraction        | 0.363     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.74     |
|    explained_variance   | 0.208     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.00203  |
|    n_updates            | 1225      |
|    policy_gradient_loss | -0.0552   |
|    value_loss           | 0.223     |
---------------------------------------
Ep done - 170000.
Eval num_timesteps=3940000, episode_reward=0.26 +/- 0.96
Episode length: 30.08 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.255      |
| time/                   |            |
|    total_timesteps      | 3940000    |
| train/                  |            |
|    approx_kl            | 0.24989863 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.734     |
|    explained_variance   | 0.25       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0202    |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0567    |
|    value_loss           | 0.217      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 56
Ep done - 171000.
Eval num_timesteps=3960000, episode_reward=0.04 +/- 0.98
Episode length: 29.93 +/- 0.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.04      |
| time/                   |           |
|    total_timesteps      | 3960000   |
| train/                  |           |
|    approx_kl            | 0.2487156 |
|    clip_fraction        | 0.363     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.74     |
|    explained_variance   | 0.258     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0364   |
|    n_updates            | 1235      |
|    policy_gradient_loss | -0.057    |
|    value_loss           | 0.222     |
---------------------------------------
Ep done - 172000.
Eval num_timesteps=3980000, episode_reward=0.15 +/- 0.98
Episode length: 30.00 +/- 0.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 3980000    |
| train/                  |            |
|    approx_kl            | 0.24322605 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.265      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0358    |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 173000.
Eval num_timesteps=4000000, episode_reward=0.15 +/- 0.98
Episode length: 30.04 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 4000000    |
| train/                  |            |
|    approx_kl            | 0.24163428 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.734     |
|    explained_variance   | 0.265      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0381    |
|    n_updates            | 1245       |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.217      |
----------------------------------------
Ep done - 174000.
Eval num_timesteps=4020000, episode_reward=0.26 +/- 0.96
Episode length: 30.00 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.26      |
| time/                   |           |
|    total_timesteps      | 4020000   |
| train/                  |           |
|    approx_kl            | 0.2714278 |
|    clip_fraction        | 0.359     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.728    |
|    explained_variance   | 0.257     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0173   |
|    n_updates            | 1250      |
|    policy_gradient_loss | -0.0561   |
|    value_loss           | 0.224     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 57
Ep done - 175000.
Eval num_timesteps=4040000, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.105     |
| time/                   |           |
|    total_timesteps      | 4040000   |
| train/                  |           |
|    approx_kl            | 0.2545273 |
|    clip_fraction        | 0.367     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.737    |
|    explained_variance   | 0.252     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0146   |
|    n_updates            | 1255      |
|    policy_gradient_loss | -0.056    |
|    value_loss           | 0.223     |
---------------------------------------
Eval num_timesteps=4060000, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.145     |
| time/                   |           |
|    total_timesteps      | 4060000   |
| train/                  |           |
|    approx_kl            | 0.2601063 |
|    clip_fraction        | 0.356     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.738    |
|    explained_variance   | 0.228     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.00543   |
|    n_updates            | 1260      |
|    policy_gradient_loss | -0.0549   |
|    value_loss           | 0.231     |
---------------------------------------
Ep done - 176000.
Eval num_timesteps=4080000, episode_reward=0.12 +/- 0.97
Episode length: 29.99 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 4080000    |
| train/                  |            |
|    approx_kl            | 0.24546456 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.734     |
|    explained_variance   | 0.248      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0256    |
|    n_updates            | 1265       |
|    policy_gradient_loss | -0.0571    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 177000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 200      |
|    time_elapsed    | 24630    |
|    total_timesteps | 4096000  |
---------------------------------
Eval num_timesteps=4100000, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 4100000    |
| train/                  |            |
|    approx_kl            | 0.25952202 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.731     |
|    explained_variance   | 0.234      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0247    |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 178000.
Eval num_timesteps=4120000, episode_reward=0.17 +/- 0.98
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.175      |
| time/                   |            |
|    total_timesteps      | 4120000    |
| train/                  |            |
|    approx_kl            | 0.26842207 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.257      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0047    |
|    n_updates            | 1275       |
|    policy_gradient_loss | -0.0555    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 179000.
Eval num_timesteps=4140000, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.145     |
| time/                   |           |
|    total_timesteps      | 4140000   |
| train/                  |           |
|    approx_kl            | 0.2650463 |
|    clip_fraction        | 0.361     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.738    |
|    explained_variance   | 0.227     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.00348  |
|    n_updates            | 1280      |
|    policy_gradient_loss | -0.0549   |
|    value_loss           | 0.233     |
---------------------------------------
Ep done - 180000.
Eval num_timesteps=4160000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 4160000    |
| train/                  |            |
|    approx_kl            | 0.27014208 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.731     |
|    explained_variance   | 0.306      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.004     |
|    n_updates            | 1285       |
|    policy_gradient_loss | -0.0568    |
|    value_loss           | 0.211      |
----------------------------------------
Ep done - 181000.
Eval num_timesteps=4180000, episode_reward=0.21 +/- 0.97
Episode length: 30.02 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.215      |
| time/                   |            |
|    total_timesteps      | 4180000    |
| train/                  |            |
|    approx_kl            | 0.24697009 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.265      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.00662    |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.224      |
----------------------------------------
Eval num_timesteps=4200000, episode_reward=0.12 +/- 0.99
Episode length: 30.09 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 4200000    |
| train/                  |            |
|    approx_kl            | 0.24997887 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.278      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0321    |
|    n_updates            | 1295       |
|    policy_gradient_loss | -0.0555    |
|    value_loss           | 0.223      |
----------------------------------------
Ep done - 182000.
Eval num_timesteps=4220000, episode_reward=0.22 +/- 0.95
Episode length: 30.02 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.22       |
| time/                   |            |
|    total_timesteps      | 4220000    |
| train/                  |            |
|    approx_kl            | 0.24640246 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.278      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0386    |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.0527    |
|    value_loss           | 0.222      |
----------------------------------------
Ep done - 183000.
Eval num_timesteps=4240000, episode_reward=0.21 +/- 0.96
Episode length: 30.02 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 4240000    |
| train/                  |            |
|    approx_kl            | 0.26640853 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.741     |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00663   |
|    n_updates            | 1305       |
|    policy_gradient_loss | -0.056     |
|    value_loss           | 0.225      |
----------------------------------------
Ep done - 184000.
Eval num_timesteps=4260000, episode_reward=0.18 +/- 0.96
Episode length: 30.09 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.18       |
| time/                   |            |
|    total_timesteps      | 4260000    |
| train/                  |            |
|    approx_kl            | 0.24370742 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.21       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.015     |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0542    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 185000.
Eval num_timesteps=4280000, episode_reward=0.21 +/- 0.95
Episode length: 29.99 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.215    |
| time/              |          |
|    total_timesteps | 4280000  |
---------------------------------
Ep done - 186000.
Eval num_timesteps=4300000, episode_reward=0.40 +/- 0.91
Episode length: 30.04 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.395      |
| time/                   |            |
|    total_timesteps      | 4300000    |
| train/                  |            |
|    approx_kl            | 0.27447113 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.735     |
|    explained_variance   | 0.274      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0384     |
|    n_updates            | 1315       |
|    policy_gradient_loss | -0.0572    |
|    value_loss           | 0.219      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.395
SELFPLAY: new best model, bumping up generation to 58
Ep done - 187000.
Eval num_timesteps=4320000, episode_reward=0.27 +/- 0.95
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 4320000    |
| train/                  |            |
|    approx_kl            | 0.28793898 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.229      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00347   |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.235      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 59
Ep done - 188000.
Eval num_timesteps=4340000, episode_reward=0.17 +/- 0.98
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.17       |
| time/                   |            |
|    total_timesteps      | 4340000    |
| train/                  |            |
|    approx_kl            | 0.26477087 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.729     |
|    explained_variance   | 0.258      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0308    |
|    n_updates            | 1325       |
|    policy_gradient_loss | -0.0576    |
|    value_loss           | 0.22       |
----------------------------------------
Eval num_timesteps=4360000, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 4360000    |
| train/                  |            |
|    approx_kl            | 0.27255815 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.736     |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0238    |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0558    |
|    value_loss           | 0.218      |
----------------------------------------
Ep done - 189000.
Eval num_timesteps=4380000, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.60
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.14     |
| time/                   |          |
|    total_timesteps      | 4380000  |
| train/                  |          |
|    approx_kl            | 0.261568 |
|    clip_fraction        | 0.363    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.721   |
|    explained_variance   | 0.264    |
|    learning_rate        | 0.0001   |
|    loss                 | -0.0387  |
|    n_updates            | 1335     |
|    policy_gradient_loss | -0.0553  |
|    value_loss           | 0.224    |
--------------------------------------
Ep done - 190000.
Eval num_timesteps=4400000, episode_reward=0.23 +/- 0.94
Episode length: 29.99 +/- 0.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.235      |
| time/                   |            |
|    total_timesteps      | 4400000    |
| train/                  |            |
|    approx_kl            | 0.28150243 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.718     |
|    explained_variance   | 0.302      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0197    |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.212      |
----------------------------------------
Ep done - 191000.
Eval num_timesteps=4420000, episode_reward=0.21 +/- 0.96
Episode length: 29.97 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 4420000    |
| train/                  |            |
|    approx_kl            | 0.33154792 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.261      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0423    |
|    n_updates            | 1345       |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.222      |
----------------------------------------
Ep done - 192000.
Eval num_timesteps=4440000, episode_reward=0.31 +/- 0.94
Episode length: 30.07 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.31       |
| time/                   |            |
|    total_timesteps      | 4440000    |
| train/                  |            |
|    approx_kl            | 0.28116423 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0217    |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.224      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 60
Ep done - 193000.
Eval num_timesteps=4460000, episode_reward=0.23 +/- 0.96
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 4460000    |
| train/                  |            |
|    approx_kl            | 0.26777667 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.244      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.00688    |
|    n_updates            | 1355       |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 194000.
Eval num_timesteps=4480000, episode_reward=0.23 +/- 0.96
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 4480000    |
| train/                  |            |
|    approx_kl            | 0.29274052 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.254      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0288    |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.219      |
----------------------------------------
Eval num_timesteps=4500000, episode_reward=0.23 +/- 0.97
Episode length: 30.03 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 4500000    |
| train/                  |            |
|    approx_kl            | 0.28970855 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.293      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0234    |
|    n_updates            | 1365       |
|    policy_gradient_loss | -0.0545    |
|    value_loss           | 0.221      |
----------------------------------------
Ep done - 195000.
Eval num_timesteps=4520000, episode_reward=0.21 +/- 0.97
Episode length: 30.02 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.215      |
| time/                   |            |
|    total_timesteps      | 4520000    |
| train/                  |            |
|    approx_kl            | 0.27160987 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.295      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0234    |
|    n_updates            | 1370       |
|    policy_gradient_loss | -0.0535    |
|    value_loss           | 0.217      |
----------------------------------------
Ep done - 196000.
Eval num_timesteps=4540000, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.105      |
| time/                   |            |
|    total_timesteps      | 4540000    |
| train/                  |            |
|    approx_kl            | 0.25392896 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.726     |
|    explained_variance   | 0.305      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0272    |
|    n_updates            | 1375       |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.211      |
----------------------------------------
Ep done - 197000.
Eval num_timesteps=4560000, episode_reward=0.19 +/- 0.96
Episode length: 30.04 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.19       |
| time/                   |            |
|    total_timesteps      | 4560000    |
| train/                  |            |
|    approx_kl            | 0.27019173 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.266      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0164    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.223      |
----------------------------------------
Ep done - 198000.
Eval num_timesteps=4580000, episode_reward=0.24 +/- 0.96
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.24       |
| time/                   |            |
|    total_timesteps      | 4580000    |
| train/                  |            |
|    approx_kl            | 0.27865344 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.719     |
|    explained_variance   | 0.217      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.043     |
|    n_updates            | 1385       |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.223      |
----------------------------------------
Ep done - 199000.
Eval num_timesteps=4600000, episode_reward=0.20 +/- 0.97
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 4600000    |
| train/                  |            |
|    approx_kl            | 0.27730107 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.223      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0122    |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0544    |
|    value_loss           | 0.222      |
----------------------------------------
Ep done - 200000.
Eval num_timesteps=4620000, episode_reward=0.20 +/- 0.97
Episode length: 30.09 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 4620000    |
| train/                  |            |
|    approx_kl            | 0.28046498 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.707     |
|    explained_variance   | 0.255      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.00229    |
|    n_updates            | 1395       |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.221      |
----------------------------------------
Ep done - 201000.
Eval num_timesteps=4640000, episode_reward=0.32 +/- 0.94
Episode length: 30.07 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.32       |
| time/                   |            |
|    total_timesteps      | 4640000    |
| train/                  |            |
|    approx_kl            | 0.28507042 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.717     |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.021      |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.223      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.32
SELFPLAY: new best model, bumping up generation to 61
Eval num_timesteps=4660000, episode_reward=0.06 +/- 0.99
Episode length: 29.95 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 4660000    |
| train/                  |            |
|    approx_kl            | 0.29144153 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.24       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0179    |
|    n_updates            | 1405       |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.22       |
----------------------------------------
Ep done - 202000.
Eval num_timesteps=4680000, episode_reward=0.23 +/- 0.96
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 4680000    |
| train/                  |            |
|    approx_kl            | 0.28423342 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.217      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0207    |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.221      |
----------------------------------------
Ep done - 203000.
Eval num_timesteps=4700000, episode_reward=0.17 +/- 0.96
Episode length: 30.00 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.17       |
| time/                   |            |
|    total_timesteps      | 4700000    |
| train/                  |            |
|    approx_kl            | 0.26936215 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.203      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0341    |
|    n_updates            | 1415       |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.218      |
----------------------------------------
Ep done - 204000.
Eval num_timesteps=4720000, episode_reward=0.14 +/- 0.97
Episode length: 29.94 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 4720000    |
| train/                  |            |
|    approx_kl            | 0.28840232 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.726     |
|    explained_variance   | 0.289      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0446    |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 0.209      |
----------------------------------------
Ep done - 205000.
Eval num_timesteps=4740000, episode_reward=0.18 +/- 0.96
Episode length: 29.93 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 4740000    |
| train/                  |            |
|    approx_kl            | 0.26539525 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.245      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0412    |
|    n_updates            | 1425       |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.211      |
----------------------------------------
Ep done - 206000.
Eval num_timesteps=4760000, episode_reward=0.12 +/- 0.98
Episode length: 29.91 +/- 1.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 4760000    |
| train/                  |            |
|    approx_kl            | 0.27458468 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.718     |
|    explained_variance   | 0.188      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0325    |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.0527    |
|    value_loss           | 0.222      |
----------------------------------------
Ep done - 207000.
Eval num_timesteps=4780000, episode_reward=0.14 +/- 0.97
Episode length: 29.99 +/- 0.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 4780000    |
| train/                  |            |
|    approx_kl            | 0.28071487 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.709     |
|    explained_variance   | 0.24       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0361    |
|    n_updates            | 1435       |
|    policy_gradient_loss | -0.0531    |
|    value_loss           | 0.217      |
----------------------------------------
Eval num_timesteps=4800000, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.11      |
| time/                   |           |
|    total_timesteps      | 4800000   |
| train/                  |           |
|    approx_kl            | 0.2803269 |
|    clip_fraction        | 0.365     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.723    |
|    explained_variance   | 0.199     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0462   |
|    n_updates            | 1440      |
|    policy_gradient_loss | -0.0531   |
|    value_loss           | 0.221     |
---------------------------------------
Ep done - 208000.
Eval num_timesteps=4820000, episode_reward=0.18 +/- 0.98
Episode length: 29.99 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 4820000    |
| train/                  |            |
|    approx_kl            | 0.28630292 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.238      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0251    |
|    n_updates            | 1445       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.211      |
----------------------------------------
Ep done - 209000.
Eval num_timesteps=4840000, episode_reward=0.28 +/- 0.94
Episode length: 29.93 +/- 1.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.285      |
| time/                   |            |
|    total_timesteps      | 4840000    |
| train/                  |            |
|    approx_kl            | 0.28809637 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.186      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0264    |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.0524    |
|    value_loss           | 0.219      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 62
Ep done - 210000.
Eval num_timesteps=4860000, episode_reward=0.23 +/- 0.96
Episode length: 30.07 +/- 0.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.23      |
| time/                   |           |
|    total_timesteps      | 4860000   |
| train/                  |           |
|    approx_kl            | 0.2731115 |
|    clip_fraction        | 0.366     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.729    |
|    explained_variance   | 0.182     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0526   |
|    n_updates            | 1455      |
|    policy_gradient_loss | -0.052    |
|    value_loss           | 0.212     |
---------------------------------------
Ep done - 211000.
Eval num_timesteps=4880000, episode_reward=0.30 +/- 0.95
Episode length: 30.00 +/- 0.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4880000   |
| train/                  |           |
|    approx_kl            | 0.2758198 |
|    clip_fraction        | 0.364     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.742    |
|    explained_variance   | 0.224     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0183   |
|    n_updates            | 1460      |
|    policy_gradient_loss | -0.0539   |
|    value_loss           | 0.214     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 63
Ep done - 212000.
Eval num_timesteps=4900000, episode_reward=0.21 +/- 0.97
Episode length: 30.00 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.21      |
| time/                   |           |
|    total_timesteps      | 4900000   |
| train/                  |           |
|    approx_kl            | 0.2792902 |
|    clip_fraction        | 0.368     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.738    |
|    explained_variance   | 0.234     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0192   |
|    n_updates            | 1465      |
|    policy_gradient_loss | -0.055    |
|    value_loss           | 0.215     |
---------------------------------------
Ep done - 213000.
Eval num_timesteps=4920000, episode_reward=0.28 +/- 0.95
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.275      |
| time/                   |            |
|    total_timesteps      | 4920000    |
| train/                  |            |
|    approx_kl            | 0.27031156 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.746     |
|    explained_variance   | 0.224      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0408    |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.215      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 64
Ep done - 214000.
Eval num_timesteps=4940000, episode_reward=0.28 +/- 0.95
Episode length: 30.07 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.28       |
| time/                   |            |
|    total_timesteps      | 4940000    |
| train/                  |            |
|    approx_kl            | 0.26757008 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.745     |
|    explained_variance   | 0.237      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0183    |
|    n_updates            | 1475       |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.213      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 65
Eval num_timesteps=4960000, episode_reward=0.29 +/- 0.95
Episode length: 30.01 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.29      |
| time/                   |           |
|    total_timesteps      | 4960000   |
| train/                  |           |
|    approx_kl            | 0.2787684 |
|    clip_fraction        | 0.364     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.741    |
|    explained_variance   | 0.263     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0281   |
|    n_updates            | 1480      |
|    policy_gradient_loss | -0.0526   |
|    value_loss           | 0.212     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.29
SELFPLAY: new best model, bumping up generation to 66
Ep done - 215000.
Eval num_timesteps=4980000, episode_reward=0.17 +/- 0.98
Episode length: 30.07 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 4980000    |
| train/                  |            |
|    approx_kl            | 0.28279504 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.203      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0212    |
|    n_updates            | 1485       |
|    policy_gradient_loss | -0.0533    |
|    value_loss           | 0.217      |
----------------------------------------
Ep done - 216000.
Eval num_timesteps=5000000, episode_reward=0.15 +/- 0.97
Episode length: 30.04 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 5000000    |
| train/                  |            |
|    approx_kl            | 0.30182382 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.214      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.034     |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.218      |
----------------------------------------
Ep done - 217000.
Eval num_timesteps=5020000, episode_reward=0.21 +/- 0.97
Episode length: 29.97 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 5020000    |
| train/                  |            |
|    approx_kl            | 0.27963158 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0114    |
|    n_updates            | 1495       |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 218000.
Eval num_timesteps=5040000, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.085      |
| time/                   |            |
|    total_timesteps      | 5040000    |
| train/                  |            |
|    approx_kl            | 0.27717903 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.224      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0154    |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.21       |
----------------------------------------
Ep done - 219000.
Eval num_timesteps=5060000, episode_reward=0.17 +/- 0.98
Episode length: 30.05 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.17       |
| time/                   |            |
|    total_timesteps      | 5060000    |
| train/                  |            |
|    approx_kl            | 0.27138746 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.175      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0316    |
|    n_updates            | 1505       |
|    policy_gradient_loss | -0.0519    |
|    value_loss           | 0.227      |
----------------------------------------
Ep done - 220000.
Eval num_timesteps=5080000, episode_reward=0.23 +/- 0.96
Episode length: 29.97 +/- 1.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.225     |
| time/                   |           |
|    total_timesteps      | 5080000   |
| train/                  |           |
|    approx_kl            | 0.2958687 |
|    clip_fraction        | 0.368     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.729    |
|    explained_variance   | 0.242     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0229   |
|    n_updates            | 1510      |
|    policy_gradient_loss | -0.0529   |
|    value_loss           | 0.218     |
---------------------------------------
Eval num_timesteps=5100000, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.055      |
| time/                   |            |
|    total_timesteps      | 5100000    |
| train/                  |            |
|    approx_kl            | 0.28552037 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.239      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00808   |
|    n_updates            | 1515       |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.222      |
----------------------------------------
Ep done - 221000.
Eval num_timesteps=5120000, episode_reward=0.17 +/- 0.98
Episode length: 30.02 +/- 0.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.175    |
| time/              |          |
|    total_timesteps | 5120000  |
---------------------------------
Ep done - 222000.
Eval num_timesteps=5140000, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.065      |
| time/                   |            |
|    total_timesteps      | 5140000    |
| train/                  |            |
|    approx_kl            | 0.28593665 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.199      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0469    |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 0.227      |
----------------------------------------
Ep done - 223000.
Eval num_timesteps=5160000, episode_reward=0.10 +/- 0.99
Episode length: 29.96 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.095     |
| time/                   |           |
|    total_timesteps      | 5160000   |
| train/                  |           |
|    approx_kl            | 0.2916855 |
|    clip_fraction        | 0.366     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.732    |
|    explained_variance   | 0.266     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0524   |
|    n_updates            | 1525      |
|    policy_gradient_loss | -0.0532   |
|    value_loss           | 0.215     |
---------------------------------------
Ep done - 224000.
Eval num_timesteps=5180000, episode_reward=0.04 +/- 0.97
Episode length: 29.95 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 5180000    |
| train/                  |            |
|    approx_kl            | 0.28298503 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.258      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0175    |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.212      |
----------------------------------------
Ep done - 225000.
Eval num_timesteps=5200000, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.165     |
| time/                   |           |
|    total_timesteps      | 5200000   |
| train/                  |           |
|    approx_kl            | 0.2787784 |
|    clip_fraction        | 0.356     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.715    |
|    explained_variance   | 0.205     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.1e-05   |
|    n_updates            | 1535      |
|    policy_gradient_loss | -0.0517   |
|    value_loss           | 0.22      |
---------------------------------------
Ep done - 226000.
Eval num_timesteps=5220000, episode_reward=0.06 +/- 0.99
Episode length: 30.07 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.055     |
| time/                   |           |
|    total_timesteps      | 5220000   |
| train/                  |           |
|    approx_kl            | 0.2766223 |
|    clip_fraction        | 0.356     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.719    |
|    explained_variance   | 0.232     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0143   |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.0533   |
|    value_loss           | 0.221     |
---------------------------------------
Ep done - 227000.
Eval num_timesteps=5240000, episode_reward=0.14 +/- 0.98
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.14       |
| time/                   |            |
|    total_timesteps      | 5240000    |
| train/                  |            |
|    approx_kl            | 0.31502652 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.719     |
|    explained_variance   | 0.196      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0114    |
|    n_updates            | 1545       |
|    policy_gradient_loss | -0.0521    |
|    value_loss           | 0.222      |
----------------------------------------
Eval num_timesteps=5260000, episode_reward=0.24 +/- 0.97
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.24       |
| time/                   |            |
|    total_timesteps      | 5260000    |
| train/                  |            |
|    approx_kl            | 0.27534923 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.719     |
|    explained_variance   | 0.234      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0303    |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.215      |
----------------------------------------
Ep done - 228000.
Eval num_timesteps=5280000, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 5280000    |
| train/                  |            |
|    approx_kl            | 0.26238447 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.21       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.013      |
|    n_updates            | 1555       |
|    policy_gradient_loss | -0.0526    |
|    value_loss           | 0.225      |
----------------------------------------
Ep done - 229000.
Eval num_timesteps=5300000, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 5300000    |
| train/                  |            |
|    approx_kl            | 0.25491735 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.241      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0341    |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0546    |
|    value_loss           | 0.218      |
----------------------------------------
Ep done - 230000.
Eval num_timesteps=5320000, episode_reward=0.12 +/- 0.99
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 5320000    |
| train/                  |            |
|    approx_kl            | 0.27534825 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.199      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0192    |
|    n_updates            | 1565       |
|    policy_gradient_loss | -0.0527    |
|    value_loss           | 0.229      |
----------------------------------------
Ep done - 231000.
Eval num_timesteps=5340000, episode_reward=0.09 +/- 0.98
Episode length: 29.95 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.09      |
| time/                   |           |
|    total_timesteps      | 5340000   |
| train/                  |           |
|    approx_kl            | 0.3011605 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.709    |
|    explained_variance   | 0.227     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0173    |
|    n_updates            | 1570      |
|    policy_gradient_loss | -0.0532   |
|    value_loss           | 0.221     |
---------------------------------------
Ep done - 232000.
Eval num_timesteps=5360000, episode_reward=0.10 +/- 0.99
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.095      |
| time/                   |            |
|    total_timesteps      | 5360000    |
| train/                  |            |
|    approx_kl            | 0.28536376 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.716     |
|    explained_variance   | 0.227      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0281    |
|    n_updates            | 1575       |
|    policy_gradient_loss | -0.0535    |
|    value_loss           | 0.221      |
----------------------------------------
Ep done - 233000.
Eval num_timesteps=5380000, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.05      |
| time/                   |           |
|    total_timesteps      | 5380000   |
| train/                  |           |
|    approx_kl            | 0.2985739 |
|    clip_fraction        | 0.367     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.725    |
|    explained_variance   | 0.232     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0014   |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.0528   |
|    value_loss           | 0.223     |
---------------------------------------
Eval num_timesteps=5400000, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.075      |
| time/                   |            |
|    total_timesteps      | 5400000    |
| train/                  |            |
|    approx_kl            | 0.28596672 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.241      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0139    |
|    n_updates            | 1585       |
|    policy_gradient_loss | -0.0542    |
|    value_loss           | 0.222      |
----------------------------------------
Ep done - 234000.
Eval num_timesteps=5420000, episode_reward=0.20 +/- 0.96
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 5420000    |
| train/                  |            |
|    approx_kl            | 0.28979212 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.707     |
|    explained_variance   | 0.261      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0353    |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.216      |
----------------------------------------
Ep done - 235000.
Eval num_timesteps=5440000, episode_reward=0.12 +/- 0.97
Episode length: 30.04 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.12      |
| time/                   |           |
|    total_timesteps      | 5440000   |
| train/                  |           |
|    approx_kl            | 0.2732886 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.714    |
|    explained_variance   | 0.223     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0281   |
|    n_updates            | 1595      |
|    policy_gradient_loss | -0.0529   |
|    value_loss           | 0.229     |
---------------------------------------
Ep done - 236000.
Eval num_timesteps=5460000, episode_reward=0.10 +/- 0.98
Episode length: 29.93 +/- 1.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 5460000   |
| train/                  |           |
|    approx_kl            | 0.2787284 |
|    clip_fraction        | 0.357     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.715    |
|    explained_variance   | 0.265     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0156   |
|    n_updates            | 1600      |
|    policy_gradient_loss | -0.0539   |
|    value_loss           | 0.214     |
---------------------------------------
Ep done - 237000.
Eval num_timesteps=5480000, episode_reward=0.20 +/- 0.96
Episode length: 30.04 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 5480000    |
| train/                  |            |
|    approx_kl            | 0.30085605 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.256      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.000493  |
|    n_updates            | 1605       |
|    policy_gradient_loss | -0.0558    |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 238000.
Eval num_timesteps=5500000, episode_reward=0.17 +/- 0.97
Episode length: 30.00 +/- 0.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.17      |
| time/                   |           |
|    total_timesteps      | 5500000   |
| train/                  |           |
|    approx_kl            | 0.2763334 |
|    clip_fraction        | 0.357     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.707    |
|    explained_variance   | 0.245     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.000285 |
|    n_updates            | 1610      |
|    policy_gradient_loss | -0.054    |
|    value_loss           | 0.231     |
---------------------------------------
Ep done - 239000.
Eval num_timesteps=5520000, episode_reward=0.06 +/- 0.99
Episode length: 29.95 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 5520000    |
| train/                  |            |
|    approx_kl            | 0.27977502 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.286      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0166     |
|    n_updates            | 1615       |
|    policy_gradient_loss | -0.0572    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 240000.
Eval num_timesteps=5540000, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 5540000    |
| train/                  |            |
|    approx_kl            | 0.26703265 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.717     |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0173    |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0568    |
|    value_loss           | 0.226      |
----------------------------------------
Eval num_timesteps=5560000, episode_reward=0.15 +/- 0.98
Episode length: 29.92 +/- 1.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 5560000    |
| train/                  |            |
|    approx_kl            | 0.27386484 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.697     |
|    explained_variance   | 0.261      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0147     |
|    n_updates            | 1625       |
|    policy_gradient_loss | -0.0546    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 241000.
Eval num_timesteps=5580000, episode_reward=0.17 +/- 0.97
Episode length: 30.05 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.175      |
| time/                   |            |
|    total_timesteps      | 5580000    |
| train/                  |            |
|    approx_kl            | 0.29414672 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.243      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00192   |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.223      |
----------------------------------------
Ep done - 242000.
Eval num_timesteps=5600000, episode_reward=0.10 +/- 0.97
Episode length: 30.02 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.095      |
| time/                   |            |
|    total_timesteps      | 5600000    |
| train/                  |            |
|    approx_kl            | 0.28275114 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.699     |
|    explained_variance   | 0.259      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0175    |
|    n_updates            | 1635       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 243000.
Eval num_timesteps=5620000, episode_reward=0.15 +/- 0.98
Episode length: 29.93 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 5620000    |
| train/                  |            |
|    approx_kl            | 0.27016833 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.704     |
|    explained_variance   | 0.228      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0147     |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 244000.
Eval num_timesteps=5640000, episode_reward=0.20 +/- 0.97
Episode length: 30.05 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 5640000    |
| train/                  |            |
|    approx_kl            | 0.27908662 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.701     |
|    explained_variance   | 0.268      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0218    |
|    n_updates            | 1645       |
|    policy_gradient_loss | -0.0558    |
|    value_loss           | 0.225      |
----------------------------------------
Ep done - 245000.
Eval num_timesteps=5660000, episode_reward=0.23 +/- 0.96
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 5660000    |
| train/                  |            |
|    approx_kl            | 0.28681213 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.702     |
|    explained_variance   | 0.267      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0132     |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.228      |
----------------------------------------
Ep done - 246000.
Eval num_timesteps=5680000, episode_reward=0.26 +/- 0.95
Episode length: 30.09 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.255     |
| time/                   |           |
|    total_timesteps      | 5680000   |
| train/                  |           |
|    approx_kl            | 0.2820317 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.704    |
|    explained_variance   | 0.277     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0231   |
|    n_updates            | 1655      |
|    policy_gradient_loss | -0.0534   |
|    value_loss           | 0.218     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 67
Eval num_timesteps=5700000, episode_reward=0.14 +/- 0.98
Episode length: 30.03 +/- 0.60
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.14     |
| time/                   |          |
|    total_timesteps      | 5700000  |
| train/                  |          |
|    approx_kl            | 0.283608 |
|    clip_fraction        | 0.364    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.712   |
|    explained_variance   | 0.229    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.00537  |
|    n_updates            | 1660     |
|    policy_gradient_loss | -0.0531  |
|    value_loss           | 0.224    |
--------------------------------------
Ep done - 247000.
Eval num_timesteps=5720000, episode_reward=0.14 +/- 0.98
Episode length: 29.93 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.135     |
| time/                   |           |
|    total_timesteps      | 5720000   |
| train/                  |           |
|    approx_kl            | 0.3142076 |
|    clip_fraction        | 0.364     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.709    |
|    explained_variance   | 0.253     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0257   |
|    n_updates            | 1665      |
|    policy_gradient_loss | -0.0532   |
|    value_loss           | 0.229     |
---------------------------------------
Ep done - 248000.
Eval num_timesteps=5740000, episode_reward=0.14 +/- 0.98
Episode length: 29.97 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.135      |
| time/                   |            |
|    total_timesteps      | 5740000    |
| train/                  |            |
|    approx_kl            | 0.28462005 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.226      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.000806  |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.229      |
----------------------------------------
Ep done - 249000.
Eval num_timesteps=5760000, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.55
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.1      |
| time/                   |          |
|    total_timesteps      | 5760000  |
| train/                  |          |
|    approx_kl            | 0.311519 |
|    clip_fraction        | 0.361    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.707   |
|    explained_variance   | 0.244    |
|    learning_rate        | 0.0001   |
|    loss                 | 0.00863  |
|    n_updates            | 1675     |
|    policy_gradient_loss | -0.0538  |
|    value_loss           | 0.224    |
--------------------------------------
Ep done - 250000.
Eval num_timesteps=5780000, episode_reward=0.17 +/- 0.97
Episode length: 30.05 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 5780000    |
| train/                  |            |
|    approx_kl            | 0.28094864 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.719     |
|    explained_variance   | 0.284      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0272    |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.218      |
----------------------------------------
Ep done - 251000.
Eval num_timesteps=5800000, episode_reward=0.14 +/- 0.99
Episode length: 30.00 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.145      |
| time/                   |            |
|    total_timesteps      | 5800000    |
| train/                  |            |
|    approx_kl            | 0.30493855 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.226      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00375   |
|    n_updates            | 1685       |
|    policy_gradient_loss | -0.0549    |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 252000.
Eval num_timesteps=5820000, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 5820000    |
| train/                  |            |
|    approx_kl            | 0.27799162 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.235      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00704   |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 253000.
Eval num_timesteps=5840000, episode_reward=-0.06 +/- 0.98
Episode length: 29.95 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.06      |
| time/                   |            |
|    total_timesteps      | 5840000    |
| train/                  |            |
|    approx_kl            | 0.29769546 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.231      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.00604    |
|    n_updates            | 1695       |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.232      |
----------------------------------------
Eval num_timesteps=5860000, episode_reward=0.18 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.18       |
| time/                   |            |
|    total_timesteps      | 5860000    |
| train/                  |            |
|    approx_kl            | 0.30570465 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.299      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00202   |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0553    |
|    value_loss           | 0.223      |
----------------------------------------
Ep done - 254000.
Eval num_timesteps=5880000, episode_reward=0.24 +/- 0.96
Episode length: 30.10 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.245      |
| time/                   |            |
|    total_timesteps      | 5880000    |
| train/                  |            |
|    approx_kl            | 0.37341368 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.26       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0381    |
|    n_updates            | 1705       |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.228      |
----------------------------------------
Ep done - 255000.
Eval num_timesteps=5900000, episode_reward=0.17 +/- 0.97
Episode length: 30.01 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 5900000    |
| train/                  |            |
|    approx_kl            | 0.29146323 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.722     |
|    explained_variance   | 0.253      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0146     |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.053     |
|    value_loss           | 0.233      |
----------------------------------------
Ep done - 256000.
Eval num_timesteps=5920000, episode_reward=0.09 +/- 0.97
Episode length: 30.02 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.09      |
| time/                   |           |
|    total_timesteps      | 5920000   |
| train/                  |           |
|    approx_kl            | 0.2966312 |
|    clip_fraction        | 0.372     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.715    |
|    explained_variance   | 0.273     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0186    |
|    n_updates            | 1715      |
|    policy_gradient_loss | -0.0562   |
|    value_loss           | 0.222     |
---------------------------------------
Ep done - 257000.
Eval num_timesteps=5940000, episode_reward=0.06 +/- 0.99
Episode length: 29.93 +/- 0.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.055      |
| time/                   |            |
|    total_timesteps      | 5940000    |
| train/                  |            |
|    approx_kl            | 0.28672403 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0144     |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.0533    |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 258000.
Eval num_timesteps=5960000, episode_reward=0.20 +/- 0.97
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.195      |
| time/                   |            |
|    total_timesteps      | 5960000    |
| train/                  |            |
|    approx_kl            | 0.29147163 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.256      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0291    |
|    n_updates            | 1725       |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.236      |
----------------------------------------
Ep done - 259000.
Eval num_timesteps=5980000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30       |
|    mean_reward     | 0.125    |
| time/              |          |
|    total_timesteps | 5980000  |
---------------------------------
Eval num_timesteps=6000000, episode_reward=0.21 +/- 0.96
Episode length: 30.09 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.215     |
| time/                   |           |
|    total_timesteps      | 6000000   |
| train/                  |           |
|    approx_kl            | 0.2972662 |
|    clip_fraction        | 0.363     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.721    |
|    explained_variance   | 0.276     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.00768  |
|    n_updates            | 1730      |
|    policy_gradient_loss | -0.0549   |
|    value_loss           | 0.229     |
---------------------------------------
Ep done - 260000.
Eval num_timesteps=6020000, episode_reward=0.23 +/- 0.96
Episode length: 30.09 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.235     |
| time/                   |           |
|    total_timesteps      | 6020000   |
| train/                  |           |
|    approx_kl            | 0.2829133 |
|    clip_fraction        | 0.358     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.721    |
|    explained_variance   | 0.271     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0133    |
|    n_updates            | 1735      |
|    policy_gradient_loss | -0.0546   |
|    value_loss           | 0.226     |
---------------------------------------
Ep done - 261000.
Eval num_timesteps=6040000, episode_reward=0.15 +/- 0.97
Episode length: 29.98 +/- 0.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.15      |
| time/                   |           |
|    total_timesteps      | 6040000   |
| train/                  |           |
|    approx_kl            | 0.2769943 |
|    clip_fraction        | 0.362     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.725    |
|    explained_variance   | 0.263     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0044    |
|    n_updates            | 1740      |
|    policy_gradient_loss | -0.0554   |
|    value_loss           | 0.228     |
---------------------------------------
Ep done - 262000.
Eval num_timesteps=6060000, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.195     |
| time/                   |           |
|    total_timesteps      | 6060000   |
| train/                  |           |
|    approx_kl            | 0.2834885 |
|    clip_fraction        | 0.369     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.717    |
|    explained_variance   | 0.252     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.00175   |
|    n_updates            | 1745      |
|    policy_gradient_loss | -0.0543   |
|    value_loss           | 0.23      |
---------------------------------------
Ep done - 263000.
Eval num_timesteps=6080000, episode_reward=0.09 +/- 0.99
Episode length: 29.98 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 6080000    |
| train/                  |            |
|    approx_kl            | 0.28347582 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.714     |
|    explained_variance   | 0.215      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0224     |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.229      |
----------------------------------------
Ep done - 264000.
Eval num_timesteps=6100000, episode_reward=0.18 +/- 0.95
Episode length: 29.91 +/- 1.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.18      |
| time/                   |           |
|    total_timesteps      | 6100000   |
| train/                  |           |
|    approx_kl            | 0.2702398 |
|    clip_fraction        | 0.355     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.708    |
|    explained_variance   | 0.231     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.00187  |
|    n_updates            | 1755      |
|    policy_gradient_loss | -0.0524   |
|    value_loss           | 0.223     |
---------------------------------------
Ep done - 265000.
Eval num_timesteps=6120000, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 6120000    |
| train/                  |            |
|    approx_kl            | 0.27877244 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0124     |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.234      |
----------------------------------------
Ep done - 266000.
Eval num_timesteps=6140000, episode_reward=0.12 +/- 0.96
Episode length: 30.04 +/- 0.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.125     |
| time/                   |           |
|    total_timesteps      | 6140000   |
| train/                  |           |
|    approx_kl            | 0.2888344 |
|    clip_fraction        | 0.364     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.719    |
|    explained_variance   | 0.21      |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0287   |
|    n_updates            | 1765      |
|    policy_gradient_loss | -0.054    |
|    value_loss           | 0.221     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 161      |
|    iterations      | 300      |
|    time_elapsed    | 38013    |
|    total_timesteps | 6144000  |
---------------------------------
Eval num_timesteps=6160000, episode_reward=0.23 +/- 0.95
Episode length: 30.00 +/- 0.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.225     |
| time/                   |           |
|    total_timesteps      | 6160000   |
| train/                  |           |
|    approx_kl            | 0.2919437 |
|    clip_fraction        | 0.357     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.711    |
|    explained_variance   | 0.243     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0385   |
|    n_updates            | 1770      |
|    policy_gradient_loss | -0.054    |
|    value_loss           | 0.223     |
---------------------------------------
Ep done - 267000.
Eval num_timesteps=6180000, episode_reward=0.04 +/- 0.98
Episode length: 29.88 +/- 1.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.045     |
| time/                   |           |
|    total_timesteps      | 6180000   |
| train/                  |           |
|    approx_kl            | 0.2701622 |
|    clip_fraction        | 0.358     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.719    |
|    explained_variance   | 0.232     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0426   |
|    n_updates            | 1775      |
|    policy_gradient_loss | -0.0541   |
|    value_loss           | 0.215     |
---------------------------------------
Ep done - 268000.
Eval num_timesteps=6200000, episode_reward=0.16 +/- 0.97
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.16       |
| time/                   |            |
|    total_timesteps      | 6200000    |
| train/                  |            |
|    approx_kl            | 0.29875535 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.194      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.033     |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 269000.
Eval num_timesteps=6220000, episode_reward=0.20 +/- 0.97
Episode length: 29.98 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.195     |
| time/                   |           |
|    total_timesteps      | 6220000   |
| train/                  |           |
|    approx_kl            | 0.2804958 |
|    clip_fraction        | 0.359     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.718    |
|    explained_variance   | 0.23      |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0226   |
|    n_updates            | 1785      |
|    policy_gradient_loss | -0.0538   |
|    value_loss           | 0.22      |
---------------------------------------
Ep done - 270000.
Eval num_timesteps=6240000, episode_reward=0.23 +/- 0.96
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 6240000    |
| train/                  |            |
|    approx_kl            | 0.28652492 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.224      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0153    |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.222      |
----------------------------------------
Ep done - 271000.
Eval num_timesteps=6260000, episode_reward=0.07 +/- 0.98
Episode length: 29.93 +/- 0.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.07      |
| time/                   |           |
|    total_timesteps      | 6260000   |
| train/                  |           |
|    approx_kl            | 0.2876881 |
|    clip_fraction        | 0.355     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.703    |
|    explained_variance   | 0.213     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0149   |
|    n_updates            | 1795      |
|    policy_gradient_loss | -0.0523   |
|    value_loss           | 0.223     |
---------------------------------------
Ep done - 272000.
Eval num_timesteps=6280000, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.08      |
| time/                   |           |
|    total_timesteps      | 6280000   |
| train/                  |           |
|    approx_kl            | 0.2804802 |
|    clip_fraction        | 0.353     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.706    |
|    explained_variance   | 0.201     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.00154  |
|    n_updates            | 1800      |
|    policy_gradient_loss | -0.0518   |
|    value_loss           | 0.227     |
---------------------------------------
Eval num_timesteps=6300000, episode_reward=0.08 +/- 0.98
Episode length: 29.99 +/- 0.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 6300000    |
| train/                  |            |
|    approx_kl            | 0.29110417 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.714     |
|    explained_variance   | 0.209      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0202    |
|    n_updates            | 1805       |
|    policy_gradient_loss | -0.0524    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 273000.
Eval num_timesteps=6320000, episode_reward=0.17 +/- 0.98
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.17       |
| time/                   |            |
|    total_timesteps      | 6320000    |
| train/                  |            |
|    approx_kl            | 0.28711814 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0114    |
|    n_updates            | 1810       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.225      |
----------------------------------------
Ep done - 274000.
Eval num_timesteps=6340000, episode_reward=0.14 +/- 0.98
Episode length: 29.98 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.135     |
| time/                   |           |
|    total_timesteps      | 6340000   |
| train/                  |           |
|    approx_kl            | 0.2883273 |
|    clip_fraction        | 0.356     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.706    |
|    explained_variance   | 0.218     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0325   |
|    n_updates            | 1815      |
|    policy_gradient_loss | -0.0535   |
|    value_loss           | 0.224     |
---------------------------------------
Ep done - 275000.
Eval num_timesteps=6360000, episode_reward=0.17 +/- 0.98
Episode length: 30.02 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 6360000    |
| train/                  |            |
|    approx_kl            | 0.28656334 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.698     |
|    explained_variance   | 0.223      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0396    |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 0.225      |
----------------------------------------
Ep done - 276000.
Eval num_timesteps=6380000, episode_reward=0.23 +/- 0.96
Episode length: 30.04 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 6380000    |
| train/                  |            |
|    approx_kl            | 0.27315444 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.696     |
|    explained_variance   | 0.213      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0343    |
|    n_updates            | 1825       |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.219      |
----------------------------------------
Ep done - 277000.
Eval num_timesteps=6400000, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.065      |
| time/                   |            |
|    total_timesteps      | 6400000    |
| train/                  |            |
|    approx_kl            | 0.28402385 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.695     |
|    explained_variance   | 0.214      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00373   |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 278000.
Eval num_timesteps=6420000, episode_reward=0.20 +/- 0.97
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.195      |
| time/                   |            |
|    total_timesteps      | 6420000    |
| train/                  |            |
|    approx_kl            | 0.31859905 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.689     |
|    explained_variance   | 0.198      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.036     |
|    n_updates            | 1835       |
|    policy_gradient_loss | -0.0486    |
|    value_loss           | 0.223      |
----------------------------------------
Ep done - 279000.
Eval num_timesteps=6440000, episode_reward=0.15 +/- 0.97
Episode length: 30.04 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 6440000    |
| train/                  |            |
|    approx_kl            | 0.31489992 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.689     |
|    explained_variance   | 0.208      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0234    |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.0531    |
|    value_loss           | 0.22       |
----------------------------------------
Eval num_timesteps=6460000, episode_reward=0.15 +/- 0.97
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 6460000    |
| train/                  |            |
|    approx_kl            | 0.28812233 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.687     |
|    explained_variance   | 0.249      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.00133    |
|    n_updates            | 1845       |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 280000.
Eval num_timesteps=6480000, episode_reward=0.15 +/- 0.98
Episode length: 30.04 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 6480000    |
| train/                  |            |
|    approx_kl            | 0.30821776 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.679     |
|    explained_variance   | 0.23       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.000129   |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 281000.
Eval num_timesteps=6500000, episode_reward=0.20 +/- 0.96
Episode length: 29.95 +/- 1.44
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 29.9     |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 6500000  |
| train/                  |          |
|    approx_kl            | 0.28843  |
|    clip_fraction        | 0.344    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.662   |
|    explained_variance   | 0.227    |
|    learning_rate        | 0.0001   |
|    loss                 | -0.00848 |
|    n_updates            | 1855     |
|    policy_gradient_loss | -0.0517  |
|    value_loss           | 0.23     |
--------------------------------------
Ep done - 282000.
Eval num_timesteps=6520000, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.065      |
| time/                   |            |
|    total_timesteps      | 6520000    |
| train/                  |            |
|    approx_kl            | 0.29252785 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.666     |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0161     |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0535    |
|    value_loss           | 0.223      |
----------------------------------------
Ep done - 283000.
Eval num_timesteps=6540000, episode_reward=0.20 +/- 0.95
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.195      |
| time/                   |            |
|    total_timesteps      | 6540000    |
| train/                  |            |
|    approx_kl            | 0.30237666 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.658     |
|    explained_variance   | 0.216      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0101     |
|    n_updates            | 1865       |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 284000.
Eval num_timesteps=6560000, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 6560000   |
| train/                  |           |
|    approx_kl            | 0.3063031 |
|    clip_fraction        | 0.353     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.663    |
|    explained_variance   | 0.221     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0191    |
|    n_updates            | 1870      |
|    policy_gradient_loss | -0.0518   |
|    value_loss           | 0.226     |
---------------------------------------
Ep done - 285000.
Eval num_timesteps=6580000, episode_reward=0.04 +/- 0.99
Episode length: 29.91 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 6580000    |
| train/                  |            |
|    approx_kl            | 0.30820885 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.25       |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0313    |
|    n_updates            | 1875       |
|    policy_gradient_loss | -0.0518    |
|    value_loss           | 0.224      |
----------------------------------------
Eval num_timesteps=6600000, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.085      |
| time/                   |            |
|    total_timesteps      | 6600000    |
| train/                  |            |
|    approx_kl            | 0.29534274 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.666     |
|    explained_variance   | 0.228      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0137    |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0489    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 286000.
Eval num_timesteps=6620000, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 6620000    |
| train/                  |            |
|    approx_kl            | 0.30788547 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.000594  |
|    n_updates            | 1885       |
|    policy_gradient_loss | -0.0506    |
|    value_loss           | 0.225      |
----------------------------------------
Ep done - 287000.
Eval num_timesteps=6640000, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 6640000    |
| train/                  |            |
|    approx_kl            | 0.32115906 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.659     |
|    explained_variance   | 0.231      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00355   |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 288000.
Eval num_timesteps=6660000, episode_reward=0.00 +/- 0.98
Episode length: 29.93 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 6660000    |
| train/                  |            |
|    approx_kl            | 0.30657592 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0118     |
|    n_updates            | 1895       |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.233      |
----------------------------------------
Ep done - 289000.
Eval num_timesteps=6680000, episode_reward=0.13 +/- 0.99
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 6680000    |
| train/                  |            |
|    approx_kl            | 0.28622603 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.666     |
|    explained_variance   | 0.248      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.00577    |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0521    |
|    value_loss           | 0.219      |
----------------------------------------
Ep done - 290000.
Eval num_timesteps=6700000, episode_reward=0.00 +/- 0.99
Episode length: 30.03 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 6700000    |
| train/                  |            |
|    approx_kl            | 0.34229153 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.665     |
|    explained_variance   | 0.236      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0163    |
|    n_updates            | 1905       |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.221      |
----------------------------------------
Ep done - 291000.
Eval num_timesteps=6720000, episode_reward=0.03 +/- 0.99
Episode length: 29.89 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.025      |
| time/                   |            |
|    total_timesteps      | 6720000    |
| train/                  |            |
|    approx_kl            | 0.29092765 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.663     |
|    explained_variance   | 0.187      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0181    |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.0489    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 292000.
Eval num_timesteps=6740000, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 6740000    |
| train/                  |            |
|    approx_kl            | 0.35315046 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.662     |
|    explained_variance   | 0.214      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0328    |
|    n_updates            | 1915       |
|    policy_gradient_loss | -0.0499    |
|    value_loss           | 0.225      |
----------------------------------------
slurmstepd-n16: error: *** JOB 682 ON n16 CANCELLED AT 2024-06-16T05:41:43 ***
