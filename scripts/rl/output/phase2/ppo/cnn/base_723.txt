CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
CUDA available: True
seed: 12 
num_timesteps: 12000000 
eval_freq: 61500 
eval_episoded: 200 
best_threshold: 0.22 
logdir: scripts/rl/output/phase2/ppo/cnn/base-v3/ 
cnn_policy: True 
continueFrom_model: scripts/rl/output/phase2/ppo/cnn/base-v2/history_0062

params: {'learning_rate': <__main__.LinearSchedule object at 0x7fe9127c5d60>, 'n_steps': 61440, 'n_epochs': 5, 'clip_range': 0.18, 'batch_size': 128, 'ent_coef': 0.01, 'verbose': 100, 'seed': 12}

Ep done - 1000.
Ep done - 2000.
Eval num_timesteps=61500, episode_reward=0.17 +/- 0.97
Episode length: 29.99 +/- 0.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.175     |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.4005672 |
|    clip_fraction        | 0.483     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.742    |
|    explained_variance   | 0.194     |
|    learning_rate        | 0.000199  |
|    loss                 | 0.0286    |
|    n_updates            | 1455      |
|    policy_gradient_loss | -0.0116   |
|    value_loss           | 0.192     |
---------------------------------------
Ep done - 3000.
Ep done - 4000.
Eval num_timesteps=123000, episode_reward=0.36 +/- 0.92
Episode length: 30.07 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.365      |
| time/                   |            |
|    total_timesteps      | 123000     |
| train/                  |            |
|    approx_kl            | 0.39447695 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.155      |
|    learning_rate        | 0.000198   |
|    loss                 | 0.00196    |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 0.196      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.365
SELFPLAY: new best model, bumping up generation to 1
Ep done - 5000.
Ep done - 6000.
Eval num_timesteps=184500, episode_reward=0.20 +/- 0.96
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.195      |
| time/                   |            |
|    total_timesteps      | 184500     |
| train/                  |            |
|    approx_kl            | 0.35245132 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.775     |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.000197   |
|    loss                 | -0.0306    |
|    n_updates            | 1465       |
|    policy_gradient_loss | -0.0235    |
|    value_loss           | 0.185      |
----------------------------------------
Ep done - 7000.
Ep done - 8000.
Ep done - 9000.
Eval num_timesteps=246000, episode_reward=0.20 +/- 0.96
Episode length: 30.07 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 246000     |
| train/                  |            |
|    approx_kl            | 0.32173696 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.785     |
|    explained_variance   | 0.144      |
|    learning_rate        | 0.000196   |
|    loss                 | -0.00918   |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0258    |
|    value_loss           | 0.193      |
----------------------------------------
Ep done - 10000.
Ep done - 11000.
Eval num_timesteps=307500, episode_reward=0.30 +/- 0.93
Episode length: 30.03 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 307500     |
| train/                  |            |
|    approx_kl            | 0.29712674 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.797     |
|    explained_variance   | 0.172      |
|    learning_rate        | 0.000195   |
|    loss                 | -0.0337    |
|    n_updates            | 1475       |
|    policy_gradient_loss | -0.0301    |
|    value_loss           | 0.191      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 2
Ep done - 12000.
Ep done - 13000.
Eval num_timesteps=369000, episode_reward=0.27 +/- 0.96
Episode length: 30.06 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.265      |
| time/                   |            |
|    total_timesteps      | 369000     |
| train/                  |            |
|    approx_kl            | 0.29914278 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.819     |
|    explained_variance   | 0.167      |
|    learning_rate        | 0.000194   |
|    loss                 | -0.0317    |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0327    |
|    value_loss           | 0.189      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 3
Ep done - 14000.
Ep done - 15000.
Eval num_timesteps=430500, episode_reward=0.30 +/- 0.94
Episode length: 30.05 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 430500     |
| train/                  |            |
|    approx_kl            | 0.27696732 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.813     |
|    explained_variance   | 0.181      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.031     |
|    n_updates            | 1485       |
|    policy_gradient_loss | -0.0315    |
|    value_loss           | 0.192      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 4
Ep done - 16000.
Ep done - 17000.
Ep done - 18000.
Eval num_timesteps=492000, episode_reward=0.21 +/- 0.96
Episode length: 30.02 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 492000     |
| train/                  |            |
|    approx_kl            | 0.26057708 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.809     |
|    explained_variance   | 0.0998     |
|    learning_rate        | 0.000192   |
|    loss                 | -0.0253    |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0321    |
|    value_loss           | 0.2        |
----------------------------------------
Ep done - 19000.
Ep done - 20000.
Eval num_timesteps=553500, episode_reward=0.25 +/- 0.96
Episode length: 30.05 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.25      |
| time/                   |           |
|    total_timesteps      | 553500    |
| train/                  |           |
|    approx_kl            | 0.2645089 |
|    clip_fraction        | 0.434     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.804    |
|    explained_variance   | 0.115     |
|    learning_rate        | 0.000191  |
|    loss                 | -0.0492   |
|    n_updates            | 1495      |
|    policy_gradient_loss | -0.033    |
|    value_loss           | 0.194     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.25
SELFPLAY: new best model, bumping up generation to 5
Ep done - 21000.
Ep done - 22000.
Eval num_timesteps=615000, episode_reward=0.22 +/- 0.96
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.22       |
| time/                   |            |
|    total_timesteps      | 615000     |
| train/                  |            |
|    approx_kl            | 0.27152565 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.802     |
|    explained_variance   | 0.129      |
|    learning_rate        | 0.00019    |
|    loss                 | -0.0318    |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0325    |
|    value_loss           | 0.194      |
----------------------------------------
Ep done - 23000.
Ep done - 24000.
Eval num_timesteps=676500, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.105      |
| time/                   |            |
|    total_timesteps      | 676500     |
| train/                  |            |
|    approx_kl            | 0.26422733 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.817     |
|    explained_variance   | 0.13       |
|    learning_rate        | 0.000189   |
|    loss                 | 0.000766   |
|    n_updates            | 1505       |
|    policy_gradient_loss | -0.0351    |
|    value_loss           | 0.199      |
----------------------------------------
Ep done - 25000.
Ep done - 26000.
Ep done - 27000.
Eval num_timesteps=738000, episode_reward=0.15 +/- 0.96
Episode length: 29.98 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 738000     |
| train/                  |            |
|    approx_kl            | 0.26480764 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.82      |
|    explained_variance   | 0.128      |
|    learning_rate        | 0.000188   |
|    loss                 | -0.00722   |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.0362    |
|    value_loss           | 0.2        |
----------------------------------------
Ep done - 28000.
Ep done - 29000.
Eval num_timesteps=799500, episode_reward=0.10 +/- 0.97
Episode length: 30.01 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.105      |
| time/                   |            |
|    total_timesteps      | 799500     |
| train/                  |            |
|    approx_kl            | 0.26826364 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.816     |
|    explained_variance   | 0.118      |
|    learning_rate        | 0.000187   |
|    loss                 | -0.0135    |
|    n_updates            | 1515       |
|    policy_gradient_loss | -0.0363    |
|    value_loss           | 0.202      |
----------------------------------------
Ep done - 30000.
Ep done - 31000.
Eval num_timesteps=861000, episode_reward=0.33 +/- 0.92
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.33       |
| time/                   |            |
|    total_timesteps      | 861000     |
| train/                  |            |
|    approx_kl            | 0.27512208 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.809     |
|    explained_variance   | 0.107      |
|    learning_rate        | 0.000186   |
|    loss                 | -0.02      |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.036     |
|    value_loss           | 0.2        |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.33
SELFPLAY: new best model, bumping up generation to 6
Ep done - 32000.
Ep done - 33000.
Eval num_timesteps=922500, episode_reward=0.06 +/- 0.99
Episode length: 30.03 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 922500     |
| train/                  |            |
|    approx_kl            | 0.26559672 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.809     |
|    explained_variance   | 0.0836     |
|    learning_rate        | 0.000185   |
|    loss                 | -0.036     |
|    n_updates            | 1525       |
|    policy_gradient_loss | -0.0354    |
|    value_loss           | 0.2        |
----------------------------------------
Ep done - 34000.
Ep done - 35000.
Ep done - 36000.
Eval num_timesteps=984000, episode_reward=0.13 +/- 0.98
Episode length: 29.96 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 984000     |
| train/                  |            |
|    approx_kl            | 0.28231525 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.807     |
|    explained_variance   | 0.127      |
|    learning_rate        | 0.000184   |
|    loss                 | -0.0203    |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.0357    |
|    value_loss           | 0.201      |
----------------------------------------
Ep done - 37000.
Ep done - 38000.
Eval num_timesteps=1045500, episode_reward=0.15 +/- 0.98
Episode length: 30.02 +/- 0.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.15      |
| time/                   |           |
|    total_timesteps      | 1045500   |
| train/                  |           |
|    approx_kl            | 0.2714788 |
|    clip_fraction        | 0.429     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.807    |
|    explained_variance   | 0.107     |
|    learning_rate        | 0.000183  |
|    loss                 | -0.0091   |
|    n_updates            | 1535      |
|    policy_gradient_loss | -0.0365   |
|    value_loss           | 0.2       |
---------------------------------------
Ep done - 39000.
Ep done - 40000.
Eval num_timesteps=1107000, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.175      |
| time/                   |            |
|    total_timesteps      | 1107000    |
| train/                  |            |
|    approx_kl            | 0.27305335 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.797     |
|    explained_variance   | 0.137      |
|    learning_rate        | 0.000182   |
|    loss                 | -0.00818   |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.0375    |
|    value_loss           | 0.198      |
----------------------------------------
Ep done - 41000.
Ep done - 42000.
Eval num_timesteps=1168500, episode_reward=0.15 +/- 0.98
Episode length: 30.04 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 1168500    |
| train/                  |            |
|    approx_kl            | 0.27834216 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.819     |
|    explained_variance   | 0.108      |
|    learning_rate        | 0.000181   |
|    loss                 | -0.014     |
|    n_updates            | 1545       |
|    policy_gradient_loss | -0.0377    |
|    value_loss           | 0.201      |
----------------------------------------
Ep done - 43000.
Ep done - 44000.
Ep done - 45000.
Eval num_timesteps=1230000, episode_reward=0.09 +/- 0.98
Episode length: 29.95 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.09      |
| time/                   |           |
|    total_timesteps      | 1230000   |
| train/                  |           |
|    approx_kl            | 0.2588949 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.814    |
|    explained_variance   | 0.16      |
|    learning_rate        | 0.00018   |
|    loss                 | -0.0358   |
|    n_updates            | 1550      |
|    policy_gradient_loss | -0.0385   |
|    value_loss           | 0.194     |
---------------------------------------
Ep done - 46000.
Ep done - 47000.
Eval num_timesteps=1291500, episode_reward=0.17 +/- 0.98
Episode length: 30.00 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.175      |
| time/                   |            |
|    total_timesteps      | 1291500    |
| train/                  |            |
|    approx_kl            | 0.26118132 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.813     |
|    explained_variance   | 0.106      |
|    learning_rate        | 0.000178   |
|    loss                 | -0.00267   |
|    n_updates            | 1555       |
|    policy_gradient_loss | -0.0359    |
|    value_loss           | 0.206      |
----------------------------------------
Ep done - 48000.
Ep done - 49000.
Eval num_timesteps=1353000, episode_reward=0.17 +/- 0.98
Episode length: 29.98 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.165     |
| time/                   |           |
|    total_timesteps      | 1353000   |
| train/                  |           |
|    approx_kl            | 0.2637204 |
|    clip_fraction        | 0.425     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.814    |
|    explained_variance   | 0.129     |
|    learning_rate        | 0.000177  |
|    loss                 | -0.0131   |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.039    |
|    value_loss           | 0.203     |
---------------------------------------
Ep done - 50000.
Ep done - 51000.
Eval num_timesteps=1414500, episode_reward=0.12 +/- 0.98
Episode length: 29.95 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 1414500    |
| train/                  |            |
|    approx_kl            | 0.26748306 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.14       |
|    learning_rate        | 0.000176   |
|    loss                 | -0.0371    |
|    n_updates            | 1565       |
|    policy_gradient_loss | -0.0386    |
|    value_loss           | 0.205      |
----------------------------------------
Ep done - 52000.
Ep done - 53000.
Ep done - 54000.
Eval num_timesteps=1476000, episode_reward=0.27 +/- 0.96
Episode length: 30.07 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.265      |
| time/                   |            |
|    total_timesteps      | 1476000    |
| train/                  |            |
|    approx_kl            | 0.24654831 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.822     |
|    explained_variance   | 0.0887     |
|    learning_rate        | 0.000175   |
|    loss                 | -0.0103    |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0378    |
|    value_loss           | 0.208      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 7
Ep done - 55000.
Ep done - 56000.
Eval num_timesteps=1537500, episode_reward=0.27 +/- 0.94
Episode length: 30.04 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 1537500    |
| train/                  |            |
|    approx_kl            | 0.24385554 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.828     |
|    explained_variance   | 0.167      |
|    learning_rate        | 0.000174   |
|    loss                 | -0.00805   |
|    n_updates            | 1575       |
|    policy_gradient_loss | -0.0394    |
|    value_loss           | 0.207      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 8
Ep done - 57000.
Ep done - 58000.
Eval num_timesteps=1599000, episode_reward=0.21 +/- 0.96
Episode length: 30.05 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.215     |
| time/                   |           |
|    total_timesteps      | 1599000   |
| train/                  |           |
|    approx_kl            | 0.2575349 |
|    clip_fraction        | 0.418     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.818    |
|    explained_variance   | 0.155     |
|    learning_rate        | 0.000173  |
|    loss                 | -0.0259   |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.0417   |
|    value_loss           | 0.206     |
---------------------------------------
Ep done - 59000.
Ep done - 60000.
Eval num_timesteps=1660500, episode_reward=0.13 +/- 0.97
Episode length: 30.04 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 1660500    |
| train/                  |            |
|    approx_kl            | 0.24324541 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.811     |
|    explained_variance   | 0.132      |
|    learning_rate        | 0.000172   |
|    loss                 | 0.00776    |
|    n_updates            | 1585       |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.208      |
----------------------------------------
Ep done - 61000.
Ep done - 62000.
Ep done - 63000.
Eval num_timesteps=1722000, episode_reward=0.23 +/- 0.95
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.235      |
| time/                   |            |
|    total_timesteps      | 1722000    |
| train/                  |            |
|    approx_kl            | 0.24745616 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.805     |
|    explained_variance   | 0.149      |
|    learning_rate        | 0.000171   |
|    loss                 | -0.0214    |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.209      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 9
Ep done - 64000.
Ep done - 65000.
Eval num_timesteps=1783500, episode_reward=0.28 +/- 0.95
Episode length: 30.07 +/- 0.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.275      |
| time/                   |            |
|    total_timesteps      | 1783500    |
| train/                  |            |
|    approx_kl            | 0.24058387 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.806     |
|    explained_variance   | 0.161      |
|    learning_rate        | 0.00017    |
|    loss                 | -0.0161    |
|    n_updates            | 1595       |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.211      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 10
Ep done - 66000.
Ep done - 67000.
Eval num_timesteps=1845000, episode_reward=0.15 +/- 0.98
Episode length: 30.04 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 1845000    |
| train/                  |            |
|    approx_kl            | 0.25348386 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.798     |
|    explained_variance   | 0.152      |
|    learning_rate        | 0.000169   |
|    loss                 | -0.00341   |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0408    |
|    value_loss           | 0.214      |
----------------------------------------
Ep done - 68000.
Ep done - 69000.
Eval num_timesteps=1906500, episode_reward=0.10 +/- 0.98
Episode length: 30.08 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.105      |
| time/                   |            |
|    total_timesteps      | 1906500    |
| train/                  |            |
|    approx_kl            | 0.27109528 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.79      |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.000168   |
|    loss                 | 1.97e-05   |
|    n_updates            | 1605       |
|    policy_gradient_loss | -0.0406    |
|    value_loss           | 0.21       |
----------------------------------------
Ep done - 70000.
Ep done - 71000.
Ep done - 72000.
Eval num_timesteps=1968000, episode_reward=0.18 +/- 0.97
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 1968000    |
| train/                  |            |
|    approx_kl            | 0.27006182 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.158      |
|    learning_rate        | 0.000167   |
|    loss                 | -0.0109    |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0409    |
|    value_loss           | 0.213      |
----------------------------------------
Ep done - 73000.
Ep done - 74000.
Eval num_timesteps=2029500, episode_reward=0.20 +/- 0.95
Episode length: 29.95 +/- 0.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 2029500    |
| train/                  |            |
|    approx_kl            | 0.26249477 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.786     |
|    explained_variance   | 0.186      |
|    learning_rate        | 0.000166   |
|    loss                 | 0.00541    |
|    n_updates            | 1615       |
|    policy_gradient_loss | -0.0412    |
|    value_loss           | 0.213      |
----------------------------------------
Ep done - 75000.
Ep done - 76000.
Eval num_timesteps=2091000, episode_reward=0.18 +/- 0.97
Episode length: 30.02 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 2091000    |
| train/                  |            |
|    approx_kl            | 0.26094097 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.789     |
|    explained_variance   | 0.175      |
|    learning_rate        | 0.000165   |
|    loss                 | -0.00968   |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.041     |
|    value_loss           | 0.216      |
----------------------------------------
Ep done - 77000.
Ep done - 78000.
Eval num_timesteps=2152500, episode_reward=0.18 +/- 0.97
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.18       |
| time/                   |            |
|    total_timesteps      | 2152500    |
| train/                  |            |
|    approx_kl            | 0.26189226 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.784     |
|    explained_variance   | 0.166      |
|    learning_rate        | 0.000164   |
|    loss                 | -0.0127    |
|    n_updates            | 1625       |
|    policy_gradient_loss | -0.041     |
|    value_loss           | 0.223      |
----------------------------------------
Ep done - 79000.
Ep done - 80000.
Ep done - 81000.
Eval num_timesteps=2214000, episode_reward=0.26 +/- 0.95
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.26       |
| time/                   |            |
|    total_timesteps      | 2214000    |
| train/                  |            |
|    approx_kl            | 0.26234245 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.777     |
|    explained_variance   | 0.194      |
|    learning_rate        | 0.000163   |
|    loss                 | -0.0286    |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.0403    |
|    value_loss           | 0.216      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 11
Ep done - 82000.
Ep done - 83000.
Eval num_timesteps=2275500, episode_reward=0.27 +/- 0.96
Episode length: 30.02 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.265      |
| time/                   |            |
|    total_timesteps      | 2275500    |
| train/                  |            |
|    approx_kl            | 0.26396388 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.789     |
|    explained_variance   | 0.156      |
|    learning_rate        | 0.000162   |
|    loss                 | -0.04      |
|    n_updates            | 1635       |
|    policy_gradient_loss | -0.0408    |
|    value_loss           | 0.218      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 12
Ep done - 84000.
Ep done - 85000.
Eval num_timesteps=2337000, episode_reward=0.12 +/- 0.97
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 2337000    |
| train/                  |            |
|    approx_kl            | 0.26486105 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.193      |
|    learning_rate        | 0.000161   |
|    loss                 | -0.0128    |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0424    |
|    value_loss           | 0.212      |
----------------------------------------
Ep done - 86000.
Ep done - 87000.
Eval num_timesteps=2398500, episode_reward=0.12 +/- 0.99
Episode length: 30.07 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 2398500    |
| train/                  |            |
|    approx_kl            | 0.24586669 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.166      |
|    learning_rate        | 0.00016    |
|    loss                 | -0.0132    |
|    n_updates            | 1645       |
|    policy_gradient_loss | -0.0429    |
|    value_loss           | 0.221      |
----------------------------------------
Ep done - 88000.
Ep done - 89000.
Ep done - 90000.
Eval num_timesteps=2460000, episode_reward=0.06 +/- 0.99
Episode length: 29.91 +/- 1.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.055      |
| time/                   |            |
|    total_timesteps      | 2460000    |
| train/                  |            |
|    approx_kl            | 0.25364408 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.79      |
|    explained_variance   | 0.172      |
|    learning_rate        | 0.000159   |
|    loss                 | -0.00776   |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0424    |
|    value_loss           | 0.219      |
----------------------------------------
Ep done - 91000.
Ep done - 92000.
Eval num_timesteps=2521500, episode_reward=0.17 +/- 0.97
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.175      |
| time/                   |            |
|    total_timesteps      | 2521500    |
| train/                  |            |
|    approx_kl            | 0.25809485 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.79      |
|    explained_variance   | 0.188      |
|    learning_rate        | 0.000158   |
|    loss                 | -0.0227    |
|    n_updates            | 1655       |
|    policy_gradient_loss | -0.0436    |
|    value_loss           | 0.218      |
----------------------------------------
Ep done - 93000.
Ep done - 94000.
Eval num_timesteps=2583000, episode_reward=0.15 +/- 0.98
Episode length: 29.94 +/- 0.61
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 29.9     |
|    mean_reward          | 0.155    |
| time/                   |          |
|    total_timesteps      | 2583000  |
| train/                  |          |
|    approx_kl            | 0.258155 |
|    clip_fraction        | 0.401    |
|    clip_range           | 0.18     |
|    entropy_loss         | -0.788   |
|    explained_variance   | 0.161    |
|    learning_rate        | 0.000157 |
|    loss                 | -0.0131  |
|    n_updates            | 1660     |
|    policy_gradient_loss | -0.0414  |
|    value_loss           | 0.215    |
--------------------------------------
Ep done - 95000.
Ep done - 96000.
Eval num_timesteps=2644500, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.03       |
| time/                   |            |
|    total_timesteps      | 2644500    |
| train/                  |            |
|    approx_kl            | 0.24771048 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.786     |
|    explained_variance   | 0.171      |
|    learning_rate        | 0.000156   |
|    loss                 | -0.0144    |
|    n_updates            | 1665       |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.219      |
----------------------------------------
Ep done - 97000.
Ep done - 98000.
Ep done - 99000.
Eval num_timesteps=2706000, episode_reward=0.19 +/- 0.96
Episode length: 29.91 +/- 1.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.19       |
| time/                   |            |
|    total_timesteps      | 2706000    |
| train/                  |            |
|    approx_kl            | 0.25950173 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.788     |
|    explained_variance   | 0.172      |
|    learning_rate        | 0.000155   |
|    loss                 | 0.00234    |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0419    |
|    value_loss           | 0.214      |
----------------------------------------
Ep done - 100000.
Ep done - 101000.
Eval num_timesteps=2767500, episode_reward=0.14 +/- 0.98
Episode length: 30.07 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.14      |
| time/                   |           |
|    total_timesteps      | 2767500   |
| train/                  |           |
|    approx_kl            | 0.2521917 |
|    clip_fraction        | 0.406     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.788    |
|    explained_variance   | 0.142     |
|    learning_rate        | 0.000154  |
|    loss                 | -0.015    |
|    n_updates            | 1675      |
|    policy_gradient_loss | -0.0433   |
|    value_loss           | 0.215     |
---------------------------------------
Ep done - 102000.
Ep done - 103000.
Eval num_timesteps=2829000, episode_reward=0.22 +/- 0.96
Episode length: 30.01 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.22       |
| time/                   |            |
|    total_timesteps      | 2829000    |
| train/                  |            |
|    approx_kl            | 0.24747038 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.78      |
|    explained_variance   | 0.146      |
|    learning_rate        | 0.000153   |
|    loss                 | -0.0217    |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0425    |
|    value_loss           | 0.214      |
----------------------------------------
Ep done - 104000.
Ep done - 105000.
Eval num_timesteps=2890500, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.195     |
| time/                   |           |
|    total_timesteps      | 2890500   |
| train/                  |           |
|    approx_kl            | 0.2581376 |
|    clip_fraction        | 0.39      |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.764    |
|    explained_variance   | 0.135     |
|    learning_rate        | 0.000152  |
|    loss                 | -0.00315  |
|    n_updates            | 1685      |
|    policy_gradient_loss | -0.0425   |
|    value_loss           | 0.213     |
---------------------------------------
Ep done - 106000.
Ep done - 107000.
Ep done - 108000.
Eval num_timesteps=2952000, episode_reward=0.18 +/- 0.97
Episode length: 30.03 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.18       |
| time/                   |            |
|    total_timesteps      | 2952000    |
| train/                  |            |
|    approx_kl            | 0.25105917 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.148      |
|    learning_rate        | 0.000151   |
|    loss                 | -0.0119    |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0414    |
|    value_loss           | 0.211      |
----------------------------------------
Ep done - 109000.
Ep done - 110000.
Eval num_timesteps=3013500, episode_reward=0.23 +/- 0.96
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.235      |
| time/                   |            |
|    total_timesteps      | 3013500    |
| train/                  |            |
|    approx_kl            | 0.25103492 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.751     |
|    explained_variance   | 0.128      |
|    learning_rate        | 0.00015    |
|    loss                 | -0.0225    |
|    n_updates            | 1695       |
|    policy_gradient_loss | -0.0409    |
|    value_loss           | 0.211      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 13
Ep done - 111000.
Ep done - 112000.
Eval num_timesteps=3075000, episode_reward=0.14 +/- 0.97
Episode length: 30.01 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.135      |
| time/                   |            |
|    total_timesteps      | 3075000    |
| train/                  |            |
|    approx_kl            | 0.25144866 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.161      |
|    learning_rate        | 0.000149   |
|    loss                 | -0.00734   |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0416    |
|    value_loss           | 0.214      |
----------------------------------------
Ep done - 113000.
Ep done - 114000.
Eval num_timesteps=3136500, episode_reward=0.12 +/- 0.97
Episode length: 29.95 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 3136500    |
| train/                  |            |
|    approx_kl            | 0.25995043 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.141      |
|    learning_rate        | 0.000148   |
|    loss                 | 0.00252    |
|    n_updates            | 1705       |
|    policy_gradient_loss | -0.042     |
|    value_loss           | 0.217      |
----------------------------------------
Ep done - 115000.
Ep done - 116000.
Ep done - 117000.
Eval num_timesteps=3198000, episode_reward=0.15 +/- 0.97
Episode length: 30.00 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.15      |
| time/                   |           |
|    total_timesteps      | 3198000   |
| train/                  |           |
|    approx_kl            | 0.2482543 |
|    clip_fraction        | 0.387     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.757    |
|    explained_variance   | 0.163     |
|    learning_rate        | 0.000147  |
|    loss                 | -0.00992  |
|    n_updates            | 1710      |
|    policy_gradient_loss | -0.0419   |
|    value_loss           | 0.215     |
---------------------------------------
Ep done - 118000.
Ep done - 119000.
Eval num_timesteps=3259500, episode_reward=0.16 +/- 0.97
Episode length: 29.95 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.16      |
| time/                   |           |
|    total_timesteps      | 3259500   |
| train/                  |           |
|    approx_kl            | 0.2554002 |
|    clip_fraction        | 0.386     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.761    |
|    explained_variance   | 0.181     |
|    learning_rate        | 0.000146  |
|    loss                 | -0.00492  |
|    n_updates            | 1715      |
|    policy_gradient_loss | -0.0428   |
|    value_loss           | 0.217     |
---------------------------------------
Ep done - 120000.
Ep done - 121000.
Eval num_timesteps=3321000, episode_reward=0.14 +/- 0.97
Episode length: 30.07 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.145      |
| time/                   |            |
|    total_timesteps      | 3321000    |
| train/                  |            |
|    approx_kl            | 0.24965258 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.761     |
|    explained_variance   | 0.187      |
|    learning_rate        | 0.000145   |
|    loss                 | 0.00509    |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.0427    |
|    value_loss           | 0.221      |
----------------------------------------
Ep done - 122000.
Ep done - 123000.
Eval num_timesteps=3382500, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.56
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30       |
|    mean_reward          | 0.1      |
| time/                   |          |
|    total_timesteps      | 3382500  |
| train/                  |          |
|    approx_kl            | 0.248362 |
|    clip_fraction        | 0.386    |
|    clip_range           | 0.18     |
|    entropy_loss         | -0.758   |
|    explained_variance   | 0.179    |
|    learning_rate        | 0.000144 |
|    loss                 | -0.0201  |
|    n_updates            | 1725     |
|    policy_gradient_loss | -0.0442  |
|    value_loss           | 0.222    |
--------------------------------------
Ep done - 124000.
Ep done - 125000.
Ep done - 126000.
Eval num_timesteps=3444000, episode_reward=0.14 +/- 0.97
Episode length: 30.04 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.145      |
| time/                   |            |
|    total_timesteps      | 3444000    |
| train/                  |            |
|    approx_kl            | 0.24263522 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.000143   |
|    loss                 | -0.00652   |
|    n_updates            | 1730       |
|    policy_gradient_loss | -0.0436    |
|    value_loss           | 0.221      |
----------------------------------------
Ep done - 127000.
Ep done - 128000.
Eval num_timesteps=3505500, episode_reward=0.07 +/- 0.99
Episode length: 30.00 +/- 0.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.075     |
| time/                   |           |
|    total_timesteps      | 3505500   |
| train/                  |           |
|    approx_kl            | 0.2565745 |
|    clip_fraction        | 0.383     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.743    |
|    explained_variance   | 0.149     |
|    learning_rate        | 0.000142  |
|    loss                 | 0.0226    |
|    n_updates            | 1735      |
|    policy_gradient_loss | -0.0423   |
|    value_loss           | 0.223     |
---------------------------------------
Ep done - 129000.
Ep done - 130000.
Eval num_timesteps=3567000, episode_reward=0.18 +/- 0.97
Episode length: 29.98 +/- 0.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.18      |
| time/                   |           |
|    total_timesteps      | 3567000   |
| train/                  |           |
|    approx_kl            | 0.2637971 |
|    clip_fraction        | 0.385     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.742    |
|    explained_variance   | 0.195     |
|    learning_rate        | 0.000141  |
|    loss                 | 0.00953   |
|    n_updates            | 1740      |
|    policy_gradient_loss | -0.0426   |
|    value_loss           | 0.22      |
---------------------------------------
Ep done - 131000.
Ep done - 132000.
Eval num_timesteps=3628500, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 3628500    |
| train/                  |            |
|    approx_kl            | 0.25073442 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.747     |
|    explained_variance   | 0.198      |
|    learning_rate        | 0.00014    |
|    loss                 | 0.0078     |
|    n_updates            | 1745       |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.221      |
----------------------------------------
Ep done - 133000.
Ep done - 134000.
Ep done - 135000.
Eval num_timesteps=3690000, episode_reward=0.26 +/- 0.95
Episode length: 30.09 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.26       |
| time/                   |            |
|    total_timesteps      | 3690000    |
| train/                  |            |
|    approx_kl            | 0.25173965 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.742     |
|    explained_variance   | 0.162      |
|    learning_rate        | 0.000139   |
|    loss                 | 0.00956    |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.225      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 14
Ep done - 136000.
Ep done - 137000.
Eval num_timesteps=3751500, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.165     |
| time/                   |           |
|    total_timesteps      | 3751500   |
| train/                  |           |
|    approx_kl            | 0.2601114 |
|    clip_fraction        | 0.392     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.758    |
|    explained_variance   | 0.189     |
|    learning_rate        | 0.000138  |
|    loss                 | -0.00488  |
|    n_updates            | 1755      |
|    policy_gradient_loss | -0.043    |
|    value_loss           | 0.223     |
---------------------------------------
Ep done - 138000.
Ep done - 139000.
Eval num_timesteps=3813000, episode_reward=0.18 +/- 0.96
Episode length: 30.05 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 3813000    |
| train/                  |            |
|    approx_kl            | 0.25285488 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.000137   |
|    loss                 | -0.0185    |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0427    |
|    value_loss           | 0.22       |
----------------------------------------
Ep done - 140000.
Ep done - 141000.
Eval num_timesteps=3874500, episode_reward=0.17 +/- 0.96
Episode length: 30.02 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 3874500    |
| train/                  |            |
|    approx_kl            | 0.25149634 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.178      |
|    learning_rate        | 0.000135   |
|    loss                 | 0.0118     |
|    n_updates            | 1765       |
|    policy_gradient_loss | -0.0433    |
|    value_loss           | 0.224      |
----------------------------------------
Ep done - 142000.
Ep done - 143000.
Ep done - 144000.
Eval num_timesteps=3936000, episode_reward=0.14 +/- 0.97
Episode length: 30.01 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.14      |
| time/                   |           |
|    total_timesteps      | 3936000   |
| train/                  |           |
|    approx_kl            | 0.2526494 |
|    clip_fraction        | 0.383     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.744    |
|    explained_variance   | 0.192     |
|    learning_rate        | 0.000134  |
|    loss                 | -0.00214  |
|    n_updates            | 1770      |
|    policy_gradient_loss | -0.0434   |
|    value_loss           | 0.221     |
---------------------------------------
Ep done - 145000.
Ep done - 146000.
Eval num_timesteps=3997500, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.095      |
| time/                   |            |
|    total_timesteps      | 3997500    |
| train/                  |            |
|    approx_kl            | 0.24806446 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.207      |
|    learning_rate        | 0.000133   |
|    loss                 | -0.0032    |
|    n_updates            | 1775       |
|    policy_gradient_loss | -0.0427    |
|    value_loss           | 0.226      |
----------------------------------------
Ep done - 147000.
Ep done - 148000.
Eval num_timesteps=4059000, episode_reward=0.16 +/- 0.97
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.16       |
| time/                   |            |
|    total_timesteps      | 4059000    |
| train/                  |            |
|    approx_kl            | 0.24765788 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.215      |
|    learning_rate        | 0.000132   |
|    loss                 | 0.0178     |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0443    |
|    value_loss           | 0.22       |
----------------------------------------
Ep done - 149000.
Ep done - 150000.
Eval num_timesteps=4120500, episode_reward=0.27 +/- 0.95
Episode length: 30.07 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 4120500    |
| train/                  |            |
|    approx_kl            | 0.24089439 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.209      |
|    learning_rate        | 0.000131   |
|    loss                 | 0.0102     |
|    n_updates            | 1785       |
|    policy_gradient_loss | -0.0452    |
|    value_loss           | 0.228      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 15
Ep done - 151000.
Ep done - 152000.
Ep done - 153000.
Eval num_timesteps=4182000, episode_reward=0.23 +/- 0.97
Episode length: 30.09 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.225     |
| time/                   |           |
|    total_timesteps      | 4182000   |
| train/                  |           |
|    approx_kl            | 0.2469729 |
|    clip_fraction        | 0.384     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.758    |
|    explained_variance   | 0.203     |
|    learning_rate        | 0.00013   |
|    loss                 | -0.0105   |
|    n_updates            | 1790      |
|    policy_gradient_loss | -0.0455   |
|    value_loss           | 0.232     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 16
Ep done - 154000.
Ep done - 155000.
Eval num_timesteps=4243500, episode_reward=0.07 +/- 0.98
Episode length: 30.09 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.075      |
| time/                   |            |
|    total_timesteps      | 4243500    |
| train/                  |            |
|    approx_kl            | 0.24464223 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.238      |
|    learning_rate        | 0.000129   |
|    loss                 | -0.00795   |
|    n_updates            | 1795       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 156000.
Ep done - 157000.
Eval num_timesteps=4305000, episode_reward=0.23 +/- 0.96
Episode length: 30.05 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.235      |
| time/                   |            |
|    total_timesteps      | 4305000    |
| train/                  |            |
|    approx_kl            | 0.26142523 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.229      |
|    learning_rate        | 0.000128   |
|    loss                 | 0.00843    |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.226      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 17
Ep done - 158000.
Ep done - 159000.
Eval num_timesteps=4366500, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 4366500    |
| train/                  |            |
|    approx_kl            | 0.24089175 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.216      |
|    learning_rate        | 0.000127   |
|    loss                 | 0.0232     |
|    n_updates            | 1805       |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.228      |
----------------------------------------
Ep done - 160000.
Ep done - 161000.
Ep done - 162000.
Eval num_timesteps=4428000, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.135      |
| time/                   |            |
|    total_timesteps      | 4428000    |
| train/                  |            |
|    approx_kl            | 0.24328487 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.208      |
|    learning_rate        | 0.000126   |
|    loss                 | -0.0114    |
|    n_updates            | 1810       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.234      |
----------------------------------------
Ep done - 163000.
Ep done - 164000.
Eval num_timesteps=4489500, episode_reward=0.21 +/- 0.96
Episode length: 30.09 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.215      |
| time/                   |            |
|    total_timesteps      | 4489500    |
| train/                  |            |
|    approx_kl            | 0.25962618 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.737     |
|    explained_variance   | 0.239      |
|    learning_rate        | 0.000125   |
|    loss                 | 0.0275     |
|    n_updates            | 1815       |
|    policy_gradient_loss | -0.0458    |
|    value_loss           | 0.227      |
----------------------------------------
Ep done - 165000.
Ep done - 166000.
Eval num_timesteps=4551000, episode_reward=0.28 +/- 0.95
Episode length: 30.09 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.285      |
| time/                   |            |
|    total_timesteps      | 4551000    |
| train/                  |            |
|    approx_kl            | 0.25644264 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.734     |
|    explained_variance   | 0.234      |
|    learning_rate        | 0.000124   |
|    loss                 | 0.0154     |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.231      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 18
Ep done - 167000.
Ep done - 168000.
Eval num_timesteps=4612500, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 4612500    |
| train/                  |            |
|    approx_kl            | 0.24405572 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.227      |
|    learning_rate        | 0.000123   |
|    loss                 | 0.00684    |
|    n_updates            | 1825       |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 169000.
Ep done - 170000.
Ep done - 171000.
Eval num_timesteps=4674000, episode_reward=0.20 +/- 0.96
Episode length: 30.02 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.205     |
| time/                   |           |
|    total_timesteps      | 4674000   |
| train/                  |           |
|    approx_kl            | 0.2375596 |
|    clip_fraction        | 0.367     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.73     |
|    explained_variance   | 0.247     |
|    learning_rate        | 0.000122  |
|    loss                 | 0.0106    |
|    n_updates            | 1830      |
|    policy_gradient_loss | -0.0447   |
|    value_loss           | 0.229     |
---------------------------------------
Ep done - 172000.
Ep done - 173000.
Eval num_timesteps=4735500, episode_reward=0.22 +/- 0.95
Episode length: 30.02 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.22       |
| time/                   |            |
|    total_timesteps      | 4735500    |
| train/                  |            |
|    approx_kl            | 0.23762117 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.235      |
|    learning_rate        | 0.000121   |
|    loss                 | -0.0223    |
|    n_updates            | 1835       |
|    policy_gradient_loss | -0.0452    |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 174000.
Ep done - 175000.
Eval num_timesteps=4797000, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.045      |
| time/                   |            |
|    total_timesteps      | 4797000    |
| train/                  |            |
|    approx_kl            | 0.24243546 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.233      |
|    learning_rate        | 0.00012    |
|    loss                 | -0.00574   |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 176000.
Ep done - 177000.
Eval num_timesteps=4858500, episode_reward=0.09 +/- 0.99
Episode length: 29.96 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 4858500    |
| train/                  |            |
|    approx_kl            | 0.24791428 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.244      |
|    learning_rate        | 0.000119   |
|    loss                 | -0.00264   |
|    n_updates            | 1845       |
|    policy_gradient_loss | -0.0457    |
|    value_loss           | 0.233      |
----------------------------------------
Ep done - 178000.
Ep done - 179000.
Ep done - 180000.
Eval num_timesteps=4920000, episode_reward=0.12 +/- 0.99
Episode length: 29.95 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 4920000    |
| train/                  |            |
|    approx_kl            | 0.23882529 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.722     |
|    explained_variance   | 0.263      |
|    learning_rate        | 0.000118   |
|    loss                 | 0.0139     |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.0466    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 181000.
Ep done - 182000.
Eval num_timesteps=4981500, episode_reward=0.26 +/- 0.94
Episode length: 30.07 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.26       |
| time/                   |            |
|    total_timesteps      | 4981500    |
| train/                  |            |
|    approx_kl            | 0.23257884 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.267      |
|    learning_rate        | 0.000117   |
|    loss                 | 0.0165     |
|    n_updates            | 1855       |
|    policy_gradient_loss | -0.0464    |
|    value_loss           | 0.23       |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 19
Ep done - 183000.
Ep done - 184000.
Eval num_timesteps=5043000, episode_reward=0.19 +/- 0.96
Episode length: 30.05 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.19       |
| time/                   |            |
|    total_timesteps      | 5043000    |
| train/                  |            |
|    approx_kl            | 0.23587142 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.27       |
|    learning_rate        | 0.000116   |
|    loss                 | 0.0181     |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0459    |
|    value_loss           | 0.231      |
----------------------------------------
Ep done - 185000.
Ep done - 186000.
Eval num_timesteps=5104500, episode_reward=0.20 +/- 0.97
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 5104500    |
| train/                  |            |
|    approx_kl            | 0.22166371 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.259      |
|    learning_rate        | 0.000115   |
|    loss                 | 0.0121     |
|    n_updates            | 1865       |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.235      |
----------------------------------------
Ep done - 187000.
Ep done - 188000.
Ep done - 189000.
Eval num_timesteps=5166000, episode_reward=0.27 +/- 0.95
Episode length: 30.07 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.265      |
| time/                   |            |
|    total_timesteps      | 5166000    |
| train/                  |            |
|    approx_kl            | 0.23112656 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.234      |
|    learning_rate        | 0.000114   |
|    loss                 | 0.0173     |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.237      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 20
Ep done - 190000.
Ep done - 191000.
Eval num_timesteps=5227500, episode_reward=0.24 +/- 0.96
Episode length: 30.03 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.245      |
| time/                   |            |
|    total_timesteps      | 5227500    |
| train/                  |            |
|    approx_kl            | 0.21310116 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.714     |
|    explained_variance   | 0.256      |
|    learning_rate        | 0.000113   |
|    loss                 | 0.0157     |
|    n_updates            | 1875       |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.237      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.245
SELFPLAY: new best model, bumping up generation to 21
Ep done - 192000.
Ep done - 193000.
Eval num_timesteps=5289000, episode_reward=0.12 +/- 0.99
Episode length: 29.89 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 5289000    |
| train/                  |            |
|    approx_kl            | 0.22381483 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.000112   |
|    loss                 | 0.00912    |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0462    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 194000.
Ep done - 195000.
Eval num_timesteps=5350500, episode_reward=0.30 +/- 0.93
Episode length: 30.04 +/- 0.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5350500   |
| train/                  |           |
|    approx_kl            | 0.2140913 |
|    clip_fraction        | 0.35      |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.712    |
|    explained_variance   | 0.29      |
|    learning_rate        | 0.000111  |
|    loss                 | 0.0279    |
|    n_updates            | 1885      |
|    policy_gradient_loss | -0.046    |
|    value_loss           | 0.234     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 22
Ep done - 196000.
Ep done - 197000.
Ep done - 198000.
Eval num_timesteps=5412000, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 5412000    |
| train/                  |            |
|    approx_kl            | 0.22063844 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.701     |
|    explained_variance   | 0.292      |
|    learning_rate        | 0.00011    |
|    loss                 | 0.049      |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0457    |
|    value_loss           | 0.232      |
----------------------------------------
Ep done - 199000.
Ep done - 200000.
Eval num_timesteps=5473500, episode_reward=0.22 +/- 0.97
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.22       |
| time/                   |            |
|    total_timesteps      | 5473500    |
| train/                  |            |
|    approx_kl            | 0.21035676 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.698     |
|    explained_variance   | 0.304      |
|    learning_rate        | 0.000109   |
|    loss                 | 0.079      |
|    n_updates            | 1895       |
|    policy_gradient_loss | -0.0464    |
|    value_loss           | 0.23       |
----------------------------------------
Ep done - 201000.
Ep done - 202000.
Eval num_timesteps=5535000, episode_reward=0.12 +/- 0.99
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 5535000    |
| train/                  |            |
|    approx_kl            | 0.21719716 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.694     |
|    explained_variance   | 0.286      |
|    learning_rate        | 0.000108   |
|    loss                 | 0.0197     |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.235      |
----------------------------------------
Ep done - 203000.
Ep done - 204000.
Eval num_timesteps=5596500, episode_reward=0.23 +/- 0.95
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.23       |
| time/                   |            |
|    total_timesteps      | 5596500    |
| train/                  |            |
|    approx_kl            | 0.21104258 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.272      |
|    learning_rate        | 0.000107   |
|    loss                 | 0.00861    |
|    n_updates            | 1905       |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.236      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.23
SELFPLAY: new best model, bumping up generation to 23
Ep done - 205000.
Ep done - 206000.
Ep done - 207000.
Eval num_timesteps=5658000, episode_reward=0.14 +/- 0.97
Episode length: 30.00 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.135     |
| time/                   |           |
|    total_timesteps      | 5658000   |
| train/                  |           |
|    approx_kl            | 0.2148095 |
|    clip_fraction        | 0.338     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.691    |
|    explained_variance   | 0.299     |
|    learning_rate        | 0.000106  |
|    loss                 | 0.00814   |
|    n_updates            | 1910      |
|    policy_gradient_loss | -0.0448   |
|    value_loss           | 0.235     |
---------------------------------------
Ep done - 208000.
Ep done - 209000.
Eval num_timesteps=5719500, episode_reward=0.19 +/- 0.97
Episode length: 30.05 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.19       |
| time/                   |            |
|    total_timesteps      | 5719500    |
| train/                  |            |
|    approx_kl            | 0.20841153 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.29       |
|    learning_rate        | 0.000105   |
|    loss                 | 0.0378     |
|    n_updates            | 1915       |
|    policy_gradient_loss | -0.0449    |
|    value_loss           | 0.236      |
----------------------------------------
Ep done - 210000.
Ep done - 211000.
Eval num_timesteps=5781000, episode_reward=0.20 +/- 0.97
Episode length: 30.05 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 5781000    |
| train/                  |            |
|    approx_kl            | 0.20783256 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.685     |
|    explained_variance   | 0.283      |
|    learning_rate        | 0.000104   |
|    loss                 | 0.0364     |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.239      |
----------------------------------------
Ep done - 212000.
Ep done - 213000.
Eval num_timesteps=5842500, episode_reward=0.16 +/- 0.97
Episode length: 29.98 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.16       |
| time/                   |            |
|    total_timesteps      | 5842500    |
| train/                  |            |
|    approx_kl            | 0.20729609 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.685     |
|    explained_variance   | 0.281      |
|    learning_rate        | 0.000103   |
|    loss                 | 0.0198     |
|    n_updates            | 1925       |
|    policy_gradient_loss | -0.0452    |
|    value_loss           | 0.236      |
----------------------------------------
Ep done - 214000.
Ep done - 215000.
Ep done - 216000.
Eval num_timesteps=5904000, episode_reward=0.20 +/- 0.97
Episode length: 30.08 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.205     |
| time/                   |           |
|    total_timesteps      | 5904000   |
| train/                  |           |
|    approx_kl            | 0.2003927 |
|    clip_fraction        | 0.333     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.688    |
|    explained_variance   | 0.284     |
|    learning_rate        | 0.000102  |
|    loss                 | 0.0715    |
|    n_updates            | 1930      |
|    policy_gradient_loss | -0.0454   |
|    value_loss           | 0.232     |
---------------------------------------
Ep done - 217000.
Ep done - 218000.
Eval num_timesteps=5965500, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.075      |
| time/                   |            |
|    total_timesteps      | 5965500    |
| train/                  |            |
|    approx_kl            | 0.19624013 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.676     |
|    explained_variance   | 0.277      |
|    learning_rate        | 0.000101   |
|    loss                 | 0.0193     |
|    n_updates            | 1935       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.236      |
----------------------------------------
Ep done - 219000.
Ep done - 220000.
Eval num_timesteps=6027000, episode_reward=0.19 +/- 0.97
Episode length: 30.07 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.19       |
| time/                   |            |
|    total_timesteps      | 6027000    |
| train/                  |            |
|    approx_kl            | 0.20133232 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.675     |
|    explained_variance   | 0.278      |
|    learning_rate        | 9.96e-05   |
|    loss                 | 0.0266     |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.0459    |
|    value_loss           | 0.234      |
----------------------------------------
Ep done - 221000.
Ep done - 222000.
Eval num_timesteps=6088500, episode_reward=0.12 +/- 0.98
Episode length: 30.08 +/- 0.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.12      |
| time/                   |           |
|    total_timesteps      | 6088500   |
| train/                  |           |
|    approx_kl            | 0.1966408 |
|    clip_fraction        | 0.325     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.672    |
|    explained_variance   | 0.291     |
|    learning_rate        | 9.86e-05  |
|    loss                 | 0.00249   |
|    n_updates            | 1945      |
|    policy_gradient_loss | -0.0452   |
|    value_loss           | 0.228     |
---------------------------------------
Ep done - 223000.
Ep done - 224000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 100      |
|    time_elapsed    | 35105    |
|    total_timesteps | 6144000  |
---------------------------------
Ep done - 225000.
Eval num_timesteps=6150000, episode_reward=0.23 +/- 0.96
Episode length: 29.98 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 6150000    |
| train/                  |            |
|    approx_kl            | 0.20299967 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.67      |
|    explained_variance   | 0.294      |
|    learning_rate        | 9.76e-05   |
|    loss                 | 0.0205     |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.234      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 24
Ep done - 226000.
Ep done - 227000.
Eval num_timesteps=6211500, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 6211500    |
| train/                  |            |
|    approx_kl            | 0.19465855 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.668     |
|    explained_variance   | 0.283      |
|    learning_rate        | 9.66e-05   |
|    loss                 | 0.00939    |
|    n_updates            | 1955       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 228000.
Ep done - 229000.
Eval num_timesteps=6273000, episode_reward=0.03 +/- 0.97
Episode length: 29.96 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.03       |
| time/                   |            |
|    total_timesteps      | 6273000    |
| train/                  |            |
|    approx_kl            | 0.20602986 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.664     |
|    explained_variance   | 0.306      |
|    learning_rate        | 9.56e-05   |
|    loss                 | 0.0371     |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.233      |
----------------------------------------
Ep done - 230000.
Ep done - 231000.
Eval num_timesteps=6334500, episode_reward=0.12 +/- 0.98
Episode length: 30.04 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 6334500    |
| train/                  |            |
|    approx_kl            | 0.20156555 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.663     |
|    explained_variance   | 0.298      |
|    learning_rate        | 9.45e-05   |
|    loss                 | 0.0187     |
|    n_updates            | 1965       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.233      |
----------------------------------------
Ep done - 232000.
Ep done - 233000.
Ep done - 234000.
Eval num_timesteps=6396000, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 6396000    |
| train/                  |            |
|    approx_kl            | 0.21281649 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.658     |
|    explained_variance   | 0.292      |
|    learning_rate        | 9.35e-05   |
|    loss                 | 0.0308     |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0458    |
|    value_loss           | 0.234      |
----------------------------------------
Ep done - 235000.
Ep done - 236000.
Eval num_timesteps=6457500, episode_reward=0.13 +/- 0.99
Episode length: 29.99 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.13      |
| time/                   |           |
|    total_timesteps      | 6457500   |
| train/                  |           |
|    approx_kl            | 0.1955232 |
|    clip_fraction        | 0.322     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.655    |
|    explained_variance   | 0.276     |
|    learning_rate        | 9.25e-05  |
|    loss                 | 0.0558    |
|    n_updates            | 1975      |
|    policy_gradient_loss | -0.0447   |
|    value_loss           | 0.239     |
---------------------------------------
Ep done - 237000.
Ep done - 238000.
Eval num_timesteps=6519000, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 6519000    |
| train/                  |            |
|    approx_kl            | 0.19961445 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.3        |
|    learning_rate        | 9.15e-05   |
|    loss                 | 0.00868    |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.0449    |
|    value_loss           | 0.236      |
----------------------------------------
Ep done - 239000.
Ep done - 240000.
Eval num_timesteps=6580500, episode_reward=0.06 +/- 0.98
Episode length: 29.97 +/- 0.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.06      |
| time/                   |           |
|    total_timesteps      | 6580500   |
| train/                  |           |
|    approx_kl            | 0.2008586 |
|    clip_fraction        | 0.319     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.65     |
|    explained_variance   | 0.289     |
|    learning_rate        | 9.04e-05  |
|    loss                 | 0.0322    |
|    n_updates            | 1985      |
|    policy_gradient_loss | -0.0442   |
|    value_loss           | 0.24      |
---------------------------------------
Ep done - 241000.
Ep done - 242000.
Ep done - 243000.
Eval num_timesteps=6642000, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.145      |
| time/                   |            |
|    total_timesteps      | 6642000    |
| train/                  |            |
|    approx_kl            | 0.19368626 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.298      |
|    learning_rate        | 8.94e-05   |
|    loss                 | 0.0121     |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.0457    |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 244000.
Ep done - 245000.
Eval num_timesteps=6703500, episode_reward=0.17 +/- 0.98
Episode length: 30.01 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.175     |
| time/                   |           |
|    total_timesteps      | 6703500   |
| train/                  |           |
|    approx_kl            | 0.2023732 |
|    clip_fraction        | 0.317     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.643    |
|    explained_variance   | 0.294     |
|    learning_rate        | 8.84e-05  |
|    loss                 | 0.0354    |
|    n_updates            | 1995      |
|    policy_gradient_loss | -0.0448   |
|    value_loss           | 0.24      |
---------------------------------------
Ep done - 246000.
Ep done - 247000.
Eval num_timesteps=6765000, episode_reward=0.15 +/- 0.95
Episode length: 29.98 +/- 0.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 6765000    |
| train/                  |            |
|    approx_kl            | 0.19764929 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.645     |
|    explained_variance   | 0.309      |
|    learning_rate        | 8.74e-05   |
|    loss                 | 0.0378     |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.0449    |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 248000.
Ep done - 249000.
Eval num_timesteps=6826500, episode_reward=0.15 +/- 0.97
Episode length: 30.05 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.1      |
|    mean_reward          | 0.15      |
| time/                   |           |
|    total_timesteps      | 6826500   |
| train/                  |           |
|    approx_kl            | 0.1931028 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.639    |
|    explained_variance   | 0.29      |
|    learning_rate        | 8.63e-05  |
|    loss                 | 0.0268    |
|    n_updates            | 2005      |
|    policy_gradient_loss | -0.0449   |
|    value_loss           | 0.239     |
---------------------------------------
Ep done - 250000.
Ep done - 251000.
Ep done - 252000.
Eval num_timesteps=6888000, episode_reward=0.26 +/- 0.94
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.255      |
| time/                   |            |
|    total_timesteps      | 6888000    |
| train/                  |            |
|    approx_kl            | 0.19610819 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.64      |
|    explained_variance   | 0.303      |
|    learning_rate        | 8.53e-05   |
|    loss                 | 0.0195     |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.0455    |
|    value_loss           | 0.235      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 25
Ep done - 253000.
Ep done - 254000.
Eval num_timesteps=6949500, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.165      |
| time/                   |            |
|    total_timesteps      | 6949500    |
| train/                  |            |
|    approx_kl            | 0.18869959 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.642     |
|    explained_variance   | 0.294      |
|    learning_rate        | 8.43e-05   |
|    loss                 | 0.0284     |
|    n_updates            | 2015       |
|    policy_gradient_loss | -0.0439    |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 255000.
Ep done - 256000.
Eval num_timesteps=7011000, episode_reward=0.04 +/- 0.98
Episode length: 29.92 +/- 1.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.9      |
|    mean_reward          | 0.045     |
| time/                   |           |
|    total_timesteps      | 7011000   |
| train/                  |           |
|    approx_kl            | 0.2071837 |
|    clip_fraction        | 0.31      |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.639    |
|    explained_variance   | 0.32      |
|    learning_rate        | 8.33e-05  |
|    loss                 | 0.0329    |
|    n_updates            | 2020      |
|    policy_gradient_loss | -0.0452   |
|    value_loss           | 0.233     |
---------------------------------------
Ep done - 257000.
Ep done - 258000.
Eval num_timesteps=7072500, episode_reward=0.02 +/- 0.99
Episode length: 30.02 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 7072500    |
| train/                  |            |
|    approx_kl            | 0.18812254 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.645     |
|    explained_variance   | 0.322      |
|    learning_rate        | 8.22e-05   |
|    loss                 | 0.0289     |
|    n_updates            | 2025       |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.236      |
----------------------------------------
Ep done - 259000.
Ep done - 260000.
Ep done - 261000.
Eval num_timesteps=7134000, episode_reward=0.21 +/- 0.97
Episode length: 29.98 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 7134000    |
| train/                  |            |
|    approx_kl            | 0.18113998 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.647     |
|    explained_variance   | 0.307      |
|    learning_rate        | 8.12e-05   |
|    loss                 | 0.0241     |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.242      |
----------------------------------------
Ep done - 262000.
Ep done - 263000.
Eval num_timesteps=7195500, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.055     |
| time/                   |           |
|    total_timesteps      | 7195500   |
| train/                  |           |
|    approx_kl            | 0.1807161 |
|    clip_fraction        | 0.306     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.639    |
|    explained_variance   | 0.3       |
|    learning_rate        | 8.02e-05  |
|    loss                 | 0.0369    |
|    n_updates            | 2035      |
|    policy_gradient_loss | -0.0447   |
|    value_loss           | 0.238     |
---------------------------------------
Ep done - 264000.
Ep done - 265000.
Eval num_timesteps=7257000, episode_reward=0.15 +/- 0.96
Episode length: 29.99 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.15       |
| time/                   |            |
|    total_timesteps      | 7257000    |
| train/                  |            |
|    approx_kl            | 0.18528749 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.641     |
|    explained_variance   | 0.295      |
|    learning_rate        | 7.92e-05   |
|    loss                 | 0.0197     |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.0455    |
|    value_loss           | 0.242      |
----------------------------------------
Ep done - 266000.
Ep done - 267000.
Eval num_timesteps=7318500, episode_reward=0.20 +/- 0.97
Episode length: 30.04 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.205      |
| time/                   |            |
|    total_timesteps      | 7318500    |
| train/                  |            |
|    approx_kl            | 0.18132763 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.639     |
|    explained_variance   | 0.322      |
|    learning_rate        | 7.81e-05   |
|    loss                 | 0.0575     |
|    n_updates            | 2045       |
|    policy_gradient_loss | -0.0455    |
|    value_loss           | 0.236      |
----------------------------------------
Ep done - 268000.
Ep done - 269000.
Ep done - 270000.
Eval num_timesteps=7380000, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 7380000    |
| train/                  |            |
|    approx_kl            | 0.17897816 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.636     |
|    explained_variance   | 0.316      |
|    learning_rate        | 7.71e-05   |
|    loss                 | 0.0453     |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 271000.
Ep done - 272000.
Eval num_timesteps=7441500, episode_reward=0.18 +/- 0.96
Episode length: 30.05 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.18       |
| time/                   |            |
|    total_timesteps      | 7441500    |
| train/                  |            |
|    approx_kl            | 0.18091136 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.639     |
|    explained_variance   | 0.33       |
|    learning_rate        | 7.61e-05   |
|    loss                 | 0.021      |
|    n_updates            | 2055       |
|    policy_gradient_loss | -0.0458    |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 273000.
Ep done - 274000.
Eval num_timesteps=7503000, episode_reward=0.19 +/- 0.97
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.19       |
| time/                   |            |
|    total_timesteps      | 7503000    |
| train/                  |            |
|    approx_kl            | 0.17464443 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.639     |
|    explained_variance   | 0.313      |
|    learning_rate        | 7.51e-05   |
|    loss                 | 0.0141     |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.239      |
----------------------------------------
Ep done - 275000.
Ep done - 276000.
Eval num_timesteps=7564500, episode_reward=0.10 +/- 0.98
Episode length: 30.04 +/- 0.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.095     |
| time/                   |           |
|    total_timesteps      | 7564500   |
| train/                  |           |
|    approx_kl            | 0.1745369 |
|    clip_fraction        | 0.301     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.644    |
|    explained_variance   | 0.325     |
|    learning_rate        | 7.4e-05   |
|    loss                 | 0.0299    |
|    n_updates            | 2065      |
|    policy_gradient_loss | -0.046    |
|    value_loss           | 0.235     |
---------------------------------------
Ep done - 277000.
Ep done - 278000.
Ep done - 279000.
Eval num_timesteps=7626000, episode_reward=0.08 +/- 0.99
Episode length: 30.01 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 7626000    |
| train/                  |            |
|    approx_kl            | 0.17006817 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.64      |
|    explained_variance   | 0.32       |
|    learning_rate        | 7.3e-05    |
|    loss                 | 0.0214     |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.0459    |
|    value_loss           | 0.239      |
----------------------------------------
Ep done - 280000.
Ep done - 281000.
Eval num_timesteps=7687500, episode_reward=0.12 +/- 0.98
Episode length: 30.05 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 7687500    |
| train/                  |            |
|    approx_kl            | 0.17106304 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.305      |
|    learning_rate        | 7.2e-05    |
|    loss                 | 0.0208     |
|    n_updates            | 2075       |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.241      |
----------------------------------------
Ep done - 282000.
Ep done - 283000.
Eval num_timesteps=7749000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 7749000    |
| train/                  |            |
|    approx_kl            | 0.17945036 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.631     |
|    explained_variance   | 0.317      |
|    learning_rate        | 7.1e-05    |
|    loss                 | 0.00399    |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.0457    |
|    value_loss           | 0.235      |
----------------------------------------
Ep done - 284000.
Ep done - 285000.
Eval num_timesteps=7810500, episode_reward=0.21 +/- 0.96
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.215      |
| time/                   |            |
|    total_timesteps      | 7810500    |
| train/                  |            |
|    approx_kl            | 0.17014137 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.628     |
|    explained_variance   | 0.315      |
|    learning_rate        | 7e-05      |
|    loss                 | 0.0364     |
|    n_updates            | 2085       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.24       |
----------------------------------------
Ep done - 286000.
Ep done - 287000.
Ep done - 288000.
Eval num_timesteps=7872000, episode_reward=-0.02 +/- 0.98
Episode length: 29.96 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 7872000    |
| train/                  |            |
|    approx_kl            | 0.17286286 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.626     |
|    explained_variance   | 0.32       |
|    learning_rate        | 6.89e-05   |
|    loss                 | 0.0258     |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.0455    |
|    value_loss           | 0.236      |
----------------------------------------
Ep done - 289000.
Ep done - 290000.
Eval num_timesteps=7933500, episode_reward=0.21 +/- 0.97
Episode length: 30.06 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 7933500    |
| train/                  |            |
|    approx_kl            | 0.16798496 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.625     |
|    explained_variance   | 0.321      |
|    learning_rate        | 6.79e-05   |
|    loss                 | 0.0265     |
|    n_updates            | 2095       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.242      |
----------------------------------------
Ep done - 291000.
Ep done - 292000.
Eval num_timesteps=7995000, episode_reward=0.27 +/- 0.95
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 7995000    |
| train/                  |            |
|    approx_kl            | 0.16509534 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.627     |
|    explained_variance   | 0.319      |
|    learning_rate        | 6.69e-05   |
|    loss                 | 0.0211     |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.24       |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 26
Ep done - 293000.
Ep done - 294000.
Eval num_timesteps=8056500, episode_reward=0.21 +/- 0.96
Episode length: 30.07 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.215      |
| time/                   |            |
|    total_timesteps      | 8056500    |
| train/                  |            |
|    approx_kl            | 0.15579942 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.628     |
|    explained_variance   | 0.33       |
|    learning_rate        | 6.59e-05   |
|    loss                 | 0.0153     |
|    n_updates            | 2105       |
|    policy_gradient_loss | -0.0457    |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 295000.
Ep done - 296000.
Ep done - 297000.
Eval num_timesteps=8118000, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 8118000    |
| train/                  |            |
|    approx_kl            | 0.15925722 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.63      |
|    explained_variance   | 0.317      |
|    learning_rate        | 6.48e-05   |
|    loss                 | 0.0603     |
|    n_updates            | 2110       |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 298000.
Ep done - 299000.
Eval num_timesteps=8179500, episode_reward=0.18 +/- 0.98
Episode length: 30.06 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 8179500    |
| train/                  |            |
|    approx_kl            | 0.15188998 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.626     |
|    explained_variance   | 0.329      |
|    learning_rate        | 6.38e-05   |
|    loss                 | 0.0277     |
|    n_updates            | 2115       |
|    policy_gradient_loss | -0.0449    |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 300000.
Ep done - 301000.
Eval num_timesteps=8241000, episode_reward=0.04 +/- 0.99
Episode length: 30.07 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.045      |
| time/                   |            |
|    total_timesteps      | 8241000    |
| train/                  |            |
|    approx_kl            | 0.15966024 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.628     |
|    explained_variance   | 0.317      |
|    learning_rate        | 6.28e-05   |
|    loss                 | 0.0312     |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.242      |
----------------------------------------
Ep done - 302000.
Ep done - 303000.
Eval num_timesteps=8302500, episode_reward=0.15 +/- 0.98
Episode length: 29.96 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 8302500    |
| train/                  |            |
|    approx_kl            | 0.14737396 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.321      |
|    learning_rate        | 6.18e-05   |
|    loss                 | 0.033      |
|    n_updates            | 2125       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 304000.
Ep done - 305000.
Ep done - 306000.
Eval num_timesteps=8364000, episode_reward=0.12 +/- 0.98
Episode length: 29.95 +/- 0.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total_timesteps      | 8364000    |
| train/                  |            |
|    approx_kl            | 0.14692488 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.636     |
|    explained_variance   | 0.308      |
|    learning_rate        | 6.07e-05   |
|    loss                 | 0.0563     |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.239      |
----------------------------------------
Ep done - 307000.
Ep done - 308000.
Eval num_timesteps=8425500, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 8425500    |
| train/                  |            |
|    approx_kl            | 0.14690667 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.321      |
|    learning_rate        | 5.97e-05   |
|    loss                 | 0.0751     |
|    n_updates            | 2135       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 309000.
Ep done - 310000.
Eval num_timesteps=8487000, episode_reward=0.08 +/- 0.99
Episode length: 30.00 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 8487000    |
| train/                  |            |
|    approx_kl            | 0.14396222 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.631     |
|    explained_variance   | 0.312      |
|    learning_rate        | 5.87e-05   |
|    loss                 | 0.018      |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.24       |
----------------------------------------
Ep done - 311000.
Ep done - 312000.
Eval num_timesteps=8548500, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.075      |
| time/                   |            |
|    total_timesteps      | 8548500    |
| train/                  |            |
|    approx_kl            | 0.14428557 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.629     |
|    explained_variance   | 0.323      |
|    learning_rate        | 5.77e-05   |
|    loss                 | 0.0252     |
|    n_updates            | 2145       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.233      |
----------------------------------------
Ep done - 313000.
Ep done - 314000.
Ep done - 315000.
Eval num_timesteps=8610000, episode_reward=0.07 +/- 0.98
Episode length: 30.04 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.07      |
| time/                   |           |
|    total_timesteps      | 8610000   |
| train/                  |           |
|    approx_kl            | 0.1481298 |
|    clip_fraction        | 0.27      |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.63     |
|    explained_variance   | 0.321     |
|    learning_rate        | 5.66e-05  |
|    loss                 | 0.0643    |
|    n_updates            | 2150      |
|    policy_gradient_loss | -0.0447   |
|    value_loss           | 0.241     |
---------------------------------------
Ep done - 316000.
Ep done - 317000.
Eval num_timesteps=8671500, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.085      |
| time/                   |            |
|    total_timesteps      | 8671500    |
| train/                  |            |
|    approx_kl            | 0.13945036 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.312      |
|    learning_rate        | 5.56e-05   |
|    loss                 | 0.0235     |
|    n_updates            | 2155       |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.243      |
----------------------------------------
Ep done - 318000.
Ep done - 319000.
Eval num_timesteps=8733000, episode_reward=0.12 +/- 0.97
Episode length: 30.09 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 8733000    |
| train/                  |            |
|    approx_kl            | 0.13463752 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.635     |
|    explained_variance   | 0.322      |
|    learning_rate        | 5.46e-05   |
|    loss                 | 0.0172     |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.234      |
----------------------------------------
Ep done - 320000.
Ep done - 321000.
Eval num_timesteps=8794500, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 8794500    |
| train/                  |            |
|    approx_kl            | 0.13180739 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.628     |
|    explained_variance   | 0.324      |
|    learning_rate        | 5.36e-05   |
|    loss                 | 0.0346     |
|    n_updates            | 2165       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 322000.
Ep done - 323000.
Ep done - 324000.
Eval num_timesteps=8856000, episode_reward=0.15 +/- 0.98
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 8856000    |
| train/                  |            |
|    approx_kl            | 0.13276866 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.326      |
|    learning_rate        | 5.25e-05   |
|    loss                 | 0.0254     |
|    n_updates            | 2170       |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 325000.
Ep done - 326000.
Eval num_timesteps=8917500, episode_reward=0.23 +/- 0.96
Episode length: 30.01 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 8917500    |
| train/                  |            |
|    approx_kl            | 0.12961273 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.63      |
|    explained_variance   | 0.326      |
|    learning_rate        | 5.15e-05   |
|    loss                 | 0.0578     |
|    n_updates            | 2175       |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.243      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 27
Ep done - 327000.
Ep done - 328000.
Eval num_timesteps=8979000, episode_reward=-0.01 +/- 0.98
Episode length: 29.94 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.01      |
| time/                   |            |
|    total_timesteps      | 8979000    |
| train/                  |            |
|    approx_kl            | 0.13109106 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.642     |
|    explained_variance   | 0.338      |
|    learning_rate        | 5.05e-05   |
|    loss                 | 0.079      |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.0464    |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 329000.
Ep done - 330000.
Eval num_timesteps=9040500, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.11       |
| time/                   |            |
|    total_timesteps      | 9040500    |
| train/                  |            |
|    approx_kl            | 0.12577079 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.642     |
|    explained_variance   | 0.325      |
|    learning_rate        | 4.95e-05   |
|    loss                 | 0.0259     |
|    n_updates            | 2185       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.242      |
----------------------------------------
Ep done - 331000.
Ep done - 332000.
Ep done - 333000.
Eval num_timesteps=9102000, episode_reward=0.04 +/- 0.98
Episode length: 30.01 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.035      |
| time/                   |            |
|    total_timesteps      | 9102000    |
| train/                  |            |
|    approx_kl            | 0.12391262 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.638     |
|    explained_variance   | 0.322      |
|    learning_rate        | 4.84e-05   |
|    loss                 | 0.0308     |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.243      |
----------------------------------------
Ep done - 334000.
Ep done - 335000.
Eval num_timesteps=9163500, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.105      |
| time/                   |            |
|    total_timesteps      | 9163500    |
| train/                  |            |
|    approx_kl            | 0.12537445 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.64      |
|    explained_variance   | 0.316      |
|    learning_rate        | 4.74e-05   |
|    loss                 | 0.0388     |
|    n_updates            | 2195       |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.243      |
----------------------------------------
Ep done - 336000.
Ep done - 337000.
Eval num_timesteps=9225000, episode_reward=0.13 +/- 0.97
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 9225000    |
| train/                  |            |
|    approx_kl            | 0.11903973 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.63      |
|    explained_variance   | 0.326      |
|    learning_rate        | 4.64e-05   |
|    loss                 | 0.0174     |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.0452    |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 338000.
Ep done - 339000.
Eval num_timesteps=9286500, episode_reward=0.10 +/- 0.97
Episode length: 29.98 +/- 0.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.095     |
| time/                   |           |
|    total_timesteps      | 9286500   |
| train/                  |           |
|    approx_kl            | 0.1162581 |
|    clip_fraction        | 0.252     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.625    |
|    explained_variance   | 0.324     |
|    learning_rate        | 4.54e-05  |
|    loss                 | 0.0327    |
|    n_updates            | 2205      |
|    policy_gradient_loss | -0.0447   |
|    value_loss           | 0.238     |
---------------------------------------
Ep done - 340000.
Ep done - 341000.
Ep done - 342000.
Eval num_timesteps=9348000, episode_reward=0.04 +/- 0.98
Episode length: 30.03 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.045      |
| time/                   |            |
|    total_timesteps      | 9348000    |
| train/                  |            |
|    approx_kl            | 0.12816526 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.626     |
|    explained_variance   | 0.332      |
|    learning_rate        | 4.44e-05   |
|    loss                 | 0.0373     |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.24       |
----------------------------------------
Ep done - 343000.
Ep done - 344000.
Eval num_timesteps=9409500, episode_reward=0.11 +/- 0.97
Episode length: 30.07 +/- 0.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 9409500     |
| train/                  |             |
|    approx_kl            | 0.111521535 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.622      |
|    explained_variance   | 0.319       |
|    learning_rate        | 4.33e-05    |
|    loss                 | 0.0383      |
|    n_updates            | 2215        |
|    policy_gradient_loss | -0.0446     |
|    value_loss           | 0.242       |
-----------------------------------------
Ep done - 345000.
Ep done - 346000.
Eval num_timesteps=9471000, episode_reward=0.28 +/- 0.95
Episode length: 30.05 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.28       |
| time/                   |            |
|    total_timesteps      | 9471000    |
| train/                  |            |
|    approx_kl            | 0.11498284 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.615     |
|    explained_variance   | 0.335      |
|    learning_rate        | 4.23e-05   |
|    loss                 | 0.0459     |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.238      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 28
Ep done - 347000.
Ep done - 348000.
Eval num_timesteps=9532500, episode_reward=0.04 +/- 0.99
Episode length: 29.93 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 9532500    |
| train/                  |            |
|    approx_kl            | 0.11401308 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.623     |
|    explained_variance   | 0.328      |
|    learning_rate        | 4.13e-05   |
|    loss                 | 0.0465     |
|    n_updates            | 2225       |
|    policy_gradient_loss | -0.0443    |
|    value_loss           | 0.24       |
----------------------------------------
Ep done - 349000.
Ep done - 350000.
Ep done - 351000.
Eval num_timesteps=9594000, episode_reward=0.08 +/- 0.99
Episode length: 29.97 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 9594000    |
| train/                  |            |
|    approx_kl            | 0.11144827 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.624     |
|    explained_variance   | 0.343      |
|    learning_rate        | 4.03e-05   |
|    loss                 | 0.0498     |
|    n_updates            | 2230       |
|    policy_gradient_loss | -0.0449    |
|    value_loss           | 0.235      |
----------------------------------------
Ep done - 352000.
Ep done - 353000.
Eval num_timesteps=9655500, episode_reward=0.08 +/- 0.99
Episode length: 29.95 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 9655500     |
| train/                  |             |
|    approx_kl            | 0.099439375 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.337       |
|    learning_rate        | 3.92e-05    |
|    loss                 | 0.0712      |
|    n_updates            | 2235        |
|    policy_gradient_loss | -0.0443     |
|    value_loss           | 0.239       |
-----------------------------------------
Ep done - 354000.
Ep done - 355000.
Eval num_timesteps=9717000, episode_reward=0.17 +/- 0.97
Episode length: 30.01 +/- 0.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.175      |
| time/                   |            |
|    total_timesteps      | 9717000    |
| train/                  |            |
|    approx_kl            | 0.10310604 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.621     |
|    explained_variance   | 0.328      |
|    learning_rate        | 3.82e-05   |
|    loss                 | 0.0344     |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0439    |
|    value_loss           | 0.24       |
----------------------------------------
Ep done - 356000.
Ep done - 357000.
Eval num_timesteps=9778500, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.095      |
| time/                   |            |
|    total_timesteps      | 9778500    |
| train/                  |            |
|    approx_kl            | 0.10158307 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.618     |
|    explained_variance   | 0.331      |
|    learning_rate        | 3.72e-05   |
|    loss                 | 0.0336     |
|    n_updates            | 2245       |
|    policy_gradient_loss | -0.0445    |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 358000.
Ep done - 359000.
Ep done - 360000.
Eval num_timesteps=9840000, episode_reward=0.13 +/- 0.99
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.13       |
| time/                   |            |
|    total_timesteps      | 9840000    |
| train/                  |            |
|    approx_kl            | 0.09871529 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.62      |
|    explained_variance   | 0.335      |
|    learning_rate        | 3.62e-05   |
|    loss                 | 0.0383     |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.044     |
|    value_loss           | 0.239      |
----------------------------------------
Ep done - 361000.
Ep done - 362000.
Eval num_timesteps=9901500, episode_reward=0.09 +/- 0.99
Episode length: 30.00 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.085       |
| time/                   |             |
|    total_timesteps      | 9901500     |
| train/                  |             |
|    approx_kl            | 0.091537155 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.35        |
|    learning_rate        | 3.51e-05    |
|    loss                 | 0.0453      |
|    n_updates            | 2255        |
|    policy_gradient_loss | -0.0439     |
|    value_loss           | 0.235       |
-----------------------------------------
Ep done - 363000.
Ep done - 364000.
Eval num_timesteps=9963000, episode_reward=0.15 +/- 0.98
Episode length: 30.02 +/- 0.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.15      |
| time/                   |           |
|    total_timesteps      | 9963000   |
| train/                  |           |
|    approx_kl            | 0.0889852 |
|    clip_fraction        | 0.225     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.608    |
|    explained_variance   | 0.337     |
|    learning_rate        | 3.41e-05  |
|    loss                 | 0.0755    |
|    n_updates            | 2260      |
|    policy_gradient_loss | -0.0433   |
|    value_loss           | 0.242     |
---------------------------------------
Ep done - 365000.
Ep done - 366000.
Eval num_timesteps=10024500, episode_reward=0.14 +/- 0.97
Episode length: 29.96 +/- 0.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.145     |
| time/                   |           |
|    total_timesteps      | 10024500  |
| train/                  |           |
|    approx_kl            | 0.0868007 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.608    |
|    explained_variance   | 0.343     |
|    learning_rate        | 3.31e-05  |
|    loss                 | 0.0255    |
|    n_updates            | 2265      |
|    policy_gradient_loss | -0.0429   |
|    value_loss           | 0.236     |
---------------------------------------
Ep done - 367000.
Ep done - 368000.
Ep done - 369000.
Eval num_timesteps=10086000, episode_reward=0.21 +/- 0.97
Episode length: 30.04 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.21       |
| time/                   |            |
|    total_timesteps      | 10086000   |
| train/                  |            |
|    approx_kl            | 0.08644586 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.614     |
|    explained_variance   | 0.339      |
|    learning_rate        | 3.21e-05   |
|    loss                 | 0.0288     |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.239      |
----------------------------------------
Ep done - 370000.
Ep done - 371000.
Eval num_timesteps=10147500, episode_reward=0.01 +/- 0.98
Episode length: 29.90 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.01       |
| time/                   |            |
|    total_timesteps      | 10147500   |
| train/                  |            |
|    approx_kl            | 0.08422061 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.613     |
|    explained_variance   | 0.332      |
|    learning_rate        | 3.1e-05    |
|    loss                 | 0.0681     |
|    n_updates            | 2275       |
|    policy_gradient_loss | -0.0433    |
|    value_loss           | 0.242      |
----------------------------------------
Ep done - 372000.
Ep done - 373000.
Eval num_timesteps=10209000, episode_reward=0.01 +/- 0.99
Episode length: 30.01 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.01       |
| time/                   |            |
|    total_timesteps      | 10209000   |
| train/                  |            |
|    approx_kl            | 0.08891424 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.616     |
|    explained_variance   | 0.337      |
|    learning_rate        | 3e-05      |
|    loss                 | 0.0396     |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0438    |
|    value_loss           | 0.239      |
----------------------------------------
Ep done - 374000.
Ep done - 375000.
Eval num_timesteps=10270500, episode_reward=0.18 +/- 0.97
Episode length: 29.94 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 10270500   |
| train/                  |            |
|    approx_kl            | 0.08123317 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.61      |
|    explained_variance   | 0.327      |
|    learning_rate        | 2.9e-05    |
|    loss                 | 0.0655     |
|    n_updates            | 2285       |
|    policy_gradient_loss | -0.0428    |
|    value_loss           | 0.243      |
----------------------------------------
Ep done - 376000.
Ep done - 377000.
Ep done - 378000.
Eval num_timesteps=10332000, episode_reward=0.28 +/- 0.94
Episode length: 30.08 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.28       |
| time/                   |            |
|    total_timesteps      | 10332000   |
| train/                  |            |
|    approx_kl            | 0.07540547 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.601     |
|    explained_variance   | 0.338      |
|    learning_rate        | 2.8e-05    |
|    loss                 | 0.0729     |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.237      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 29
Ep done - 379000.
Ep done - 380000.
Eval num_timesteps=10393500, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.095      |
| time/                   |            |
|    total_timesteps      | 10393500   |
| train/                  |            |
|    approx_kl            | 0.07277905 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.605     |
|    explained_variance   | 0.34       |
|    learning_rate        | 2.69e-05   |
|    loss                 | 0.0534     |
|    n_updates            | 2295       |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 0.237      |
----------------------------------------
Ep done - 381000.
Ep done - 382000.
Eval num_timesteps=10455000, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 10455000   |
| train/                  |            |
|    approx_kl            | 0.06875065 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.601     |
|    explained_variance   | 0.328      |
|    learning_rate        | 2.59e-05   |
|    loss                 | 0.0388     |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.0412    |
|    value_loss           | 0.244      |
----------------------------------------
Ep done - 383000.
Ep done - 384000.
Eval num_timesteps=10516500, episode_reward=0.12 +/- 0.99
Episode length: 30.07 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.12       |
| time/                   |            |
|    total_timesteps      | 10516500   |
| train/                  |            |
|    approx_kl            | 0.06533203 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.603     |
|    explained_variance   | 0.325      |
|    learning_rate        | 2.49e-05   |
|    loss                 | 0.05       |
|    n_updates            | 2305       |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.243      |
----------------------------------------
Ep done - 385000.
Ep done - 386000.
Ep done - 387000.
Eval num_timesteps=10578000, episode_reward=-0.12 +/- 0.98
Episode length: 29.93 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.12       |
| time/                   |             |
|    total_timesteps      | 10578000    |
| train/                  |             |
|    approx_kl            | 0.061910607 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.346       |
|    learning_rate        | 2.39e-05    |
|    loss                 | 0.0397      |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0411     |
|    value_loss           | 0.236       |
-----------------------------------------
Ep done - 388000.
Ep done - 389000.
Eval num_timesteps=10639500, episode_reward=0.17 +/- 0.96
Episode length: 30.04 +/- 0.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.175     |
| time/                   |           |
|    total_timesteps      | 10639500  |
| train/                  |           |
|    approx_kl            | 0.0590372 |
|    clip_fraction        | 0.187     |
|    clip_range           | 0.18      |
|    entropy_loss         | -0.599    |
|    explained_variance   | 0.324     |
|    learning_rate        | 2.28e-05  |
|    loss                 | 0.0944    |
|    n_updates            | 2315      |
|    policy_gradient_loss | -0.0401   |
|    value_loss           | 0.242     |
---------------------------------------
Ep done - 390000.
Ep done - 391000.
Eval num_timesteps=10701000, episode_reward=0.21 +/- 0.96
Episode length: 30.12 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.21        |
| time/                   |             |
|    total_timesteps      | 10701000    |
| train/                  |             |
|    approx_kl            | 0.054293092 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.338       |
|    learning_rate        | 2.18e-05    |
|    loss                 | 0.049       |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 0.24        |
-----------------------------------------
Ep done - 392000.
Ep done - 393000.
Eval num_timesteps=10762500, episode_reward=0.12 +/- 0.97
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.125       |
| time/                   |             |
|    total_timesteps      | 10762500    |
| train/                  |             |
|    approx_kl            | 0.050888598 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.602      |
|    explained_variance   | 0.33        |
|    learning_rate        | 2.08e-05    |
|    loss                 | 0.062       |
|    n_updates            | 2325        |
|    policy_gradient_loss | -0.04       |
|    value_loss           | 0.242       |
-----------------------------------------
Ep done - 394000.
Ep done - 395000.
Ep done - 396000.
Eval num_timesteps=10824000, episode_reward=0.27 +/- 0.94
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.27       |
| time/                   |            |
|    total_timesteps      | 10824000   |
| train/                  |            |
|    approx_kl            | 0.04911415 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.599     |
|    explained_variance   | 0.322      |
|    learning_rate        | 1.98e-05   |
|    loss                 | 0.043      |
|    n_updates            | 2330       |
|    policy_gradient_loss | -0.0392    |
|    value_loss           | 0.244      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 30
Ep done - 397000.
Ep done - 398000.
Eval num_timesteps=10885500, episode_reward=0.15 +/- 0.98
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.155      |
| time/                   |            |
|    total_timesteps      | 10885500   |
| train/                  |            |
|    approx_kl            | 0.04664905 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.611     |
|    explained_variance   | 0.331      |
|    learning_rate        | 1.88e-05   |
|    loss                 | 0.043      |
|    n_updates            | 2335       |
|    policy_gradient_loss | -0.0397    |
|    value_loss           | 0.239      |
----------------------------------------
Ep done - 399000.
Ep done - 400000.
Eval num_timesteps=10947000, episode_reward=0.09 +/- 0.97
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.085      |
| time/                   |            |
|    total_timesteps      | 10947000   |
| train/                  |            |
|    approx_kl            | 0.04275559 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.613     |
|    explained_variance   | 0.337      |
|    learning_rate        | 1.77e-05   |
|    loss                 | 0.0359     |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0389    |
|    value_loss           | 0.24       |
----------------------------------------
Ep done - 401000.
Ep done - 402000.
Eval num_timesteps=11008500, episode_reward=0.12 +/- 0.99
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.115       |
| time/                   |             |
|    total_timesteps      | 11008500    |
| train/                  |             |
|    approx_kl            | 0.039786395 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.347       |
|    learning_rate        | 1.67e-05    |
|    loss                 | 0.029       |
|    n_updates            | 2345        |
|    policy_gradient_loss | -0.0383     |
|    value_loss           | 0.238       |
-----------------------------------------
Ep done - 403000.
Ep done - 404000.
Ep done - 405000.
Eval num_timesteps=11070000, episode_reward=0.12 +/- 0.97
Episode length: 30.07 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.125      |
| time/                   |            |
|    total_timesteps      | 11070000   |
| train/                  |            |
|    approx_kl            | 0.03586023 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.612     |
|    explained_variance   | 0.344      |
|    learning_rate        | 1.57e-05   |
|    loss                 | 0.0396     |
|    n_updates            | 2350       |
|    policy_gradient_loss | -0.0385    |
|    value_loss           | 0.236      |
----------------------------------------
Ep done - 406000.
Ep done - 407000.
Eval num_timesteps=11131500, episode_reward=0.06 +/- 0.98
Episode length: 30.07 +/- 0.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 11131500   |
| train/                  |            |
|    approx_kl            | 0.03451618 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.611     |
|    explained_variance   | 0.36       |
|    learning_rate        | 1.47e-05   |
|    loss                 | 0.0725     |
|    n_updates            | 2355       |
|    policy_gradient_loss | -0.0377    |
|    value_loss           | 0.235      |
----------------------------------------
Ep done - 408000.
Ep done - 409000.
Eval num_timesteps=11193000, episode_reward=0.23 +/- 0.96
Episode length: 30.08 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.225      |
| time/                   |            |
|    total_timesteps      | 11193000   |
| train/                  |            |
|    approx_kl            | 0.03220118 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.611     |
|    explained_variance   | 0.332      |
|    learning_rate        | 1.36e-05   |
|    loss                 | 0.0504     |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.0364    |
|    value_loss           | 0.245      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 31
Ep done - 410000.
Ep done - 411000.
Eval num_timesteps=11254500, episode_reward=0.14 +/- 0.98
Episode length: 30.07 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.14        |
| time/                   |             |
|    total_timesteps      | 11254500    |
| train/                  |             |
|    approx_kl            | 0.028919036 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.619      |
|    explained_variance   | 0.346       |
|    learning_rate        | 1.26e-05    |
|    loss                 | 0.0655      |
|    n_updates            | 2365        |
|    policy_gradient_loss | -0.0358     |
|    value_loss           | 0.238       |
-----------------------------------------
Ep done - 412000.
Ep done - 413000.
Ep done - 414000.
Eval num_timesteps=11316000, episode_reward=0.09 +/- 0.98
Episode length: 29.95 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 11316000    |
| train/                  |             |
|    approx_kl            | 0.025452506 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.326       |
|    learning_rate        | 1.16e-05    |
|    loss                 | 0.0469      |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.243       |
-----------------------------------------
Ep done - 415000.
Ep done - 416000.
Eval num_timesteps=11377500, episode_reward=0.05 +/- 1.00
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 11377500   |
| train/                  |            |
|    approx_kl            | 0.02222081 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.618     |
|    explained_variance   | 0.348      |
|    learning_rate        | 1.06e-05   |
|    loss                 | 0.045      |
|    n_updates            | 2375       |
|    policy_gradient_loss | -0.0343    |
|    value_loss           | 0.238      |
----------------------------------------
Ep done - 417000.
Ep done - 418000.
Eval num_timesteps=11439000, episode_reward=0.05 +/- 0.99
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 11439000    |
| train/                  |             |
|    approx_kl            | 0.019545976 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.622      |
|    explained_variance   | 0.329       |
|    learning_rate        | 9.54e-06    |
|    loss                 | 0.0481      |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 0.246       |
-----------------------------------------
Ep done - 419000.
Ep done - 420000.
Eval num_timesteps=11500500, episode_reward=0.03 +/- 0.99
Episode length: 30.04 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.025       |
| time/                   |             |
|    total_timesteps      | 11500500    |
| train/                  |             |
|    approx_kl            | 0.017612778 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.329       |
|    learning_rate        | 8.51e-06    |
|    loss                 | 0.0665      |
|    n_updates            | 2385        |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.243       |
-----------------------------------------
Ep done - 421000.
Ep done - 422000.
Ep done - 423000.
Eval num_timesteps=11562000, episode_reward=0.07 +/- 0.98
Episode length: 29.93 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.065       |
| time/                   |             |
|    total_timesteps      | 11562000    |
| train/                  |             |
|    approx_kl            | 0.014400078 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.329       |
|    learning_rate        | 7.49e-06    |
|    loss                 | 0.09        |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0297     |
|    value_loss           | 0.245       |
-----------------------------------------
Ep done - 424000.
Ep done - 425000.
Eval num_timesteps=11623500, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 11623500    |
| train/                  |             |
|    approx_kl            | 0.011953046 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.627      |
|    explained_variance   | 0.342       |
|    learning_rate        | 6.46e-06    |
|    loss                 | 0.0715      |
|    n_updates            | 2395        |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.241       |
-----------------------------------------
Ep done - 426000.
Ep done - 427000.
Eval num_timesteps=11685000, episode_reward=0.15 +/- 0.97
Episode length: 30.05 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 11685000    |
| train/                  |             |
|    approx_kl            | 0.009876582 |
|    clip_fraction        | 0.0786      |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.335       |
|    learning_rate        | 5.44e-06    |
|    loss                 | 0.0828      |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.244       |
-----------------------------------------
Ep done - 428000.
Ep done - 429000.
Eval num_timesteps=11746500, episode_reward=0.09 +/- 0.97
Episode length: 30.01 +/- 0.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.085        |
| time/                   |              |
|    total_timesteps      | 11746500     |
| train/                  |              |
|    approx_kl            | 0.0075469953 |
|    clip_fraction        | 0.0644       |
|    clip_range           | 0.18         |
|    entropy_loss         | -0.625       |
|    explained_variance   | 0.325        |
|    learning_rate        | 4.42e-06     |
|    loss                 | 0.0776       |
|    n_updates            | 2405         |
|    policy_gradient_loss | -0.0237      |
|    value_loss           | 0.246        |
------------------------------------------
Ep done - 430000.
Ep done - 431000.
Ep done - 432000.
Eval num_timesteps=11808000, episode_reward=0.18 +/- 0.95
Episode length: 30.00 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.185      |
| time/                   |            |
|    total_timesteps      | 11808000   |
| train/                  |            |
|    approx_kl            | 0.00539724 |
|    clip_fraction        | 0.0482     |
|    clip_range           | 0.18       |
|    entropy_loss         | -0.627     |
|    explained_variance   | 0.336      |
|    learning_rate        | 3.39e-06   |
|    loss                 | 0.102      |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.0202    |
|    value_loss           | 0.245      |
----------------------------------------
Ep done - 433000.
Ep done - 434000.
Eval num_timesteps=11869500, episode_reward=0.15 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 11869500    |
| train/                  |             |
|    approx_kl            | 0.003657636 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.18        |
|    entropy_loss         | -0.63       |
|    explained_variance   | 0.353       |
|    learning_rate        | 2.37e-06    |
|    loss                 | 0.0998      |
|    n_updates            | 2415        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.241       |
-----------------------------------------
Ep done - 435000.
Ep done - 436000.
Eval num_timesteps=11931000, episode_reward=0.01 +/- 0.99
Episode length: 30.01 +/- 0.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.015        |
| time/                   |              |
|    total_timesteps      | 11931000     |
| train/                  |              |
|    approx_kl            | 0.0017739318 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.18         |
|    entropy_loss         | -0.628       |
|    explained_variance   | 0.35         |
|    learning_rate        | 1.34e-06     |
|    loss                 | 0.0658       |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.244        |
------------------------------------------
Ep done - 437000.
Ep done - 438000.
Eval num_timesteps=11992500, episode_reward=0.06 +/- 0.99
Episode length: 30.04 +/- 0.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 30            |
|    mean_reward          | 0.055         |
| time/                   |               |
|    total_timesteps      | 11992500      |
| train/                  |               |
|    approx_kl            | 0.00031462312 |
|    clip_fraction        | 0.00102       |
|    clip_range           | 0.18          |
|    entropy_loss         | -0.631        |
|    explained_variance   | 0.346         |
|    learning_rate        | 3.2e-07       |
|    loss                 | 0.151         |
|    n_updates            | 2425          |
|    policy_gradient_loss | -0.00327      |
|    value_loss           | 0.251         |
-------------------------------------------
/home/student/pantrasa/project/venv2/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.[0m
  logger.warn(
/home/student/pantrasa/project/venv2/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
Ep done - 439000.
Ep done - 440000.
Elapsed time: 19h 7m 22s
