/bin/bash: /opt/miniconda/3.10/lib/libtinfo.so.6: no version information available (required by /bin/bash)
CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
CUDA available: True
seed: 13 
num_timesteps: 30000000 
eval_freq: 21000 
eval_episoded: 200 
best_threshold: 0.25 
logdir: scripts/rl/output/phase2/trpo/mlp/base/ 
cnn_policy: False 
continueFrom_model: None
/home/student/pantrasa/project/venv310/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.[0m
  logger.warn(
Using cuda device

params: {'learning_rate': 1e-05, 'n_steps': 20480, 'batch_size': 128, 'verbose': 100, 'seed': 13, 'policy_kwargs': {'net_arch': {'pi': [128, 128, 128, 128, 128, 128, 128, 128], 'vf': [64, 64, 64, 64, 64, 64, 64, 64]}}}

Eval num_timesteps=21000, episode_reward=0.62 +/- 0.76
Episode length: 30.03 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.62     |
| time/                     |          |
|    total_timesteps        | 21000    |
| train/                    |          |
|    explained_variance     | -0.427   |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00357  |
|    learning_rate          | 1e-05    |
|    n_updates              | 1        |
|    policy_objective       | 0.0309   |
|    value_loss             | 0.299    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.62
SELFPLAY: new best model, bumping up generation to 1
Ep done - 1000.
Eval num_timesteps=42000, episode_reward=0.72 +/- 0.67
Episode length: 30.09 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.725    |
| time/                     |          |
|    total_timesteps        | 42000    |
| train/                    |          |
|    explained_variance     | -0.0191  |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00314  |
|    learning_rate          | 1e-05    |
|    n_updates              | 2        |
|    policy_objective       | 0.0253   |
|    value_loss             | 0.27     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.725
SELFPLAY: new best model, bumping up generation to 2
Ep done - 2000.
Eval num_timesteps=63000, episode_reward=0.66 +/- 0.74
Episode length: 30.05 +/- 0.67
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.66     |
| time/                     |          |
|    total_timesteps        | 63000    |
| train/                    |          |
|    explained_variance     | 0.145    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00411  |
|    learning_rate          | 1e-05    |
|    n_updates              | 3        |
|    policy_objective       | 0.0255   |
|    value_loss             | 0.241    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.66
SELFPLAY: new best model, bumping up generation to 3
Ep done - 3000.
Eval num_timesteps=84000, episode_reward=0.71 +/- 0.68
Episode length: 30.08 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.71     |
| time/                     |          |
|    total_timesteps        | 84000    |
| train/                    |          |
|    explained_variance     | 0.16     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00386  |
|    learning_rate          | 1e-05    |
|    n_updates              | 4        |
|    policy_objective       | 0.0235   |
|    value_loss             | 0.252    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.71
SELFPLAY: new best model, bumping up generation to 4
Ep done - 4000.
Eval num_timesteps=105000, episode_reward=0.73 +/- 0.66
Episode length: 30.01 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.735    |
| time/                     |          |
|    total_timesteps        | 105000   |
| train/                    |          |
|    explained_variance     | 0.192    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00347  |
|    learning_rate          | 1e-05    |
|    n_updates              | 5        |
|    policy_objective       | 0.0222   |
|    value_loss             | 0.246    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.735
SELFPLAY: new best model, bumping up generation to 5
Ep done - 5000.
Eval num_timesteps=126000, episode_reward=0.70 +/- 0.70
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.705    |
| time/                     |          |
|    total_timesteps        | 126000   |
| train/                    |          |
|    explained_variance     | 0.275    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0034   |
|    learning_rate          | 1e-05    |
|    n_updates              | 6        |
|    policy_objective       | 0.0233   |
|    value_loss             | 0.237    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.705
SELFPLAY: new best model, bumping up generation to 6
Ep done - 6000.
Eval num_timesteps=147000, episode_reward=0.69 +/- 0.71
Episode length: 30.00 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.685    |
| time/                     |          |
|    total_timesteps        | 147000   |
| train/                    |          |
|    explained_variance     | 0.291    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00353  |
|    learning_rate          | 1e-05    |
|    n_updates              | 7        |
|    policy_objective       | 0.0234   |
|    value_loss             | 0.239    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.685
SELFPLAY: new best model, bumping up generation to 7
Ep done - 7000.
Eval num_timesteps=168000, episode_reward=0.68 +/- 0.71
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.675    |
| time/                     |          |
|    total_timesteps        | 168000   |
| train/                    |          |
|    explained_variance     | 0.314    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00341  |
|    learning_rate          | 1e-05    |
|    n_updates              | 8        |
|    policy_objective       | 0.0235   |
|    value_loss             | 0.236    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.675
SELFPLAY: new best model, bumping up generation to 8
Ep done - 8000.
Eval num_timesteps=189000, episode_reward=0.74 +/- 0.66
Episode length: 29.99 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.745    |
| time/                     |          |
|    total_timesteps        | 189000   |
| train/                    |          |
|    explained_variance     | 0.324    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00347  |
|    learning_rate          | 1e-05    |
|    n_updates              | 9        |
|    policy_objective       | 0.0249   |
|    value_loss             | 0.234    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.745
SELFPLAY: new best model, bumping up generation to 9
Eval num_timesteps=210000, episode_reward=0.74 +/- 0.65
Episode length: 30.02 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.745    |
| time/                     |          |
|    total_timesteps        | 210000   |
| train/                    |          |
|    explained_variance     | 0.328    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.003    |
|    learning_rate          | 1e-05    |
|    n_updates              | 10       |
|    policy_objective       | 0.025    |
|    value_loss             | 0.237    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.745
SELFPLAY: new best model, bumping up generation to 10
Ep done - 9000.
Eval num_timesteps=231000, episode_reward=0.67 +/- 0.72
Episode length: 30.02 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.67     |
| time/                     |          |
|    total_timesteps        | 231000   |
| train/                    |          |
|    explained_variance     | 0.321    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00257  |
|    learning_rate          | 1e-05    |
|    n_updates              | 11       |
|    policy_objective       | 0.0258   |
|    value_loss             | 0.239    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.67
SELFPLAY: new best model, bumping up generation to 11
Ep done - 10000.
Eval num_timesteps=252000, episode_reward=0.68 +/- 0.73
Episode length: 29.98 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.675    |
| time/                     |          |
|    total_timesteps        | 252000   |
| train/                    |          |
|    explained_variance     | 0.334    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00266  |
|    learning_rate          | 1e-05    |
|    n_updates              | 12       |
|    policy_objective       | 0.0274   |
|    value_loss             | 0.238    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.675
SELFPLAY: new best model, bumping up generation to 12
Ep done - 11000.
Eval num_timesteps=273000, episode_reward=0.68 +/- 0.71
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.675    |
| time/                     |          |
|    total_timesteps        | 273000   |
| train/                    |          |
|    explained_variance     | 0.336    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00284  |
|    learning_rate          | 1e-05    |
|    n_updates              | 13       |
|    policy_objective       | 0.0307   |
|    value_loss             | 0.236    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.675
SELFPLAY: new best model, bumping up generation to 13
Ep done - 12000.
Eval num_timesteps=294000, episode_reward=0.69 +/- 0.69
Episode length: 29.96 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.695    |
| time/                     |          |
|    total_timesteps        | 294000   |
| train/                    |          |
|    explained_variance     | 0.342    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00257  |
|    learning_rate          | 1e-05    |
|    n_updates              | 14       |
|    policy_objective       | 0.0316   |
|    value_loss             | 0.233    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.695
SELFPLAY: new best model, bumping up generation to 14
Ep done - 13000.
Eval num_timesteps=315000, episode_reward=0.56 +/- 0.81
Episode length: 30.02 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.56     |
| time/                     |          |
|    total_timesteps        | 315000   |
| train/                    |          |
|    explained_variance     | 0.28     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00268  |
|    learning_rate          | 1e-05    |
|    n_updates              | 15       |
|    policy_objective       | 0.0315   |
|    value_loss             | 0.246    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.56
SELFPLAY: new best model, bumping up generation to 15
Ep done - 14000.
Eval num_timesteps=336000, episode_reward=0.58 +/- 0.80
Episode length: 29.91 +/- 1.29
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.58     |
| time/                     |          |
|    total_timesteps        | 336000   |
| train/                    |          |
|    explained_variance     | 0.351    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00271  |
|    learning_rate          | 1e-05    |
|    n_updates              | 16       |
|    policy_objective       | 0.0345   |
|    value_loss             | 0.228    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.58
SELFPLAY: new best model, bumping up generation to 16
Ep done - 15000.
Eval num_timesteps=357000, episode_reward=0.55 +/- 0.82
Episode length: 30.00 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.545    |
| time/                     |          |
|    total_timesteps        | 357000   |
| train/                    |          |
|    explained_variance     | 0.34     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0029   |
|    learning_rate          | 1e-05    |
|    n_updates              | 17       |
|    policy_objective       | 0.0372   |
|    value_loss             | 0.234    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.545
SELFPLAY: new best model, bumping up generation to 17
Ep done - 16000.
Eval num_timesteps=378000, episode_reward=0.57 +/- 0.80
Episode length: 29.98 +/- 1.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.575    |
| time/                     |          |
|    total_timesteps        | 378000   |
| train/                    |          |
|    explained_variance     | 0.369    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00299  |
|    learning_rate          | 1e-05    |
|    n_updates              | 18       |
|    policy_objective       | 0.0343   |
|    value_loss             | 0.231    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.575
SELFPLAY: new best model, bumping up generation to 18
Ep done - 17000.
Eval num_timesteps=399000, episode_reward=0.50 +/- 0.85
Episode length: 30.03 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.5      |
| time/                     |          |
|    total_timesteps        | 399000   |
| train/                    |          |
|    explained_variance     | 0.351    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0036   |
|    learning_rate          | 1e-05    |
|    n_updates              | 19       |
|    policy_objective       | 0.0657   |
|    value_loss             | 0.233    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.5
SELFPLAY: new best model, bumping up generation to 19
Eval num_timesteps=420000, episode_reward=0.42 +/- 0.89
Episode length: 30.03 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.425    |
| time/                     |          |
|    total_timesteps        | 420000   |
| train/                    |          |
|    explained_variance     | 0.324    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00303  |
|    learning_rate          | 1e-05    |
|    n_updates              | 20       |
|    policy_objective       | 0.0373   |
|    value_loss             | 0.24     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.425
SELFPLAY: new best model, bumping up generation to 20
Ep done - 18000.
Eval num_timesteps=441000, episode_reward=0.56 +/- 0.81
Episode length: 30.05 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.555    |
| time/                     |          |
|    total_timesteps        | 441000   |
| train/                    |          |
|    explained_variance     | 0.365    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00259  |
|    learning_rate          | 1e-05    |
|    n_updates              | 21       |
|    policy_objective       | 0.0458   |
|    value_loss             | 0.23     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.555
SELFPLAY: new best model, bumping up generation to 21
Ep done - 19000.
Eval num_timesteps=462000, episode_reward=0.43 +/- 0.88
Episode length: 30.04 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.435    |
| time/                     |          |
|    total_timesteps        | 462000   |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00262  |
|    learning_rate          | 1e-05    |
|    n_updates              | 22       |
|    policy_objective       | 0.0577   |
|    value_loss             | 0.232    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.435
SELFPLAY: new best model, bumping up generation to 22
Ep done - 20000.
Eval num_timesteps=483000, episode_reward=0.45 +/- 0.89
Episode length: 30.11 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.445    |
| time/                     |          |
|    total_timesteps        | 483000   |
| train/                    |          |
|    explained_variance     | 0.368    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00229  |
|    learning_rate          | 1e-05    |
|    n_updates              | 23       |
|    policy_objective       | 0.0465   |
|    value_loss             | 0.234    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.445
SELFPLAY: new best model, bumping up generation to 23
Ep done - 21000.
Eval num_timesteps=504000, episode_reward=0.41 +/- 0.89
Episode length: 30.09 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.405    |
| time/                     |          |
|    total_timesteps        | 504000   |
| train/                    |          |
|    explained_variance     | 0.381    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00215  |
|    learning_rate          | 1e-05    |
|    n_updates              | 24       |
|    policy_objective       | 0.0522   |
|    value_loss             | 0.232    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.405
SELFPLAY: new best model, bumping up generation to 24
Ep done - 22000.
Eval num_timesteps=525000, episode_reward=0.40 +/- 0.90
Episode length: 29.98 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.395    |
| time/                     |          |
|    total_timesteps        | 525000   |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0028   |
|    learning_rate          | 1e-05    |
|    n_updates              | 25       |
|    policy_objective       | 0.045    |
|    value_loss             | 0.23     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.395
SELFPLAY: new best model, bumping up generation to 25
Ep done - 23000.
Eval num_timesteps=546000, episode_reward=0.47 +/- 0.88
Episode length: 30.04 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.47     |
| time/                     |          |
|    total_timesteps        | 546000   |
| train/                    |          |
|    explained_variance     | 0.411    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00439  |
|    learning_rate          | 1e-05    |
|    n_updates              | 26       |
|    policy_objective       | 0.345    |
|    value_loss             | 0.223    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.47
SELFPLAY: new best model, bumping up generation to 26
Ep done - 24000.
Eval num_timesteps=567000, episode_reward=0.36 +/- 0.92
Episode length: 30.00 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.36     |
| time/                     |          |
|    total_timesteps        | 567000   |
| train/                    |          |
|    explained_variance     | 0.369    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00259  |
|    learning_rate          | 1e-05    |
|    n_updates              | 27       |
|    policy_objective       | 0.0761   |
|    value_loss             | 0.239    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.36
SELFPLAY: new best model, bumping up generation to 27
Ep done - 25000.
Eval num_timesteps=588000, episode_reward=0.25 +/- 0.96
Episode length: 30.04 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 588000   |
| train/                    |          |
|    explained_variance     | 0.337    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00266  |
|    learning_rate          | 1e-05    |
|    n_updates              | 28       |
|    policy_objective       | 0.0553   |
|    value_loss             | 0.242    |
----------------------------------------
Ep done - 26000.
Eval num_timesteps=609000, episode_reward=0.20 +/- 0.97
Episode length: 29.93 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 609000   |
| train/                    |          |
|    explained_variance     | 0.409    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00212  |
|    learning_rate          | 1e-05    |
|    n_updates              | 29       |
|    policy_objective       | 0.0638   |
|    value_loss             | 0.222    |
----------------------------------------
Eval num_timesteps=630000, episode_reward=0.45 +/- 0.88
Episode length: 30.02 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.45     |
| time/                     |          |
|    total_timesteps        | 630000   |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00242  |
|    learning_rate          | 1e-05    |
|    n_updates              | 30       |
|    policy_objective       | 0.211    |
|    value_loss             | 0.228    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.45
SELFPLAY: new best model, bumping up generation to 28
Ep done - 27000.
Eval num_timesteps=651000, episode_reward=0.12 +/- 0.99
Episode length: 30.03 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 651000   |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00226  |
|    learning_rate          | 1e-05    |
|    n_updates              | 31       |
|    policy_objective       | 0.0624   |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 28000.
Eval num_timesteps=672000, episode_reward=0.28 +/- 0.95
Episode length: 30.02 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.28     |
| time/                     |          |
|    total_timesteps        | 672000   |
| train/                    |          |
|    explained_variance     | 0.383    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00357  |
|    learning_rate          | 1e-05    |
|    n_updates              | 32       |
|    policy_objective       | 2.21     |
|    value_loss             | 0.234    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 29
Ep done - 29000.
Eval num_timesteps=693000, episode_reward=0.28 +/- 0.94
Episode length: 30.05 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.275    |
| time/                     |          |
|    total_timesteps        | 693000   |
| train/                    |          |
|    explained_variance     | 0.4      |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00288  |
|    learning_rate          | 1e-05    |
|    n_updates              | 33       |
|    policy_objective       | 0.0869   |
|    value_loss             | 0.225    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 30
Ep done - 30000.
Eval num_timesteps=714000, episode_reward=0.39 +/- 0.91
Episode length: 30.02 +/- 0.37
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.39     |
| time/                     |          |
|    total_timesteps        | 714000   |
| train/                    |          |
|    explained_variance     | 0.426    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00237  |
|    learning_rate          | 1e-05    |
|    n_updates              | 34       |
|    policy_objective       | 0.296    |
|    value_loss             | 0.223    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.39
SELFPLAY: new best model, bumping up generation to 31
Ep done - 31000.
Eval num_timesteps=735000, episode_reward=0.35 +/- 0.93
Episode length: 30.02 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.35     |
| time/                     |          |
|    total_timesteps        | 735000   |
| train/                    |          |
|    explained_variance     | 0.407    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00306  |
|    learning_rate          | 1e-05    |
|    n_updates              | 35       |
|    policy_objective       | 0.116    |
|    value_loss             | 0.23     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.35
SELFPLAY: new best model, bumping up generation to 32
Ep done - 32000.
Eval num_timesteps=756000, episode_reward=0.24 +/- 0.96
Episode length: 30.06 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.245    |
| time/                     |          |
|    total_timesteps        | 756000   |
| train/                    |          |
|    explained_variance     | 0.409    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00255  |
|    learning_rate          | 1e-05    |
|    n_updates              | 36       |
|    policy_objective       | 0.334    |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 33000.
Eval num_timesteps=777000, episode_reward=0.21 +/- 0.97
Episode length: 29.98 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.21     |
| time/                     |          |
|    total_timesteps        | 777000   |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00277  |
|    learning_rate          | 1e-05    |
|    n_updates              | 37       |
|    policy_objective       | 11.6     |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 34000.
Eval num_timesteps=798000, episode_reward=0.32 +/- 0.93
Episode length: 30.03 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.32     |
| time/                     |          |
|    total_timesteps        | 798000   |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00257  |
|    learning_rate          | 1e-05    |
|    n_updates              | 38       |
|    policy_objective       | 0.277    |
|    value_loss             | 0.224    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.32
SELFPLAY: new best model, bumping up generation to 33
Ep done - 35000.
Eval num_timesteps=819000, episode_reward=0.41 +/- 0.90
Episode length: 30.00 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.415    |
| time/                     |          |
|    total_timesteps        | 819000   |
| train/                    |          |
|    explained_variance     | 0.388    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00312  |
|    learning_rate          | 1e-05    |
|    n_updates              | 39       |
|    policy_objective       | 9.89     |
|    value_loss             | 0.232    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.415
SELFPLAY: new best model, bumping up generation to 34
Eval num_timesteps=840000, episode_reward=0.34 +/- 0.93
Episode length: 30.05 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.34     |
| time/                     |          |
|    total_timesteps        | 840000   |
| train/                    |          |
|    explained_variance     | 0.392    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0039   |
|    learning_rate          | 1e-05    |
|    n_updates              | 41       |
|    policy_objective       | 82.4     |
|    value_loss             | 0.237    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.34
SELFPLAY: new best model, bumping up generation to 35
Ep done - 36000.
Eval num_timesteps=861000, episode_reward=0.14 +/- 0.97
Episode length: 29.99 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 861000   |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00293  |
|    learning_rate          | 1e-05    |
|    n_updates              | 42       |
|    policy_objective       | 24.1     |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 37000.
Eval num_timesteps=882000, episode_reward=0.17 +/- 0.97
Episode length: 30.05 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 882000   |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00407  |
|    learning_rate          | 1e-05    |
|    n_updates              | 43       |
|    policy_objective       | 36.3     |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 38000.
Eval num_timesteps=903000, episode_reward=0.39 +/- 0.91
Episode length: 30.04 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.39     |
| time/                     |          |
|    total_timesteps        | 903000   |
| train/                    |          |
|    explained_variance     | 0.381    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00311  |
|    learning_rate          | 1e-05    |
|    n_updates              | 44       |
|    policy_objective       | 0.379    |
|    value_loss             | 0.233    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.39
SELFPLAY: new best model, bumping up generation to 36
Ep done - 39000.
Eval num_timesteps=924000, episode_reward=0.33 +/- 0.92
Episode length: 30.01 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.33     |
| time/                     |          |
|    total_timesteps        | 924000   |
| train/                    |          |
|    explained_variance     | 0.365    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0037   |
|    learning_rate          | 1e-05    |
|    n_updates              | 45       |
|    policy_objective       | 1.36e+03 |
|    value_loss             | 0.237    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.33
SELFPLAY: new best model, bumping up generation to 37
Ep done - 40000.
Eval num_timesteps=945000, episode_reward=0.12 +/- 0.98
Episode length: 29.96 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 945000   |
| train/                    |          |
|    explained_variance     | 0.384    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00363  |
|    learning_rate          | 1e-05    |
|    n_updates              | 46       |
|    policy_objective       | 6.22     |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 41000.
Eval num_timesteps=966000, episode_reward=0.23 +/- 0.96
Episode length: 30.02 +/- 0.38
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 966000   |
| train/                    |          |
|    explained_variance     | 0.392    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00276  |
|    learning_rate          | 1e-05    |
|    n_updates              | 47       |
|    policy_objective       | 0.902    |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 42000.
Eval num_timesteps=987000, episode_reward=0.16 +/- 0.97
Episode length: 30.02 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 987000   |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00397  |
|    learning_rate          | 1e-05    |
|    n_updates              | 48       |
|    policy_objective       | 20.3     |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 43000.
Eval num_timesteps=1008000, episode_reward=0.29 +/- 0.94
Episode length: 30.00 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.295    |
| time/                     |          |
|    total_timesteps        | 1008000  |
| train/                    |          |
|    explained_variance     | 0.405    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00366  |
|    learning_rate          | 1e-05    |
|    n_updates              | 49       |
|    policy_objective       | 33.3     |
|    value_loss             | 0.232    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.295
SELFPLAY: new best model, bumping up generation to 38
Ep done - 44000.
Eval num_timesteps=1029000, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.41
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 1029000  |
| train/                    |          |
|    explained_variance     | 0.391    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00515  |
|    learning_rate          | 1e-05    |
|    n_updates              | 50       |
|    policy_objective       | 786      |
|    value_loss             | 0.234    |
----------------------------------------
Eval num_timesteps=1050000, episode_reward=0.18 +/- 0.98
Episode length: 30.05 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 1050000  |
| train/                    |          |
|    explained_variance     | 0.38     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00343  |
|    learning_rate          | 1e-05    |
|    n_updates              | 51       |
|    policy_objective       | 21.2     |
|    value_loss             | 0.238    |
----------------------------------------
Ep done - 45000.
Eval num_timesteps=1071000, episode_reward=0.20 +/- 0.96
Episode length: 30.04 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 1071000  |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.004    |
|    learning_rate          | 1e-05    |
|    n_updates              | 52       |
|    policy_objective       | 319      |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 46000.
Eval num_timesteps=1092000, episode_reward=0.39 +/- 0.91
Episode length: 30.00 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.39     |
| time/                     |          |
|    total_timesteps        | 1092000  |
| train/                    |          |
|    explained_variance     | 0.356    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00377  |
|    learning_rate          | 1e-05    |
|    n_updates              | 53       |
|    policy_objective       | 283      |
|    value_loss             | 0.24     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.39
SELFPLAY: new best model, bumping up generation to 39
Ep done - 47000.
Eval num_timesteps=1113000, episode_reward=0.23 +/- 0.96
Episode length: 30.00 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 1113000  |
| train/                    |          |
|    explained_variance     | 0.37     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00394  |
|    learning_rate          | 1e-05    |
|    n_updates              | 54       |
|    policy_objective       | 1.14e+03 |
|    value_loss             | 0.236    |
----------------------------------------
Ep done - 48000.
Eval num_timesteps=1134000, episode_reward=0.18 +/- 0.97
Episode length: 30.04 +/- 0.40
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 1134000  |
| train/                    |          |
|    explained_variance     | 0.407    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00428  |
|    learning_rate          | 1e-05    |
|    n_updates              | 55       |
|    policy_objective       | 5.7e+03  |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 49000.
Eval num_timesteps=1155000, episode_reward=0.24 +/- 0.95
Episode length: 30.02 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.245    |
| time/                     |          |
|    total_timesteps        | 1155000  |
| train/                    |          |
|    explained_variance     | 0.402    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00497  |
|    learning_rate          | 1e-05    |
|    n_updates              | 56       |
|    policy_objective       | 2.47e+03 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 50000.
Eval num_timesteps=1176000, episode_reward=0.23 +/- 0.97
Episode length: 30.01 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 1176000  |
| train/                    |          |
|    explained_variance     | 0.412    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00441  |
|    learning_rate          | 1e-05    |
|    n_updates              | 57       |
|    policy_objective       | 1.52e+03 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 51000.
Eval num_timesteps=1197000, episode_reward=0.34 +/- 0.93
Episode length: 30.05 +/- 0.37
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.335    |
| time/                     |          |
|    total_timesteps        | 1197000  |
| train/                    |          |
|    explained_variance     | 0.421    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00389  |
|    learning_rate          | 1e-05    |
|    n_updates              | 58       |
|    policy_objective       | 291      |
|    value_loss             | 0.229    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.335
SELFPLAY: new best model, bumping up generation to 40
Ep done - 52000.
Eval num_timesteps=1218000, episode_reward=0.24 +/- 0.95
Episode length: 30.05 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.245    |
| time/                     |          |
|    total_timesteps        | 1218000  |
| train/                    |          |
|    explained_variance     | 0.366    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00431  |
|    learning_rate          | 1e-05    |
|    n_updates              | 59       |
|    policy_objective       | 43.3     |
|    value_loss             | 0.241    |
----------------------------------------
Ep done - 53000.
Eval num_timesteps=1239000, episode_reward=0.28 +/- 0.94
Episode length: 30.04 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.28     |
| time/                     |          |
|    total_timesteps        | 1239000  |
| train/                    |          |
|    explained_variance     | 0.389    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0044   |
|    learning_rate          | 1e-05    |
|    n_updates              | 60       |
|    policy_objective       | 1.87e+03 |
|    value_loss             | 0.231    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 41
Eval num_timesteps=1260000, episode_reward=0.36 +/- 0.92
Episode length: 29.95 +/- 1.08
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.365    |
| time/                     |          |
|    total_timesteps        | 1260000  |
| train/                    |          |
|    explained_variance     | 0.388    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00412  |
|    learning_rate          | 1e-05    |
|    n_updates              | 61       |
|    policy_objective       | 12.9     |
|    value_loss             | 0.234    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.365
SELFPLAY: new best model, bumping up generation to 42
Ep done - 54000.
Eval num_timesteps=1281000, episode_reward=0.23 +/- 0.96
Episode length: 29.96 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.235    |
| time/                     |          |
|    total_timesteps        | 1281000  |
| train/                    |          |
|    explained_variance     | 0.428    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00624  |
|    learning_rate          | 1e-05    |
|    n_updates              | 62       |
|    policy_objective       | 1.61e+04 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 55000.
Eval num_timesteps=1302000, episode_reward=0.21 +/- 0.96
Episode length: 30.09 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 1302000  |
| train/                    |          |
|    explained_variance     | 0.367    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00492  |
|    learning_rate          | 1e-05    |
|    n_updates              | 63       |
|    policy_objective       | 1.05e+03 |
|    value_loss             | 0.241    |
----------------------------------------
Ep done - 56000.
Eval num_timesteps=1323000, episode_reward=0.26 +/- 0.96
Episode length: 30.01 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.26     |
| time/                     |          |
|    total_timesteps        | 1323000  |
| train/                    |          |
|    explained_variance     | 0.375    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00625  |
|    learning_rate          | 1e-05    |
|    n_updates              | 64       |
|    policy_objective       | 1.45e+05 |
|    value_loss             | 0.237    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 43
Ep done - 57000.
Eval num_timesteps=1344000, episode_reward=0.18 +/- 0.96
Episode length: 29.98 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 1344000  |
| train/                    |          |
|    explained_variance     | 0.404    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00432  |
|    learning_rate          | 1e-05    |
|    n_updates              | 65       |
|    policy_objective       | 1.6e+04  |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 58000.
Eval num_timesteps=1365000, episode_reward=0.23 +/- 0.96
Episode length: 29.98 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 1365000  |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00494  |
|    learning_rate          | 1e-05    |
|    n_updates              | 66       |
|    policy_objective       | 4.62e+05 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 59000.
Eval num_timesteps=1386000, episode_reward=0.20 +/- 0.97
Episode length: 30.04 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 1386000  |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00515  |
|    learning_rate          | 1e-05    |
|    n_updates              | 67       |
|    policy_objective       | 114      |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 60000.
Eval num_timesteps=1407000, episode_reward=0.12 +/- 0.97
Episode length: 30.06 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 1407000  |
| train/                    |          |
|    explained_variance     | 0.389    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00445  |
|    learning_rate          | 1e-05    |
|    n_updates              | 68       |
|    policy_objective       | 103      |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 61000.
Eval num_timesteps=1428000, episode_reward=0.34 +/- 0.93
Episode length: 29.98 +/- 1.08
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.335    |
| time/                     |          |
|    total_timesteps        | 1428000  |
| train/                    |          |
|    explained_variance     | 0.44     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00449  |
|    learning_rate          | 1e-05    |
|    n_updates              | 69       |
|    policy_objective       | 1.06e+03 |
|    value_loss             | 0.22     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.335
SELFPLAY: new best model, bumping up generation to 44
Ep done - 62000.
Eval num_timesteps=1449000, episode_reward=0.28 +/- 0.95
Episode length: 30.05 +/- 0.41
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.28     |
| time/                     |          |
|    total_timesteps        | 1449000  |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00623  |
|    learning_rate          | 1e-05    |
|    n_updates              | 70       |
|    policy_objective       | 3.53e+05 |
|    value_loss             | 0.234    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 45
Eval num_timesteps=1470000, episode_reward=0.23 +/- 0.96
Episode length: 30.09 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.235    |
| time/                     |          |
|    total_timesteps        | 1470000  |
| train/                    |          |
|    explained_variance     | 0.4      |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00581  |
|    learning_rate          | 1e-05    |
|    n_updates              | 71       |
|    policy_objective       | 2.17e+05 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 63000.
Eval num_timesteps=1491000, episode_reward=0.25 +/- 0.96
Episode length: 30.04 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 1491000  |
| train/                    |          |
|    explained_variance     | 0.414    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00475  |
|    learning_rate          | 1e-05    |
|    n_updates              | 72       |
|    policy_objective       | 1.47e+05 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 64000.
Eval num_timesteps=1512000, episode_reward=0.17 +/- 0.96
Episode length: 30.01 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 1512000  |
| train/                    |          |
|    explained_variance     | 0.388    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00704  |
|    learning_rate          | 1e-05    |
|    n_updates              | 73       |
|    policy_objective       | 2.03e+07 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 65000.
Eval num_timesteps=1533000, episode_reward=0.37 +/- 0.91
Episode length: 30.04 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.37     |
| time/                     |          |
|    total_timesteps        | 1533000  |
| train/                    |          |
|    explained_variance     | 0.42     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00473  |
|    learning_rate          | 1e-05    |
|    n_updates              | 74       |
|    policy_objective       | 1.55e+03 |
|    value_loss             | 0.226    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.37
SELFPLAY: new best model, bumping up generation to 46
Ep done - 66000.
Eval num_timesteps=1554000, episode_reward=0.28 +/- 0.93
Episode length: 30.07 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.285    |
| time/                     |          |
|    total_timesteps        | 1554000  |
| train/                    |          |
|    explained_variance     | 0.42     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00493  |
|    learning_rate          | 1e-05    |
|    n_updates              | 75       |
|    policy_objective       | 42.2     |
|    value_loss             | 0.225    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 47
Ep done - 67000.
Eval num_timesteps=1575000, episode_reward=0.29 +/- 0.93
Episode length: 30.07 +/- 0.41
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.295    |
| time/                     |          |
|    total_timesteps        | 1575000  |
| train/                    |          |
|    explained_variance     | 0.423    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00524  |
|    learning_rate          | 1e-05    |
|    n_updates              | 76       |
|    policy_objective       | 7.23e+04 |
|    value_loss             | 0.227    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.295
SELFPLAY: new best model, bumping up generation to 48
Ep done - 68000.
Eval num_timesteps=1596000, episode_reward=0.15 +/- 0.97
Episode length: 29.92 +/- 1.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 1596000  |
| train/                    |          |
|    explained_variance     | 0.394    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00562  |
|    learning_rate          | 1e-05    |
|    n_updates              | 77       |
|    policy_objective       | 2.56e+07 |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 69000.
Eval num_timesteps=1617000, episode_reward=0.16 +/- 0.98
Episode length: 30.07 +/- 0.40
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 1617000  |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00558  |
|    learning_rate          | 1e-05    |
|    n_updates              | 78       |
|    policy_objective       | 4.28e+08 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 70000.
Eval num_timesteps=1638000, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.04     |
| time/                     |          |
|    total_timesteps        | 1638000  |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00491  |
|    learning_rate          | 1e-05    |
|    n_updates              | 79       |
|    policy_objective       | 2.48e+05 |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 71000.
Eval num_timesteps=1659000, episode_reward=0.22 +/- 0.96
Episode length: 30.02 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 1659000  |
| train/                    |          |
|    explained_variance     | 0.393    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00544  |
|    learning_rate          | 1e-05    |
|    n_updates              | 81       |
|    policy_objective       | 1.07e+08 |
|    value_loss             | 0.227    |
----------------------------------------
Eval num_timesteps=1680000, episode_reward=0.15 +/- 0.98
Episode length: 30.07 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 1680000  |
| train/                    |          |
|    explained_variance     | 0.392    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0054   |
|    learning_rate          | 1e-05    |
|    n_updates              | 82       |
|    policy_objective       | 3.99e+04 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 72000.
Eval num_timesteps=1701000, episode_reward=0.30 +/- 0.94
Episode length: 30.04 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.3      |
| time/                     |          |
|    total_timesteps        | 1701000  |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0061   |
|    learning_rate          | 1e-05    |
|    n_updates              | 83       |
|    policy_objective       | 2.88e+05 |
|    value_loss             | 0.233    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 49
Ep done - 73000.
Eval num_timesteps=1722000, episode_reward=0.20 +/- 0.97
Episode length: 30.05 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 1722000  |
| train/                    |          |
|    explained_variance     | 0.423    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00545  |
|    learning_rate          | 1e-05    |
|    n_updates              | 84       |
|    policy_objective       | 1.38e+04 |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 74000.
Eval num_timesteps=1743000, episode_reward=0.17 +/- 0.97
Episode length: 30.00 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 1743000  |
| train/                    |          |
|    explained_variance     | 0.4      |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00636  |
|    learning_rate          | 1e-05    |
|    n_updates              | 85       |
|    policy_objective       | 5.43e+06 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 75000.
Eval num_timesteps=1764000, episode_reward=0.14 +/- 0.98
Episode length: 30.04 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 1764000  |
| train/                    |          |
|    explained_variance     | 0.402    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00575  |
|    learning_rate          | 1e-05    |
|    n_updates              | 86       |
|    policy_objective       | 273      |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 76000.
Eval num_timesteps=1785000, episode_reward=0.23 +/- 0.94
Episode length: 30.02 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.235    |
| time/                     |          |
|    total_timesteps        | 1785000  |
| train/                    |          |
|    explained_variance     | 0.365    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00764  |
|    learning_rate          | 1e-05    |
|    n_updates              | 87       |
|    policy_objective       | 6.35e+09 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 77000.
Eval num_timesteps=1806000, episode_reward=0.23 +/- 0.96
Episode length: 30.02 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 1806000  |
| train/                    |          |
|    explained_variance     | 0.388    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0067   |
|    learning_rate          | 1e-05    |
|    n_updates              | 88       |
|    policy_objective       | 3.12e+06 |
|    value_loss             | 0.236    |
----------------------------------------
Ep done - 78000.
Eval num_timesteps=1827000, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 1827000  |
| train/                    |          |
|    explained_variance     | 0.401    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00556  |
|    learning_rate          | 1e-05    |
|    n_updates              | 89       |
|    policy_objective       | 8.77e+06 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 79000.
Eval num_timesteps=1848000, episode_reward=0.01 +/- 0.99
Episode length: 30.00 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.005    |
| time/                     |          |
|    total_timesteps        | 1848000  |
| train/                    |          |
|    explained_variance     | 0.399    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00826  |
|    learning_rate          | 1e-05    |
|    n_updates              | 90       |
|    policy_objective       | 1.4e+10  |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 80000.
Eval num_timesteps=1869000, episode_reward=-0.04 +/- 0.99
Episode length: 30.00 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | -0.04    |
| time/                     |          |
|    total_timesteps        | 1869000  |
| train/                    |          |
|    explained_variance     | 0.401    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00596  |
|    learning_rate          | 1e-05    |
|    n_updates              | 91       |
|    policy_objective       | 1.47e+07 |
|    value_loss             | 0.231    |
----------------------------------------
Eval num_timesteps=1890000, episode_reward=0.21 +/- 0.95
Episode length: 30.01 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.21     |
| time/                     |          |
|    total_timesteps        | 1890000  |
| train/                    |          |
|    explained_variance     | 0.393    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00863  |
|    learning_rate          | 1e-05    |
|    n_updates              | 92       |
|    policy_objective       | 5.37e+10 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 81000.
Eval num_timesteps=1911000, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.005    |
| time/                     |          |
|    total_timesteps        | 1911000  |
| train/                    |          |
|    explained_variance     | 0.402    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00601  |
|    learning_rate          | 1e-05    |
|    n_updates              | 93       |
|    policy_objective       | 3.5e+06  |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 82000.
Eval num_timesteps=1932000, episode_reward=0.12 +/- 0.99
Episode length: 29.98 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 1932000  |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00649  |
|    learning_rate          | 1e-05    |
|    n_updates              | 94       |
|    policy_objective       | 3.35e+06 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 83000.
Eval num_timesteps=1953000, episode_reward=0.06 +/- 0.99
Episode length: 30.02 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 1953000  |
| train/                    |          |
|    explained_variance     | 0.404    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00887  |
|    learning_rate          | 1e-05    |
|    n_updates              | 95       |
|    policy_objective       | 7.2e+07  |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 84000.
Eval num_timesteps=1974000, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.075    |
| time/                     |          |
|    total_timesteps        | 1974000  |
| train/                    |          |
|    explained_variance     | 0.411    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00663  |
|    learning_rate          | 1e-05    |
|    n_updates              | 96       |
|    policy_objective       | 1.28e+09 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 85000.
Eval num_timesteps=1995000, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 1995000  |
| train/                    |          |
|    explained_variance     | 0.396    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00769  |
|    learning_rate          | 1e-05    |
|    n_updates              | 97       |
|    policy_objective       | 6.18e+05 |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 86000.
Eval num_timesteps=2016000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 2016000  |
| train/                    |          |
|    explained_variance     | 0.411    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00667  |
|    learning_rate          | 1e-05    |
|    n_updates              | 98       |
|    policy_objective       | 3.73e+10 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 87000.
Eval num_timesteps=2037000, episode_reward=0.18 +/- 0.97
Episode length: 30.05 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 2037000  |
| train/                    |          |
|    explained_variance     | 0.373    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00813  |
|    learning_rate          | 1e-05    |
|    n_updates              | 99       |
|    policy_objective       | 4.64e+14 |
|    value_loss             | 0.244    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 201      |
|    iterations      | 100      |
|    time_elapsed    | 10164    |
|    total_timesteps | 2048000  |
---------------------------------
Ep done - 88000.
Eval num_timesteps=2058000, episode_reward=0.12 +/- 0.99
Episode length: 30.09 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 2058000  |
| train/                    |          |
|    explained_variance     | 0.392    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00827  |
|    learning_rate          | 1e-05    |
|    n_updates              | 100      |
|    policy_objective       | 1.55e+14 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 89000.
Eval num_timesteps=2079000, episode_reward=0.20 +/- 0.97
Episode length: 30.05 +/- 0.41
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 2079000  |
| train/                    |          |
|    explained_variance     | 0.419    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00682  |
|    learning_rate          | 1e-05    |
|    n_updates              | 101      |
|    policy_objective       | 2.44e+07 |
|    value_loss             | 0.221    |
----------------------------------------
Eval num_timesteps=2100000, episode_reward=0.12 +/- 0.97
Episode length: 30.00 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 2100000  |
| train/                    |          |
|    explained_variance     | 0.444    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00852  |
|    learning_rate          | 1e-05    |
|    n_updates              | 102      |
|    policy_objective       | 2.06e+08 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 90000.
Eval num_timesteps=2121000, episode_reward=0.21 +/- 0.96
Episode length: 30.02 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 2121000  |
| train/                    |          |
|    explained_variance     | 0.397    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00664  |
|    learning_rate          | 1e-05    |
|    n_updates              | 103      |
|    policy_objective       | 3.25e+12 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 91000.
Eval num_timesteps=2142000, episode_reward=0.20 +/- 0.97
Episode length: 29.84 +/- 1.70
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.8     |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 2142000  |
| train/                    |          |
|    explained_variance     | 0.386    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00633  |
|    learning_rate          | 1e-05    |
|    n_updates              | 104      |
|    policy_objective       | 1.56e+09 |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 92000.
Eval num_timesteps=2163000, episode_reward=0.27 +/- 0.96
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.265    |
| time/                     |          |
|    total_timesteps        | 2163000  |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00564  |
|    learning_rate          | 1e-05    |
|    n_updates              | 105      |
|    policy_objective       | 3.67e+06 |
|    value_loss             | 0.231    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 50
Ep done - 93000.
Eval num_timesteps=2184000, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.06     |
| time/                     |          |
|    total_timesteps        | 2184000  |
| train/                    |          |
|    explained_variance     | 0.394    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00601  |
|    learning_rate          | 1e-05    |
|    n_updates              | 106      |
|    policy_objective       | 1.81e+08 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 94000.
Eval num_timesteps=2205000, episode_reward=0.18 +/- 0.98
Episode length: 30.03 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 2205000  |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00649  |
|    learning_rate          | 1e-05    |
|    n_updates              | 107      |
|    policy_objective       | 1.1e+05  |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 95000.
Eval num_timesteps=2226000, episode_reward=0.13 +/- 0.98
Episode length: 29.98 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 2226000  |
| train/                    |          |
|    explained_variance     | 0.382    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00945  |
|    learning_rate          | 1e-05    |
|    n_updates              | 108      |
|    policy_objective       | 1.31e+10 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 96000.
Eval num_timesteps=2247000, episode_reward=0.16 +/- 0.97
Episode length: 30.02 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 2247000  |
| train/                    |          |
|    explained_variance     | 0.397    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00816  |
|    learning_rate          | 1e-05    |
|    n_updates              | 109      |
|    policy_objective       | 4.22e+09 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 97000.
Eval num_timesteps=2268000, episode_reward=0.21 +/- 0.97
Episode length: 30.04 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 2268000  |
| train/                    |          |
|    explained_variance     | 0.433    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00853  |
|    learning_rate          | 1e-05    |
|    n_updates              | 110      |
|    policy_objective       | 2.22e+09 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 98000.
Eval num_timesteps=2289000, episode_reward=0.18 +/- 0.97
Episode length: 30.02 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 2289000  |
| train/                    |          |
|    explained_variance     | 0.403    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0056   |
|    learning_rate          | 1e-05    |
|    n_updates              | 111      |
|    policy_objective       | 2.37e+04 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 99000.
Eval num_timesteps=2310000, episode_reward=0.18 +/- 0.98
Episode length: 29.96 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 2310000  |
| train/                    |          |
|    explained_variance     | 0.421    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0088   |
|    learning_rate          | 1e-05    |
|    n_updates              | 112      |
|    policy_objective       | 5.02e+14 |
|    value_loss             | 0.226    |
----------------------------------------
Eval num_timesteps=2331000, episode_reward=0.17 +/- 0.98
Episode length: 30.00 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 2331000  |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00733  |
|    learning_rate          | 1e-05    |
|    n_updates              | 113      |
|    policy_objective       | 1.01e+09 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 100000.
Eval num_timesteps=2352000, episode_reward=0.17 +/- 0.98
Episode length: 30.04 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 2352000  |
| train/                    |          |
|    explained_variance     | 0.38     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00932  |
|    learning_rate          | 1e-05    |
|    n_updates              | 114      |
|    policy_objective       | 1.06e+10 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 101000.
Eval num_timesteps=2373000, episode_reward=0.22 +/- 0.96
Episode length: 30.02 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 2373000  |
| train/                    |          |
|    explained_variance     | 0.404    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0079   |
|    learning_rate          | 1e-05    |
|    n_updates              | 115      |
|    policy_objective       | 1.11e+08 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 102000.
Eval num_timesteps=2394000, episode_reward=0.28 +/- 0.95
Episode length: 30.07 +/- 0.40
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.285    |
| time/                     |          |
|    total_timesteps        | 2394000  |
| train/                    |          |
|    explained_variance     | 0.367    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0072   |
|    learning_rate          | 1e-05    |
|    n_updates              | 116      |
|    policy_objective       | 9.43e+12 |
|    value_loss             | 0.238    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 51
Ep done - 103000.
Eval num_timesteps=2415000, episode_reward=0.21 +/- 0.96
Episode length: 30.03 +/- 0.41
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 2415000  |
| train/                    |          |
|    explained_variance     | 0.412    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0051   |
|    learning_rate          | 1e-05    |
|    n_updates              | 117      |
|    policy_objective       | 5.23e+06 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 104000.
Eval num_timesteps=2436000, episode_reward=0.12 +/- 0.99
Episode length: 30.02 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 2436000  |
| train/                    |          |
|    explained_variance     | 0.399    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00582  |
|    learning_rate          | 1e-05    |
|    n_updates              | 118      |
|    policy_objective       | 9.3e+12  |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 105000.
Eval num_timesteps=2457000, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 2457000  |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00625  |
|    learning_rate          | 1e-05    |
|    n_updates              | 119      |
|    policy_objective       | 5.1e+10  |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 106000.
Eval num_timesteps=2478000, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 2478000  |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00898  |
|    learning_rate          | 1e-05    |
|    n_updates              | 120      |
|    policy_objective       | 6.29e+11 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 107000.
Eval num_timesteps=2499000, episode_reward=0.09 +/- 0.97
Episode length: 30.05 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 2499000  |
| train/                    |          |
|    explained_variance     | 0.447    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00566  |
|    learning_rate          | 1e-05    |
|    n_updates              | 122      |
|    policy_objective       | 3.88e+11 |
|    value_loss             | 0.218    |
----------------------------------------
Eval num_timesteps=2520000, episode_reward=0.34 +/- 0.93
Episode length: 29.98 +/- 0.97
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.335    |
| time/                     |          |
|    total_timesteps        | 2520000  |
| train/                    |          |
|    explained_variance     | 0.444    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0096   |
|    learning_rate          | 1e-05    |
|    n_updates              | 123      |
|    policy_objective       | 2.89e+13 |
|    value_loss             | 0.225    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.335
SELFPLAY: new best model, bumping up generation to 52
Ep done - 108000.
Eval num_timesteps=2541000, episode_reward=0.33 +/- 0.93
Episode length: 30.05 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.33     |
| time/                     |          |
|    total_timesteps        | 2541000  |
| train/                    |          |
|    explained_variance     | 0.389    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00583  |
|    learning_rate          | 1e-05    |
|    n_updates              | 124      |
|    policy_objective       | 1.52e+12 |
|    value_loss             | 0.237    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.33
SELFPLAY: new best model, bumping up generation to 53
Ep done - 109000.
Eval num_timesteps=2562000, episode_reward=0.12 +/- 0.97
Episode length: 30.06 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 2562000  |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00783  |
|    learning_rate          | 1e-05    |
|    n_updates              | 125      |
|    policy_objective       | 2.48e+14 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 110000.
Eval num_timesteps=2583000, episode_reward=0.11 +/- 0.98
Episode length: 30.05 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 2583000  |
| train/                    |          |
|    explained_variance     | 0.414    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00655  |
|    learning_rate          | 1e-05    |
|    n_updates              | 126      |
|    policy_objective       | 5.52e+11 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 111000.
Eval num_timesteps=2604000, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 2604000  |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00619  |
|    learning_rate          | 1e-05    |
|    n_updates              | 127      |
|    policy_objective       | 5.35e+05 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 112000.
Eval num_timesteps=2625000, episode_reward=0.15 +/- 0.98
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 2625000  |
| train/                    |          |
|    explained_variance     | 0.396    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00945  |
|    learning_rate          | 1e-05    |
|    n_updates              | 128      |
|    policy_objective       | 4.12e+09 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 113000.
Eval num_timesteps=2646000, episode_reward=0.20 +/- 0.96
Episode length: 30.02 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 2646000  |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00936  |
|    learning_rate          | 1e-05    |
|    n_updates              | 129      |
|    policy_objective       | 9.27e+08 |
|    value_loss             | 0.236    |
----------------------------------------
Ep done - 114000.
Eval num_timesteps=2667000, episode_reward=0.26 +/- 0.96
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.26     |
| time/                     |          |
|    total_timesteps        | 2667000  |
| train/                    |          |
|    explained_variance     | 0.405    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00648  |
|    learning_rate          | 1e-05    |
|    n_updates              | 130      |
|    policy_objective       | 4.28e+08 |
|    value_loss             | 0.228    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 54
Ep done - 115000.
Eval num_timesteps=2688000, episode_reward=0.13 +/- 0.97
Episode length: 30.06 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 2688000  |
| train/                    |          |
|    explained_variance     | 0.432    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00615  |
|    learning_rate          | 1e-05    |
|    n_updates              | 131      |
|    policy_objective       | 1.28e+10 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 116000.
Eval num_timesteps=2709000, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 2709000  |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00985  |
|    learning_rate          | 1e-05    |
|    n_updates              | 132      |
|    policy_objective       | 2.14e+09 |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 117000.
Eval num_timesteps=2730000, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 2730000  |
| train/                    |          |
|    explained_variance     | 0.424    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00823  |
|    learning_rate          | 1e-05    |
|    n_updates              | 133      |
|    policy_objective       | 8.98e+10 |
|    value_loss             | 0.226    |
----------------------------------------
Eval num_timesteps=2751000, episode_reward=0.26 +/- 0.97
Episode length: 30.06 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.26     |
| time/                     |          |
|    total_timesteps        | 2751000  |
| train/                    |          |
|    explained_variance     | 0.37     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00879  |
|    learning_rate          | 1e-05    |
|    n_updates              | 134      |
|    policy_objective       | 1.75e+14 |
|    value_loss             | 0.24     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 55
Ep done - 118000.
Eval num_timesteps=2772000, episode_reward=0.18 +/- 0.97
Episode length: 30.03 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 2772000  |
| train/                    |          |
|    explained_variance     | 0.402    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00749  |
|    learning_rate          | 1e-05    |
|    n_updates              | 135      |
|    policy_objective       | 1.13e+12 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 119000.
Eval num_timesteps=2793000, episode_reward=0.25 +/- 0.96
Episode length: 30.01 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 2793000  |
| train/                    |          |
|    explained_variance     | 0.432    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00576  |
|    learning_rate          | 1e-05    |
|    n_updates              | 136      |
|    policy_objective       | 1.61e+08 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 120000.
Eval num_timesteps=2814000, episode_reward=0.31 +/- 0.93
Episode length: 30.05 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.31     |
| time/                     |          |
|    total_timesteps        | 2814000  |
| train/                    |          |
|    explained_variance     | 0.387    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00866  |
|    learning_rate          | 1e-05    |
|    n_updates              | 137      |
|    policy_objective       | 2.49e+10 |
|    value_loss             | 0.239    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 56
Ep done - 121000.
Eval num_timesteps=2835000, episode_reward=0.20 +/- 0.97
Episode length: 29.98 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 2835000  |
| train/                    |          |
|    explained_variance     | 0.435    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00528  |
|    learning_rate          | 1e-05    |
|    n_updates              | 138      |
|    policy_objective       | 7.48e+11 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 122000.
Eval num_timesteps=2856000, episode_reward=0.19 +/- 0.96
Episode length: 30.03 +/- 0.37
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 2856000  |
| train/                    |          |
|    explained_variance     | 0.4      |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00652  |
|    learning_rate          | 1e-05    |
|    n_updates              | 139      |
|    policy_objective       | 4.92e+07 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 123000.
Eval num_timesteps=2877000, episode_reward=0.23 +/- 0.97
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 2877000  |
| train/                    |          |
|    explained_variance     | 0.438    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00801  |
|    learning_rate          | 1e-05    |
|    n_updates              | 140      |
|    policy_objective       | 2.87e+07 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 124000.
Eval num_timesteps=2898000, episode_reward=0.14 +/- 0.97
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 2898000  |
| train/                    |          |
|    explained_variance     | 0.432    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00626  |
|    learning_rate          | 1e-05    |
|    n_updates              | 141      |
|    policy_objective       | 1.08e+11 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 125000.
Eval num_timesteps=2919000, episode_reward=0.21 +/- 0.97
Episode length: 30.09 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.21     |
| time/                     |          |
|    total_timesteps        | 2919000  |
| train/                    |          |
|    explained_variance     | 0.405    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0097   |
|    learning_rate          | 1e-05    |
|    n_updates              | 142      |
|    policy_objective       | 1.98e+10 |
|    value_loss             | 0.229    |
----------------------------------------
Eval num_timesteps=2940000, episode_reward=0.16 +/- 0.96
Episode length: 29.98 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 2940000  |
| train/                    |          |
|    explained_variance     | 0.396    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00885  |
|    learning_rate          | 1e-05    |
|    n_updates              | 143      |
|    policy_objective       | 7.2e+10  |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 126000.
Eval num_timesteps=2961000, episode_reward=0.23 +/- 0.97
Episode length: 29.98 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 2961000  |
| train/                    |          |
|    explained_variance     | 0.411    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00872  |
|    learning_rate          | 1e-05    |
|    n_updates              | 144      |
|    policy_objective       | 5.84e+11 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 127000.
Eval num_timesteps=2982000, episode_reward=0.27 +/- 0.96
Episode length: 30.02 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.265    |
| time/                     |          |
|    total_timesteps        | 2982000  |
| train/                    |          |
|    explained_variance     | 0.422    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0098   |
|    learning_rate          | 1e-05    |
|    n_updates              | 145      |
|    policy_objective       | 7.26e+09 |
|    value_loss             | 0.229    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 57
Ep done - 128000.
Eval num_timesteps=3003000, episode_reward=0.06 +/- 0.99
Episode length: 29.96 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 3003000  |
| train/                    |          |
|    explained_variance     | 0.438    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00781  |
|    learning_rate          | 1e-05    |
|    n_updates              | 146      |
|    policy_objective       | 1.01e+08 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 129000.
Eval num_timesteps=3024000, episode_reward=0.12 +/- 0.97
Episode length: 29.96 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 3024000  |
| train/                    |          |
|    explained_variance     | 0.4      |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00691  |
|    learning_rate          | 1e-05    |
|    n_updates              | 147      |
|    policy_objective       | 9.9e+09  |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 130000.
Eval num_timesteps=3045000, episode_reward=0.23 +/- 0.96
Episode length: 30.00 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 3045000  |
| train/                    |          |
|    explained_variance     | 0.392    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00523  |
|    learning_rate          | 1e-05    |
|    n_updates              | 148      |
|    policy_objective       | 2.05e+10 |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 131000.
Eval num_timesteps=3066000, episode_reward=0.28 +/- 0.95
Episode length: 29.99 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.285    |
| time/                     |          |
|    total_timesteps        | 3066000  |
| train/                    |          |
|    explained_variance     | 0.419    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00963  |
|    learning_rate          | 1e-05    |
|    n_updates              | 149      |
|    policy_objective       | 2.21e+10 |
|    value_loss             | 0.223    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 58
Ep done - 132000.
Eval num_timesteps=3087000, episode_reward=0.23 +/- 0.96
Episode length: 30.05 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 3087000  |
| train/                    |          |
|    explained_variance     | 0.405    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00814  |
|    learning_rate          | 1e-05    |
|    n_updates              | 150      |
|    policy_objective       | 3.54e+11 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 133000.
Eval num_timesteps=3108000, episode_reward=0.21 +/- 0.96
Episode length: 29.98 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 3108000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00882  |
|    learning_rate          | 1e-05    |
|    n_updates              | 151      |
|    policy_objective       | 1.54e+09 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 134000.
Eval num_timesteps=3129000, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | -0.01    |
| time/                     |          |
|    total_timesteps        | 3129000  |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00682  |
|    learning_rate          | 1e-05    |
|    n_updates              | 152      |
|    policy_objective       | 8.48e+14 |
|    value_loss             | 0.229    |
----------------------------------------
Eval num_timesteps=3150000, episode_reward=0.17 +/- 0.98
Episode length: 30.07 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 3150000  |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0066   |
|    learning_rate          | 1e-05    |
|    n_updates              | 153      |
|    policy_objective       | 1.13e+08 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 135000.
Eval num_timesteps=3171000, episode_reward=0.14 +/- 0.97
Episode length: 30.06 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 3171000  |
| train/                    |          |
|    explained_variance     | 0.424    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00772  |
|    learning_rate          | 1e-05    |
|    n_updates              | 154      |
|    policy_objective       | 4.24e+13 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 136000.
Eval num_timesteps=3192000, episode_reward=0.17 +/- 0.98
Episode length: 30.04 +/- 0.37
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 3192000  |
| train/                    |          |
|    explained_variance     | 0.398    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00977  |
|    learning_rate          | 1e-05    |
|    n_updates              | 155      |
|    policy_objective       | 2.43e+09 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 137000.
Eval num_timesteps=3213000, episode_reward=0.21 +/- 0.97
Episode length: 30.05 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 3213000  |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00617  |
|    learning_rate          | 1e-05    |
|    n_updates              | 156      |
|    policy_objective       | 1e+11    |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 138000.
Eval num_timesteps=3234000, episode_reward=0.15 +/- 0.96
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 3234000  |
| train/                    |          |
|    explained_variance     | 0.409    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00593  |
|    learning_rate          | 1e-05    |
|    n_updates              | 157      |
|    policy_objective       | 9.46e+08 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 139000.
Eval num_timesteps=3255000, episode_reward=0.19 +/- 0.97
Episode length: 30.05 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 3255000  |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00818  |
|    learning_rate          | 1e-05    |
|    n_updates              | 158      |
|    policy_objective       | 1.73e+07 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 140000.
Eval num_timesteps=3276000, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 3276000  |
| train/                    |          |
|    explained_variance     | 0.403    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0091   |
|    learning_rate          | 1e-05    |
|    n_updates              | 159      |
|    policy_objective       | 3.82e+12 |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 141000.
Eval num_timesteps=3297000, episode_reward=0.26 +/- 0.95
Episode length: 29.99 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.255    |
| time/                     |          |
|    total_timesteps        | 3297000  |
| train/                    |          |
|    explained_variance     | 0.429    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00869  |
|    learning_rate          | 1e-05    |
|    n_updates              | 160      |
|    policy_objective       | 7.78e+14 |
|    value_loss             | 0.221    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 59
Ep done - 142000.
Eval num_timesteps=3318000, episode_reward=0.14 +/- 0.98
Episode length: 30.03 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 3318000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00868  |
|    learning_rate          | 1e-05    |
|    n_updates              | 162      |
|    policy_objective       | 1.94e+08 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 143000.
Eval num_timesteps=3339000, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.065    |
| time/                     |          |
|    total_timesteps        | 3339000  |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00453  |
|    learning_rate          | 1e-05    |
|    n_updates              | 163      |
|    policy_objective       | 6.03e+08 |
|    value_loss             | 0.228    |
----------------------------------------
Eval num_timesteps=3360000, episode_reward=0.17 +/- 0.96
Episode length: 30.07 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 3360000  |
| train/                    |          |
|    explained_variance     | 0.435    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00961  |
|    learning_rate          | 1e-05    |
|    n_updates              | 164      |
|    policy_objective       | 6.12e+11 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 144000.
Eval num_timesteps=3381000, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.195    |
| time/                     |          |
|    total_timesteps        | 3381000  |
| train/                    |          |
|    explained_variance     | 0.41     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00733  |
|    learning_rate          | 1e-05    |
|    n_updates              | 165      |
|    policy_objective       | 492      |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 145000.
Eval num_timesteps=3402000, episode_reward=0.13 +/- 0.96
Episode length: 30.02 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 3402000  |
| train/                    |          |
|    explained_variance     | 0.435    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0074   |
|    learning_rate          | 1e-05    |
|    n_updates              | 166      |
|    policy_objective       | 1.4e+09  |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 146000.
Eval num_timesteps=3423000, episode_reward=0.15 +/- 0.95
Episode length: 30.00 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 3423000  |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0072   |
|    learning_rate          | 1e-05    |
|    n_updates              | 167      |
|    policy_objective       | 2.7e+10  |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 147000.
Eval num_timesteps=3444000, episode_reward=0.07 +/- 0.99
Episode length: 30.03 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.07     |
| time/                     |          |
|    total_timesteps        | 3444000  |
| train/                    |          |
|    explained_variance     | 0.431    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00917  |
|    learning_rate          | 1e-05    |
|    n_updates              | 168      |
|    policy_objective       | 7.23e+11 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 148000.
Eval num_timesteps=3465000, episode_reward=0.12 +/- 0.97
Episode length: 29.98 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 3465000  |
| train/                    |          |
|    explained_variance     | 0.443    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00974  |
|    learning_rate          | 1e-05    |
|    n_updates              | 169      |
|    policy_objective       | 1.71e+13 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 149000.
Eval num_timesteps=3486000, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 3486000  |
| train/                    |          |
|    explained_variance     | 0.456    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00774  |
|    learning_rate          | 1e-05    |
|    n_updates              | 170      |
|    policy_objective       | 5.41e+10 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 150000.
Eval num_timesteps=3507000, episode_reward=0.10 +/- 0.98
Episode length: 30.03 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.105    |
| time/                     |          |
|    total_timesteps        | 3507000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00627  |
|    learning_rate          | 1e-05    |
|    n_updates              | 171      |
|    policy_objective       | 2.02e+08 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 151000.
Eval num_timesteps=3528000, episode_reward=0.21 +/- 0.97
Episode length: 30.00 +/- 1.07
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 3528000  |
| train/                    |          |
|    explained_variance     | 0.459    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00872  |
|    learning_rate          | 1e-05    |
|    n_updates              | 172      |
|    policy_objective       | 1.07e+07 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 152000.
Eval num_timesteps=3549000, episode_reward=0.15 +/- 0.98
Episode length: 30.03 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 3549000  |
| train/                    |          |
|    explained_variance     | 0.437    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.006    |
|    learning_rate          | 1e-05    |
|    n_updates              | 173      |
|    policy_objective       | 3.73e+08 |
|    value_loss             | 0.223    |
----------------------------------------
Eval num_timesteps=3570000, episode_reward=0.26 +/- 0.95
Episode length: 30.03 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.255    |
| time/                     |          |
|    total_timesteps        | 3570000  |
| train/                    |          |
|    explained_variance     | 0.421    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0096   |
|    learning_rate          | 1e-05    |
|    n_updates              | 174      |
|    policy_objective       | 3.93e+15 |
|    value_loss             | 0.229    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 60
Ep done - 153000.
Eval num_timesteps=3591000, episode_reward=0.03 +/- 0.98
Episode length: 29.98 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.03     |
| time/                     |          |
|    total_timesteps        | 3591000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00648  |
|    learning_rate          | 1e-05    |
|    n_updates              | 175      |
|    policy_objective       | 2.46e+09 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 154000.
Eval num_timesteps=3612000, episode_reward=0.23 +/- 0.96
Episode length: 30.04 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 3612000  |
| train/                    |          |
|    explained_variance     | 0.378    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00693  |
|    learning_rate          | 1e-05    |
|    n_updates              | 176      |
|    policy_objective       | 9.23e+08 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 155000.
Eval num_timesteps=3633000, episode_reward=0.09 +/- 0.98
Episode length: 30.07 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.085    |
| time/                     |          |
|    total_timesteps        | 3633000  |
| train/                    |          |
|    explained_variance     | 0.426    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00924  |
|    learning_rate          | 1e-05    |
|    n_updates              | 177      |
|    policy_objective       | 1.32e+17 |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 156000.
Eval num_timesteps=3654000, episode_reward=0.26 +/- 0.96
Episode length: 30.08 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.26     |
| time/                     |          |
|    total_timesteps        | 3654000  |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00526  |
|    learning_rate          | 1e-05    |
|    n_updates              | 178      |
|    policy_objective       | 4.11e+13 |
|    value_loss             | 0.224    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 61
Ep done - 157000.
Eval num_timesteps=3675000, episode_reward=0.23 +/- 0.96
Episode length: 30.09 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 3675000  |
| train/                    |          |
|    explained_variance     | 0.42     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0056   |
|    learning_rate          | 1e-05    |
|    n_updates              | 179      |
|    policy_objective       | 1.37e+15 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 158000.
Eval num_timesteps=3696000, episode_reward=0.22 +/- 0.96
Episode length: 29.95 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 3696000  |
| train/                    |          |
|    explained_variance     | 0.419    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00574  |
|    learning_rate          | 1e-05    |
|    n_updates              | 180      |
|    policy_objective       | 5.22e+07 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 159000.
Eval num_timesteps=3717000, episode_reward=0.23 +/- 0.96
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 3717000  |
| train/                    |          |
|    explained_variance     | 0.404    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00864  |
|    learning_rate          | 1e-05    |
|    n_updates              | 181      |
|    policy_objective       | 3.99e+09 |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 160000.
Eval num_timesteps=3738000, episode_reward=0.23 +/- 0.96
Episode length: 30.03 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 3738000  |
| train/                    |          |
|    explained_variance     | 0.432    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00821  |
|    learning_rate          | 1e-05    |
|    n_updates              | 182      |
|    policy_objective       | 1.72e+07 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 161000.
Eval num_timesteps=3759000, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 3759000  |
| train/                    |          |
|    explained_variance     | 0.44     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00696  |
|    learning_rate          | 1e-05    |
|    n_updates              | 183      |
|    policy_objective       | 1.05e+16 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 162000.
Eval num_timesteps=3780000, episode_reward=0.27 +/- 0.95
Episode length: 30.00 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.265    |
| time/                     |          |
|    total_timesteps        | 3780000  |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00948  |
|    learning_rate          | 1e-05    |
|    n_updates              | 184      |
|    policy_objective       | 2.49e+13 |
|    value_loss             | 0.232    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 62
Eval num_timesteps=3801000, episode_reward=0.16 +/- 0.98
Episode length: 29.95 +/- 1.36
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 3801000  |
| train/                    |          |
|    explained_variance     | 0.42     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00929  |
|    learning_rate          | 1e-05    |
|    n_updates              | 185      |
|    policy_objective       | 2.99e+09 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 163000.
Eval num_timesteps=3822000, episode_reward=0.17 +/- 0.97
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 3822000  |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00623  |
|    learning_rate          | 1e-05    |
|    n_updates              | 186      |
|    policy_objective       | 5.31e+07 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 164000.
Eval num_timesteps=3843000, episode_reward=0.21 +/- 0.95
Episode length: 30.09 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 3843000  |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00942  |
|    learning_rate          | 1e-05    |
|    n_updates              | 187      |
|    policy_objective       | 6.52e+13 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 165000.
Eval num_timesteps=3864000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 3864000  |
| train/                    |          |
|    explained_variance     | 0.435    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0062   |
|    learning_rate          | 1e-05    |
|    n_updates              | 188      |
|    policy_objective       | 1.31e+06 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 166000.
Eval num_timesteps=3885000, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 3885000  |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00742  |
|    learning_rate          | 1e-05    |
|    n_updates              | 189      |
|    policy_objective       | 1.38e+11 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 167000.
Eval num_timesteps=3906000, episode_reward=0.15 +/- 0.97
Episode length: 29.96 +/- 1.38
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 3906000  |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00903  |
|    learning_rate          | 1e-05    |
|    n_updates              | 190      |
|    policy_objective       | 2.07e+09 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 168000.
Eval num_timesteps=3927000, episode_reward=0.15 +/- 0.98
Episode length: 29.96 +/- 0.95
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 3927000  |
| train/                    |          |
|    explained_variance     | 0.419    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00967  |
|    learning_rate          | 1e-05    |
|    n_updates              | 191      |
|    policy_objective       | 2.18e+10 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 169000.
Eval num_timesteps=3948000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 3948000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00659  |
|    learning_rate          | 1e-05    |
|    n_updates              | 192      |
|    policy_objective       | 8.35e+12 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 170000.
Eval num_timesteps=3969000, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 3969000  |
| train/                    |          |
|    explained_variance     | 0.386    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00851  |
|    learning_rate          | 1e-05    |
|    n_updates              | 193      |
|    policy_objective       | 2.22e+12 |
|    value_loss             | 0.24     |
----------------------------------------
Ep done - 171000.
Eval num_timesteps=3990000, episode_reward=0.20 +/- 0.97
Episode length: 30.05 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 3990000  |
| train/                    |          |
|    explained_variance     | 0.455    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00829  |
|    learning_rate          | 1e-05    |
|    n_updates              | 194      |
|    policy_objective       | 1.45e+12 |
|    value_loss             | 0.213    |
----------------------------------------
Eval num_timesteps=4011000, episode_reward=0.16 +/- 0.98
Episode length: 30.05 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 4011000  |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00819  |
|    learning_rate          | 1e-05    |
|    n_updates              | 195      |
|    policy_objective       | 2.52e+11 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 172000.
Eval num_timesteps=4032000, episode_reward=0.12 +/- 0.97
Episode length: 30.00 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 4032000  |
| train/                    |          |
|    explained_variance     | 0.41     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00757  |
|    learning_rate          | 1e-05    |
|    n_updates              | 196      |
|    policy_objective       | 1.18e+11 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 173000.
Eval num_timesteps=4053000, episode_reward=0.18 +/- 0.97
Episode length: 30.04 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 4053000  |
| train/                    |          |
|    explained_variance     | 0.373    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00835  |
|    learning_rate          | 1e-05    |
|    n_updates              | 197      |
|    policy_objective       | 7.47e+16 |
|    value_loss             | 0.24     |
----------------------------------------
Ep done - 174000.
Eval num_timesteps=4074000, episode_reward=0.24 +/- 0.96
Episode length: 30.02 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.245    |
| time/                     |          |
|    total_timesteps        | 4074000  |
| train/                    |          |
|    explained_variance     | 0.392    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00948  |
|    learning_rate          | 1e-05    |
|    n_updates              | 198      |
|    policy_objective       | 7.83e+13 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 175000.
Eval num_timesteps=4095000, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.195    |
| time/                     |          |
|    total_timesteps        | 4095000  |
| train/                    |          |
|    explained_variance     | 0.414    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00773  |
|    learning_rate          | 1e-05    |
|    n_updates              | 199      |
|    policy_objective       | 1.8e+06  |
|    value_loss             | 0.228    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 210      |
|    iterations      | 200      |
|    time_elapsed    | 19465    |
|    total_timesteps | 4096000  |
---------------------------------
Ep done - 176000.
Eval num_timesteps=4116000, episode_reward=0.12 +/- 0.98
Episode length: 30.01 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 4116000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0063   |
|    learning_rate          | 1e-05    |
|    n_updates              | 200      |
|    policy_objective       | 6.43e+11 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 177000.
Eval num_timesteps=4137000, episode_reward=0.18 +/- 0.97
Episode length: 30.07 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 4137000  |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00704  |
|    learning_rate          | 1e-05    |
|    n_updates              | 202      |
|    policy_objective       | 1.11e+08 |
|    value_loss             | 0.217    |
----------------------------------------
Ep done - 178000.
Eval num_timesteps=4158000, episode_reward=0.22 +/- 0.97
Episode length: 30.02 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 4158000  |
| train/                    |          |
|    explained_variance     | 0.442    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00633  |
|    learning_rate          | 1e-05    |
|    n_updates              | 203      |
|    policy_objective       | 4.51e+10 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 179000.
Eval num_timesteps=4179000, episode_reward=0.14 +/- 0.98
Episode length: 29.96 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 4179000  |
| train/                    |          |
|    explained_variance     | 0.426    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00854  |
|    learning_rate          | 1e-05    |
|    n_updates              | 204      |
|    policy_objective       | 7.49e+15 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 180000.
Eval num_timesteps=4200000, episode_reward=0.14 +/- 0.98
Episode length: 29.93 +/- 1.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 4200000  |
| train/                    |          |
|    explained_variance     | 0.426    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00614  |
|    learning_rate          | 1e-05    |
|    n_updates              | 205      |
|    policy_objective       | 4.4e+08  |
|    value_loss             | 0.225    |
----------------------------------------
Eval num_timesteps=4221000, episode_reward=0.12 +/- 0.99
Episode length: 29.96 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 4221000  |
| train/                    |          |
|    explained_variance     | 0.405    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00633  |
|    learning_rate          | 1e-05    |
|    n_updates              | 206      |
|    policy_objective       | 8.13e+08 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 181000.
Eval num_timesteps=4242000, episode_reward=0.17 +/- 0.97
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 4242000  |
| train/                    |          |
|    explained_variance     | 0.404    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0072   |
|    learning_rate          | 1e-05    |
|    n_updates              | 207      |
|    policy_objective       | 2.49e+11 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 182000.
Eval num_timesteps=4263000, episode_reward=0.08 +/- 0.99
Episode length: 30.01 +/- 0.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 4263000  |
| train/                    |          |
|    explained_variance     | 0.395    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00798  |
|    learning_rate          | 1e-05    |
|    n_updates              | 208      |
|    policy_objective       | 1.38e+21 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 183000.
Eval num_timesteps=4284000, episode_reward=0.23 +/- 0.95
Episode length: 30.00 +/- 0.70
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 4284000  |
| train/                    |          |
|    explained_variance     | 0.403    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00547  |
|    learning_rate          | 1e-05    |
|    n_updates              | 209      |
|    policy_objective       | 5.54e+12 |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 184000.
Eval num_timesteps=4305000, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.04     |
| time/                     |          |
|    total_timesteps        | 4305000  |
| train/                    |          |
|    explained_variance     | 0.387    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00927  |
|    learning_rate          | 1e-05    |
|    n_updates              | 210      |
|    policy_objective       | 3.22e+18 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 185000.
Eval num_timesteps=4326000, episode_reward=0.16 +/- 0.97
Episode length: 30.02 +/- 0.65
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 4326000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00784  |
|    learning_rate          | 1e-05    |
|    n_updates              | 211      |
|    policy_objective       | 9.41e+12 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 186000.
Eval num_timesteps=4347000, episode_reward=0.23 +/- 0.96
Episode length: 29.96 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.235    |
| time/                     |          |
|    total_timesteps        | 4347000  |
| train/                    |          |
|    explained_variance     | 0.45     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00694  |
|    learning_rate          | 1e-05    |
|    n_updates              | 212      |
|    policy_objective       | 1.86e+08 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 187000.
Eval num_timesteps=4368000, episode_reward=0.14 +/- 0.97
Episode length: 29.96 +/- 0.98
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 4368000  |
| train/                    |          |
|    explained_variance     | 0.403    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00994  |
|    learning_rate          | 1e-05    |
|    n_updates              | 213      |
|    policy_objective       | 1.43e+09 |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 188000.
Eval num_timesteps=4389000, episode_reward=0.20 +/- 0.97
Episode length: 30.07 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 4389000  |
| train/                    |          |
|    explained_variance     | 0.389    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00538  |
|    learning_rate          | 1e-05    |
|    n_updates              | 214      |
|    policy_objective       | 9.71e+11 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 189000.
Eval num_timesteps=4410000, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 4410000  |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00599  |
|    learning_rate          | 1e-05    |
|    n_updates              | 215      |
|    policy_objective       | 2.01e+09 |
|    value_loss             | 0.225    |
----------------------------------------
Eval num_timesteps=4431000, episode_reward=0.13 +/- 0.98
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 4431000  |
| train/                    |          |
|    explained_variance     | 0.421    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00619  |
|    learning_rate          | 1e-05    |
|    n_updates              | 216      |
|    policy_objective       | 3.57e+10 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 190000.
Eval num_timesteps=4452000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.44
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 4452000  |
| train/                    |          |
|    explained_variance     | 0.411    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00886  |
|    learning_rate          | 1e-05    |
|    n_updates              | 217      |
|    policy_objective       | 1.2e+13  |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 191000.
Eval num_timesteps=4473000, episode_reward=0.25 +/- 0.96
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 4473000  |
| train/                    |          |
|    explained_variance     | 0.407    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00751  |
|    learning_rate          | 1e-05    |
|    n_updates              | 218      |
|    policy_objective       | 2.77e+08 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 192000.
Eval num_timesteps=4494000, episode_reward=0.03 +/- 0.98
Episode length: 29.89 +/- 1.03
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.025    |
| time/                     |          |
|    total_timesteps        | 4494000  |
| train/                    |          |
|    explained_variance     | 0.383    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00739  |
|    learning_rate          | 1e-05    |
|    n_updates              | 219      |
|    policy_objective       | 2.5e+12  |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 193000.
Eval num_timesteps=4515000, episode_reward=0.12 +/- 0.97
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 4515000  |
| train/                    |          |
|    explained_variance     | 0.404    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00804  |
|    learning_rate          | 1e-05    |
|    n_updates              | 220      |
|    policy_objective       | 2.26e+19 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 194000.
Eval num_timesteps=4536000, episode_reward=0.25 +/- 0.95
Episode length: 29.98 +/- 0.96
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 4536000  |
| train/                    |          |
|    explained_variance     | 0.395    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00705  |
|    learning_rate          | 1e-05    |
|    n_updates              | 221      |
|    policy_objective       | 1.5e+07  |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 195000.
Eval num_timesteps=4557000, episode_reward=0.20 +/- 0.94
Episode length: 30.07 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 4557000  |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00838  |
|    learning_rate          | 1e-05    |
|    n_updates              | 222      |
|    policy_objective       | 4.52e+07 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 196000.
Eval num_timesteps=4578000, episode_reward=0.15 +/- 0.98
Episode length: 30.00 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 4578000  |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00705  |
|    learning_rate          | 1e-05    |
|    n_updates              | 223      |
|    policy_objective       | 1.59e+09 |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 197000.
Eval num_timesteps=4599000, episode_reward=0.16 +/- 0.97
Episode length: 30.00 +/- 0.64
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 4599000  |
| train/                    |          |
|    explained_variance     | 0.419    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00608  |
|    learning_rate          | 1e-05    |
|    n_updates              | 224      |
|    policy_objective       | 1.32e+08 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 198000.
Eval num_timesteps=4620000, episode_reward=0.14 +/- 0.97
Episode length: 29.96 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 4620000  |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00849  |
|    learning_rate          | 1e-05    |
|    n_updates              | 225      |
|    policy_objective       | 1.22e+10 |
|    value_loss             | 0.228    |
----------------------------------------
Eval num_timesteps=4641000, episode_reward=0.23 +/- 0.96
Episode length: 30.02 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 4641000  |
| train/                    |          |
|    explained_variance     | 0.422    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00922  |
|    learning_rate          | 1e-05    |
|    n_updates              | 226      |
|    policy_objective       | 3.85e+08 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 199000.
Eval num_timesteps=4662000, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 4662000  |
| train/                    |          |
|    explained_variance     | 0.392    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00745  |
|    learning_rate          | 1e-05    |
|    n_updates              | 227      |
|    policy_objective       | 8.35e+11 |
|    value_loss             | 0.236    |
----------------------------------------
Ep done - 200000.
Eval num_timesteps=4683000, episode_reward=0.15 +/- 0.97
Episode length: 29.98 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 4683000  |
| train/                    |          |
|    explained_variance     | 0.421    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00677  |
|    learning_rate          | 1e-05    |
|    n_updates              | 228      |
|    policy_objective       | 1.01e+10 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 201000.
Eval num_timesteps=4704000, episode_reward=0.22 +/- 0.95
Episode length: 30.00 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 4704000  |
| train/                    |          |
|    explained_variance     | 0.41     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00645  |
|    learning_rate          | 1e-05    |
|    n_updates              | 229      |
|    policy_objective       | 1.49e+14 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 202000.
Eval num_timesteps=4725000, episode_reward=0.12 +/- 0.98
Episode length: 29.92 +/- 0.43
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 4725000  |
| train/                    |          |
|    explained_variance     | 0.407    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00903  |
|    learning_rate          | 1e-05    |
|    n_updates              | 230      |
|    policy_objective       | 9.17e+09 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 203000.
Eval num_timesteps=4746000, episode_reward=0.31 +/- 0.93
Episode length: 30.07 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.31     |
| time/                     |          |
|    total_timesteps        | 4746000  |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00649  |
|    learning_rate          | 1e-05    |
|    n_updates              | 231      |
|    policy_objective       | 5.19e+06 |
|    value_loss             | 0.234    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 63
Ep done - 204000.
Eval num_timesteps=4767000, episode_reward=0.15 +/- 0.97
Episode length: 30.00 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 4767000  |
| train/                    |          |
|    explained_variance     | 0.398    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00685  |
|    learning_rate          | 1e-05    |
|    n_updates              | 232      |
|    policy_objective       | 1.84e+07 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 205000.
Eval num_timesteps=4788000, episode_reward=0.20 +/- 0.97
Episode length: 30.03 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 4788000  |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00971  |
|    learning_rate          | 1e-05    |
|    n_updates              | 233      |
|    policy_objective       | 4.17e+10 |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 206000.
Eval num_timesteps=4809000, episode_reward=0.21 +/- 0.96
Episode length: 29.98 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.21     |
| time/                     |          |
|    total_timesteps        | 4809000  |
| train/                    |          |
|    explained_variance     | 0.428    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00809  |
|    learning_rate          | 1e-05    |
|    n_updates              | 234      |
|    policy_objective       | 1.39e+16 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 207000.
Eval num_timesteps=4830000, episode_reward=0.12 +/- 0.97
Episode length: 29.95 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 4830000  |
| train/                    |          |
|    explained_variance     | 0.431    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00742  |
|    learning_rate          | 1e-05    |
|    n_updates              | 235      |
|    policy_objective       | 2.53e+13 |
|    value_loss             | 0.223    |
----------------------------------------
Eval num_timesteps=4851000, episode_reward=0.26 +/- 0.95
Episode length: 30.05 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.26     |
| time/                     |          |
|    total_timesteps        | 4851000  |
| train/                    |          |
|    explained_variance     | 0.449    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00964  |
|    learning_rate          | 1e-05    |
|    n_updates              | 236      |
|    policy_objective       | 1.62e+09 |
|    value_loss             | 0.222    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 64
Ep done - 208000.
Eval num_timesteps=4872000, episode_reward=0.15 +/- 0.97
Episode length: 30.05 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 4872000  |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00974  |
|    learning_rate          | 1e-05    |
|    n_updates              | 237      |
|    policy_objective       | 4.93e+12 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 209000.
Eval num_timesteps=4893000, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 4893000  |
| train/                    |          |
|    explained_variance     | 0.423    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00794  |
|    learning_rate          | 1e-05    |
|    n_updates              | 238      |
|    policy_objective       | 3.1e+14  |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 210000.
Eval num_timesteps=4914000, episode_reward=-0.01 +/- 0.98
Episode length: 29.95 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | -0.015   |
| time/                     |          |
|    total_timesteps        | 4914000  |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00898  |
|    learning_rate          | 1e-05    |
|    n_updates              | 239      |
|    policy_objective       | 1.37e+10 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 211000.
Eval num_timesteps=4935000, episode_reward=0.15 +/- 0.98
Episode length: 29.98 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 4935000  |
| train/                    |          |
|    explained_variance     | 0.43     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00994  |
|    learning_rate          | 1e-05    |
|    n_updates              | 240      |
|    policy_objective       | 1.18e+10 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 212000.
Eval num_timesteps=4956000, episode_reward=0.12 +/- 0.98
Episode length: 30.05 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 4956000  |
| train/                    |          |
|    explained_variance     | 0.442    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00964  |
|    learning_rate          | 1e-05    |
|    n_updates              | 241      |
|    policy_objective       | 1.81e+10 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 213000.
Eval num_timesteps=4977000, episode_reward=0.12 +/- 0.98
Episode length: 30.06 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 4977000  |
| train/                    |          |
|    explained_variance     | 0.394    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00908  |
|    learning_rate          | 1e-05    |
|    n_updates              | 243      |
|    policy_objective       | 7.33e+06 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 214000.
Eval num_timesteps=4998000, episode_reward=0.23 +/- 0.96
Episode length: 30.04 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 4998000  |
| train/                    |          |
|    explained_variance     | 0.428    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00636  |
|    learning_rate          | 1e-05    |
|    n_updates              | 244      |
|    policy_objective       | 1.31e+06 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 215000.
Eval num_timesteps=5019000, episode_reward=0.14 +/- 0.97
Episode length: 29.99 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 5019000  |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00972  |
|    learning_rate          | 1e-05    |
|    n_updates              | 245      |
|    policy_objective       | 3.07e+14 |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 216000.
Eval num_timesteps=5040000, episode_reward=0.26 +/- 0.96
Episode length: 30.08 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.255    |
| time/                     |          |
|    total_timesteps        | 5040000  |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00857  |
|    learning_rate          | 1e-05    |
|    n_updates              | 246      |
|    policy_objective       | 5.29e+10 |
|    value_loss             | 0.225    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 65
Eval num_timesteps=5061000, episode_reward=0.38 +/- 0.91
Episode length: 30.03 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.375    |
| time/                     |          |
|    total_timesteps        | 5061000  |
| train/                    |          |
|    explained_variance     | 0.442    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00743  |
|    learning_rate          | 1e-05    |
|    n_updates              | 247      |
|    policy_objective       | 4.05e+07 |
|    value_loss             | 0.222    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.375
SELFPLAY: new best model, bumping up generation to 66
Ep done - 217000.
Eval num_timesteps=5082000, episode_reward=0.20 +/- 0.97
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.195    |
| time/                     |          |
|    total_timesteps        | 5082000  |
| train/                    |          |
|    explained_variance     | 0.403    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00605  |
|    learning_rate          | 1e-05    |
|    n_updates              | 248      |
|    policy_objective       | 8.32e+16 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 218000.
Eval num_timesteps=5103000, episode_reward=0.15 +/- 0.98
Episode length: 30.05 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 5103000  |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00765  |
|    learning_rate          | 1e-05    |
|    n_updates              | 249      |
|    policy_objective       | 1e+09    |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 219000.
Eval num_timesteps=5124000, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 5124000  |
| train/                    |          |
|    explained_variance     | 0.426    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00802  |
|    learning_rate          | 1e-05    |
|    n_updates              | 250      |
|    policy_objective       | 4.35e+10 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 220000.
Eval num_timesteps=5145000, episode_reward=0.28 +/- 0.94
Episode length: 30.04 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.285    |
| time/                     |          |
|    total_timesteps        | 5145000  |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00698  |
|    learning_rate          | 1e-05    |
|    n_updates              | 251      |
|    policy_objective       | 7.28e+14 |
|    value_loss             | 0.23     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 67
Ep done - 221000.
Eval num_timesteps=5166000, episode_reward=0.07 +/- 0.99
Episode length: 29.96 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.07     |
| time/                     |          |
|    total_timesteps        | 5166000  |
| train/                    |          |
|    explained_variance     | 0.419    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00892  |
|    learning_rate          | 1e-05    |
|    n_updates              | 252      |
|    policy_objective       | 5.59e+06 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 222000.
Eval num_timesteps=5187000, episode_reward=0.19 +/- 0.97
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 5187000  |
| train/                    |          |
|    explained_variance     | 0.435    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00796  |
|    learning_rate          | 1e-05    |
|    n_updates              | 253      |
|    policy_objective       | 1.2e+11  |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 223000.
Eval num_timesteps=5208000, episode_reward=0.23 +/- 0.96
Episode length: 30.02 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 5208000  |
| train/                    |          |
|    explained_variance     | 0.402    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00949  |
|    learning_rate          | 1e-05    |
|    n_updates              | 254      |
|    policy_objective       | 1.54e+10 |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 224000.
Eval num_timesteps=5229000, episode_reward=0.20 +/- 0.94
Episode length: 30.02 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 5229000  |
| train/                    |          |
|    explained_variance     | 0.429    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00752  |
|    learning_rate          | 1e-05    |
|    n_updates              | 255      |
|    policy_objective       | 3.99e+12 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 225000.
Eval num_timesteps=5250000, episode_reward=0.20 +/- 0.97
Episode length: 29.98 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.195    |
| time/                     |          |
|    total_timesteps        | 5250000  |
| train/                    |          |
|    explained_variance     | 0.429    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00946  |
|    learning_rate          | 1e-05    |
|    n_updates              | 256      |
|    policy_objective       | 3.51e+14 |
|    value_loss             | 0.225    |
----------------------------------------
Eval num_timesteps=5271000, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 5271000  |
| train/                    |          |
|    explained_variance     | 0.433    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00846  |
|    learning_rate          | 1e-05    |
|    n_updates              | 257      |
|    policy_objective       | 2.33e+11 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 226000.
Eval num_timesteps=5292000, episode_reward=0.34 +/- 0.93
Episode length: 30.07 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.345    |
| time/                     |          |
|    total_timesteps        | 5292000  |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00614  |
|    learning_rate          | 1e-05    |
|    n_updates              | 258      |
|    policy_objective       | 6.14e+06 |
|    value_loss             | 0.22     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.345
SELFPLAY: new best model, bumping up generation to 68
Ep done - 227000.
Eval num_timesteps=5313000, episode_reward=0.19 +/- 0.96
Episode length: 30.05 +/- 0.64
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 5313000  |
| train/                    |          |
|    explained_variance     | 0.454    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00931  |
|    learning_rate          | 1e-05    |
|    n_updates              | 259      |
|    policy_objective       | 7.98e+16 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 228000.
Eval num_timesteps=5334000, episode_reward=0.11 +/- 0.97
Episode length: 29.91 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 5334000  |
| train/                    |          |
|    explained_variance     | 0.402    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00787  |
|    learning_rate          | 1e-05    |
|    n_updates              | 260      |
|    policy_objective       | 1.65e+11 |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 229000.
Eval num_timesteps=5355000, episode_reward=0.15 +/- 0.98
Episode length: 29.93 +/- 1.23
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 5355000  |
| train/                    |          |
|    explained_variance     | 0.45     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00972  |
|    learning_rate          | 1e-05    |
|    n_updates              | 261      |
|    policy_objective       | 4.11e+08 |
|    value_loss             | 0.214    |
----------------------------------------
Ep done - 230000.
Eval num_timesteps=5376000, episode_reward=0.09 +/- 0.98
Episode length: 29.94 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 5376000  |
| train/                    |          |
|    explained_variance     | 0.391    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00878  |
|    learning_rate          | 1e-05    |
|    n_updates              | 262      |
|    policy_objective       | 7.74e+06 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 231000.
Eval num_timesteps=5397000, episode_reward=0.12 +/- 0.97
Episode length: 29.98 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 5397000  |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00929  |
|    learning_rate          | 1e-05    |
|    n_updates              | 263      |
|    policy_objective       | 1.2e+11  |
|    value_loss             | 0.238    |
----------------------------------------
Ep done - 232000.
Eval num_timesteps=5418000, episode_reward=0.18 +/- 0.98
Episode length: 29.95 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 5418000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00793  |
|    learning_rate          | 1e-05    |
|    n_updates              | 264      |
|    policy_objective       | 4.51e+11 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 233000.
Eval num_timesteps=5439000, episode_reward=0.17 +/- 0.97
Episode length: 30.07 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 5439000  |
| train/                    |          |
|    explained_variance     | 0.463    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0085   |
|    learning_rate          | 1e-05    |
|    n_updates              | 265      |
|    policy_objective       | 3.93e+10 |
|    value_loss             | 0.214    |
----------------------------------------
Ep done - 234000.
Eval num_timesteps=5460000, episode_reward=0.21 +/- 0.94
Episode length: 30.00 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 5460000  |
| train/                    |          |
|    explained_variance     | 0.41     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00933  |
|    learning_rate          | 1e-05    |
|    n_updates              | 266      |
|    policy_objective       | 1.47e+09 |
|    value_loss             | 0.235    |
----------------------------------------
Eval num_timesteps=5481000, episode_reward=0.24 +/- 0.96
Episode length: 30.04 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.24     |
| time/                     |          |
|    total_timesteps        | 5481000  |
| train/                    |          |
|    explained_variance     | 0.429    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00922  |
|    learning_rate          | 1e-05    |
|    n_updates              | 267      |
|    policy_objective       | 4.08e+10 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 235000.
Eval num_timesteps=5502000, episode_reward=0.18 +/- 0.98
Episode length: 30.00 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 5502000  |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00721  |
|    learning_rate          | 1e-05    |
|    n_updates              | 268      |
|    policy_objective       | 4.63e+13 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 236000.
Eval num_timesteps=5523000, episode_reward=0.17 +/- 0.97
Episode length: 30.02 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 5523000  |
| train/                    |          |
|    explained_variance     | 0.388    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0077   |
|    learning_rate          | 1e-05    |
|    n_updates              | 269      |
|    policy_objective       | 6.75e+10 |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 237000.
Eval num_timesteps=5544000, episode_reward=0.26 +/- 0.96
Episode length: 30.05 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.26     |
| time/                     |          |
|    total_timesteps        | 5544000  |
| train/                    |          |
|    explained_variance     | 0.409    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00502  |
|    learning_rate          | 1e-05    |
|    n_updates              | 270      |
|    policy_objective       | 5.77e+11 |
|    value_loss             | 0.228    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 69
Ep done - 238000.
Eval num_timesteps=5565000, episode_reward=0.24 +/- 0.95
Episode length: 30.04 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.24     |
| time/                     |          |
|    total_timesteps        | 5565000  |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00858  |
|    learning_rate          | 1e-05    |
|    n_updates              | 271      |
|    policy_objective       | 2.9e+16  |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 239000.
Eval num_timesteps=5586000, episode_reward=0.17 +/- 0.96
Episode length: 30.02 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 5586000  |
| train/                    |          |
|    explained_variance     | 0.407    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00636  |
|    learning_rate          | 1e-05    |
|    n_updates              | 272      |
|    policy_objective       | 7.67e+09 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 240000.
Eval num_timesteps=5607000, episode_reward=0.18 +/- 0.96
Episode length: 30.02 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 5607000  |
| train/                    |          |
|    explained_variance     | 0.435    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00889  |
|    learning_rate          | 1e-05    |
|    n_updates              | 273      |
|    policy_objective       | 1.84e+15 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 241000.
Eval num_timesteps=5628000, episode_reward=0.14 +/- 0.97
Episode length: 29.98 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 5628000  |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00839  |
|    learning_rate          | 1e-05    |
|    n_updates              | 274      |
|    policy_objective       | 8.03e+15 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 242000.
Eval num_timesteps=5649000, episode_reward=0.17 +/- 0.98
Episode length: 29.96 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 5649000  |
| train/                    |          |
|    explained_variance     | 0.429    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00589  |
|    learning_rate          | 1e-05    |
|    n_updates              | 275      |
|    policy_objective       | 6.77e+05 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 243000.
Eval num_timesteps=5670000, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 5670000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00666  |
|    learning_rate          | 1e-05    |
|    n_updates              | 276      |
|    policy_objective       | 1.07e+10 |
|    value_loss             | 0.227    |
----------------------------------------
Eval num_timesteps=5691000, episode_reward=0.09 +/- 0.98
Episode length: 29.95 +/- 1.00
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 5691000  |
| train/                    |          |
|    explained_variance     | 0.455    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00707  |
|    learning_rate          | 1e-05    |
|    n_updates              | 277      |
|    policy_objective       | 2.93e+09 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 244000.
Eval num_timesteps=5712000, episode_reward=0.08 +/- 0.98
Episode length: 29.93 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 5712000  |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00873  |
|    learning_rate          | 1e-05    |
|    n_updates              | 278      |
|    policy_objective       | 1.05e+08 |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 245000.
Eval num_timesteps=5733000, episode_reward=0.26 +/- 0.95
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.255    |
| time/                     |          |
|    total_timesteps        | 5733000  |
| train/                    |          |
|    explained_variance     | 0.43     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00908  |
|    learning_rate          | 1e-05    |
|    n_updates              | 279      |
|    policy_objective       | 1.17e+08 |
|    value_loss             | 0.22     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 70
Ep done - 246000.
Eval num_timesteps=5754000, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 5754000  |
| train/                    |          |
|    explained_variance     | 0.436    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00886  |
|    learning_rate          | 1e-05    |
|    n_updates              | 280      |
|    policy_objective       | 1.95e+14 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 247000.
Eval num_timesteps=5775000, episode_reward=0.28 +/- 0.95
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.275    |
| time/                     |          |
|    total_timesteps        | 5775000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0098   |
|    learning_rate          | 1e-05    |
|    n_updates              | 281      |
|    policy_objective       | 3.31e+12 |
|    value_loss             | 0.224    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 71
Ep done - 248000.
Eval num_timesteps=5796000, episode_reward=0.23 +/- 0.96
Episode length: 30.00 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 5796000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00667  |
|    learning_rate          | 1e-05    |
|    n_updates              | 283      |
|    policy_objective       | 1.16e+13 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 249000.
Eval num_timesteps=5817000, episode_reward=0.25 +/- 0.96
Episode length: 30.04 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 5817000  |
| train/                    |          |
|    explained_variance     | 0.433    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0098   |
|    learning_rate          | 1e-05    |
|    n_updates              | 284      |
|    policy_objective       | 1.3e+17  |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 250000.
Eval num_timesteps=5838000, episode_reward=0.26 +/- 0.96
Episode length: 30.05 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.255    |
| time/                     |          |
|    total_timesteps        | 5838000  |
| train/                    |          |
|    explained_variance     | 0.455    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00793  |
|    learning_rate          | 1e-05    |
|    n_updates              | 285      |
|    policy_objective       | 3.91e+10 |
|    value_loss             | 0.221    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 72
Ep done - 251000.
Eval num_timesteps=5859000, episode_reward=0.03 +/- 0.98
Episode length: 29.95 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.03     |
| time/                     |          |
|    total_timesteps        | 5859000  |
| train/                    |          |
|    explained_variance     | 0.426    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00915  |
|    learning_rate          | 1e-05    |
|    n_updates              | 286      |
|    policy_objective       | 3.2e+12  |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 252000.
Eval num_timesteps=5880000, episode_reward=0.21 +/- 0.96
Episode length: 30.01 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 5880000  |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00819  |
|    learning_rate          | 1e-05    |
|    n_updates              | 287      |
|    policy_objective       | 3.1e+07  |
|    value_loss             | 0.229    |
----------------------------------------
Eval num_timesteps=5901000, episode_reward=0.18 +/- 0.97
Episode length: 29.98 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 5901000  |
| train/                    |          |
|    explained_variance     | 0.388    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00529  |
|    learning_rate          | 1e-05    |
|    n_updates              | 288      |
|    policy_objective       | 1.23e+12 |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 253000.
Eval num_timesteps=5922000, episode_reward=0.26 +/- 0.94
Episode length: 30.02 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.255    |
| time/                     |          |
|    total_timesteps        | 5922000  |
| train/                    |          |
|    explained_variance     | 0.466    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00641  |
|    learning_rate          | 1e-05    |
|    n_updates              | 289      |
|    policy_objective       | 1.84e+03 |
|    value_loss             | 0.209    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 73
Ep done - 254000.
Eval num_timesteps=5943000, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 5943000  |
| train/                    |          |
|    explained_variance     | 0.438    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00613  |
|    learning_rate          | 1e-05    |
|    n_updates              | 290      |
|    policy_objective       | 9.05e+15 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 255000.
Eval num_timesteps=5964000, episode_reward=0.17 +/- 0.96
Episode length: 30.03 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 5964000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00751  |
|    learning_rate          | 1e-05    |
|    n_updates              | 291      |
|    policy_objective       | 3.3e+10  |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 256000.
Eval num_timesteps=5985000, episode_reward=0.24 +/- 0.97
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.245    |
| time/                     |          |
|    total_timesteps        | 5985000  |
| train/                    |          |
|    explained_variance     | 0.432    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00811  |
|    learning_rate          | 1e-05    |
|    n_updates              | 292      |
|    policy_objective       | 2.15e+04 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 257000.
Eval num_timesteps=6006000, episode_reward=0.24 +/- 0.96
Episode length: 30.00 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.245    |
| time/                     |          |
|    total_timesteps        | 6006000  |
| train/                    |          |
|    explained_variance     | 0.414    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00737  |
|    learning_rate          | 1e-05    |
|    n_updates              | 293      |
|    policy_objective       | 5.27e+06 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 258000.
Eval num_timesteps=6027000, episode_reward=0.26 +/- 0.94
Episode length: 29.91 +/- 1.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.26     |
| time/                     |          |
|    total_timesteps        | 6027000  |
| train/                    |          |
|    explained_variance     | 0.436    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00736  |
|    learning_rate          | 1e-05    |
|    n_updates              | 294      |
|    policy_objective       | 1.42e+09 |
|    value_loss             | 0.217    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 74
Ep done - 259000.
Eval num_timesteps=6048000, episode_reward=0.24 +/- 0.95
Episode length: 30.02 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.24     |
| time/                     |          |
|    total_timesteps        | 6048000  |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00733  |
|    learning_rate          | 1e-05    |
|    n_updates              | 295      |
|    policy_objective       | 1.34e+12 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 260000.
Eval num_timesteps=6069000, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.195    |
| time/                     |          |
|    total_timesteps        | 6069000  |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.006    |
|    learning_rate          | 1e-05    |
|    n_updates              | 296      |
|    policy_objective       | 6.85e+11 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 261000.
Eval num_timesteps=6090000, episode_reward=0.30 +/- 0.94
Episode length: 30.04 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.3      |
| time/                     |          |
|    total_timesteps        | 6090000  |
| train/                    |          |
|    explained_variance     | 0.437    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00917  |
|    learning_rate          | 1e-05    |
|    n_updates              | 297      |
|    policy_objective       | 6.77e+08 |
|    value_loss             | 0.219    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 75
Eval num_timesteps=6111000, episode_reward=0.23 +/- 0.94
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 6111000  |
| train/                    |          |
|    explained_variance     | 0.428    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00801  |
|    learning_rate          | 1e-05    |
|    n_updates              | 298      |
|    policy_objective       | 3.38e+11 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 262000.
Eval num_timesteps=6132000, episode_reward=0.22 +/- 0.97
Episode length: 30.00 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 6132000  |
| train/                    |          |
|    explained_variance     | 0.454    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.006    |
|    learning_rate          | 1e-05    |
|    n_updates              | 299      |
|    policy_objective       | 5.86e+06 |
|    value_loss             | 0.217    |
----------------------------------------
Ep done - 263000.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 300      |
|    time_elapsed    | 28749    |
|    total_timesteps | 6144000  |
---------------------------------
Eval num_timesteps=6153000, episode_reward=0.15 +/- 0.98
Episode length: 30.03 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 6153000  |
| train/                    |          |
|    explained_variance     | 0.402    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00489  |
|    learning_rate          | 1e-05    |
|    n_updates              | 300      |
|    policy_objective       | 2.02e+12 |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 264000.
Eval num_timesteps=6174000, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.06     |
| time/                     |          |
|    total_timesteps        | 6174000  |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0098   |
|    learning_rate          | 1e-05    |
|    n_updates              | 301      |
|    policy_objective       | 6.9e+11  |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 265000.
Eval num_timesteps=6195000, episode_reward=0.22 +/- 0.96
Episode length: 29.98 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 6195000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.009    |
|    learning_rate          | 1e-05    |
|    n_updates              | 302      |
|    policy_objective       | 1.48e+07 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 266000.
Eval num_timesteps=6216000, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 6216000  |
| train/                    |          |
|    explained_variance     | 0.456    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00677  |
|    learning_rate          | 1e-05    |
|    n_updates              | 303      |
|    policy_objective       | 6.79e+09 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 267000.
Eval num_timesteps=6237000, episode_reward=0.23 +/- 0.94
Episode length: 30.04 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 6237000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00627  |
|    learning_rate          | 1e-05    |
|    n_updates              | 304      |
|    policy_objective       | 3.52e+07 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 268000.
Eval num_timesteps=6258000, episode_reward=0.27 +/- 0.94
Episode length: 30.02 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.265    |
| time/                     |          |
|    total_timesteps        | 6258000  |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00775  |
|    learning_rate          | 1e-05    |
|    n_updates              | 305      |
|    policy_objective       | 8.91e+12 |
|    value_loss             | 0.23     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 76
Ep done - 269000.
Eval num_timesteps=6279000, episode_reward=0.17 +/- 0.99
Episode length: 30.02 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 6279000  |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00746  |
|    learning_rate          | 1e-05    |
|    n_updates              | 306      |
|    policy_objective       | 2.32e+18 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 270000.
Eval num_timesteps=6300000, episode_reward=0.21 +/- 0.96
Episode length: 30.03 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 6300000  |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00873  |
|    learning_rate          | 1e-05    |
|    n_updates              | 307      |
|    policy_objective       | 1.39e+10 |
|    value_loss             | 0.228    |
----------------------------------------
Eval num_timesteps=6321000, episode_reward=0.21 +/- 0.96
Episode length: 30.03 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 6321000  |
| train/                    |          |
|    explained_variance     | 0.401    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00831  |
|    learning_rate          | 1e-05    |
|    n_updates              | 308      |
|    policy_objective       | 2.2e+14  |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 271000.
Eval num_timesteps=6342000, episode_reward=0.24 +/- 0.95
Episode length: 30.04 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.24     |
| time/                     |          |
|    total_timesteps        | 6342000  |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00941  |
|    learning_rate          | 1e-05    |
|    n_updates              | 309      |
|    policy_objective       | 7.03e+09 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 272000.
Eval num_timesteps=6363000, episode_reward=0.28 +/- 0.94
Episode length: 29.95 +/- 0.85
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.275    |
| time/                     |          |
|    total_timesteps        | 6363000  |
| train/                    |          |
|    explained_variance     | 0.426    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00778  |
|    learning_rate          | 1e-05    |
|    n_updates              | 310      |
|    policy_objective       | 4.74e+12 |
|    value_loss             | 0.221    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 77
Ep done - 273000.
Eval num_timesteps=6384000, episode_reward=0.21 +/- 0.95
Episode length: 30.04 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 6384000  |
| train/                    |          |
|    explained_variance     | 0.433    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00823  |
|    learning_rate          | 1e-05    |
|    n_updates              | 311      |
|    policy_objective       | 8.15e+12 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 274000.
Eval num_timesteps=6405000, episode_reward=0.27 +/- 0.95
Episode length: 29.96 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.27     |
| time/                     |          |
|    total_timesteps        | 6405000  |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00898  |
|    learning_rate          | 1e-05    |
|    n_updates              | 312      |
|    policy_objective       | 6.36e+17 |
|    value_loss             | 0.232    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 78
Ep done - 275000.
Eval num_timesteps=6426000, episode_reward=0.19 +/- 0.97
Episode length: 29.95 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 6426000  |
| train/                    |          |
|    explained_variance     | 0.421    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00986  |
|    learning_rate          | 1e-05    |
|    n_updates              | 313      |
|    policy_objective       | 1.62e+09 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 276000.
Eval num_timesteps=6447000, episode_reward=0.24 +/- 0.95
Episode length: 30.08 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.245    |
| time/                     |          |
|    total_timesteps        | 6447000  |
| train/                    |          |
|    explained_variance     | 0.452    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00563  |
|    learning_rate          | 1e-05    |
|    n_updates              | 314      |
|    policy_objective       | 1.83e+07 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 277000.
Eval num_timesteps=6468000, episode_reward=0.23 +/- 0.95
Episode length: 29.98 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 6468000  |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0093   |
|    learning_rate          | 1e-05    |
|    n_updates              | 315      |
|    policy_objective       | 2.32e+13 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 278000.
Eval num_timesteps=6489000, episode_reward=0.28 +/- 0.95
Episode length: 29.89 +/- 1.62
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.285    |
| time/                     |          |
|    total_timesteps        | 6489000  |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00811  |
|    learning_rate          | 1e-05    |
|    n_updates              | 316      |
|    policy_objective       | 1.07e+10 |
|    value_loss             | 0.225    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 79
Ep done - 279000.
Eval num_timesteps=6510000, episode_reward=0.29 +/- 0.93
Episode length: 30.02 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.29     |
| time/                     |          |
|    total_timesteps        | 6510000  |
| train/                    |          |
|    explained_variance     | 0.444    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00796  |
|    learning_rate          | 1e-05    |
|    n_updates              | 317      |
|    policy_objective       | 6.36e+15 |
|    value_loss             | 0.218    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.29
SELFPLAY: new best model, bumping up generation to 80
Eval num_timesteps=6531000, episode_reward=0.15 +/- 0.97
Episode length: 29.98 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 6531000  |
| train/                    |          |
|    explained_variance     | 0.458    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00669  |
|    learning_rate          | 1e-05    |
|    n_updates              | 318      |
|    policy_objective       | 1.11e+12 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 280000.
Eval num_timesteps=6552000, episode_reward=0.17 +/- 0.97
Episode length: 29.98 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 6552000  |
| train/                    |          |
|    explained_variance     | 0.455    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0086   |
|    learning_rate          | 1e-05    |
|    n_updates              | 319      |
|    policy_objective       | 6.27e+08 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 281000.
Eval num_timesteps=6573000, episode_reward=0.25 +/- 0.95
Episode length: 30.05 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 6573000  |
| train/                    |          |
|    explained_variance     | 0.42     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00789  |
|    learning_rate          | 1e-05    |
|    n_updates              | 320      |
|    policy_objective       | 2.42e+09 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 282000.
Eval num_timesteps=6594000, episode_reward=0.20 +/- 0.96
Episode length: 29.95 +/- 0.98
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.195    |
| time/                     |          |
|    total_timesteps        | 6594000  |
| train/                    |          |
|    explained_variance     | 0.399    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00695  |
|    learning_rate          | 1e-05    |
|    n_updates              | 321      |
|    policy_objective       | 4.33e+04 |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 283000.
Eval num_timesteps=6615000, episode_reward=0.18 +/- 0.97
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 6615000  |
| train/                    |          |
|    explained_variance     | 0.389    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00626  |
|    learning_rate          | 1e-05    |
|    n_updates              | 322      |
|    policy_objective       | 1.25e+09 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 284000.
Eval num_timesteps=6636000, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.105    |
| time/                     |          |
|    total_timesteps        | 6636000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00725  |
|    learning_rate          | 1e-05    |
|    n_updates              | 324      |
|    policy_objective       | 2.59e+11 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 285000.
Eval num_timesteps=6657000, episode_reward=0.09 +/- 0.97
Episode length: 29.94 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.085    |
| time/                     |          |
|    total_timesteps        | 6657000  |
| train/                    |          |
|    explained_variance     | 0.424    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00766  |
|    learning_rate          | 1e-05    |
|    n_updates              | 325      |
|    policy_objective       | 2.96e+07 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 286000.
Eval num_timesteps=6678000, episode_reward=0.12 +/- 0.99
Episode length: 29.96 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 6678000  |
| train/                    |          |
|    explained_variance     | 0.422    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00759  |
|    learning_rate          | 1e-05    |
|    n_updates              | 326      |
|    policy_objective       | 4.82e+08 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 287000.
Eval num_timesteps=6699000, episode_reward=0.18 +/- 0.96
Episode length: 30.03 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 6699000  |
| train/                    |          |
|    explained_variance     | 0.384    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00741  |
|    learning_rate          | 1e-05    |
|    n_updates              | 327      |
|    policy_objective       | 8.78e+14 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 288000.
Eval num_timesteps=6720000, episode_reward=0.18 +/- 0.98
Episode length: 30.04 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 6720000  |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0088   |
|    learning_rate          | 1e-05    |
|    n_updates              | 328      |
|    policy_objective       | 1.4e+14  |
|    value_loss             | 0.226    |
----------------------------------------
Eval num_timesteps=6741000, episode_reward=0.30 +/- 0.94
Episode length: 30.04 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.3      |
| time/                     |          |
|    total_timesteps        | 6741000  |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00667  |
|    learning_rate          | 1e-05    |
|    n_updates              | 329      |
|    policy_objective       | 1.39e+10 |
|    value_loss             | 0.228    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 81
Ep done - 289000.
Eval num_timesteps=6762000, episode_reward=0.18 +/- 0.97
Episode length: 29.95 +/- 1.02
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 6762000  |
| train/                    |          |
|    explained_variance     | 0.453    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00791  |
|    learning_rate          | 1e-05    |
|    n_updates              | 330      |
|    policy_objective       | 4.13e+05 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 290000.
Eval num_timesteps=6783000, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 6783000  |
| train/                    |          |
|    explained_variance     | 0.457    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0058   |
|    learning_rate          | 1e-05    |
|    n_updates              | 331      |
|    policy_objective       | 1.45e+08 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 291000.
Eval num_timesteps=6804000, episode_reward=0.13 +/- 0.98
Episode length: 30.00 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 6804000  |
| train/                    |          |
|    explained_variance     | 0.388    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00964  |
|    learning_rate          | 1e-05    |
|    n_updates              | 332      |
|    policy_objective       | 4.31e+09 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 292000.
Eval num_timesteps=6825000, episode_reward=0.20 +/- 0.96
Episode length: 30.05 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 6825000  |
| train/                    |          |
|    explained_variance     | 0.443    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00745  |
|    learning_rate          | 1e-05    |
|    n_updates              | 333      |
|    policy_objective       | 4.05e+07 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 293000.
Eval num_timesteps=6846000, episode_reward=0.31 +/- 0.94
Episode length: 30.07 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.31     |
| time/                     |          |
|    total_timesteps        | 6846000  |
| train/                    |          |
|    explained_variance     | 0.444    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00681  |
|    learning_rate          | 1e-05    |
|    n_updates              | 334      |
|    policy_objective       | 3.64e+09 |
|    value_loss             | 0.225    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 82
Ep done - 294000.
Eval num_timesteps=6867000, episode_reward=0.17 +/- 0.97
Episode length: 29.93 +/- 0.87
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 6867000  |
| train/                    |          |
|    explained_variance     | 0.419    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00927  |
|    learning_rate          | 1e-05    |
|    n_updates              | 335      |
|    policy_objective       | 1.85e+10 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 295000.
Eval num_timesteps=6888000, episode_reward=0.13 +/- 0.98
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 6888000  |
| train/                    |          |
|    explained_variance     | 0.422    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00989  |
|    learning_rate          | 1e-05    |
|    n_updates              | 336      |
|    policy_objective       | 2.4e+13  |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 296000.
Eval num_timesteps=6909000, episode_reward=0.23 +/- 0.96
Episode length: 29.95 +/- 1.39
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 6909000  |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00823  |
|    learning_rate          | 1e-05    |
|    n_updates              | 337      |
|    policy_objective       | 6.97e+10 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 297000.
Eval num_timesteps=6930000, episode_reward=0.23 +/- 0.96
Episode length: 29.98 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 6930000  |
| train/                    |          |
|    explained_variance     | 0.442    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00682  |
|    learning_rate          | 1e-05    |
|    n_updates              | 338      |
|    policy_objective       | 8.81e+15 |
|    value_loss             | 0.22     |
----------------------------------------
Eval num_timesteps=6951000, episode_reward=0.36 +/- 0.91
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.36     |
| time/                     |          |
|    total_timesteps        | 6951000  |
| train/                    |          |
|    explained_variance     | 0.396    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00762  |
|    learning_rate          | 1e-05    |
|    n_updates              | 339      |
|    policy_objective       | 1.32e+16 |
|    value_loss             | 0.235    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.36
SELFPLAY: new best model, bumping up generation to 83
Ep done - 298000.
Eval num_timesteps=6972000, episode_reward=0.23 +/- 0.96
Episode length: 30.03 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 6972000  |
| train/                    |          |
|    explained_variance     | 0.397    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00738  |
|    learning_rate          | 1e-05    |
|    n_updates              | 340      |
|    policy_objective       | 1.18e+09 |
|    value_loss             | 0.229    |
----------------------------------------
Ep done - 299000.
Eval num_timesteps=6993000, episode_reward=0.29 +/- 0.95
Episode length: 29.93 +/- 1.36
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.295    |
| time/                     |          |
|    total_timesteps        | 6993000  |
| train/                    |          |
|    explained_variance     | 0.439    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00861  |
|    learning_rate          | 1e-05    |
|    n_updates              | 341      |
|    policy_objective       | 6.28e+07 |
|    value_loss             | 0.218    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.295
SELFPLAY: new best model, bumping up generation to 84
Ep done - 300000.
Eval num_timesteps=7014000, episode_reward=0.23 +/- 0.96
Episode length: 29.91 +/- 1.26
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 7014000  |
| train/                    |          |
|    explained_variance     | 0.455    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00895  |
|    learning_rate          | 1e-05    |
|    n_updates              | 342      |
|    policy_objective       | 1.04e+10 |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 301000.
Eval num_timesteps=7035000, episode_reward=0.34 +/- 0.93
Episode length: 30.05 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.345    |
| time/                     |          |
|    total_timesteps        | 7035000  |
| train/                    |          |
|    explained_variance     | 0.431    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00743  |
|    learning_rate          | 1e-05    |
|    n_updates              | 343      |
|    policy_objective       | 1.89e+08 |
|    value_loss             | 0.229    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.345
SELFPLAY: new best model, bumping up generation to 85
Ep done - 302000.
Eval num_timesteps=7056000, episode_reward=0.22 +/- 0.96
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 7056000  |
| train/                    |          |
|    explained_variance     | 0.451    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00959  |
|    learning_rate          | 1e-05    |
|    n_updates              | 344      |
|    policy_objective       | 2.94e+11 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 303000.
Eval num_timesteps=7077000, episode_reward=0.21 +/- 0.96
Episode length: 30.00 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 7077000  |
| train/                    |          |
|    explained_variance     | 0.462    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00659  |
|    learning_rate          | 1e-05    |
|    n_updates              | 345      |
|    policy_objective       | 7.41e+07 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 304000.
Eval num_timesteps=7098000, episode_reward=0.20 +/- 0.97
Episode length: 30.05 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 7098000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00599  |
|    learning_rate          | 1e-05    |
|    n_updates              | 346      |
|    policy_objective       | 2.27e+11 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 305000.
Eval num_timesteps=7119000, episode_reward=0.17 +/- 0.97
Episode length: 30.03 +/- 0.41
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 7119000  |
| train/                    |          |
|    explained_variance     | 0.387    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00815  |
|    learning_rate          | 1e-05    |
|    n_updates              | 347      |
|    policy_objective       | 6.74e+07 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 306000.
Eval num_timesteps=7140000, episode_reward=0.27 +/- 0.94
Episode length: 30.04 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.27     |
| time/                     |          |
|    total_timesteps        | 7140000  |
| train/                    |          |
|    explained_variance     | 0.449    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00674  |
|    learning_rate          | 1e-05    |
|    n_updates              | 348      |
|    policy_objective       | 1.39e+04 |
|    value_loss             | 0.216    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 86
Eval num_timesteps=7161000, episode_reward=0.30 +/- 0.94
Episode length: 30.07 +/- 0.41
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.3      |
| time/                     |          |
|    total_timesteps        | 7161000  |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00823  |
|    learning_rate          | 1e-05    |
|    n_updates              | 349      |
|    policy_objective       | 5.88e+07 |
|    value_loss             | 0.226    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 87
Ep done - 307000.
Eval num_timesteps=7182000, episode_reward=0.18 +/- 0.97
Episode length: 29.96 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 7182000  |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00988  |
|    learning_rate          | 1e-05    |
|    n_updates              | 350      |
|    policy_objective       | 4.73e+13 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 308000.
Eval num_timesteps=7203000, episode_reward=0.34 +/- 0.92
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.34     |
| time/                     |          |
|    total_timesteps        | 7203000  |
| train/                    |          |
|    explained_variance     | 0.433    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0052   |
|    learning_rate          | 1e-05    |
|    n_updates              | 351      |
|    policy_objective       | 4.1e+15  |
|    value_loss             | 0.226    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.34
SELFPLAY: new best model, bumping up generation to 88
Ep done - 309000.
Eval num_timesteps=7224000, episode_reward=0.23 +/- 0.95
Episode length: 30.06 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.235    |
| time/                     |          |
|    total_timesteps        | 7224000  |
| train/                    |          |
|    explained_variance     | 0.423    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00723  |
|    learning_rate          | 1e-05    |
|    n_updates              | 352      |
|    policy_objective       | 7.76e+11 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 310000.
Eval num_timesteps=7245000, episode_reward=0.14 +/- 0.97
Episode length: 29.98 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 7245000  |
| train/                    |          |
|    explained_variance     | 0.409    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00858  |
|    learning_rate          | 1e-05    |
|    n_updates              | 353      |
|    policy_objective       | 5.16e+15 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 311000.
Eval num_timesteps=7266000, episode_reward=0.15 +/- 0.98
Episode length: 29.98 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 7266000  |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00774  |
|    learning_rate          | 1e-05    |
|    n_updates              | 354      |
|    policy_objective       | 3.26e+09 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 312000.
Eval num_timesteps=7287000, episode_reward=0.28 +/- 0.95
Episode length: 30.02 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.275    |
| time/                     |          |
|    total_timesteps        | 7287000  |
| train/                    |          |
|    explained_variance     | 0.402    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00797  |
|    learning_rate          | 1e-05    |
|    n_updates              | 355      |
|    policy_objective       | 7.71e+11 |
|    value_loss             | 0.232    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 89
Ep done - 313000.
Eval num_timesteps=7308000, episode_reward=0.17 +/- 0.96
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 7308000  |
| train/                    |          |
|    explained_variance     | 0.436    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00997  |
|    learning_rate          | 1e-05    |
|    n_updates              | 356      |
|    policy_objective       | 3.94e+11 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 314000.
Eval num_timesteps=7329000, episode_reward=0.13 +/- 0.98
Episode length: 29.93 +/- 1.00
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 7329000  |
| train/                    |          |
|    explained_variance     | 0.449    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00645  |
|    learning_rate          | 1e-05    |
|    n_updates              | 357      |
|    policy_objective       | 4.41e+09 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 315000.
Eval num_timesteps=7350000, episode_reward=0.21 +/- 0.96
Episode length: 29.99 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 7350000  |
| train/                    |          |
|    explained_variance     | 0.444    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0073   |
|    learning_rate          | 1e-05    |
|    n_updates              | 358      |
|    policy_objective       | 7.27e+07 |
|    value_loss             | 0.222    |
----------------------------------------
Eval num_timesteps=7371000, episode_reward=0.28 +/- 0.95
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.28     |
| time/                     |          |
|    total_timesteps        | 7371000  |
| train/                    |          |
|    explained_variance     | 0.424    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00737  |
|    learning_rate          | 1e-05    |
|    n_updates              | 359      |
|    policy_objective       | 7.88e+09 |
|    value_loss             | 0.225    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.28
SELFPLAY: new best model, bumping up generation to 90
Ep done - 316000.
Eval num_timesteps=7392000, episode_reward=0.18 +/- 0.97
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 7392000  |
| train/                    |          |
|    explained_variance     | 0.45     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00901  |
|    learning_rate          | 1e-05    |
|    n_updates              | 360      |
|    policy_objective       | 3.3e+09  |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 317000.
Eval num_timesteps=7413000, episode_reward=0.20 +/- 0.96
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 7413000  |
| train/                    |          |
|    explained_variance     | 0.478    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00968  |
|    learning_rate          | 1e-05    |
|    n_updates              | 361      |
|    policy_objective       | 1.07e+11 |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 318000.
Eval num_timesteps=7434000, episode_reward=0.20 +/- 0.97
Episode length: 30.01 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 7434000  |
| train/                    |          |
|    explained_variance     | 0.438    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00924  |
|    learning_rate          | 1e-05    |
|    n_updates              | 362      |
|    policy_objective       | 9.46e+12 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 319000.
Eval num_timesteps=7455000, episode_reward=0.22 +/- 0.96
Episode length: 30.01 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 7455000  |
| train/                    |          |
|    explained_variance     | 0.444    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00928  |
|    learning_rate          | 1e-05    |
|    n_updates              | 364      |
|    policy_objective       | 8.75e+09 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 320000.
Eval num_timesteps=7476000, episode_reward=0.25 +/- 0.95
Episode length: 30.04 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 7476000  |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00843  |
|    learning_rate          | 1e-05    |
|    n_updates              | 365      |
|    policy_objective       | 4.53e+08 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 321000.
Eval num_timesteps=7497000, episode_reward=0.15 +/- 0.98
Episode length: 29.99 +/- 0.45
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 7497000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00928  |
|    learning_rate          | 1e-05    |
|    n_updates              | 366      |
|    policy_objective       | 2.07e+12 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 322000.
Eval num_timesteps=7518000, episode_reward=0.20 +/- 0.96
Episode length: 30.00 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 7518000  |
| train/                    |          |
|    explained_variance     | 0.479    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00749  |
|    learning_rate          | 1e-05    |
|    n_updates              | 367      |
|    policy_objective       | 4.52e+13 |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 323000.
Eval num_timesteps=7539000, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.195    |
| time/                     |          |
|    total_timesteps        | 7539000  |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00684  |
|    learning_rate          | 1e-05    |
|    n_updates              | 368      |
|    policy_objective       | 4.27e+04 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 324000.
Eval num_timesteps=7560000, episode_reward=0.14 +/- 0.97
Episode length: 29.96 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 7560000  |
| train/                    |          |
|    explained_variance     | 0.428    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00767  |
|    learning_rate          | 1e-05    |
|    n_updates              | 369      |
|    policy_objective       | 3.42e+13 |
|    value_loss             | 0.228    |
----------------------------------------
Eval num_timesteps=7581000, episode_reward=0.30 +/- 0.94
Episode length: 30.08 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.3      |
| time/                     |          |
|    total_timesteps        | 7581000  |
| train/                    |          |
|    explained_variance     | 0.439    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00617  |
|    learning_rate          | 1e-05    |
|    n_updates              | 370      |
|    policy_objective       | 2.54e+14 |
|    value_loss             | 0.222    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.3
SELFPLAY: new best model, bumping up generation to 91
Ep done - 325000.
Eval num_timesteps=7602000, episode_reward=0.20 +/- 0.97
Episode length: 30.01 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 7602000  |
| train/                    |          |
|    explained_variance     | 0.445    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00897  |
|    learning_rate          | 1e-05    |
|    n_updates              | 371      |
|    policy_objective       | 4.57e+09 |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 326000.
Eval num_timesteps=7623000, episode_reward=0.23 +/- 0.94
Episode length: 30.00 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 7623000  |
| train/                    |          |
|    explained_variance     | 0.464    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00982  |
|    learning_rate          | 1e-05    |
|    n_updates              | 372      |
|    policy_objective       | 1.39e+09 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 327000.
Eval num_timesteps=7644000, episode_reward=0.23 +/- 0.96
Episode length: 30.02 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.235    |
| time/                     |          |
|    total_timesteps        | 7644000  |
| train/                    |          |
|    explained_variance     | 0.414    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00775  |
|    learning_rate          | 1e-05    |
|    n_updates              | 373      |
|    policy_objective       | 3.73e+08 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 328000.
Eval num_timesteps=7665000, episode_reward=0.17 +/- 0.98
Episode length: 30.00 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 7665000  |
| train/                    |          |
|    explained_variance     | 0.44     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00737  |
|    learning_rate          | 1e-05    |
|    n_updates              | 374      |
|    policy_objective       | 2.85e+08 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 329000.
Eval num_timesteps=7686000, episode_reward=0.20 +/- 0.97
Episode length: 30.00 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 7686000  |
| train/                    |          |
|    explained_variance     | 0.407    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00903  |
|    learning_rate          | 1e-05    |
|    n_updates              | 375      |
|    policy_objective       | 5.25e+12 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 330000.
Eval num_timesteps=7707000, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 7707000  |
| train/                    |          |
|    explained_variance     | 0.445    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00744  |
|    learning_rate          | 1e-05    |
|    n_updates              | 376      |
|    policy_objective       | 1.29e+11 |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 331000.
Eval num_timesteps=7728000, episode_reward=0.19 +/- 0.98
Episode length: 29.99 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 7728000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00684  |
|    learning_rate          | 1e-05    |
|    n_updates              | 377      |
|    policy_objective       | 8.38e+06 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 332000.
Eval num_timesteps=7749000, episode_reward=0.20 +/- 0.96
Episode length: 30.05 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 7749000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0062   |
|    learning_rate          | 1e-05    |
|    n_updates              | 378      |
|    policy_objective       | 9.04e+10 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 333000.
Eval num_timesteps=7770000, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 7770000  |
| train/                    |          |
|    explained_variance     | 0.454    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00544  |
|    learning_rate          | 1e-05    |
|    n_updates              | 379      |
|    policy_objective       | 9e+10    |
|    value_loss             | 0.215    |
----------------------------------------
Eval num_timesteps=7791000, episode_reward=0.21 +/- 0.96
Episode length: 30.03 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 7791000  |
| train/                    |          |
|    explained_variance     | 0.412    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00788  |
|    learning_rate          | 1e-05    |
|    n_updates              | 380      |
|    policy_objective       | 1.73e+08 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 334000.
Eval num_timesteps=7812000, episode_reward=0.17 +/- 0.97
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 7812000  |
| train/                    |          |
|    explained_variance     | 0.428    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00611  |
|    learning_rate          | 1e-05    |
|    n_updates              | 381      |
|    policy_objective       | 2.11e+06 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 335000.
Eval num_timesteps=7833000, episode_reward=0.18 +/- 0.97
Episode length: 30.00 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 7833000  |
| train/                    |          |
|    explained_variance     | 0.457    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0062   |
|    learning_rate          | 1e-05    |
|    n_updates              | 382      |
|    policy_objective       | 2.92e+09 |
|    value_loss             | 0.217    |
----------------------------------------
Ep done - 336000.
Eval num_timesteps=7854000, episode_reward=0.17 +/- 0.98
Episode length: 29.98 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 7854000  |
| train/                    |          |
|    explained_variance     | 0.461    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00875  |
|    learning_rate          | 1e-05    |
|    n_updates              | 383      |
|    policy_objective       | 1.95e+11 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 337000.
Eval num_timesteps=7875000, episode_reward=0.23 +/- 0.96
Episode length: 29.99 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 7875000  |
| train/                    |          |
|    explained_variance     | 0.436    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00793  |
|    learning_rate          | 1e-05    |
|    n_updates              | 384      |
|    policy_objective       | 1.39e+11 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 338000.
Eval num_timesteps=7896000, episode_reward=0.29 +/- 0.95
Episode length: 30.00 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.29     |
| time/                     |          |
|    total_timesteps        | 7896000  |
| train/                    |          |
|    explained_variance     | 0.432    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0076   |
|    learning_rate          | 1e-05    |
|    n_updates              | 385      |
|    policy_objective       | 1.43e+18 |
|    value_loss             | 0.221    |
----------------------------------------
slurmstepd-n16: error: *** JOB 698 ON n16 CANCELLED AT 2024-06-16T06:30:16 ***
