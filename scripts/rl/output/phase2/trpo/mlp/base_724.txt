CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
CUDA available: True
seed: 13 
num_timesteps: 30000000 
eval_freq: 61500 
eval_episoded: 200 
best_threshold: 0.19 
logdir: scripts/rl/output/phase2/trpo/mlp/base2/ 
cnn_policy: False 
continueFrom_model: scripts/rl/output/phase2/trpo/mlp/base/history_0092
/home/student/pantrasa/project/venv310/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.[0m
  logger.warn(

params: {'learning_rate': <__main__.LinearSchedule object at 0x7f38633b4310>, 'n_steps': 6144, 'batch_size': 64, 'verbose': 100, 'seed': 13}

Ep done - 1000.
Ep done - 2000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.14    |
| time/                     |          |
|    fps                    | 254      |
|    iterations             | 10       |
|    time_elapsed           | 241      |
|    total_timesteps        | 61440    |
| train/                    |          |
|    explained_variance     | 0.275    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00682  |
|    learning_rate          | 9.98e-05 |
|    n_updates              | 394      |
|    policy_objective       | 1.68e+10 |
|    value_loss             | 0.167    |
----------------------------------------
Eval num_timesteps=61500, episode_reward=0.28 +/- 0.93
Episode length: 30.01 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.275    |
| time/                     |          |
|    total_timesteps        | 61500    |
| train/                    |          |
|    explained_variance     | 0.305    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00926  |
|    learning_rate          | 9.98e-05 |
|    n_updates              | 395      |
|    policy_objective       | 5.09e+11 |
|    value_loss             | 0.174    |
----------------------------------------
/home/student/pantrasa/project/venv310/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.[0m
  logger.warn(
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 1
Ep done - 3000.
Ep done - 4000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.09     |
| time/                     |          |
|    fps                    | 243      |
|    iterations             | 20       |
|    time_elapsed           | 504      |
|    total_timesteps        | 122880   |
| train/                    |          |
|    explained_variance     | 0.266    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00665  |
|    learning_rate          | 9.96e-05 |
|    n_updates              | 404      |
|    policy_objective       | 4.22e+14 |
|    value_loss             | 0.176    |
----------------------------------------
Eval num_timesteps=123000, episode_reward=0.12 +/- 0.99
Episode length: 29.96 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 123000   |
| train/                    |          |
|    explained_variance     | 0.248    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00478  |
|    learning_rate          | 9.96e-05 |
|    n_updates              | 405      |
|    policy_objective       | 7.95e+23 |
|    value_loss             | 0.175    |
----------------------------------------
Ep done - 5000.
Ep done - 6000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 241      |
|    iterations             | 30       |
|    time_elapsed           | 761      |
|    total_timesteps        | 184320   |
| train/                    |          |
|    explained_variance     | 0.304    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00513  |
|    learning_rate          | 9.94e-05 |
|    n_updates              | 414      |
|    policy_objective       | 1.42e+08 |
|    value_loss             | 0.165    |
----------------------------------------
Eval num_timesteps=184500, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 184500   |
| train/                    |          |
|    explained_variance     | 0.279    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00572  |
|    learning_rate          | 9.94e-05 |
|    n_updates              | 415      |
|    policy_objective       | 2.11e+05 |
|    value_loss             | 0.171    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.205
SELFPLAY: new best model, bumping up generation to 2
Ep done - 7000.
Ep done - 8000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.01    |
| time/                     |          |
|    fps                    | 240      |
|    iterations             | 40       |
|    time_elapsed           | 1022     |
|    total_timesteps        | 245760   |
| train/                    |          |
|    explained_variance     | 0.302    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00464  |
|    learning_rate          | 9.92e-05 |
|    n_updates              | 424      |
|    policy_objective       | 8.82e+10 |
|    value_loss             | 0.162    |
----------------------------------------
Ep done - 9000.
Eval num_timesteps=246000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 246000   |
| train/                    |          |
|    explained_variance     | 0.282    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00951  |
|    learning_rate          | 9.92e-05 |
|    n_updates              | 425      |
|    policy_objective       | 4.66e+10 |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 10000.
Ep done - 11000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.06     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 50       |
|    time_elapsed           | 1281     |
|    total_timesteps        | 307200   |
| train/                    |          |
|    explained_variance     | 0.286    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00436  |
|    learning_rate          | 9.9e-05  |
|    n_updates              | 434      |
|    policy_objective       | 2.3e+07  |
|    value_loss             | 0.17     |
----------------------------------------
Eval num_timesteps=307500, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 307500   |
| train/                    |          |
|    explained_variance     | 0.247    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00878  |
|    learning_rate          | 9.9e-05  |
|    n_updates              | 435      |
|    policy_objective       | 9.13e+08 |
|    value_loss             | 0.169    |
----------------------------------------
Ep done - 12000.
Ep done - 13000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.02     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 60       |
|    time_elapsed           | 1541     |
|    total_timesteps        | 368640   |
| train/                    |          |
|    explained_variance     | 0.243    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00904  |
|    learning_rate          | 9.88e-05 |
|    n_updates              | 444      |
|    policy_objective       | 4.67e+08 |
|    value_loss             | 0.168    |
----------------------------------------
Eval num_timesteps=369000, episode_reward=0.23 +/- 0.94
Episode length: 30.04 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.235    |
| time/                     |          |
|    total_timesteps        | 369000   |
| train/                    |          |
|    explained_variance     | 0.375    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00492  |
|    learning_rate          | 9.88e-05 |
|    n_updates              | 445      |
|    policy_objective       | 6.83e+14 |
|    value_loss             | 0.16     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 3
Ep done - 14000.
Ep done - 15000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 70       |
|    time_elapsed           | 1796     |
|    total_timesteps        | 430080   |
| train/                    |          |
|    explained_variance     | 0.319    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00776  |
|    learning_rate          | 9.86e-05 |
|    n_updates              | 454      |
|    policy_objective       | 3.54e+13 |
|    value_loss             | 0.169    |
----------------------------------------
Eval num_timesteps=430500, episode_reward=0.12 +/- 0.98
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 430500   |
| train/                    |          |
|    explained_variance     | 0.344    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00643  |
|    learning_rate          | 9.86e-05 |
|    n_updates              | 455      |
|    policy_objective       | 1.76e+12 |
|    value_loss             | 0.17     |
----------------------------------------
Ep done - 16000.
Ep done - 17000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.2      |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 80       |
|    time_elapsed           | 2056     |
|    total_timesteps        | 491520   |
| train/                    |          |
|    explained_variance     | 0.251    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00636  |
|    learning_rate          | 9.84e-05 |
|    n_updates              | 464      |
|    policy_objective       | 2.21e+17 |
|    value_loss             | 0.163    |
----------------------------------------
Ep done - 18000.
Eval num_timesteps=492000, episode_reward=0.25 +/- 0.95
Episode length: 29.95 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 492000   |
| train/                    |          |
|    explained_variance     | 0.248    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00572  |
|    learning_rate          | 9.84e-05 |
|    n_updates              | 465      |
|    policy_objective       | 6.46e+05 |
|    value_loss             | 0.175    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.25
SELFPLAY: new best model, bumping up generation to 4
Ep done - 19000.
Ep done - 20000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 90       |
|    time_elapsed           | 2315     |
|    total_timesteps        | 552960   |
| train/                    |          |
|    explained_variance     | 0.276    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00723  |
|    learning_rate          | 9.82e-05 |
|    n_updates              | 474      |
|    policy_objective       | 1.43e+22 |
|    value_loss             | 0.183    |
----------------------------------------
Eval num_timesteps=553500, episode_reward=0.25 +/- 0.96
Episode length: 30.03 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 553500   |
| train/                    |          |
|    explained_variance     | 0.222    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00413  |
|    learning_rate          | 9.82e-05 |
|    n_updates              | 475      |
|    policy_objective       | 1.78e+13 |
|    value_loss             | 0.174    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.25
SELFPLAY: new best model, bumping up generation to 5
Ep done - 21000.
Ep done - 22000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.12    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 100      |
|    time_elapsed           | 2572     |
|    total_timesteps        | 614400   |
| train/                    |          |
|    explained_variance     | 0.337    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00779  |
|    learning_rate          | 9.8e-05  |
|    n_updates              | 484      |
|    policy_objective       | 5.96e+04 |
|    value_loss             | 0.148    |
----------------------------------------
Eval num_timesteps=615000, episode_reward=0.31 +/- 0.93
Episode length: 30.06 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.31     |
| time/                     |          |
|    total_timesteps        | 615000   |
| train/                    |          |
|    explained_variance     | 0.208    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00776  |
|    learning_rate          | 9.8e-05  |
|    n_updates              | 485      |
|    policy_objective       | 1.86e+10 |
|    value_loss             | 0.177    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 6
Ep done - 23000.
Ep done - 24000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 110      |
|    time_elapsed           | 2831     |
|    total_timesteps        | 675840   |
| train/                    |          |
|    explained_variance     | 0.294    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0093   |
|    learning_rate          | 9.78e-05 |
|    n_updates              | 494      |
|    policy_objective       | 3.28e+12 |
|    value_loss             | 0.174    |
----------------------------------------
Eval num_timesteps=676500, episode_reward=0.30 +/- 0.93
Episode length: 30.02 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.305    |
| time/                     |          |
|    total_timesteps        | 676500   |
| train/                    |          |
|    explained_variance     | 0.179    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00745  |
|    learning_rate          | 9.77e-05 |
|    n_updates              | 495      |
|    policy_objective       | 2.91e+13 |
|    value_loss             | 0.19     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.305
SELFPLAY: new best model, bumping up generation to 7
Ep done - 25000.
Ep done - 26000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.11    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 120      |
|    time_elapsed           | 3090     |
|    total_timesteps        | 737280   |
| train/                    |          |
|    explained_variance     | 0.236    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00837  |
|    learning_rate          | 9.76e-05 |
|    n_updates              | 504      |
|    policy_objective       | 9.62e+10 |
|    value_loss             | 0.178    |
----------------------------------------
Ep done - 27000.
Eval num_timesteps=738000, episode_reward=0.33 +/- 0.93
Episode length: 30.04 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.33     |
| time/                     |          |
|    total_timesteps        | 738000   |
| train/                    |          |
|    explained_variance     | 0.248    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00691  |
|    learning_rate          | 9.75e-05 |
|    n_updates              | 505      |
|    policy_objective       | 2.61e+05 |
|    value_loss             | 0.179    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.33
SELFPLAY: new best model, bumping up generation to 8
Ep done - 28000.
Ep done - 29000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.04    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 130      |
|    time_elapsed           | 3350     |
|    total_timesteps        | 798720   |
| train/                    |          |
|    explained_variance     | 0.301    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00838  |
|    learning_rate          | 9.74e-05 |
|    n_updates              | 514      |
|    policy_objective       | 2.87e+10 |
|    value_loss             | 0.173    |
----------------------------------------
Eval num_timesteps=799500, episode_reward=0.25 +/- 0.96
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 799500   |
| train/                    |          |
|    explained_variance     | 0.343    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00937  |
|    learning_rate          | 9.73e-05 |
|    n_updates              | 515      |
|    policy_objective       | 1.05e+12 |
|    value_loss             | 0.166    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.25
SELFPLAY: new best model, bumping up generation to 9
Ep done - 30000.
Ep done - 31000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.13    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 140      |
|    time_elapsed           | 3605     |
|    total_timesteps        | 860160   |
| train/                    |          |
|    explained_variance     | 0.342    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00956  |
|    learning_rate          | 9.72e-05 |
|    n_updates              | 524      |
|    policy_objective       | 5.14e+08 |
|    value_loss             | 0.179    |
----------------------------------------
Eval num_timesteps=861000, episode_reward=0.31 +/- 0.93
Episode length: 29.89 +/- 1.32
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.31     |
| time/                     |          |
|    total_timesteps        | 861000   |
| train/                    |          |
|    explained_variance     | 0.292    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0079   |
|    learning_rate          | 9.71e-05 |
|    n_updates              | 525      |
|    policy_objective       | 1.28e+06 |
|    value_loss             | 0.185    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.31
SELFPLAY: new best model, bumping up generation to 10
Ep done - 32000.
Ep done - 33000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 150      |
|    time_elapsed           | 3865     |
|    total_timesteps        | 921600   |
| train/                    |          |
|    explained_variance     | 0.329    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00735  |
|    learning_rate          | 9.69e-05 |
|    n_updates              | 534      |
|    policy_objective       | 7.47e+09 |
|    value_loss             | 0.169    |
----------------------------------------
Eval num_timesteps=922500, episode_reward=0.14 +/- 0.98
Episode length: 29.98 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 922500   |
| train/                    |          |
|    explained_variance     | 0.384    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00635  |
|    learning_rate          | 9.69e-05 |
|    n_updates              | 535      |
|    policy_objective       | 2.42e+06 |
|    value_loss             | 0.167    |
----------------------------------------
Ep done - 34000.
Ep done - 35000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.2      |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 160      |
|    time_elapsed           | 4124     |
|    total_timesteps        | 983040   |
| train/                    |          |
|    explained_variance     | 0.291    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0054   |
|    learning_rate          | 9.67e-05 |
|    n_updates              | 544      |
|    policy_objective       | 1.06e+10 |
|    value_loss             | 0.175    |
----------------------------------------
Ep done - 36000.
Eval num_timesteps=984000, episode_reward=0.28 +/- 0.93
Episode length: 30.03 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.285    |
| time/                     |          |
|    total_timesteps        | 984000   |
| train/                    |          |
|    explained_variance     | 0.282    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00711  |
|    learning_rate          | 9.67e-05 |
|    n_updates              | 545      |
|    policy_objective       | 1.09e+20 |
|    value_loss             | 0.18     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 11
Ep done - 37000.
Ep done - 38000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 170      |
|    time_elapsed           | 4383     |
|    total_timesteps        | 1044480  |
| train/                    |          |
|    explained_variance     | 0.333    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00604  |
|    learning_rate          | 9.65e-05 |
|    n_updates              | 554      |
|    policy_objective       | 1.41e+03 |
|    value_loss             | 0.164    |
----------------------------------------
Eval num_timesteps=1045500, episode_reward=0.26 +/- 0.96
Episode length: 29.89 +/- 1.05
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.255    |
| time/                     |          |
|    total_timesteps        | 1045500  |
| train/                    |          |
|    explained_variance     | 0.276    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00556  |
|    learning_rate          | 9.65e-05 |
|    n_updates              | 555      |
|    policy_objective       | 4.74e+09 |
|    value_loss             | 0.183    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 12
Ep done - 39000.
Ep done - 40000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.09     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 180      |
|    time_elapsed           | 4638     |
|    total_timesteps        | 1105920  |
| train/                    |          |
|    explained_variance     | 0.32     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00867  |
|    learning_rate          | 9.63e-05 |
|    n_updates              | 564      |
|    policy_objective       | 5.76e+16 |
|    value_loss             | 0.17     |
----------------------------------------
Eval num_timesteps=1107000, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.06     |
| time/                     |          |
|    total_timesteps        | 1107000  |
| train/                    |          |
|    explained_variance     | 0.248    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00695  |
|    learning_rate          | 9.63e-05 |
|    n_updates              | 565      |
|    policy_objective       | 5.25e+10 |
|    value_loss             | 0.178    |
----------------------------------------
Ep done - 41000.
Ep done - 42000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 190      |
|    time_elapsed           | 4896     |
|    total_timesteps        | 1167360  |
| train/                    |          |
|    explained_variance     | 0.348    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00674  |
|    learning_rate          | 9.61e-05 |
|    n_updates              | 574      |
|    policy_objective       | 1.12e+10 |
|    value_loss             | 0.177    |
----------------------------------------
Eval num_timesteps=1168500, episode_reward=0.22 +/- 0.97
Episode length: 29.98 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 1168500  |
| train/                    |          |
|    explained_variance     | 0.29     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00673  |
|    learning_rate          | 9.61e-05 |
|    n_updates              | 575      |
|    policy_objective       | 1.64e+22 |
|    value_loss             | 0.178    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.22
SELFPLAY: new best model, bumping up generation to 13
Ep done - 43000.
Ep done - 44000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 200      |
|    time_elapsed           | 5155     |
|    total_timesteps        | 1228800  |
| train/                    |          |
|    explained_variance     | 0.237    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00951  |
|    learning_rate          | 9.59e-05 |
|    n_updates              | 584      |
|    policy_objective       | 1.29e+10 |
|    value_loss             | 0.173    |
----------------------------------------
Ep done - 45000.
Eval num_timesteps=1230000, episode_reward=0.15 +/- 0.97
Episode length: 30.00 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 1230000  |
| train/                    |          |
|    explained_variance     | 0.307    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00916  |
|    learning_rate          | 9.59e-05 |
|    n_updates              | 585      |
|    policy_objective       | 8.74e+16 |
|    value_loss             | 0.166    |
----------------------------------------
Ep done - 46000.
Ep done - 47000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.29    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 210      |
|    time_elapsed           | 5414     |
|    total_timesteps        | 1290240  |
| train/                    |          |
|    explained_variance     | 0.343    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00984  |
|    learning_rate          | 9.57e-05 |
|    n_updates              | 594      |
|    policy_objective       | 9.21e+15 |
|    value_loss             | 0.151    |
----------------------------------------
Eval num_timesteps=1291500, episode_reward=0.18 +/- 0.96
Episode length: 30.00 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 1291500  |
| train/                    |          |
|    explained_variance     | 0.331    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.008    |
|    learning_rate          | 9.57e-05 |
|    n_updates              | 595      |
|    policy_objective       | 1.13e+17 |
|    value_loss             | 0.171    |
----------------------------------------
Ep done - 48000.
Ep done - 49000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.12    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 220      |
|    time_elapsed           | 5668     |
|    total_timesteps        | 1351680  |
| train/                    |          |
|    explained_variance     | 0.202    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00651  |
|    learning_rate          | 9.55e-05 |
|    n_updates              | 604      |
|    policy_objective       | 2.74e+07 |
|    value_loss             | 0.186    |
----------------------------------------
Eval num_timesteps=1353000, episode_reward=0.19 +/- 0.97
Episode length: 29.98 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 1353000  |
| train/                    |          |
|    explained_variance     | 0.255    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00754  |
|    learning_rate          | 9.55e-05 |
|    n_updates              | 605      |
|    policy_objective       | 3.2e+10  |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 50000.
Ep done - 51000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 230      |
|    time_elapsed           | 5927     |
|    total_timesteps        | 1413120  |
| train/                    |          |
|    explained_variance     | 0.273    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00823  |
|    learning_rate          | 9.53e-05 |
|    n_updates              | 614      |
|    policy_objective       | 2.92e+07 |
|    value_loss             | 0.181    |
----------------------------------------
Eval num_timesteps=1414500, episode_reward=0.26 +/- 0.96
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.255    |
| time/                     |          |
|    total_timesteps        | 1414500  |
| train/                    |          |
|    explained_variance     | 0.364    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00539  |
|    learning_rate          | 9.53e-05 |
|    n_updates              | 615      |
|    policy_objective       | 3.56e+09 |
|    value_loss             | 0.176    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 14
Ep done - 52000.
Ep done - 53000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 240      |
|    time_elapsed           | 6184     |
|    total_timesteps        | 1474560  |
| train/                    |          |
|    explained_variance     | 0.229    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00292  |
|    learning_rate          | 9.51e-05 |
|    n_updates              | 624      |
|    policy_objective       | 6.99e+16 |
|    value_loss             | 0.176    |
----------------------------------------
Ep done - 54000.
Eval num_timesteps=1476000, episode_reward=0.19 +/- 0.97
Episode length: 29.90 +/- 1.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 1476000  |
| train/                    |          |
|    explained_variance     | 0.318    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00897  |
|    learning_rate          | 9.51e-05 |
|    n_updates              | 625      |
|    policy_objective       | 4.62e+08 |
|    value_loss             | 0.174    |
----------------------------------------
Ep done - 55000.
Ep done - 56000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.09    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 250      |
|    time_elapsed           | 6438     |
|    total_timesteps        | 1536000  |
| train/                    |          |
|    explained_variance     | 0.286    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00559  |
|    learning_rate          | 9.49e-05 |
|    n_updates              | 634      |
|    policy_objective       | 7.98e+06 |
|    value_loss             | 0.171    |
----------------------------------------
Eval num_timesteps=1537500, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 1537500  |
| train/                    |          |
|    explained_variance     | 0.299    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00383  |
|    learning_rate          | 9.49e-05 |
|    n_updates              | 635      |
|    policy_objective       | 1.7e+07  |
|    value_loss             | 0.173    |
----------------------------------------
Ep done - 57000.
Ep done - 58000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.12    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 260      |
|    time_elapsed           | 6697     |
|    total_timesteps        | 1597440  |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00483  |
|    learning_rate          | 9.47e-05 |
|    n_updates              | 644      |
|    policy_objective       | 2.26e+11 |
|    value_loss             | 0.157    |
----------------------------------------
Eval num_timesteps=1599000, episode_reward=0.24 +/- 0.96
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.24     |
| time/                     |          |
|    total_timesteps        | 1599000  |
| train/                    |          |
|    explained_variance     | 0.334    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00758  |
|    learning_rate          | 9.47e-05 |
|    n_updates              | 645      |
|    policy_objective       | 2.17e+11 |
|    value_loss             | 0.17     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.24
SELFPLAY: new best model, bumping up generation to 15
Ep done - 59000.
Ep done - 60000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.24    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 270      |
|    time_elapsed           | 6955     |
|    total_timesteps        | 1658880  |
| train/                    |          |
|    explained_variance     | 0.31     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00892  |
|    learning_rate          | 9.45e-05 |
|    n_updates              | 654      |
|    policy_objective       | 2.84e+07 |
|    value_loss             | 0.179    |
----------------------------------------
Eval num_timesteps=1660500, episode_reward=0.24 +/- 0.96
Episode length: 30.02 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.24     |
| time/                     |          |
|    total_timesteps        | 1660500  |
| train/                    |          |
|    explained_variance     | 0.209    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00806  |
|    learning_rate          | 9.45e-05 |
|    n_updates              | 655      |
|    policy_objective       | 1.87e+10 |
|    value_loss             | 0.18     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.24
SELFPLAY: new best model, bumping up generation to 16
Ep done - 61000.
Ep done - 62000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.17    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 280      |
|    time_elapsed           | 7213     |
|    total_timesteps        | 1720320  |
| train/                    |          |
|    explained_variance     | 0.339    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00936  |
|    learning_rate          | 9.43e-05 |
|    n_updates              | 664      |
|    policy_objective       | 1.61e+07 |
|    value_loss             | 0.173    |
----------------------------------------
Ep done - 63000.
Eval num_timesteps=1722000, episode_reward=0.21 +/- 0.95
Episode length: 29.95 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.21     |
| time/                     |          |
|    total_timesteps        | 1722000  |
| train/                    |          |
|    explained_variance     | 0.316    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00813  |
|    learning_rate          | 9.43e-05 |
|    n_updates              | 665      |
|    policy_objective       | 2.88e+12 |
|    value_loss             | 0.175    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.21
SELFPLAY: new best model, bumping up generation to 17
Ep done - 64000.
Ep done - 65000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.2     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 290      |
|    time_elapsed           | 7468     |
|    total_timesteps        | 1781760  |
| train/                    |          |
|    explained_variance     | 0.268    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00923  |
|    learning_rate          | 9.41e-05 |
|    n_updates              | 674      |
|    policy_objective       | 2.58e+13 |
|    value_loss             | 0.169    |
----------------------------------------
Eval num_timesteps=1783500, episode_reward=0.12 +/- 0.97
Episode length: 29.91 +/- 1.06
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 1783500  |
| train/                    |          |
|    explained_variance     | 0.266    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00823  |
|    learning_rate          | 9.41e-05 |
|    n_updates              | 675      |
|    policy_objective       | 5.82e+11 |
|    value_loss             | 0.16     |
----------------------------------------
Ep done - 66000.
Ep done - 67000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.14     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 300      |
|    time_elapsed           | 7726     |
|    total_timesteps        | 1843200  |
| train/                    |          |
|    explained_variance     | 0.298    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00692  |
|    learning_rate          | 9.39e-05 |
|    n_updates              | 684      |
|    policy_objective       | 2.45e+10 |
|    value_loss             | 0.168    |
----------------------------------------
Eval num_timesteps=1845000, episode_reward=0.10 +/- 0.99
Episode length: 29.95 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 1845000  |
| train/                    |          |
|    explained_variance     | 0.33     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00884  |
|    learning_rate          | 9.39e-05 |
|    n_updates              | 685      |
|    policy_objective       | 9.17e+05 |
|    value_loss             | 0.162    |
----------------------------------------
Ep done - 68000.
Ep done - 69000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 310      |
|    time_elapsed           | 7984     |
|    total_timesteps        | 1904640  |
| train/                    |          |
|    explained_variance     | 0.25     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00502  |
|    learning_rate          | 9.37e-05 |
|    n_updates              | 694      |
|    policy_objective       | 1.29e+10 |
|    value_loss             | 0.174    |
----------------------------------------
Eval num_timesteps=1906500, episode_reward=0.12 +/- 0.97
Episode length: 29.95 +/- 0.74
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 1906500  |
| train/                    |          |
|    explained_variance     | 0.31     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00592  |
|    learning_rate          | 9.37e-05 |
|    n_updates              | 695      |
|    policy_objective       | 3.43e+08 |
|    value_loss             | 0.165    |
----------------------------------------
Ep done - 70000.
Ep done - 71000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 320      |
|    time_elapsed           | 8243     |
|    total_timesteps        | 1966080  |
| train/                    |          |
|    explained_variance     | 0.364    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00944  |
|    learning_rate          | 9.35e-05 |
|    n_updates              | 704      |
|    policy_objective       | 5.28e+14 |
|    value_loss             | 0.162    |
----------------------------------------
Ep done - 72000.
Eval num_timesteps=1968000, episode_reward=0.14 +/- 0.99
Episode length: 29.97 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 1968000  |
| train/                    |          |
|    explained_variance     | 0.254    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00829  |
|    learning_rate          | 9.34e-05 |
|    n_updates              | 705      |
|    policy_objective       | 1.04e+10 |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 73000.
Ep done - 74000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 330      |
|    time_elapsed           | 8496     |
|    total_timesteps        | 2027520  |
| train/                    |          |
|    explained_variance     | 0.277    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00818  |
|    learning_rate          | 9.33e-05 |
|    n_updates              | 714      |
|    policy_objective       | 2.65e+06 |
|    value_loss             | 0.172    |
----------------------------------------
Eval num_timesteps=2029500, episode_reward=0.14 +/- 0.97
Episode length: 29.96 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 2029500  |
| train/                    |          |
|    explained_variance     | 0.254    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00655  |
|    learning_rate          | 9.32e-05 |
|    n_updates              | 715      |
|    policy_objective       | 2.1e+13  |
|    value_loss             | 0.165    |
----------------------------------------
Ep done - 75000.
Ep done - 76000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.08     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 340      |
|    time_elapsed           | 8755     |
|    total_timesteps        | 2088960  |
| train/                    |          |
|    explained_variance     | 0.259    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0082   |
|    learning_rate          | 9.31e-05 |
|    n_updates              | 724      |
|    policy_objective       | 2.65e+19 |
|    value_loss             | 0.169    |
----------------------------------------
Eval num_timesteps=2091000, episode_reward=0.12 +/- 0.97
Episode length: 29.96 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 2091000  |
| train/                    |          |
|    explained_variance     | 0.287    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00881  |
|    learning_rate          | 9.3e-05  |
|    n_updates              | 725      |
|    policy_objective       | 1.65e+12 |
|    value_loss             | 0.165    |
----------------------------------------
Ep done - 77000.
Ep done - 78000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.08     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 350      |
|    time_elapsed           | 9012     |
|    total_timesteps        | 2150400  |
| train/                    |          |
|    explained_variance     | 0.304    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00996  |
|    learning_rate          | 9.29e-05 |
|    n_updates              | 734      |
|    policy_objective       | 5.7e+07  |
|    value_loss             | 0.179    |
----------------------------------------
Eval num_timesteps=2152500, episode_reward=0.12 +/- 0.97
Episode length: 29.98 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 2152500  |
| train/                    |          |
|    explained_variance     | 0.309    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00621  |
|    learning_rate          | 9.28e-05 |
|    n_updates              | 735      |
|    policy_objective       | 2.24e+04 |
|    value_loss             | 0.168    |
----------------------------------------
Ep done - 79000.
Ep done - 80000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.08    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 360      |
|    time_elapsed           | 9270     |
|    total_timesteps        | 2211840  |
| train/                    |          |
|    explained_variance     | 0.283    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00722  |
|    learning_rate          | 9.26e-05 |
|    n_updates              | 744      |
|    policy_objective       | 3.64e+20 |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 81000.
Eval num_timesteps=2214000, episode_reward=0.08 +/- 0.98
Episode length: 29.97 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 2214000  |
| train/                    |          |
|    explained_variance     | 0.212    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.009    |
|    learning_rate          | 9.26e-05 |
|    n_updates              | 745      |
|    policy_objective       | 5.26e+08 |
|    value_loss             | 0.181    |
----------------------------------------
Ep done - 82000.
Ep done - 83000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 370      |
|    time_elapsed           | 9525     |
|    total_timesteps        | 2273280  |
| train/                    |          |
|    explained_variance     | 0.273    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00768  |
|    learning_rate          | 9.24e-05 |
|    n_updates              | 754      |
|    policy_objective       | 1.52e+09 |
|    value_loss             | 0.174    |
----------------------------------------
Eval num_timesteps=2275500, episode_reward=-0.01 +/- 0.99
Episode length: 29.93 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | -0.01    |
| time/                     |          |
|    total_timesteps        | 2275500  |
| train/                    |          |
|    explained_variance     | 0.37     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00237  |
|    learning_rate          | 9.24e-05 |
|    n_updates              | 755      |
|    policy_objective       | 2.17     |
|    value_loss             | 0.165    |
----------------------------------------
Ep done - 84000.
Ep done - 85000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 380      |
|    time_elapsed           | 9783     |
|    total_timesteps        | 2334720  |
| train/                    |          |
|    explained_variance     | 0.302    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00979  |
|    learning_rate          | 9.22e-05 |
|    n_updates              | 764      |
|    policy_objective       | 4.34e+14 |
|    value_loss             | 0.177    |
----------------------------------------
Eval num_timesteps=2337000, episode_reward=0.20 +/- 0.97
Episode length: 29.97 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 2337000  |
| train/                    |          |
|    explained_variance     | 0.274    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00462  |
|    learning_rate          | 9.22e-05 |
|    n_updates              | 765      |
|    policy_objective       | 8.55e+07 |
|    value_loss             | 0.198    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.205
SELFPLAY: new best model, bumping up generation to 18
Ep done - 86000.
Ep done - 87000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.13     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 390      |
|    time_elapsed           | 10041    |
|    total_timesteps        | 2396160  |
| train/                    |          |
|    explained_variance     | 0.305    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00752  |
|    learning_rate          | 9.2e-05  |
|    n_updates              | 774      |
|    policy_objective       | 7.69e+08 |
|    value_loss             | 0.17     |
----------------------------------------
Eval num_timesteps=2398500, episode_reward=0.09 +/- 0.97
Episode length: 30.03 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 2398500  |
| train/                    |          |
|    explained_variance     | 0.329    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00536  |
|    learning_rate          | 9.2e-05  |
|    n_updates              | 775      |
|    policy_objective       | 1.82e+04 |
|    value_loss             | 0.17     |
----------------------------------------
Ep done - 88000.
Ep done - 89000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 400      |
|    time_elapsed           | 10296    |
|    total_timesteps        | 2457600  |
| train/                    |          |
|    explained_variance     | 0.269    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00738  |
|    learning_rate          | 9.18e-05 |
|    n_updates              | 784      |
|    policy_objective       | 1e+11    |
|    value_loss             | 0.182    |
----------------------------------------
Ep done - 90000.
Eval num_timesteps=2460000, episode_reward=0.21 +/- 0.96
Episode length: 30.00 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 2460000  |
| train/                    |          |
|    explained_variance     | 0.207    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00682  |
|    learning_rate          | 9.18e-05 |
|    n_updates              | 785      |
|    policy_objective       | 6.52e+09 |
|    value_loss             | 0.191    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.215
SELFPLAY: new best model, bumping up generation to 19
Ep done - 91000.
Ep done - 92000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.01    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 410      |
|    time_elapsed           | 10555    |
|    total_timesteps        | 2519040  |
| train/                    |          |
|    explained_variance     | 0.372    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00924  |
|    learning_rate          | 9.16e-05 |
|    n_updates              | 794      |
|    policy_objective       | 1.66e+11 |
|    value_loss             | 0.161    |
----------------------------------------
Eval num_timesteps=2521500, episode_reward=0.15 +/- 0.96
Episode length: 29.98 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 2521500  |
| train/                    |          |
|    explained_variance     | 0.292    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00653  |
|    learning_rate          | 9.16e-05 |
|    n_updates              | 795      |
|    policy_objective       | 1.05e+12 |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 93000.
Ep done - 94000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 420      |
|    time_elapsed           | 10813    |
|    total_timesteps        | 2580480  |
| train/                    |          |
|    explained_variance     | 0.355    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00805  |
|    learning_rate          | 9.14e-05 |
|    n_updates              | 804      |
|    policy_objective       | 4.23e+12 |
|    value_loss             | 0.166    |
----------------------------------------
Eval num_timesteps=2583000, episode_reward=0.03 +/- 0.99
Episode length: 29.94 +/- 0.63
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.025    |
| time/                     |          |
|    total_timesteps        | 2583000  |
| train/                    |          |
|    explained_variance     | 0.36     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00446  |
|    learning_rate          | 9.14e-05 |
|    n_updates              | 805      |
|    policy_objective       | 8.93e+08 |
|    value_loss             | 0.158    |
----------------------------------------
Ep done - 95000.
Ep done - 96000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.7     |
|    ep_rew_mean            | -0.13    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 430      |
|    time_elapsed           | 11072    |
|    total_timesteps        | 2641920  |
| train/                    |          |
|    explained_variance     | 0.317    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00652  |
|    learning_rate          | 9.12e-05 |
|    n_updates              | 814      |
|    policy_objective       | 1.71e+16 |
|    value_loss             | 0.173    |
----------------------------------------
Eval num_timesteps=2644500, episode_reward=0.23 +/- 0.95
Episode length: 30.07 +/- 0.75
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 2644500  |
| train/                    |          |
|    explained_variance     | 0.352    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00634  |
|    learning_rate          | 9.12e-05 |
|    n_updates              | 815      |
|    policy_objective       | 2.14e+09 |
|    value_loss             | 0.178    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 20
Ep done - 97000.
Ep done - 98000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 440      |
|    time_elapsed           | 11325    |
|    total_timesteps        | 2703360  |
| train/                    |          |
|    explained_variance     | 0.353    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00663  |
|    learning_rate          | 9.1e-05  |
|    n_updates              | 824      |
|    policy_objective       | 1.07e+20 |
|    value_loss             | 0.169    |
----------------------------------------
Ep done - 99000.
Eval num_timesteps=2706000, episode_reward=0.14 +/- 0.97
Episode length: 30.01 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 2706000  |
| train/                    |          |
|    explained_variance     | 0.327    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00491  |
|    learning_rate          | 9.1e-05  |
|    n_updates              | 825      |
|    policy_objective       | 1.43e+07 |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 100000.
Ep done - 101000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.1     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 450      |
|    time_elapsed           | 11583    |
|    total_timesteps        | 2764800  |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00657  |
|    learning_rate          | 9.08e-05 |
|    n_updates              | 834      |
|    policy_objective       | 9.18e+03 |
|    value_loss             | 0.161    |
----------------------------------------
Eval num_timesteps=2767500, episode_reward=0.16 +/- 0.98
Episode length: 30.04 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 2767500  |
| train/                    |          |
|    explained_variance     | 0.36     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00685  |
|    learning_rate          | 9.08e-05 |
|    n_updates              | 835      |
|    policy_objective       | 2.08e+05 |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 102000.
Ep done - 103000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | -0.08    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 460      |
|    time_elapsed           | 11841    |
|    total_timesteps        | 2826240  |
| train/                    |          |
|    explained_variance     | 0.369    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00905  |
|    learning_rate          | 9.06e-05 |
|    n_updates              | 844      |
|    policy_objective       | 1.77e+03 |
|    value_loss             | 0.161    |
----------------------------------------
Eval num_timesteps=2829000, episode_reward=0.18 +/- 0.97
Episode length: 30.07 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 2829000  |
| train/                    |          |
|    explained_variance     | 0.33     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00467  |
|    learning_rate          | 9.06e-05 |
|    n_updates              | 845      |
|    policy_objective       | 2.53e+18 |
|    value_loss             | 0.182    |
----------------------------------------
Ep done - 104000.
Ep done - 105000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.08     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 470      |
|    time_elapsed           | 12099    |
|    total_timesteps        | 2887680  |
| train/                    |          |
|    explained_variance     | 0.397    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00697  |
|    learning_rate          | 9.04e-05 |
|    n_updates              | 854      |
|    policy_objective       | 1.74e+11 |
|    value_loss             | 0.157    |
----------------------------------------
Eval num_timesteps=2890500, episode_reward=0.25 +/- 0.95
Episode length: 30.06 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.25     |
| time/                     |          |
|    total_timesteps        | 2890500  |
| train/                    |          |
|    explained_variance     | 0.35     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00929  |
|    learning_rate          | 9.04e-05 |
|    n_updates              | 855      |
|    policy_objective       | 1.15e+07 |
|    value_loss             | 0.178    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.25
SELFPLAY: new best model, bumping up generation to 21
Ep done - 106000.
Ep done - 107000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.09     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 480      |
|    time_elapsed           | 12353    |
|    total_timesteps        | 2949120  |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00496  |
|    learning_rate          | 9.02e-05 |
|    n_updates              | 864      |
|    policy_objective       | 1.29e+07 |
|    value_loss             | 0.162    |
----------------------------------------
Ep done - 108000.
Eval num_timesteps=2952000, episode_reward=0.06 +/- 0.98
Episode length: 29.86 +/- 1.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 2952000  |
| train/                    |          |
|    explained_variance     | 0.314    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00884  |
|    learning_rate          | 9.02e-05 |
|    n_updates              | 865      |
|    policy_objective       | 2.83e+12 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 109000.
Ep done - 110000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 490      |
|    time_elapsed           | 12612    |
|    total_timesteps        | 3010560  |
| train/                    |          |
|    explained_variance     | 0.324    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00675  |
|    learning_rate          | 9e-05    |
|    n_updates              | 874      |
|    policy_objective       | 1.57e+04 |
|    value_loss             | 0.173    |
----------------------------------------
Eval num_timesteps=3013500, episode_reward=0.12 +/- 0.97
Episode length: 30.01 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 3013500  |
| train/                    |          |
|    explained_variance     | 0.37     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00776  |
|    learning_rate          | 9e-05    |
|    n_updates              | 875      |
|    policy_objective       | 1.5e+06  |
|    value_loss             | 0.175    |
----------------------------------------
Ep done - 111000.
Ep done - 112000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 500      |
|    time_elapsed           | 12870    |
|    total_timesteps        | 3072000  |
| train/                    |          |
|    explained_variance     | 0.342    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00626  |
|    learning_rate          | 8.98e-05 |
|    n_updates              | 884      |
|    policy_objective       | 2.49e+09 |
|    value_loss             | 0.181    |
----------------------------------------
Eval num_timesteps=3075000, episode_reward=0.03 +/- 0.98
Episode length: 29.95 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.025    |
| time/                     |          |
|    total_timesteps        | 3075000  |
| train/                    |          |
|    explained_variance     | 0.36     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00848  |
|    learning_rate          | 8.98e-05 |
|    n_updates              | 885      |
|    policy_objective       | 1.68e+06 |
|    value_loss             | 0.168    |
----------------------------------------
Ep done - 113000.
Ep done - 114000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.19    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 510      |
|    time_elapsed           | 13124    |
|    total_timesteps        | 3133440  |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00739  |
|    learning_rate          | 8.96e-05 |
|    n_updates              | 894      |
|    policy_objective       | 1.59e+17 |
|    value_loss             | 0.158    |
----------------------------------------
Eval num_timesteps=3136500, episode_reward=0.04 +/- 0.98
Episode length: 29.99 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.045    |
| time/                     |          |
|    total_timesteps        | 3136500  |
| train/                    |          |
|    explained_variance     | 0.262    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00663  |
|    learning_rate          | 8.96e-05 |
|    n_updates              | 895      |
|    policy_objective       | 1.32e+09 |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 115000.
Ep done - 116000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.17    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 520      |
|    time_elapsed           | 13382    |
|    total_timesteps        | 3194880  |
| train/                    |          |
|    explained_variance     | 0.289    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00594  |
|    learning_rate          | 8.94e-05 |
|    n_updates              | 904      |
|    policy_objective       | 4.95e+09 |
|    value_loss             | 0.171    |
----------------------------------------
Ep done - 117000.
Eval num_timesteps=3198000, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.045    |
| time/                     |          |
|    total_timesteps        | 3198000  |
| train/                    |          |
|    explained_variance     | 0.313    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00677  |
|    learning_rate          | 8.94e-05 |
|    n_updates              | 905      |
|    policy_objective       | 4.49e+22 |
|    value_loss             | 0.176    |
----------------------------------------
Ep done - 118000.
Ep done - 119000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 530      |
|    time_elapsed           | 13640    |
|    total_timesteps        | 3256320  |
| train/                    |          |
|    explained_variance     | 0.371    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00591  |
|    learning_rate          | 8.92e-05 |
|    n_updates              | 914      |
|    policy_objective       | 1.25e+10 |
|    value_loss             | 0.174    |
----------------------------------------
Eval num_timesteps=3259500, episode_reward=0.02 +/- 0.99
Episode length: 29.90 +/- 1.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.02     |
| time/                     |          |
|    total_timesteps        | 3259500  |
| train/                    |          |
|    explained_variance     | 0.271    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00666  |
|    learning_rate          | 8.91e-05 |
|    n_updates              | 915      |
|    policy_objective       | 1.27e+06 |
|    value_loss             | 0.176    |
----------------------------------------
Ep done - 120000.
Ep done - 121000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.1     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 540      |
|    time_elapsed           | 13898    |
|    total_timesteps        | 3317760  |
| train/                    |          |
|    explained_variance     | 0.297    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00953  |
|    learning_rate          | 8.9e-05  |
|    n_updates              | 924      |
|    policy_objective       | 1.74e+12 |
|    value_loss             | 0.182    |
----------------------------------------
Eval num_timesteps=3321000, episode_reward=0.06 +/- 0.99
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.06     |
| time/                     |          |
|    total_timesteps        | 3321000  |
| train/                    |          |
|    explained_variance     | 0.233    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00769  |
|    learning_rate          | 8.89e-05 |
|    n_updates              | 925      |
|    policy_objective       | 1.04e+11 |
|    value_loss             | 0.189    |
----------------------------------------
Ep done - 122000.
Ep done - 123000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 550      |
|    time_elapsed           | 14152    |
|    total_timesteps        | 3379200  |
| train/                    |          |
|    explained_variance     | 0.238    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00859  |
|    learning_rate          | 8.88e-05 |
|    n_updates              | 934      |
|    policy_objective       | 1.75e+08 |
|    value_loss             | 0.207    |
----------------------------------------
Eval num_timesteps=3382500, episode_reward=0.18 +/- 0.95
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 3382500  |
| train/                    |          |
|    explained_variance     | 0.322    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0075   |
|    learning_rate          | 8.87e-05 |
|    n_updates              | 935      |
|    policy_objective       | 1.72e+11 |
|    value_loss             | 0.174    |
----------------------------------------
Ep done - 124000.
Ep done - 125000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.08     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 560      |
|    time_elapsed           | 14411    |
|    total_timesteps        | 3440640  |
| train/                    |          |
|    explained_variance     | 0.404    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00642  |
|    learning_rate          | 8.86e-05 |
|    n_updates              | 944      |
|    policy_objective       | 2.99e+05 |
|    value_loss             | 0.172    |
----------------------------------------
Ep done - 126000.
Eval num_timesteps=3444000, episode_reward=0.10 +/- 0.97
Episode length: 29.96 +/- 1.14
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 3444000  |
| train/                    |          |
|    explained_variance     | 0.411    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00745  |
|    learning_rate          | 8.85e-05 |
|    n_updates              | 945      |
|    policy_objective       | 6.4e+12  |
|    value_loss             | 0.155    |
----------------------------------------
Ep done - 127000.
Ep done - 128000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.22    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 570      |
|    time_elapsed           | 14669    |
|    total_timesteps        | 3502080  |
| train/                    |          |
|    explained_variance     | 0.297    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00654  |
|    learning_rate          | 8.83e-05 |
|    n_updates              | 954      |
|    policy_objective       | 1.28e+07 |
|    value_loss             | 0.175    |
----------------------------------------
Eval num_timesteps=3505500, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 0.64
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.06     |
| time/                     |          |
|    total_timesteps        | 3505500  |
| train/                    |          |
|    explained_variance     | 0.368    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00708  |
|    learning_rate          | 8.83e-05 |
|    n_updates              | 955      |
|    policy_objective       | 1.02e+06 |
|    value_loss             | 0.174    |
----------------------------------------
Ep done - 129000.
Ep done - 130000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.15    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 580      |
|    time_elapsed           | 14927    |
|    total_timesteps        | 3563520  |
| train/                    |          |
|    explained_variance     | 0.31     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00873  |
|    learning_rate          | 8.81e-05 |
|    n_updates              | 964      |
|    policy_objective       | 9.59e+11 |
|    value_loss             | 0.185    |
----------------------------------------
Eval num_timesteps=3567000, episode_reward=0.08 +/- 0.98
Episode length: 29.93 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 3567000  |
| train/                    |          |
|    explained_variance     | 0.345    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00696  |
|    learning_rate          | 8.81e-05 |
|    n_updates              | 965      |
|    policy_objective       | 9.83e+10 |
|    value_loss             | 0.174    |
----------------------------------------
Ep done - 131000.
Ep done - 132000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 590      |
|    time_elapsed           | 15181    |
|    total_timesteps        | 3624960  |
| train/                    |          |
|    explained_variance     | 0.345    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00482  |
|    learning_rate          | 8.79e-05 |
|    n_updates              | 974      |
|    policy_objective       | 1.22e+08 |
|    value_loss             | 0.169    |
----------------------------------------
Eval num_timesteps=3628500, episode_reward=0.08 +/- 0.99
Episode length: 29.95 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 3628500  |
| train/                    |          |
|    explained_variance     | 0.328    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00799  |
|    learning_rate          | 8.79e-05 |
|    n_updates              | 975      |
|    policy_objective       | 1.06e+05 |
|    value_loss             | 0.186    |
----------------------------------------
Ep done - 133000.
Ep done - 134000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.1     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 600      |
|    time_elapsed           | 15439    |
|    total_timesteps        | 3686400  |
| train/                    |          |
|    explained_variance     | 0.314    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0078   |
|    learning_rate          | 8.77e-05 |
|    n_updates              | 984      |
|    policy_objective       | 2.82e+11 |
|    value_loss             | 0.168    |
----------------------------------------
Ep done - 135000.
Eval num_timesteps=3690000, episode_reward=0.12 +/- 0.97
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 3690000  |
| train/                    |          |
|    explained_variance     | 0.349    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00966  |
|    learning_rate          | 8.77e-05 |
|    n_updates              | 985      |
|    policy_objective       | 8e+08    |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 136000.
Ep done - 137000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.16     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 610      |
|    time_elapsed           | 15697    |
|    total_timesteps        | 3747840  |
| train/                    |          |
|    explained_variance     | 0.271    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0095   |
|    learning_rate          | 8.75e-05 |
|    n_updates              | 994      |
|    policy_objective       | 4.3e+05  |
|    value_loss             | 0.188    |
----------------------------------------
Eval num_timesteps=3751500, episode_reward=0.21 +/- 0.96
Episode length: 30.05 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.21     |
| time/                     |          |
|    total_timesteps        | 3751500  |
| train/                    |          |
|    explained_variance     | 0.272    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00919  |
|    learning_rate          | 8.75e-05 |
|    n_updates              | 995      |
|    policy_objective       | 4.19e+13 |
|    value_loss             | 0.187    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.21
SELFPLAY: new best model, bumping up generation to 22
Ep done - 138000.
Ep done - 139000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.16    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 620      |
|    time_elapsed           | 15954    |
|    total_timesteps        | 3809280  |
| train/                    |          |
|    explained_variance     | 0.287    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00724  |
|    learning_rate          | 8.73e-05 |
|    n_updates              | 1004     |
|    policy_objective       | 2.92e+13 |
|    value_loss             | 0.188    |
----------------------------------------
Eval num_timesteps=3813000, episode_reward=0.14 +/- 0.97
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 3813000  |
| train/                    |          |
|    explained_variance     | 0.394    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00809  |
|    learning_rate          | 8.73e-05 |
|    n_updates              | 1005     |
|    policy_objective       | 3.49e+11 |
|    value_loss             | 0.168    |
----------------------------------------
Ep done - 140000.
Ep done - 141000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 630      |
|    time_elapsed           | 16209    |
|    total_timesteps        | 3870720  |
| train/                    |          |
|    explained_variance     | 0.355    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00599  |
|    learning_rate          | 8.71e-05 |
|    n_updates              | 1014     |
|    policy_objective       | 2.39e+12 |
|    value_loss             | 0.184    |
----------------------------------------
Eval num_timesteps=3874500, episode_reward=0.12 +/- 0.98
Episode length: 30.04 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 3874500  |
| train/                    |          |
|    explained_variance     | 0.357    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00492  |
|    learning_rate          | 8.71e-05 |
|    n_updates              | 1015     |
|    policy_objective       | 7.33e+10 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 142000.
Ep done - 143000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 640      |
|    time_elapsed           | 16466    |
|    total_timesteps        | 3932160  |
| train/                    |          |
|    explained_variance     | 0.304    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00872  |
|    learning_rate          | 8.69e-05 |
|    n_updates              | 1024     |
|    policy_objective       | 2.53e+06 |
|    value_loss             | 0.181    |
----------------------------------------
Ep done - 144000.
Eval num_timesteps=3936000, episode_reward=0.18 +/- 0.96
Episode length: 29.89 +/- 1.37
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 3936000  |
| train/                    |          |
|    explained_variance     | 0.347    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0075   |
|    learning_rate          | 8.69e-05 |
|    n_updates              | 1025     |
|    policy_objective       | 9.77e+04 |
|    value_loss             | 0.182    |
----------------------------------------
Ep done - 145000.
Ep done - 146000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.01    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 650      |
|    time_elapsed           | 16725    |
|    total_timesteps        | 3993600  |
| train/                    |          |
|    explained_variance     | 0.343    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00997  |
|    learning_rate          | 8.67e-05 |
|    n_updates              | 1034     |
|    policy_objective       | 1.4e+13  |
|    value_loss             | 0.179    |
----------------------------------------
Eval num_timesteps=3997500, episode_reward=0.14 +/- 0.97
Episode length: 29.96 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 3997500  |
| train/                    |          |
|    explained_variance     | 0.395    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00994  |
|    learning_rate          | 8.67e-05 |
|    n_updates              | 1035     |
|    policy_objective       | 2.14e+14 |
|    value_loss             | 0.171    |
----------------------------------------
Ep done - 147000.
Ep done - 148000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 660      |
|    time_elapsed           | 16978    |
|    total_timesteps        | 4055040  |
| train/                    |          |
|    explained_variance     | 0.419    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00924  |
|    learning_rate          | 8.65e-05 |
|    n_updates              | 1044     |
|    policy_objective       | 2.15e+11 |
|    value_loss             | 0.18     |
----------------------------------------
Eval num_timesteps=4059000, episode_reward=0.09 +/- 0.98
Episode length: 29.90 +/- 1.33
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 4059000  |
| train/                    |          |
|    explained_variance     | 0.373    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0066   |
|    learning_rate          | 8.65e-05 |
|    n_updates              | 1045     |
|    policy_objective       | 2.87e+09 |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 149000.
Ep done - 150000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.09     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 670      |
|    time_elapsed           | 17237    |
|    total_timesteps        | 4116480  |
| train/                    |          |
|    explained_variance     | 0.378    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00652  |
|    learning_rate          | 8.63e-05 |
|    n_updates              | 1054     |
|    policy_objective       | 3.82e+13 |
|    value_loss             | 0.169    |
----------------------------------------
Eval num_timesteps=4120500, episode_reward=0.18 +/- 0.97
Episode length: 30.02 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 4120500  |
| train/                    |          |
|    explained_variance     | 0.375    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00745  |
|    learning_rate          | 8.63e-05 |
|    n_updates              | 1055     |
|    policy_objective       | 1.51e+10 |
|    value_loss             | 0.17     |
----------------------------------------
Ep done - 151000.
Ep done - 152000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.01    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 680      |
|    time_elapsed           | 17495    |
|    total_timesteps        | 4177920  |
| train/                    |          |
|    explained_variance     | 0.334    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00667  |
|    learning_rate          | 8.61e-05 |
|    n_updates              | 1064     |
|    policy_objective       | 7.25e+03 |
|    value_loss             | 0.17     |
----------------------------------------
Ep done - 153000.
Eval num_timesteps=4182000, episode_reward=0.14 +/- 0.97
Episode length: 29.92 +/- 1.18
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 4182000  |
| train/                    |          |
|    explained_variance     | 0.328    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.001    |
|    learning_rate          | 8.61e-05 |
|    n_updates              | 1065     |
|    policy_objective       | 298      |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 154000.
Ep done - 155000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 690      |
|    time_elapsed           | 17754    |
|    total_timesteps        | 4239360  |
| train/                    |          |
|    explained_variance     | 0.33     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.59e-05 |
|    n_updates              | 1074     |
|    policy_objective       | 2.58e-07 |
|    value_loss             | 0.183    |
----------------------------------------
Eval num_timesteps=4243500, episode_reward=0.04 +/- 0.99
Episode length: 29.94 +/- 0.76
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.045     |
| time/                     |           |
|    total_timesteps        | 4243500   |
| train/                    |           |
|    explained_variance     | 0.3       |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.59e-05  |
|    n_updates              | 1075      |
|    policy_objective       | -2.39e-07 |
|    value_loss             | 0.186     |
-----------------------------------------
Ep done - 156000.
Ep done - 157000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.12    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 700      |
|    time_elapsed           | 18008    |
|    total_timesteps        | 4300800  |
| train/                    |          |
|    explained_variance     | 0.304    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000439 |
|    learning_rate          | 8.57e-05 |
|    n_updates              | 1084     |
|    policy_objective       | 1.81     |
|    value_loss             | 0.179    |
----------------------------------------
Eval num_timesteps=4305000, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.005    |
| time/                     |          |
|    total_timesteps        | 4305000  |
| train/                    |          |
|    explained_variance     | 0.389    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.57e-05 |
|    n_updates              | 1085     |
|    policy_objective       | 1.78e-07 |
|    value_loss             | 0.17     |
----------------------------------------
Ep done - 158000.
Ep done - 159000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.11    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 710      |
|    time_elapsed           | 18266    |
|    total_timesteps        | 4362240  |
| train/                    |          |
|    explained_variance     | 0.33     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.55e-05 |
|    n_updates              | 1094     |
|    policy_objective       | -4.1e-07 |
|    value_loss             | 0.178    |
----------------------------------------
Eval num_timesteps=4366500, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.005    |
| time/                     |          |
|    total_timesteps        | 4366500  |
| train/                    |          |
|    explained_variance     | 0.401    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.55e-05 |
|    n_updates              | 1095     |
|    policy_objective       | 1.02e-07 |
|    value_loss             | 0.163    |
----------------------------------------
Ep done - 160000.
Ep done - 161000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.17    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 720      |
|    time_elapsed           | 18524    |
|    total_timesteps        | 4423680  |
| train/                    |          |
|    explained_variance     | 0.376    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000271 |
|    learning_rate          | 8.53e-05 |
|    n_updates              | 1104     |
|    policy_objective       | 0.18     |
|    value_loss             | 0.177    |
----------------------------------------
Ep done - 162000.
Eval num_timesteps=4428000, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.145     |
| time/                     |           |
|    total_timesteps        | 4428000   |
| train/                    |           |
|    explained_variance     | 0.323     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.53e-05  |
|    n_updates              | 1105      |
|    policy_objective       | -6.11e-07 |
|    value_loss             | 0.181     |
-----------------------------------------
Ep done - 163000.
Ep done - 164000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 730      |
|    time_elapsed           | 18783    |
|    total_timesteps        | 4485120  |
| train/                    |          |
|    explained_variance     | 0.343    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.51e-05 |
|    n_updates              | 1114     |
|    policy_objective       | -2.4e-07 |
|    value_loss             | 0.178    |
----------------------------------------
Eval num_timesteps=4489500, episode_reward=0.20 +/- 0.97
Episode length: 29.98 +/- 0.70
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 4489500  |
| train/                    |          |
|    explained_variance     | 0.353    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.5e-05  |
|    n_updates              | 1115     |
|    policy_objective       | 6.26e-07 |
|    value_loss             | 0.162    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.205
SELFPLAY: new best model, bumping up generation to 23
Ep done - 165000.
Ep done - 166000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.14     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 740       |
|    time_elapsed           | 19036     |
|    total_timesteps        | 4546560   |
| train/                    |           |
|    explained_variance     | 0.33      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.49e-05  |
|    n_updates              | 1124      |
|    policy_objective       | -3.17e-07 |
|    value_loss             | 0.19      |
-----------------------------------------
Eval num_timesteps=4551000, episode_reward=0.09 +/- 0.97
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.085    |
| time/                     |          |
|    total_timesteps        | 4551000  |
| train/                    |          |
|    explained_variance     | 0.287    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.48e-05 |
|    n_updates              | 1125     |
|    policy_objective       | 4.89e-07 |
|    value_loss             | 0.175    |
----------------------------------------
Ep done - 167000.
Ep done - 168000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.2     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 750      |
|    time_elapsed           | 19294    |
|    total_timesteps        | 4608000  |
| train/                    |          |
|    explained_variance     | 0.407    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.47e-05 |
|    n_updates              | 1134     |
|    policy_objective       | 7.76e-08 |
|    value_loss             | 0.169    |
----------------------------------------
Eval num_timesteps=4612500, episode_reward=0.23 +/- 0.96
Episode length: 29.91 +/- 1.38
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.235     |
| time/                     |           |
|    total_timesteps        | 4612500   |
| train/                    |           |
|    explained_variance     | 0.421     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.46e-05  |
|    n_updates              | 1135      |
|    policy_objective       | -1.36e-07 |
|    value_loss             | 0.174     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 24
Ep done - 169000.
Ep done - 170000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 760      |
|    time_elapsed           | 19552    |
|    total_timesteps        | 4669440  |
| train/                    |          |
|    explained_variance     | 0.313    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.45e-05 |
|    n_updates              | 1144     |
|    policy_objective       | 2.94e-07 |
|    value_loss             | 0.165    |
----------------------------------------
Ep done - 171000.
Eval num_timesteps=4674000, episode_reward=0.24 +/- 0.96
Episode length: 30.09 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.245    |
| time/                     |          |
|    total_timesteps        | 4674000  |
| train/                    |          |
|    explained_variance     | 0.367    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.44e-05 |
|    n_updates              | 1145     |
|    policy_objective       | 2.74e-07 |
|    value_loss             | 0.191    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.245
SELFPLAY: new best model, bumping up generation to 25
Ep done - 172000.
Ep done - 173000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.05     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 770       |
|    time_elapsed           | 19806     |
|    total_timesteps        | 4730880   |
| train/                    |           |
|    explained_variance     | 0.32      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.43e-05  |
|    n_updates              | 1154      |
|    policy_objective       | -6.15e-08 |
|    value_loss             | 0.176     |
-----------------------------------------
Eval num_timesteps=4735500, episode_reward=0.06 +/- 0.99
Episode length: 29.94 +/- 1.10
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.06      |
| time/                     |           |
|    total_timesteps        | 4735500   |
| train/                    |           |
|    explained_variance     | 0.399     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.42e-05  |
|    n_updates              | 1155      |
|    policy_objective       | -4.06e-07 |
|    value_loss             | 0.178     |
-----------------------------------------
Ep done - 174000.
Ep done - 175000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.14    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 780      |
|    time_elapsed           | 20064    |
|    total_timesteps        | 4792320  |
| train/                    |          |
|    explained_variance     | 0.191    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.4e-05  |
|    n_updates              | 1164     |
|    policy_objective       | -2.3e-07 |
|    value_loss             | 0.187    |
----------------------------------------
Eval num_timesteps=4797000, episode_reward=0.06 +/- 0.98
Episode length: 30.06 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.06     |
| time/                     |          |
|    total_timesteps        | 4797000  |
| train/                    |          |
|    explained_variance     | 0.341    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000328 |
|    learning_rate          | 8.4e-05  |
|    n_updates              | 1165     |
|    policy_objective       | 1.33     |
|    value_loss             | 0.158    |
----------------------------------------
Ep done - 176000.
Ep done - 177000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 790      |
|    time_elapsed           | 20322    |
|    total_timesteps        | 4853760  |
| train/                    |          |
|    explained_variance     | 0.318    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.38e-05 |
|    n_updates              | 1174     |
|    policy_objective       | 2.04e-07 |
|    value_loss             | 0.189    |
----------------------------------------
Eval num_timesteps=4858500, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 4858500  |
| train/                    |          |
|    explained_variance     | 0.41     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.38e-05 |
|    n_updates              | 1175     |
|    policy_objective       | 1.55e-07 |
|    value_loss             | 0.165    |
----------------------------------------
Ep done - 178000.
Ep done - 179000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.02      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 800       |
|    time_elapsed           | 20580     |
|    total_timesteps        | 4915200   |
| train/                    |           |
|    explained_variance     | 0.374     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.36e-05  |
|    n_updates              | 1184      |
|    policy_objective       | -7.48e-07 |
|    value_loss             | 0.183     |
-----------------------------------------
Ep done - 180000.
Eval num_timesteps=4920000, episode_reward=0.14 +/- 0.98
Episode length: 29.92 +/- 1.26
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 4920000  |
| train/                    |          |
|    explained_variance     | 0.296    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.36e-05 |
|    n_updates              | 1185     |
|    policy_objective       | 1.28e-07 |
|    value_loss             | 0.19     |
----------------------------------------
Ep done - 181000.
Ep done - 182000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.15     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 810       |
|    time_elapsed           | 20834     |
|    total_timesteps        | 4976640   |
| train/                    |           |
|    explained_variance     | 0.357     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.34e-05  |
|    n_updates              | 1194      |
|    policy_objective       | -2.74e-07 |
|    value_loss             | 0.169     |
-----------------------------------------
Eval num_timesteps=4981500, episode_reward=0.19 +/- 0.96
Episode length: 30.07 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 4981500  |
| train/                    |          |
|    explained_variance     | 0.381    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.34e-05 |
|    n_updates              | 1195     |
|    policy_objective       | 1.68e-07 |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 183000.
Ep done - 184000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.13    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 820      |
|    time_elapsed           | 21092    |
|    total_timesteps        | 5038080  |
| train/                    |          |
|    explained_variance     | 0.391    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.32e-05 |
|    n_updates              | 1204     |
|    policy_objective       | 2.38e-07 |
|    value_loss             | 0.175    |
----------------------------------------
Eval num_timesteps=5043000, episode_reward=-0.04 +/- 0.98
Episode length: 29.90 +/- 1.01
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | -0.04    |
| time/                     |          |
|    total_timesteps        | 5043000  |
| train/                    |          |
|    explained_variance     | 0.407    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.32e-05 |
|    n_updates              | 1205     |
|    policy_objective       | 4.48e-07 |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 185000.
Ep done - 186000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 830      |
|    time_elapsed           | 21350    |
|    total_timesteps        | 5099520  |
| train/                    |          |
|    explained_variance     | 0.442    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.3e-05  |
|    n_updates              | 1214     |
|    policy_objective       | 3.19e-07 |
|    value_loss             | 0.17     |
----------------------------------------
Eval num_timesteps=5104500, episode_reward=0.17 +/- 0.97
Episode length: 30.06 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.165     |
| time/                     |           |
|    total_timesteps        | 5104500   |
| train/                    |           |
|    explained_variance     | 0.349     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.3e-05   |
|    n_updates              | 1215      |
|    policy_objective       | -8.82e-08 |
|    value_loss             | 0.191     |
-----------------------------------------
Ep done - 187000.
Ep done - 188000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.17     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 840       |
|    time_elapsed           | 21608     |
|    total_timesteps        | 5160960   |
| train/                    |           |
|    explained_variance     | 0.289     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.28e-05  |
|    n_updates              | 1224      |
|    policy_objective       | -3.24e-08 |
|    value_loss             | 0.184     |
-----------------------------------------
Ep done - 189000.
Eval num_timesteps=5166000, episode_reward=0.21 +/- 0.95
Episode length: 30.05 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.21     |
| time/                     |          |
|    total_timesteps        | 5166000  |
| train/                    |          |
|    explained_variance     | 0.376    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.28e-05 |
|    n_updates              | 1225     |
|    policy_objective       | 2.09e-07 |
|    value_loss             | 0.173    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.21
SELFPLAY: new best model, bumping up generation to 26
Ep done - 190000.
Ep done - 191000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.16     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 850      |
|    time_elapsed           | 21862    |
|    total_timesteps        | 5222400  |
| train/                    |          |
|    explained_variance     | 0.429    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000275 |
|    learning_rate          | 8.26e-05 |
|    n_updates              | 1234     |
|    policy_objective       | 1.33e+03 |
|    value_loss             | 0.175    |
----------------------------------------
Eval num_timesteps=5227500, episode_reward=0.21 +/- 0.96
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.21     |
| time/                     |          |
|    total_timesteps        | 5227500  |
| train/                    |          |
|    explained_variance     | 0.354    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.26e-05 |
|    n_updates              | 1235     |
|    policy_objective       | 5.34e-08 |
|    value_loss             | 0.186    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.21
SELFPLAY: new best model, bumping up generation to 27
Ep done - 192000.
Ep done - 193000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.06     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 860       |
|    time_elapsed           | 22120     |
|    total_timesteps        | 5283840   |
| train/                    |           |
|    explained_variance     | 0.402     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.24e-05  |
|    n_updates              | 1244      |
|    policy_objective       | -2.57e-07 |
|    value_loss             | 0.168     |
-----------------------------------------
Eval num_timesteps=5289000, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 5289000  |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.24e-05 |
|    n_updates              | 1245     |
|    policy_objective       | -3.8e-07 |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 194000.
Ep done - 195000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 870      |
|    time_elapsed           | 22377    |
|    total_timesteps        | 5345280  |
| train/                    |          |
|    explained_variance     | 0.396    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.22e-05 |
|    n_updates              | 1254     |
|    policy_objective       | -6.1e-07 |
|    value_loss             | 0.179    |
----------------------------------------
Eval num_timesteps=5350500, episode_reward=-0.03 +/- 0.97
Episode length: 29.95 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | -0.025   |
| time/                     |          |
|    total_timesteps        | 5350500  |
| train/                    |          |
|    explained_variance     | 0.382    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.22e-05 |
|    n_updates              | 1255     |
|    policy_objective       | 3.62e-07 |
|    value_loss             | 0.176    |
----------------------------------------
Ep done - 196000.
Ep done - 197000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.1       |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 880       |
|    time_elapsed           | 22635     |
|    total_timesteps        | 5406720   |
| train/                    |           |
|    explained_variance     | 0.366     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.2e-05   |
|    n_updates              | 1264      |
|    policy_objective       | -1.59e-07 |
|    value_loss             | 0.181     |
-----------------------------------------
Ep done - 198000.
Eval num_timesteps=5412000, episode_reward=0.15 +/- 0.97
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 5412000  |
| train/                    |          |
|    explained_variance     | 0.337    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.2e-05  |
|    n_updates              | 1265     |
|    policy_objective       | 3.48e-07 |
|    value_loss             | 0.175    |
----------------------------------------
Ep done - 199000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 890      |
|    time_elapsed           | 22889    |
|    total_timesteps        | 5468160  |
| train/                    |          |
|    explained_variance     | 0.318    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.18e-05 |
|    n_updates              | 1274     |
|    policy_objective       | 5.07e-07 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 200000.
Eval num_timesteps=5473500, episode_reward=0.10 +/- 0.98
Episode length: 29.92 +/- 1.33
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 5473500  |
| train/                    |          |
|    explained_variance     | 0.451    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.18e-05 |
|    n_updates              | 1275     |
|    policy_objective       | 7.64e-08 |
|    value_loss             | 0.155    |
----------------------------------------
Ep done - 201000.
Ep done - 202000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.02     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 900      |
|    time_elapsed           | 23147    |
|    total_timesteps        | 5529600  |
| train/                    |          |
|    explained_variance     | 0.324    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.16e-05 |
|    n_updates              | 1284     |
|    policy_objective       | 7.26e-08 |
|    value_loss             | 0.191    |
----------------------------------------
Eval num_timesteps=5535000, episode_reward=-0.01 +/- 0.99
Episode length: 29.99 +/- 0.46
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | -0.015    |
| time/                     |           |
|    total_timesteps        | 5535000   |
| train/                    |           |
|    explained_variance     | 0.422     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.16e-05  |
|    n_updates              | 1285      |
|    policy_objective       | -9.87e-08 |
|    value_loss             | 0.169     |
-----------------------------------------
Ep done - 203000.
Ep done - 204000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.11     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 910      |
|    time_elapsed           | 23404    |
|    total_timesteps        | 5591040  |
| train/                    |          |
|    explained_variance     | 0.424    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.14e-05 |
|    n_updates              | 1294     |
|    policy_objective       | 5.32e-07 |
|    value_loss             | 0.173    |
----------------------------------------
Eval num_timesteps=5596500, episode_reward=0.23 +/- 0.96
Episode length: 30.05 +/- 0.66
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.225     |
| time/                     |           |
|    total_timesteps        | 5596500   |
| train/                    |           |
|    explained_variance     | 0.411     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.14e-05  |
|    n_updates              | 1295      |
|    policy_objective       | -4.19e-08 |
|    value_loss             | 0.176     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 28
Ep done - 205000.
Ep done - 206000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 920      |
|    time_elapsed           | 23658    |
|    total_timesteps        | 5652480  |
| train/                    |          |
|    explained_variance     | 0.359    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.12e-05 |
|    n_updates              | 1304     |
|    policy_objective       | 8.44e-08 |
|    value_loss             | 0.17     |
----------------------------------------
Ep done - 207000.
Eval num_timesteps=5658000, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.04      |
| time/                     |           |
|    total_timesteps        | 5658000   |
| train/                    |           |
|    explained_variance     | 0.362     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.12e-05  |
|    n_updates              | 1305      |
|    policy_objective       | -2.59e-07 |
|    value_loss             | 0.197     |
-----------------------------------------
Ep done - 208000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.2     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 930      |
|    time_elapsed           | 23917    |
|    total_timesteps        | 5713920  |
| train/                    |          |
|    explained_variance     | 0.321    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.1e-05  |
|    n_updates              | 1314     |
|    policy_objective       | 3.35e-08 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 209000.
Eval num_timesteps=5719500, episode_reward=0.14 +/- 0.96
Episode length: 30.04 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 5719500  |
| train/                    |          |
|    explained_variance     | 0.438    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.1e-05  |
|    n_updates              | 1315     |
|    policy_objective       | 4.9e-08  |
|    value_loss             | 0.164    |
----------------------------------------
Ep done - 210000.
Ep done - 211000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.22    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 940      |
|    time_elapsed           | 24176    |
|    total_timesteps        | 5775360  |
| train/                    |          |
|    explained_variance     | 0.477    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.08e-05 |
|    n_updates              | 1324     |
|    policy_objective       | 4.9e-08  |
|    value_loss             | 0.161    |
----------------------------------------
Eval num_timesteps=5781000, episode_reward=0.15 +/- 0.97
Episode length: 30.07 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 5781000  |
| train/                    |          |
|    explained_variance     | 0.397    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.07e-05 |
|    n_updates              | 1325     |
|    policy_objective       | 3.08e-07 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 212000.
Ep done - 213000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.07     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 950      |
|    time_elapsed           | 24433    |
|    total_timesteps        | 5836800  |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.06e-05 |
|    n_updates              | 1334     |
|    policy_objective       | 2.62e-07 |
|    value_loss             | 0.171    |
----------------------------------------
Eval num_timesteps=5842500, episode_reward=0.17 +/- 0.98
Episode length: 30.03 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.165     |
| time/                     |           |
|    total_timesteps        | 5842500   |
| train/                    |           |
|    explained_variance     | 0.393     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.05e-05  |
|    n_updates              | 1335      |
|    policy_objective       | -3.74e-07 |
|    value_loss             | 0.186     |
-----------------------------------------
Ep done - 214000.
Ep done - 215000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.17     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 960       |
|    time_elapsed           | 24687     |
|    total_timesteps        | 5898240   |
| train/                    |           |
|    explained_variance     | 0.286     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.04e-05  |
|    n_updates              | 1344      |
|    policy_objective       | -2.85e-07 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 216000.
Eval num_timesteps=5904000, episode_reward=0.09 +/- 0.97
Episode length: 30.00 +/- 0.53
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.085     |
| time/                     |           |
|    total_timesteps        | 5904000   |
| train/                    |           |
|    explained_variance     | 0.312     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.03e-05  |
|    n_updates              | 1345      |
|    policy_objective       | -2.86e-08 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 217000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.04    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 970      |
|    time_elapsed           | 24944    |
|    total_timesteps        | 5959680  |
| train/                    |          |
|    explained_variance     | 0.34     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000291 |
|    learning_rate          | 8.02e-05 |
|    n_updates              | 1354     |
|    policy_objective       | 27.3     |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 218000.
Eval num_timesteps=5965500, episode_reward=0.07 +/- 0.99
Episode length: 30.04 +/- 0.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.065     |
| time/                     |           |
|    total_timesteps        | 5965500   |
| train/                    |           |
|    explained_variance     | 0.348     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.01e-05  |
|    n_updates              | 1355      |
|    policy_objective       | -4.22e-07 |
|    value_loss             | 0.177     |
-----------------------------------------
Ep done - 219000.
Ep done - 220000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.04     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 980       |
|    time_elapsed           | 25202     |
|    total_timesteps        | 6021120   |
| train/                    |           |
|    explained_variance     | 0.392     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8e-05     |
|    n_updates              | 1364      |
|    policy_objective       | -1.66e-07 |
|    value_loss             | 0.173     |
-----------------------------------------
Eval num_timesteps=6027000, episode_reward=0.27 +/- 0.95
Episode length: 30.02 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.265    |
| time/                     |          |
|    total_timesteps        | 6027000  |
| train/                    |          |
|    explained_variance     | 0.316    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.99e-05 |
|    n_updates              | 1365     |
|    policy_objective       | 1.14e-07 |
|    value_loss             | 0.197    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.265
SELFPLAY: new best model, bumping up generation to 29
Ep done - 221000.
Ep done - 222000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.04     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 990       |
|    time_elapsed           | 25461     |
|    total_timesteps        | 6082560   |
| train/                    |           |
|    explained_variance     | 0.381     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.97e-05  |
|    n_updates              | 1374      |
|    policy_objective       | -1.12e-07 |
|    value_loss             | 0.192     |
-----------------------------------------
Eval num_timesteps=6088500, episode_reward=0.15 +/- 0.98
Episode length: 30.00 +/- 0.64
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 6088500  |
| train/                    |          |
|    explained_variance     | 0.276    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.97e-05 |
|    n_updates              | 1375     |
|    policy_objective       | 5.15e-07 |
|    value_loss             | 0.189    |
----------------------------------------
Ep done - 223000.
Ep done - 224000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.21     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1000      |
|    time_elapsed           | 25715     |
|    total_timesteps        | 6144000   |
| train/                    |           |
|    explained_variance     | 0.273     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.95e-05  |
|    n_updates              | 1384      |
|    policy_objective       | -6.55e-08 |
|    value_loss             | 0.189     |
-----------------------------------------
Ep done - 225000.
Eval num_timesteps=6150000, episode_reward=-0.01 +/- 0.99
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | -0.015   |
| time/                     |          |
|    total_timesteps        | 6150000  |
| train/                    |          |
|    explained_variance     | 0.372    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.95e-05 |
|    n_updates              | 1385     |
|    policy_objective       | 6.83e-09 |
|    value_loss             | 0.163    |
----------------------------------------
Ep done - 226000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.06     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1010     |
|    time_elapsed           | 25974    |
|    total_timesteps        | 6205440  |
| train/                    |          |
|    explained_variance     | 0.407    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.93e-05 |
|    n_updates              | 1394     |
|    policy_objective       | 9.28e-08 |
|    value_loss             | 0.17     |
----------------------------------------
Ep done - 227000.
Eval num_timesteps=6211500, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.025    |
| time/                     |          |
|    total_timesteps        | 6211500  |
| train/                    |          |
|    explained_variance     | 0.336    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.93e-05 |
|    n_updates              | 1395     |
|    policy_objective       | 2.02e-07 |
|    value_loss             | 0.208    |
----------------------------------------
Ep done - 228000.
Ep done - 229000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.08     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1020     |
|    time_elapsed           | 26233    |
|    total_timesteps        | 6266880  |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.91e-05 |
|    n_updates              | 1404     |
|    policy_objective       | 2.71e-07 |
|    value_loss             | 0.164    |
----------------------------------------
Eval num_timesteps=6273000, episode_reward=0.10 +/- 0.97
Episode length: 29.86 +/- 1.42
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.105     |
| time/                     |           |
|    total_timesteps        | 6273000   |
| train/                    |           |
|    explained_variance     | 0.344     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.91e-05  |
|    n_updates              | 1405      |
|    policy_objective       | -1.78e-07 |
|    value_loss             | 0.199     |
-----------------------------------------
Ep done - 230000.
Ep done - 231000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.23     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1030     |
|    time_elapsed           | 26487    |
|    total_timesteps        | 6328320  |
| train/                    |          |
|    explained_variance     | 0.357    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.89e-05 |
|    n_updates              | 1414     |
|    policy_objective       | 1.15e-07 |
|    value_loss             | 0.19     |
----------------------------------------
Eval num_timesteps=6334500, episode_reward=0.10 +/- 0.97
Episode length: 30.05 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 6334500  |
| train/                    |          |
|    explained_variance     | 0.349    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.89e-05 |
|    n_updates              | 1416     |
|    policy_objective       | 2.53e-07 |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 232000.
Ep done - 233000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.07     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1040     |
|    time_elapsed           | 26746    |
|    total_timesteps        | 6389760  |
| train/                    |          |
|    explained_variance     | 0.405    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.87e-05 |
|    n_updates              | 1424     |
|    policy_objective       | 1.74e-08 |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 234000.
Eval num_timesteps=6396000, episode_reward=0.17 +/- 0.97
Episode length: 30.06 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 6396000  |
| train/                    |          |
|    explained_variance     | 0.421    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.87e-05 |
|    n_updates              | 1426     |
|    policy_objective       | 1.43e-07 |
|    value_loss             | 0.172    |
----------------------------------------
Ep done - 235000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.11     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1050     |
|    time_elapsed           | 27004    |
|    total_timesteps        | 6451200  |
| train/                    |          |
|    explained_variance     | 0.335    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.85e-05 |
|    n_updates              | 1434     |
|    policy_objective       | 9.06e-08 |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 236000.
Eval num_timesteps=6457500, episode_reward=0.06 +/- 0.99
Episode length: 30.05 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 6457500  |
| train/                    |          |
|    explained_variance     | 0.433    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.85e-05 |
|    n_updates              | 1436     |
|    policy_objective       | -1.6e-07 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 237000.
Ep done - 238000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.1      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1060      |
|    time_elapsed           | 27262     |
|    total_timesteps        | 6512640   |
| train/                    |           |
|    explained_variance     | 0.376     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.83e-05  |
|    n_updates              | 1444      |
|    policy_objective       | -1.26e-07 |
|    value_loss             | 0.178     |
-----------------------------------------
Eval num_timesteps=6519000, episode_reward=0.12 +/- 0.97
Episode length: 29.96 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 6519000  |
| train/                    |          |
|    explained_variance     | 0.383    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.83e-05 |
|    n_updates              | 1446     |
|    policy_objective       | 1.71e-07 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 239000.
Ep done - 240000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.08     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1070      |
|    time_elapsed           | 27516     |
|    total_timesteps        | 6574080   |
| train/                    |           |
|    explained_variance     | 0.347     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.81e-05  |
|    n_updates              | 1454      |
|    policy_objective       | -2.45e-07 |
|    value_loss             | 0.194     |
-----------------------------------------
Eval num_timesteps=6580500, episode_reward=0.25 +/- 0.96
Episode length: 30.07 +/- 0.50
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.25      |
| time/                     |           |
|    total_timesteps        | 6580500   |
| train/                    |           |
|    explained_variance     | 0.42      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.81e-05  |
|    n_updates              | 1456      |
|    policy_objective       | -3.07e-07 |
|    value_loss             | 0.183     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.25
SELFPLAY: new best model, bumping up generation to 30
Ep done - 241000.
Ep done - 242000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1080     |
|    time_elapsed           | 27775    |
|    total_timesteps        | 6635520  |
| train/                    |          |
|    explained_variance     | 0.399    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.79e-05 |
|    n_updates              | 1464     |
|    policy_objective       | 6.43e-08 |
|    value_loss             | 0.174    |
----------------------------------------
Ep done - 243000.
Eval num_timesteps=6642000, episode_reward=0.09 +/- 0.98
Episode length: 29.95 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.085    |
| time/                     |          |
|    total_timesteps        | 6642000  |
| train/                    |          |
|    explained_variance     | 0.348    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.79e-05 |
|    n_updates              | 1466     |
|    policy_objective       | 2.84e-07 |
|    value_loss             | 0.201    |
----------------------------------------
Ep done - 244000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.14    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1090     |
|    time_elapsed           | 28033    |
|    total_timesteps        | 6696960  |
| train/                    |          |
|    explained_variance     | 0.351    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.77e-05 |
|    n_updates              | 1474     |
|    policy_objective       | 3.7e-07  |
|    value_loss             | 0.187    |
----------------------------------------
Ep done - 245000.
Eval num_timesteps=6703500, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 6703500  |
| train/                    |          |
|    explained_variance     | 0.399    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.77e-05 |
|    n_updates              | 1476     |
|    policy_objective       | 1.76e-07 |
|    value_loss             | 0.172    |
----------------------------------------
Ep done - 246000.
Ep done - 247000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.1      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1100      |
|    time_elapsed           | 28292     |
|    total_timesteps        | 6758400   |
| train/                    |           |
|    explained_variance     | 0.379     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.75e-05  |
|    n_updates              | 1484      |
|    policy_objective       | -3.23e-08 |
|    value_loss             | 0.194     |
-----------------------------------------
Eval num_timesteps=6765000, episode_reward=0.08 +/- 0.99
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 6765000  |
| train/                    |          |
|    explained_variance     | 0.428    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.75e-05 |
|    n_updates              | 1486     |
|    policy_objective       | 1.47e-07 |
|    value_loss             | 0.17     |
----------------------------------------
Ep done - 248000.
Ep done - 249000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.02     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1110      |
|    time_elapsed           | 28545     |
|    total_timesteps        | 6819840   |
| train/                    |           |
|    explained_variance     | 0.363     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.73e-05  |
|    n_updates              | 1494      |
|    policy_objective       | -3.11e-07 |
|    value_loss             | 0.182     |
-----------------------------------------
Eval num_timesteps=6826500, episode_reward=0.18 +/- 0.98
Episode length: 30.05 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 6826500  |
| train/                    |          |
|    explained_variance     | 0.495    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.72e-05 |
|    n_updates              | 1496     |
|    policy_objective       | 3.48e-08 |
|    value_loss             | 0.161    |
----------------------------------------
Ep done - 250000.
Ep done - 251000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.05     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1120      |
|    time_elapsed           | 28803     |
|    total_timesteps        | 6881280   |
| train/                    |           |
|    explained_variance     | 0.38      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.71e-05  |
|    n_updates              | 1504      |
|    policy_objective       | -3.88e-07 |
|    value_loss             | 0.185     |
-----------------------------------------
Ep done - 252000.
Eval num_timesteps=6888000, episode_reward=0.20 +/- 0.97
Episode length: 29.93 +/- 1.31
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 6888000  |
| train/                    |          |
|    explained_variance     | 0.293    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.7e-05  |
|    n_updates              | 1506     |
|    policy_objective       | 1.3e-07  |
|    value_loss             | 0.205    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 31
Ep done - 253000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1130     |
|    time_elapsed           | 29060    |
|    total_timesteps        | 6942720  |
| train/                    |          |
|    explained_variance     | 0.395    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.69e-05 |
|    n_updates              | 1514     |
|    policy_objective       | 1.85e-07 |
|    value_loss             | 0.181    |
----------------------------------------
Ep done - 254000.
Eval num_timesteps=6949500, episode_reward=0.06 +/- 0.99
Episode length: 29.98 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 6949500  |
| train/                    |          |
|    explained_variance     | 0.373    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00017  |
|    learning_rate          | 7.68e-05 |
|    n_updates              | 1516     |
|    policy_objective       | 0.317    |
|    value_loss             | 0.194    |
----------------------------------------
Ep done - 255000.
Ep done - 256000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.1       |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1140      |
|    time_elapsed           | 29319     |
|    total_timesteps        | 7004160   |
| train/                    |           |
|    explained_variance     | 0.271     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.67e-05  |
|    n_updates              | 1524      |
|    policy_objective       | -2.48e-09 |
|    value_loss             | 0.19      |
-----------------------------------------
Eval num_timesteps=7011000, episode_reward=0.17 +/- 0.96
Episode length: 30.03 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 7011000  |
| train/                    |          |
|    explained_variance     | 0.364    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.66e-05 |
|    n_updates              | 1526     |
|    policy_objective       | 7.67e-08 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 257000.
Ep done - 258000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.01      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1150      |
|    time_elapsed           | 29572     |
|    total_timesteps        | 7065600   |
| train/                    |           |
|    explained_variance     | 0.453     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.65e-05  |
|    n_updates              | 1534      |
|    policy_objective       | -6.32e-07 |
|    value_loss             | 0.167     |
-----------------------------------------
Eval num_timesteps=7072500, episode_reward=0.12 +/- 0.97
Episode length: 30.07 +/- 0.63
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 7072500  |
| train/                    |          |
|    explained_variance     | 0.337    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.64e-05 |
|    n_updates              | 1536     |
|    policy_objective       | 3.43e-07 |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 259000.
Ep done - 260000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.15      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1160      |
|    time_elapsed           | 29830     |
|    total_timesteps        | 7127040   |
| train/                    |           |
|    explained_variance     | 0.315     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.63e-05  |
|    n_updates              | 1544      |
|    policy_objective       | -1.01e-07 |
|    value_loss             | 0.197     |
-----------------------------------------
Ep done - 261000.
Eval num_timesteps=7134000, episode_reward=0.21 +/- 0.97
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.215     |
| time/                     |           |
|    total_timesteps        | 7134000   |
| train/                    |           |
|    explained_variance     | 0.413     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.62e-05  |
|    n_updates              | 1546      |
|    policy_objective       | -8.44e-08 |
|    value_loss             | 0.168     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.215
SELFPLAY: new best model, bumping up generation to 32
Ep done - 262000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.12      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1170      |
|    time_elapsed           | 30088     |
|    total_timesteps        | 7188480   |
| train/                    |           |
|    explained_variance     | 0.362     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.61e-05  |
|    n_updates              | 1554      |
|    policy_objective       | -1.39e-07 |
|    value_loss             | 0.188     |
-----------------------------------------
Ep done - 263000.
Eval num_timesteps=7195500, episode_reward=0.11 +/- 0.98
Episode length: 29.93 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.11      |
| time/                     |           |
|    total_timesteps        | 7195500   |
| train/                    |           |
|    explained_variance     | 0.384     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.6e-05   |
|    n_updates              | 1556      |
|    policy_objective       | -4.47e-08 |
|    value_loss             | 0.179     |
-----------------------------------------
Ep done - 264000.
Ep done - 265000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1180     |
|    time_elapsed           | 30342    |
|    total_timesteps        | 7249920  |
| train/                    |          |
|    explained_variance     | 0.35     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.59e-05 |
|    n_updates              | 1564     |
|    policy_objective       | 1.65e-07 |
|    value_loss             | 0.195    |
----------------------------------------
Eval num_timesteps=7257000, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.58
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.135     |
| time/                     |           |
|    total_timesteps        | 7257000   |
| train/                    |           |
|    explained_variance     | 0.369     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.58e-05  |
|    n_updates              | 1566      |
|    policy_objective       | -1.95e-07 |
|    value_loss             | 0.179     |
-----------------------------------------
Ep done - 266000.
Ep done - 267000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.03      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1190      |
|    time_elapsed           | 30600     |
|    total_timesteps        | 7311360   |
| train/                    |           |
|    explained_variance     | 0.428     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.56e-05  |
|    n_updates              | 1574      |
|    policy_objective       | -1.43e-08 |
|    value_loss             | 0.178     |
-----------------------------------------
Eval num_timesteps=7318500, episode_reward=0.09 +/- 0.97
Episode length: 30.00 +/- 0.65
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 7318500  |
| train/                    |          |
|    explained_variance     | 0.293    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.56e-05 |
|    n_updates              | 1576     |
|    policy_objective       | 9.31e-08 |
|    value_loss             | 0.188    |
----------------------------------------
Ep done - 268000.
Ep done - 269000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.12    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1200     |
|    time_elapsed           | 30858    |
|    total_timesteps        | 7372800  |
| train/                    |          |
|    explained_variance     | 0.433    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.54e-05 |
|    n_updates              | 1584     |
|    policy_objective       | 7.64e-08 |
|    value_loss             | 0.171    |
----------------------------------------
Ep done - 270000.
Eval num_timesteps=7380000, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.48
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.08      |
| time/                     |           |
|    total_timesteps        | 7380000   |
| train/                    |           |
|    explained_variance     | 0.366     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.54e-05  |
|    n_updates              | 1586      |
|    policy_objective       | -1.53e-07 |
|    value_loss             | 0.179     |
-----------------------------------------
Ep done - 271000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.12     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1210     |
|    time_elapsed           | 31116    |
|    total_timesteps        | 7434240  |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.52e-05 |
|    n_updates              | 1594     |
|    policy_objective       | 2.3e-07  |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 272000.
Eval num_timesteps=7441500, episode_reward=0.11 +/- 0.97
Episode length: 30.02 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 7441500  |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.52e-05 |
|    n_updates              | 1596     |
|    policy_objective       | 5.49e-07 |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 273000.
Ep done - 274000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.05      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1220      |
|    time_elapsed           | 31369     |
|    total_timesteps        | 7495680   |
| train/                    |           |
|    explained_variance     | 0.387     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.5e-05   |
|    n_updates              | 1604      |
|    policy_objective       | -1.64e-07 |
|    value_loss             | 0.192     |
-----------------------------------------
Eval num_timesteps=7503000, episode_reward=0.10 +/- 0.99
Episode length: 30.04 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.095     |
| time/                     |           |
|    total_timesteps        | 7503000   |
| train/                    |           |
|    explained_variance     | 0.43      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.5e-05   |
|    n_updates              | 1606      |
|    policy_objective       | -2.18e-07 |
|    value_loss             | 0.182     |
-----------------------------------------
Ep done - 275000.
Ep done - 276000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.04    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1230     |
|    time_elapsed           | 31627    |
|    total_timesteps        | 7557120  |
| train/                    |          |
|    explained_variance     | 0.372    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.48e-05 |
|    n_updates              | 1614     |
|    policy_objective       | 5.46e-08 |
|    value_loss             | 0.181    |
----------------------------------------
Eval num_timesteps=7564500, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.65
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.06     |
| time/                     |          |
|    total_timesteps        | 7564500  |
| train/                    |          |
|    explained_variance     | 0.362    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.48e-05 |
|    n_updates              | 1616     |
|    policy_objective       | 3.07e-07 |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 277000.
Ep done - 278000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1240      |
|    time_elapsed           | 31884     |
|    total_timesteps        | 7618560   |
| train/                    |           |
|    explained_variance     | 0.434     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.46e-05  |
|    n_updates              | 1624      |
|    policy_objective       | -2.78e-07 |
|    value_loss             | 0.182     |
-----------------------------------------
Ep done - 279000.
Eval num_timesteps=7626000, episode_reward=0.20 +/- 0.97
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.2       |
| time/                     |           |
|    total_timesteps        | 7626000   |
| train/                    |           |
|    explained_variance     | 0.383     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.46e-05  |
|    n_updates              | 1626      |
|    policy_objective       | -3.72e-07 |
|    value_loss             | 0.179     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 33
Ep done - 280000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.09    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1250     |
|    time_elapsed           | 32142    |
|    total_timesteps        | 7680000  |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.44e-05 |
|    n_updates              | 1634     |
|    policy_objective       | 1.03e-07 |
|    value_loss             | 0.19     |
----------------------------------------
Ep done - 281000.
Eval num_timesteps=7687500, episode_reward=0.07 +/- 0.99
Episode length: 30.01 +/- 0.78
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.07      |
| time/                     |           |
|    total_timesteps        | 7687500   |
| train/                    |           |
|    explained_variance     | 0.336     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.44e-05  |
|    n_updates              | 1636      |
|    policy_objective       | -2.63e-07 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 282000.
Ep done - 283000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.07      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1260      |
|    time_elapsed           | 32396     |
|    total_timesteps        | 7741440   |
| train/                    |           |
|    explained_variance     | 0.351     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.42e-05  |
|    n_updates              | 1644      |
|    policy_objective       | -8.16e-08 |
|    value_loss             | 0.183     |
-----------------------------------------
Eval num_timesteps=7749000, episode_reward=0.20 +/- 0.96
Episode length: 30.03 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 7749000  |
| train/                    |          |
|    explained_variance     | 0.396    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.42e-05 |
|    n_updates              | 1646     |
|    policy_objective       | 2.41e-07 |
|    value_loss             | 0.18     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.205
SELFPLAY: new best model, bumping up generation to 34
Ep done - 284000.
Ep done - 285000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.05     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1270      |
|    time_elapsed           | 32654     |
|    total_timesteps        | 7802880   |
| train/                    |           |
|    explained_variance     | 0.326     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.4e-05   |
|    n_updates              | 1654      |
|    policy_objective       | -1.75e-07 |
|    value_loss             | 0.203     |
-----------------------------------------
Eval num_timesteps=7810500, episode_reward=0.23 +/- 0.96
Episode length: 29.98 +/- 0.89
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.225     |
| time/                     |           |
|    total_timesteps        | 7810500   |
| train/                    |           |
|    explained_variance     | 0.297     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.4e-05   |
|    n_updates              | 1656      |
|    policy_objective       | -1.85e-07 |
|    value_loss             | 0.203     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 35
Ep done - 286000.
Ep done - 287000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0         |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1280      |
|    time_elapsed           | 32912     |
|    total_timesteps        | 7864320   |
| train/                    |           |
|    explained_variance     | 0.336     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.38e-05  |
|    n_updates              | 1664      |
|    policy_objective       | -5.05e-07 |
|    value_loss             | 0.187     |
-----------------------------------------
Ep done - 288000.
Eval num_timesteps=7872000, episode_reward=0.23 +/- 0.96
Episode length: 29.98 +/- 0.89
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.235    |
| time/                     |          |
|    total_timesteps        | 7872000  |
| train/                    |          |
|    explained_variance     | 0.352    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.38e-05 |
|    n_updates              | 1666     |
|    policy_objective       | 9.34e-08 |
|    value_loss             | 0.193    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 36
Ep done - 289000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0         |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1290      |
|    time_elapsed           | 33165     |
|    total_timesteps        | 7925760   |
| train/                    |           |
|    explained_variance     | 0.394     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.36e-05  |
|    n_updates              | 1674      |
|    policy_objective       | -5.29e-07 |
|    value_loss             | 0.174     |
-----------------------------------------
Ep done - 290000.
Eval num_timesteps=7933500, episode_reward=0.20 +/- 0.97
Episode length: 29.97 +/- 0.90
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.195     |
| time/                     |           |
|    total_timesteps        | 7933500   |
| train/                    |           |
|    explained_variance     | 0.397     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.36e-05  |
|    n_updates              | 1676      |
|    policy_objective       | -6.27e-08 |
|    value_loss             | 0.177     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.195
SELFPLAY: new best model, bumping up generation to 37
Ep done - 291000.
Ep done - 292000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.02      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1300      |
|    time_elapsed           | 33423     |
|    total_timesteps        | 7987200   |
| train/                    |           |
|    explained_variance     | 0.42      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.34e-05  |
|    n_updates              | 1684      |
|    policy_objective       | -5.81e-07 |
|    value_loss             | 0.162     |
-----------------------------------------
Eval num_timesteps=7995000, episode_reward=0.23 +/- 0.96
Episode length: 29.98 +/- 0.89
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.235     |
| time/                     |           |
|    total_timesteps        | 7995000   |
| train/                    |           |
|    explained_variance     | 0.438     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.34e-05  |
|    n_updates              | 1686      |
|    policy_objective       | -1.41e-07 |
|    value_loss             | 0.168     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 38
Ep done - 293000.
Ep done - 294000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0         |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1310      |
|    time_elapsed           | 33681     |
|    total_timesteps        | 8048640   |
| train/                    |           |
|    explained_variance     | 0.395     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.32e-05  |
|    n_updates              | 1694      |
|    policy_objective       | -4.33e-07 |
|    value_loss             | 0.167     |
-----------------------------------------
Eval num_timesteps=8056500, episode_reward=0.20 +/- 0.96
Episode length: 29.96 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.195     |
| time/                     |           |
|    total_timesteps        | 8056500   |
| train/                    |           |
|    explained_variance     | 0.398     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.32e-05  |
|    n_updates              | 1696      |
|    policy_objective       | -1.19e-07 |
|    value_loss             | 0.186     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.195
SELFPLAY: new best model, bumping up generation to 39
Ep done - 295000.
Ep done - 296000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1320     |
|    time_elapsed           | 33939    |
|    total_timesteps        | 8110080  |
| train/                    |          |
|    explained_variance     | 0.297    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.3e-05  |
|    n_updates              | 1704     |
|    policy_objective       | -3.6e-07 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 297000.
Eval num_timesteps=8118000, episode_reward=0.15 +/- 0.97
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.15      |
| time/                     |           |
|    total_timesteps        | 8118000   |
| train/                    |           |
|    explained_variance     | 0.346     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.29e-05  |
|    n_updates              | 1706      |
|    policy_objective       | -8.72e-08 |
|    value_loss             | 0.183     |
-----------------------------------------
Ep done - 298000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.02     |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1330      |
|    time_elapsed           | 34193     |
|    total_timesteps        | 8171520   |
| train/                    |           |
|    explained_variance     | 0.316     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.28e-05  |
|    n_updates              | 1714      |
|    policy_objective       | -2.96e-07 |
|    value_loss             | 0.211     |
-----------------------------------------
Ep done - 299000.
Eval num_timesteps=8179500, episode_reward=0.10 +/- 0.97
Episode length: 29.98 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 8179500  |
| train/                    |          |
|    explained_variance     | 0.331    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.27e-05 |
|    n_updates              | 1716     |
|    policy_objective       | 8.34e-08 |
|    value_loss             | 0.19     |
----------------------------------------
Ep done - 300000.
Ep done - 301000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.01      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1340      |
|    time_elapsed           | 34451     |
|    total_timesteps        | 8232960   |
| train/                    |           |
|    explained_variance     | 0.511     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.26e-05  |
|    n_updates              | 1724      |
|    policy_objective       | -3.24e-07 |
|    value_loss             | 0.164     |
-----------------------------------------
Eval num_timesteps=8241000, episode_reward=0.12 +/- 0.98
Episode length: 30.04 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.12      |
| time/                     |           |
|    total_timesteps        | 8241000   |
| train/                    |           |
|    explained_variance     | 0.338     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.25e-05  |
|    n_updates              | 1726      |
|    policy_objective       | -6.37e-07 |
|    value_loss             | 0.203     |
-----------------------------------------
Ep done - 302000.
Ep done - 303000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1350     |
|    time_elapsed           | 34709    |
|    total_timesteps        | 8294400  |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.24e-05 |
|    n_updates              | 1734     |
|    policy_objective       | 1.01e-07 |
|    value_loss             | 0.194    |
----------------------------------------
Eval num_timesteps=8302500, episode_reward=0.22 +/- 0.95
Episode length: 30.00 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.22      |
| time/                     |           |
|    total_timesteps        | 8302500   |
| train/                    |           |
|    explained_variance     | 0.367     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.23e-05  |
|    n_updates              | 1736      |
|    policy_objective       | -9.93e-09 |
|    value_loss             | 0.209     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.22
SELFPLAY: new best model, bumping up generation to 40
Ep done - 304000.
Ep done - 305000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.14      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1360      |
|    time_elapsed           | 34968     |
|    total_timesteps        | 8355840   |
| train/                    |           |
|    explained_variance     | 0.308     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.22e-05  |
|    n_updates              | 1744      |
|    policy_objective       | -2.79e-08 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 306000.
Eval num_timesteps=8364000, episode_reward=0.01 +/- 0.99
Episode length: 30.02 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.015     |
| time/                     |           |
|    total_timesteps        | 8364000   |
| train/                    |           |
|    explained_variance     | 0.407     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.21e-05  |
|    n_updates              | 1746      |
|    policy_objective       | -3.06e-07 |
|    value_loss             | 0.176     |
-----------------------------------------
Ep done - 307000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1370     |
|    time_elapsed           | 35221    |
|    total_timesteps        | 8417280  |
| train/                    |          |
|    explained_variance     | 0.355    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.2e-05  |
|    n_updates              | 1754     |
|    policy_objective       | 5.53e-08 |
|    value_loss             | 0.194    |
----------------------------------------
Ep done - 308000.
Eval num_timesteps=8425500, episode_reward=0.05 +/- 0.98
Episode length: 30.00 +/- 0.91
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.05     |
| time/                     |          |
|    total_timesteps        | 8425500  |
| train/                    |          |
|    explained_variance     | 0.356    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.19e-05 |
|    n_updates              | 1756     |
|    policy_objective       | 1.49e-08 |
|    value_loss             | 0.175    |
----------------------------------------
Ep done - 309000.
Ep done - 310000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.15      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1380      |
|    time_elapsed           | 35478     |
|    total_timesteps        | 8478720   |
| train/                    |           |
|    explained_variance     | 0.433     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.18e-05  |
|    n_updates              | 1764      |
|    policy_objective       | -1.49e-08 |
|    value_loss             | 0.182     |
-----------------------------------------
Eval num_timesteps=8487000, episode_reward=0.10 +/- 0.97
Episode length: 30.02 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 8487000  |
| train/                    |          |
|    explained_variance     | 0.354    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000816 |
|    learning_rate          | 7.17e-05 |
|    n_updates              | 1766     |
|    policy_objective       | 62.5     |
|    value_loss             | 0.2      |
----------------------------------------
Ep done - 311000.
Ep done - 312000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1390     |
|    time_elapsed           | 35736    |
|    total_timesteps        | 8540160  |
| train/                    |          |
|    explained_variance     | 0.352    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.16e-05 |
|    n_updates              | 1774     |
|    policy_objective       | 1.18e-08 |
|    value_loss             | 0.201    |
----------------------------------------
Eval num_timesteps=8548500, episode_reward=0.18 +/- 0.95
Episode length: 30.04 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 8548500  |
| train/                    |          |
|    explained_variance     | 0.42     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.15e-05 |
|    n_updates              | 1776     |
|    policy_objective       | 2.47e-07 |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 313000.
Ep done - 314000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.15    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1400     |
|    time_elapsed           | 35994    |
|    total_timesteps        | 8601600  |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.13e-05 |
|    n_updates              | 1784     |
|    policy_objective       | -1.5e-07 |
|    value_loss             | 0.158    |
----------------------------------------
Ep done - 315000.
Eval num_timesteps=8610000, episode_reward=0.32 +/- 0.94
Episode length: 30.11 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.315    |
| time/                     |          |
|    total_timesteps        | 8610000  |
| train/                    |          |
|    explained_variance     | 0.453    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.13e-05 |
|    n_updates              | 1786     |
|    policy_objective       | 3.91e-07 |
|    value_loss             | 0.171    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.315
SELFPLAY: new best model, bumping up generation to 41
Ep done - 316000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.11    |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1410     |
|    time_elapsed           | 36248    |
|    total_timesteps        | 8663040  |
| train/                    |          |
|    explained_variance     | 0.329    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.11e-05 |
|    n_updates              | 1794     |
|    policy_objective       | 4.33e-07 |
|    value_loss             | 0.203    |
----------------------------------------
Ep done - 317000.
Eval num_timesteps=8671500, episode_reward=0.04 +/- 0.98
Episode length: 30.04 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.04     |
| time/                     |          |
|    total_timesteps        | 8671500  |
| train/                    |          |
|    explained_variance     | 0.44     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.11e-05 |
|    n_updates              | 1796     |
|    policy_objective       | 1.79e-07 |
|    value_loss             | 0.176    |
----------------------------------------
Ep done - 318000.
Ep done - 319000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.11      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1420      |
|    time_elapsed           | 36505     |
|    total_timesteps        | 8724480   |
| train/                    |           |
|    explained_variance     | 0.387     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.09e-05  |
|    n_updates              | 1804      |
|    policy_objective       | -9.41e-08 |
|    value_loss             | 0.195     |
-----------------------------------------
Eval num_timesteps=8733000, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.035     |
| time/                     |           |
|    total_timesteps        | 8733000   |
| train/                    |           |
|    explained_variance     | 0.411     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.09e-05  |
|    n_updates              | 1806      |
|    policy_objective       | -3.35e-08 |
|    value_loss             | 0.188     |
-----------------------------------------
Ep done - 320000.
Ep done - 321000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.14     |
| time/                     |          |
|    fps                    | 238      |
|    iterations             | 1430     |
|    time_elapsed           | 36763    |
|    total_timesteps        | 8785920  |
| train/                    |          |
|    explained_variance     | 0.327    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.07e-05 |
|    n_updates              | 1814     |
|    policy_objective       | 6.33e-07 |
|    value_loss             | 0.203    |
----------------------------------------
Eval num_timesteps=8794500, episode_reward=0.14 +/- 0.98
Episode length: 30.05 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 8794500  |
| train/                    |          |
|    explained_variance     | 0.371    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.07e-05 |
|    n_updates              | 1816     |
|    policy_objective       | 2.26e-07 |
|    value_loss             | 0.185    |
----------------------------------------
Ep done - 322000.
Ep done - 323000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.03     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1440      |
|    time_elapsed           | 37017     |
|    total_timesteps        | 8847360   |
| train/                    |           |
|    explained_variance     | 0.394     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.05e-05  |
|    n_updates              | 1824      |
|    policy_objective       | -1.73e-07 |
|    value_loss             | 0.188     |
-----------------------------------------
Ep done - 324000.
Eval num_timesteps=8856000, episode_reward=0.12 +/- 0.97
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 8856000  |
| train/                    |          |
|    explained_variance     | 0.402    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.05e-05 |
|    n_updates              | 1826     |
|    policy_objective       | 5.19e-07 |
|    value_loss             | 0.192    |
----------------------------------------
Ep done - 325000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0         |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1450      |
|    time_elapsed           | 37274     |
|    total_timesteps        | 8908800   |
| train/                    |           |
|    explained_variance     | 0.434     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.03e-05  |
|    n_updates              | 1834      |
|    policy_objective       | -1.01e-07 |
|    value_loss             | 0.175     |
-----------------------------------------
Ep done - 326000.
Eval num_timesteps=8917500, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 8917500  |
| train/                    |          |
|    explained_variance     | 0.339    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.03e-05 |
|    n_updates              | 1836     |
|    policy_objective       | 1.59e-08 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 327000.
Ep done - 328000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.12    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1460     |
|    time_elapsed           | 37531    |
|    total_timesteps        | 8970240  |
| train/                    |          |
|    explained_variance     | 0.39     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.01e-05 |
|    n_updates              | 1844     |
|    policy_objective       | 2.22e-07 |
|    value_loss             | 0.182    |
----------------------------------------
Eval num_timesteps=8979000, episode_reward=0.06 +/- 0.99
Episode length: 29.97 +/- 0.66
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 8979000  |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.01e-05 |
|    n_updates              | 1846     |
|    policy_objective       | 4.7e-07  |
|    value_loss             | 0.182    |
----------------------------------------
Ep done - 329000.
Ep done - 330000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.06      |
| time/                     |           |
|    fps                    | 238       |
|    iterations             | 1470      |
|    time_elapsed           | 37789     |
|    total_timesteps        | 9031680   |
| train/                    |           |
|    explained_variance     | 0.331     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.99e-05  |
|    n_updates              | 1854      |
|    policy_objective       | -1.35e-07 |
|    value_loss             | 0.183     |
-----------------------------------------
Eval num_timesteps=9040500, episode_reward=0.04 +/- 0.97
Episode length: 29.88 +/- 1.32
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.04      |
| time/                     |           |
|    total_timesteps        | 9040500   |
| train/                    |           |
|    explained_variance     | 0.399     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.99e-05  |
|    n_updates              | 1856      |
|    policy_objective       | -1.32e-07 |
|    value_loss             | 0.188     |
-----------------------------------------
Ep done - 331000.
Ep done - 332000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1480      |
|    time_elapsed           | 38042     |
|    total_timesteps        | 9093120   |
| train/                    |           |
|    explained_variance     | 0.365     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.97e-05  |
|    n_updates              | 1864      |
|    policy_objective       | -1.64e-07 |
|    value_loss             | 0.196     |
-----------------------------------------
Ep done - 333000.
Eval num_timesteps=9102000, episode_reward=0.17 +/- 0.97
Episode length: 30.01 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 9102000  |
| train/                    |          |
|    explained_variance     | 0.412    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.97e-05 |
|    n_updates              | 1866     |
|    policy_objective       | 3.1e-08  |
|    value_loss             | 0.187    |
----------------------------------------
Ep done - 334000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.18      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1490      |
|    time_elapsed           | 38300     |
|    total_timesteps        | 9154560   |
| train/                    |           |
|    explained_variance     | 0.421     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.95e-05  |
|    n_updates              | 1874      |
|    policy_objective       | -2.17e-08 |
|    value_loss             | 0.179     |
-----------------------------------------
Ep done - 335000.
Eval num_timesteps=9163500, episode_reward=0.13 +/- 0.98
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 9163500  |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.95e-05 |
|    n_updates              | 1876     |
|    policy_objective       | 1.84e-07 |
|    value_loss             | 0.198    |
----------------------------------------
Ep done - 336000.
Ep done - 337000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.11    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1500     |
|    time_elapsed           | 38558    |
|    total_timesteps        | 9216000  |
| train/                    |          |
|    explained_variance     | 0.414    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.93e-05 |
|    n_updates              | 1884     |
|    policy_objective       | 2.07e-07 |
|    value_loss             | 0.19     |
----------------------------------------
Eval num_timesteps=9225000, episode_reward=0.20 +/- 0.96
Episode length: 30.00 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 9225000  |
| train/                    |          |
|    explained_variance     | 0.365    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00018  |
|    learning_rate          | 6.93e-05 |
|    n_updates              | 1886     |
|    policy_objective       | 248      |
|    value_loss             | 0.198    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 42
Ep done - 338000.
Ep done - 339000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1510      |
|    time_elapsed           | 38816     |
|    total_timesteps        | 9277440   |
| train/                    |           |
|    explained_variance     | 0.33      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.91e-05  |
|    n_updates              | 1894      |
|    policy_objective       | -3.43e-07 |
|    value_loss             | 0.201     |
-----------------------------------------
Eval num_timesteps=9286500, episode_reward=0.30 +/- 0.93
Episode length: 30.09 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.305    |
| time/                     |          |
|    total_timesteps        | 9286500  |
| train/                    |          |
|    explained_variance     | 0.385    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.91e-05 |
|    n_updates              | 1896     |
|    policy_objective       | 3.65e-07 |
|    value_loss             | 0.2      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.305
SELFPLAY: new best model, bumping up generation to 43
Ep done - 340000.
Ep done - 341000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1520      |
|    time_elapsed           | 39069     |
|    total_timesteps        | 9338880   |
| train/                    |           |
|    explained_variance     | 0.4       |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.89e-05  |
|    n_updates              | 1904      |
|    policy_objective       | -3.29e-08 |
|    value_loss             | 0.179     |
-----------------------------------------
Ep done - 342000.
Eval num_timesteps=9348000, episode_reward=0.11 +/- 0.98
Episode length: 30.10 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 9348000  |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.88e-05 |
|    n_updates              | 1906     |
|    policy_objective       | 2.35e-07 |
|    value_loss             | 0.187    |
----------------------------------------
Ep done - 343000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1530     |
|    time_elapsed           | 39327    |
|    total_timesteps        | 9400320  |
| train/                    |          |
|    explained_variance     | 0.357    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.87e-05 |
|    n_updates              | 1914     |
|    policy_objective       | 2.8e-07  |
|    value_loss             | 0.204    |
----------------------------------------
Ep done - 344000.
Eval num_timesteps=9409500, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.035     |
| time/                     |           |
|    total_timesteps        | 9409500   |
| train/                    |           |
|    explained_variance     | 0.347     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.86e-05  |
|    n_updates              | 1916      |
|    policy_objective       | -1.79e-09 |
|    value_loss             | 0.192     |
-----------------------------------------
Ep done - 345000.
Ep done - 346000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.05     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1540      |
|    time_elapsed           | 39586     |
|    total_timesteps        | 9461760   |
| train/                    |           |
|    explained_variance     | 0.455     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.85e-05  |
|    n_updates              | 1924      |
|    policy_objective       | -6.21e-09 |
|    value_loss             | 0.182     |
-----------------------------------------
Eval num_timesteps=9471000, episode_reward=0.24 +/- 0.96
Episode length: 30.05 +/- 0.61
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.24      |
| time/                     |           |
|    total_timesteps        | 9471000   |
| train/                    |           |
|    explained_variance     | 0.386     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.84e-05  |
|    n_updates              | 1926      |
|    policy_objective       | -7.45e-08 |
|    value_loss             | 0.199     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.24
SELFPLAY: new best model, bumping up generation to 44
Ep done - 347000.
Ep done - 348000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1550     |
|    time_elapsed           | 39842    |
|    total_timesteps        | 9523200  |
| train/                    |          |
|    explained_variance     | 0.409    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.83e-05 |
|    n_updates              | 1934     |
|    policy_objective       | 3.1e-09  |
|    value_loss             | 0.186    |
----------------------------------------
Eval num_timesteps=9532500, episode_reward=0.28 +/- 0.95
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.275    |
| time/                     |          |
|    total_timesteps        | 9532500  |
| train/                    |          |
|    explained_variance     | 0.397    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.82e-05 |
|    n_updates              | 1936     |
|    policy_objective       | 1.04e-07 |
|    value_loss             | 0.202    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 45
Ep done - 349000.
Ep done - 350000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.1      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1560      |
|    time_elapsed           | 40098     |
|    total_timesteps        | 9584640   |
| train/                    |           |
|    explained_variance     | 0.393     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.81e-05  |
|    n_updates              | 1944      |
|    policy_objective       | -9.27e-08 |
|    value_loss             | 0.189     |
-----------------------------------------
Ep done - 351000.
Eval num_timesteps=9594000, episode_reward=0.10 +/- 0.99
Episode length: 30.05 +/- 0.62
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 9594000  |
| train/                    |          |
|    explained_variance     | 0.382    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.8e-05  |
|    n_updates              | 1946     |
|    policy_objective       | 5.04e-07 |
|    value_loss             | 0.196    |
----------------------------------------
Ep done - 352000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.15     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1570     |
|    time_elapsed           | 40356    |
|    total_timesteps        | 9646080  |
| train/                    |          |
|    explained_variance     | 0.352    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000175 |
|    learning_rate          | 6.79e-05 |
|    n_updates              | 1954     |
|    policy_objective       | 0.74     |
|    value_loss             | 0.2      |
----------------------------------------
Ep done - 353000.
Eval num_timesteps=9655500, episode_reward=0.14 +/- 0.96
Episode length: 29.98 +/- 1.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 9655500  |
| train/                    |          |
|    explained_variance     | 0.412    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.78e-05 |
|    n_updates              | 1956     |
|    policy_objective       | 2.67e-08 |
|    value_loss             | 0.191    |
----------------------------------------
Ep done - 354000.
Ep done - 355000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.06      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1580      |
|    time_elapsed           | 40614     |
|    total_timesteps        | 9707520   |
| train/                    |           |
|    explained_variance     | 0.433     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.77e-05  |
|    n_updates              | 1964      |
|    policy_objective       | -1.24e-09 |
|    value_loss             | 0.182     |
-----------------------------------------
Eval num_timesteps=9717000, episode_reward=0.06 +/- 0.98
Episode length: 30.05 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 9717000  |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.76e-05 |
|    n_updates              | 1966     |
|    policy_objective       | 7.7e-08  |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 356000.
Ep done - 357000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.11     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1590     |
|    time_elapsed           | 40868    |
|    total_timesteps        | 9768960  |
| train/                    |          |
|    explained_variance     | 0.353    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00017  |
|    learning_rate          | 6.75e-05 |
|    n_updates              | 1974     |
|    policy_objective       | 0.363    |
|    value_loss             | 0.192    |
----------------------------------------
Eval num_timesteps=9778500, episode_reward=0.15 +/- 0.96
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.15      |
| time/                     |           |
|    total_timesteps        | 9778500   |
| train/                    |           |
|    explained_variance     | 0.355     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.74e-05  |
|    n_updates              | 1976      |
|    policy_objective       | -1.42e-07 |
|    value_loss             | 0.193     |
-----------------------------------------
Ep done - 358000.
Ep done - 359000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1600     |
|    time_elapsed           | 41126    |
|    total_timesteps        | 9830400  |
| train/                    |          |
|    explained_variance     | 0.372    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.73e-05 |
|    n_updates              | 1984     |
|    policy_objective       | 3.51e-08 |
|    value_loss             | 0.199    |
----------------------------------------
Ep done - 360000.
Eval num_timesteps=9840000, episode_reward=0.03 +/- 0.99
Episode length: 30.06 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.025    |
| time/                     |          |
|    total_timesteps        | 9840000  |
| train/                    |          |
|    explained_variance     | 0.467    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.72e-05 |
|    n_updates              | 1986     |
|    policy_objective       | 0        |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 361000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1610     |
|    time_elapsed           | 41384    |
|    total_timesteps        | 9891840  |
| train/                    |          |
|    explained_variance     | 0.346    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.7e-05  |
|    n_updates              | 1994     |
|    policy_objective       | 3.23e-07 |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 362000.
Eval num_timesteps=9901500, episode_reward=0.14 +/- 0.98
Episode length: 30.04 +/- 0.70
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.135     |
| time/                     |           |
|    total_timesteps        | 9901500   |
| train/                    |           |
|    explained_variance     | 0.442     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.7e-05   |
|    n_updates              | 1996      |
|    policy_objective       | -6.16e-07 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 363000.
Ep done - 364000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1620     |
|    time_elapsed           | 41643    |
|    total_timesteps        | 9953280  |
| train/                    |          |
|    explained_variance     | 0.339    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.68e-05 |
|    n_updates              | 2004     |
|    policy_objective       | 4.22e-08 |
|    value_loss             | 0.192    |
----------------------------------------
Eval num_timesteps=9963000, episode_reward=0.03 +/- 0.98
Episode length: 30.02 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.025     |
| time/                     |           |
|    total_timesteps        | 9963000   |
| train/                    |           |
|    explained_variance     | 0.434     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.68e-05  |
|    n_updates              | 2006      |
|    policy_objective       | -1.45e-07 |
|    value_loss             | 0.184     |
-----------------------------------------
Ep done - 365000.
Ep done - 366000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1630     |
|    time_elapsed           | 41896    |
|    total_timesteps        | 10014720 |
| train/                    |          |
|    explained_variance     | 0.414    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.66e-05 |
|    n_updates              | 2014     |
|    policy_objective       | 1.24e-07 |
|    value_loss             | 0.191    |
----------------------------------------
Eval num_timesteps=10024500, episode_reward=0.05 +/- 0.99
Episode length: 30.07 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.05      |
| time/                     |           |
|    total_timesteps        | 10024500  |
| train/                    |           |
|    explained_variance     | 0.405     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.66e-05  |
|    n_updates              | 2016      |
|    policy_objective       | -2.62e-07 |
|    value_loss             | 0.187     |
-----------------------------------------
Ep done - 367000.
Ep done - 368000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.05      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1640      |
|    time_elapsed           | 42155     |
|    total_timesteps        | 10076160  |
| train/                    |           |
|    explained_variance     | 0.362     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.64e-05  |
|    n_updates              | 2024      |
|    policy_objective       | -1.07e-07 |
|    value_loss             | 0.199     |
-----------------------------------------
Ep done - 369000.
Eval num_timesteps=10086000, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.13      |
| time/                     |           |
|    total_timesteps        | 10086000  |
| train/                    |           |
|    explained_variance     | 0.345     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.64e-05  |
|    n_updates              | 2026      |
|    policy_objective       | -2.15e-07 |
|    value_loss             | 0.202     |
-----------------------------------------
Ep done - 370000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.07      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1650      |
|    time_elapsed           | 42412     |
|    total_timesteps        | 10137600  |
| train/                    |           |
|    explained_variance     | 0.427     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.62e-05  |
|    n_updates              | 2034      |
|    policy_objective       | -1.22e-07 |
|    value_loss             | 0.186     |
-----------------------------------------
Ep done - 371000.
Eval num_timesteps=10147500, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.065    |
| time/                     |          |
|    total_timesteps        | 10147500 |
| train/                    |          |
|    explained_variance     | 0.4      |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.62e-05 |
|    n_updates              | 2036     |
|    policy_objective       | 1.87e-07 |
|    value_loss             | 0.187    |
----------------------------------------
Ep done - 372000.
Ep done - 373000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.03     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1660      |
|    time_elapsed           | 42671     |
|    total_timesteps        | 10199040  |
| train/                    |           |
|    explained_variance     | 0.342     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.6e-05   |
|    n_updates              | 2044      |
|    policy_objective       | -2.31e-07 |
|    value_loss             | 0.197     |
-----------------------------------------
Eval num_timesteps=10209000, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.125     |
| time/                     |           |
|    total_timesteps        | 10209000  |
| train/                    |           |
|    explained_variance     | 0.446     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.6e-05   |
|    n_updates              | 2046      |
|    policy_objective       | -4.27e-07 |
|    value_loss             | 0.176     |
-----------------------------------------
Ep done - 374000.
Ep done - 375000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.17    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1670     |
|    time_elapsed           | 42924    |
|    total_timesteps        | 10260480 |
| train/                    |          |
|    explained_variance     | 0.376    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.58e-05 |
|    n_updates              | 2054     |
|    policy_objective       | 8.69e-08 |
|    value_loss             | 0.19     |
----------------------------------------
Eval num_timesteps=10270500, episode_reward=0.21 +/- 0.96
Episode length: 29.96 +/- 1.04
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.215     |
| time/                     |           |
|    total_timesteps        | 10270500  |
| train/                    |           |
|    explained_variance     | 0.387     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.58e-05  |
|    n_updates              | 2056      |
|    policy_objective       | -5.28e-09 |
|    value_loss             | 0.193     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.215
SELFPLAY: new best model, bumping up generation to 46
Ep done - 376000.
Ep done - 377000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1680     |
|    time_elapsed           | 43182    |
|    total_timesteps        | 10321920 |
| train/                    |          |
|    explained_variance     | 0.382    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.56e-05 |
|    n_updates              | 2064     |
|    policy_objective       | 2.49e-07 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 378000.
Eval num_timesteps=10332000, episode_reward=0.07 +/- 0.99
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.065     |
| time/                     |           |
|    total_timesteps        | 10332000  |
| train/                    |           |
|    explained_variance     | 0.351     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.56e-05  |
|    n_updates              | 2066      |
|    policy_objective       | -7.98e-08 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 379000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1690      |
|    time_elapsed           | 43440     |
|    total_timesteps        | 10383360  |
| train/                    |           |
|    explained_variance     | 0.441     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.54e-05  |
|    n_updates              | 2074      |
|    policy_objective       | -3.63e-08 |
|    value_loss             | 0.181     |
-----------------------------------------
Ep done - 380000.
Eval num_timesteps=10393500, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 10393500 |
| train/                    |          |
|    explained_variance     | 0.337    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.54e-05 |
|    n_updates              | 2076     |
|    policy_objective       | 2.66e-07 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 381000.
Ep done - 382000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.01      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1700      |
|    time_elapsed           | 43696     |
|    total_timesteps        | 10444800  |
| train/                    |           |
|    explained_variance     | 0.379     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.52e-05  |
|    n_updates              | 2084      |
|    policy_objective       | -5.25e-08 |
|    value_loss             | 0.189     |
-----------------------------------------
Eval num_timesteps=10455000, episode_reward=0.07 +/- 0.99
Episode length: 30.05 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.065    |
| time/                     |          |
|    total_timesteps        | 10455000 |
| train/                    |          |
|    explained_variance     | 0.428    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000172 |
|    learning_rate          | 6.52e-05 |
|    n_updates              | 2086     |
|    policy_objective       | 0.176    |
|    value_loss             | 0.187    |
----------------------------------------
Ep done - 383000.
Ep done - 384000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1710     |
|    time_elapsed           | 43952    |
|    total_timesteps        | 10506240 |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.5e-05  |
|    n_updates              | 2094     |
|    policy_objective       | 2.44e-07 |
|    value_loss             | 0.181    |
----------------------------------------
Eval num_timesteps=10516500, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.58
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.14      |
| time/                     |           |
|    total_timesteps        | 10516500  |
| train/                    |           |
|    explained_variance     | 0.429     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.5e-05   |
|    n_updates              | 2096      |
|    policy_objective       | -2.68e-07 |
|    value_loss             | 0.169     |
-----------------------------------------
Ep done - 385000.
Ep done - 386000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.22     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1720      |
|    time_elapsed           | 44210     |
|    total_timesteps        | 10567680  |
| train/                    |           |
|    explained_variance     | 0.325     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.48e-05  |
|    n_updates              | 2104      |
|    policy_objective       | -1.59e-07 |
|    value_loss             | 0.207     |
-----------------------------------------
Ep done - 387000.
Eval num_timesteps=10578000, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.47
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.14      |
| time/                     |           |
|    total_timesteps        | 10578000  |
| train/                    |           |
|    explained_variance     | 0.413     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.48e-05  |
|    n_updates              | 2106      |
|    policy_objective       | -5.25e-08 |
|    value_loss             | 0.188     |
-----------------------------------------
Ep done - 388000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.12     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1730     |
|    time_elapsed           | 44469    |
|    total_timesteps        | 10629120 |
| train/                    |          |
|    explained_variance     | 0.461    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.46e-05 |
|    n_updates              | 2114     |
|    policy_objective       | -3.5e-07 |
|    value_loss             | 0.174    |
----------------------------------------
Ep done - 389000.
Eval num_timesteps=10639500, episode_reward=0.14 +/- 0.96
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 10639500 |
| train/                    |          |
|    explained_variance     | 0.381    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.45e-05 |
|    n_updates              | 2116     |
|    policy_objective       | 6.02e-08 |
|    value_loss             | 0.181    |
----------------------------------------
Ep done - 390000.
Ep done - 391000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.11    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1740     |
|    time_elapsed           | 44723    |
|    total_timesteps        | 10690560 |
| train/                    |          |
|    explained_variance     | 0.401    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.44e-05 |
|    n_updates              | 2124     |
|    policy_objective       | -6e-07   |
|    value_loss             | 0.188    |
----------------------------------------
Eval num_timesteps=10701000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 1.07
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.115     |
| time/                     |           |
|    total_timesteps        | 10701000  |
| train/                    |           |
|    explained_variance     | 0.428     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.43e-05  |
|    n_updates              | 2126      |
|    policy_objective       | -8.07e-08 |
|    value_loss             | 0.196     |
-----------------------------------------
Ep done - 392000.
Ep done - 393000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1750     |
|    time_elapsed           | 44981    |
|    total_timesteps        | 10752000 |
| train/                    |          |
|    explained_variance     | 0.444    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.42e-05 |
|    n_updates              | 2134     |
|    policy_objective       | 9.44e-08 |
|    value_loss             | 0.181    |
----------------------------------------
Eval num_timesteps=10762500, episode_reward=0.12 +/- 0.97
Episode length: 30.03 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.12      |
| time/                     |           |
|    total_timesteps        | 10762500  |
| train/                    |           |
|    explained_variance     | 0.423     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.41e-05  |
|    n_updates              | 2136      |
|    policy_objective       | -4.77e-07 |
|    value_loss             | 0.191     |
-----------------------------------------
Ep done - 394000.
Ep done - 395000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1760     |
|    time_elapsed           | 45239    |
|    total_timesteps        | 10813440 |
| train/                    |          |
|    explained_variance     | 0.376    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.4e-05  |
|    n_updates              | 2144     |
|    policy_objective       | 4.54e-07 |
|    value_loss             | 0.203    |
----------------------------------------
Ep done - 396000.
Eval num_timesteps=10824000, episode_reward=0.23 +/- 0.95
Episode length: 30.02 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.235    |
| time/                     |          |
|    total_timesteps        | 10824000 |
| train/                    |          |
|    explained_variance     | 0.384    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.39e-05 |
|    n_updates              | 2146     |
|    policy_objective       | 3.23e-07 |
|    value_loss             | 0.198    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 47
Ep done - 397000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | -0.17    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1770     |
|    time_elapsed           | 45498    |
|    total_timesteps        | 10874880 |
| train/                    |          |
|    explained_variance     | 0.387    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.38e-05 |
|    n_updates              | 2154     |
|    policy_objective       | 3.28e-07 |
|    value_loss             | 0.188    |
----------------------------------------
Ep done - 398000.
Eval num_timesteps=10885500, episode_reward=0.16 +/- 0.97
Episode length: 30.04 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 10885500 |
| train/                    |          |
|    explained_variance     | 0.372    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.37e-05 |
|    n_updates              | 2156     |
|    policy_objective       | 4.17e-07 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 399000.
Ep done - 400000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1780     |
|    time_elapsed           | 45751    |
|    total_timesteps        | 10936320 |
| train/                    |          |
|    explained_variance     | 0.403    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.36e-05 |
|    n_updates              | 2164     |
|    policy_objective       | 2.44e-07 |
|    value_loss             | 0.199    |
----------------------------------------
Eval num_timesteps=10947000, episode_reward=0.15 +/- 0.97
Episode length: 29.93 +/- 1.36
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.155     |
| time/                     |           |
|    total_timesteps        | 10947000  |
| train/                    |           |
|    explained_variance     | 0.363     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.35e-05  |
|    n_updates              | 2166      |
|    policy_objective       | -8.94e-08 |
|    value_loss             | 0.197     |
-----------------------------------------
Ep done - 401000.
Ep done - 402000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1790     |
|    time_elapsed           | 46010    |
|    total_timesteps        | 10997760 |
| train/                    |          |
|    explained_variance     | 0.394    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.34e-05 |
|    n_updates              | 2174     |
|    policy_objective       | 2.06e-07 |
|    value_loss             | 0.192    |
----------------------------------------
Eval num_timesteps=11008500, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.075     |
| time/                     |           |
|    total_timesteps        | 11008500  |
| train/                    |           |
|    explained_variance     | 0.387     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.33e-05  |
|    n_updates              | 2176      |
|    policy_objective       | -1.56e-07 |
|    value_loss             | 0.205     |
-----------------------------------------
Ep done - 403000.
Ep done - 404000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1800      |
|    time_elapsed           | 46268     |
|    total_timesteps        | 11059200  |
| train/                    |           |
|    explained_variance     | 0.346     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.32e-05  |
|    n_updates              | 2184      |
|    policy_objective       | -5.85e-08 |
|    value_loss             | 0.205     |
-----------------------------------------
Ep done - 405000.
Eval num_timesteps=11070000, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 11070000 |
| train/                    |          |
|    explained_variance     | 0.38     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.31e-05 |
|    n_updates              | 2186     |
|    policy_objective       | 1.49e-08 |
|    value_loss             | 0.182    |
----------------------------------------
Ep done - 406000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1810     |
|    time_elapsed           | 46527    |
|    total_timesteps        | 11120640 |
| train/                    |          |
|    explained_variance     | 0.451    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.3e-05  |
|    n_updates              | 2194     |
|    policy_objective       | 9.59e-08 |
|    value_loss             | 0.178    |
----------------------------------------
Ep done - 407000.
Eval num_timesteps=11131500, episode_reward=0.10 +/- 0.98
Episode length: 30.01 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 11131500 |
| train/                    |          |
|    explained_variance     | 0.392    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.29e-05 |
|    n_updates              | 2196     |
|    policy_objective       | 3.4e-07  |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 408000.
Ep done - 409000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.15     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1820      |
|    time_elapsed           | 46780     |
|    total_timesteps        | 11182080  |
| train/                    |           |
|    explained_variance     | 0.454     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.27e-05  |
|    n_updates              | 2204      |
|    policy_objective       | -2.73e-07 |
|    value_loss             | 0.18      |
-----------------------------------------
Eval num_timesteps=11193000, episode_reward=0.06 +/- 0.99
Episode length: 30.04 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.06     |
| time/                     |          |
|    total_timesteps        | 11193000 |
| train/                    |          |
|    explained_variance     | 0.436    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.27e-05 |
|    n_updates              | 2206     |
|    policy_objective       | 2.79e-07 |
|    value_loss             | 0.186    |
----------------------------------------
Ep done - 410000.
Ep done - 411000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.09     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1830     |
|    time_elapsed           | 47037    |
|    total_timesteps        | 11243520 |
| train/                    |          |
|    explained_variance     | 0.398    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.25e-05 |
|    n_updates              | 2214     |
|    policy_objective       | 1.27e-07 |
|    value_loss             | 0.189    |
----------------------------------------
Eval num_timesteps=11254500, episode_reward=0.09 +/- 0.99
Episode length: 30.02 +/- 0.47
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.09      |
| time/                     |           |
|    total_timesteps        | 11254500  |
| train/                    |           |
|    explained_variance     | 0.419     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.25e-05  |
|    n_updates              | 2216      |
|    policy_objective       | -1.59e-07 |
|    value_loss             | 0.186     |
-----------------------------------------
Ep done - 412000.
Ep done - 413000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.11     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1840     |
|    time_elapsed           | 47296    |
|    total_timesteps        | 11304960 |
| train/                    |          |
|    explained_variance     | 0.397    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.23e-05 |
|    n_updates              | 2224     |
|    policy_objective       | -2.6e-07 |
|    value_loss             | 0.192    |
----------------------------------------
Ep done - 414000.
Eval num_timesteps=11316000, episode_reward=0.14 +/- 0.98
Episode length: 30.07 +/- 0.60
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.145     |
| time/                     |           |
|    total_timesteps        | 11316000  |
| train/                    |           |
|    explained_variance     | 0.367     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.23e-05  |
|    n_updates              | 2226      |
|    policy_objective       | -8.91e-08 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 415000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.09     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1850      |
|    time_elapsed           | 47550     |
|    total_timesteps        | 11366400  |
| train/                    |           |
|    explained_variance     | 0.416     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.21e-05  |
|    n_updates              | 2234      |
|    policy_objective       | -5.25e-08 |
|    value_loss             | 0.198     |
-----------------------------------------
Ep done - 416000.
Eval num_timesteps=11377500, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.14      |
| time/                     |           |
|    total_timesteps        | 11377500  |
| train/                    |           |
|    explained_variance     | 0.381     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.21e-05  |
|    n_updates              | 2236      |
|    policy_objective       | -1.02e-07 |
|    value_loss             | 0.205     |
-----------------------------------------
Ep done - 417000.
Ep done - 418000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1860     |
|    time_elapsed           | 47808    |
|    total_timesteps        | 11427840 |
| train/                    |          |
|    explained_variance     | 0.35     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.19e-05 |
|    n_updates              | 2244     |
|    policy_objective       | 4.74e-07 |
|    value_loss             | 0.206    |
----------------------------------------
Eval num_timesteps=11439000, episode_reward=0.04 +/- 0.97
Episode length: 30.04 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.04      |
| time/                     |           |
|    total_timesteps        | 11439000  |
| train/                    |           |
|    explained_variance     | 0.412     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.19e-05  |
|    n_updates              | 2246      |
|    policy_objective       | -8.32e-08 |
|    value_loss             | 0.178     |
-----------------------------------------
Ep done - 419000.
Ep done - 420000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1870      |
|    time_elapsed           | 48066     |
|    total_timesteps        | 11489280  |
| train/                    |           |
|    explained_variance     | 0.428     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.17e-05  |
|    n_updates              | 2254      |
|    policy_objective       | -2.73e-08 |
|    value_loss             | 0.2       |
-----------------------------------------
Eval num_timesteps=11500500, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.065    |
| time/                     |          |
|    total_timesteps        | 11500500 |
| train/                    |          |
|    explained_variance     | 0.496    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.17e-05 |
|    n_updates              | 2256     |
|    policy_objective       | 3.69e-07 |
|    value_loss             | 0.174    |
----------------------------------------
Ep done - 421000.
Ep done - 422000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.8      |
|    ep_rew_mean            | -0.03     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1880      |
|    time_elapsed           | 48325     |
|    total_timesteps        | 11550720  |
| train/                    |           |
|    explained_variance     | 0.428     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.15e-05  |
|    n_updates              | 2264      |
|    policy_objective       | -2.47e-07 |
|    value_loss             | 0.191     |
-----------------------------------------
Ep done - 423000.
Eval num_timesteps=11562000, episode_reward=0.02 +/- 0.97
Episode length: 30.05 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.02     |
| time/                     |          |
|    total_timesteps        | 11562000 |
| train/                    |          |
|    explained_variance     | 0.442    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000184 |
|    learning_rate          | 6.15e-05 |
|    n_updates              | 2266     |
|    policy_objective       | 1.13e+03 |
|    value_loss             | 0.178    |
----------------------------------------
Ep done - 424000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.02     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1890     |
|    time_elapsed           | 48579    |
|    total_timesteps        | 11612160 |
| train/                    |          |
|    explained_variance     | 0.404    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.13e-05 |
|    n_updates              | 2274     |
|    policy_objective       | 2.66e-07 |
|    value_loss             | 0.195    |
----------------------------------------
Ep done - 425000.
Eval num_timesteps=11623500, episode_reward=0.19 +/- 0.96
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 11623500 |
| train/                    |          |
|    explained_variance     | 0.403    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.13e-05 |
|    n_updates              | 2276     |
|    policy_objective       | 3.91e-08 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 426000.
Ep done - 427000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.1       |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1900      |
|    time_elapsed           | 48837     |
|    total_timesteps        | 11673600  |
| train/                    |           |
|    explained_variance     | 0.397     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.11e-05  |
|    n_updates              | 2284      |
|    policy_objective       | -2.74e-07 |
|    value_loss             | 0.192     |
-----------------------------------------
Eval num_timesteps=11685000, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 11685000 |
| train/                    |          |
|    explained_variance     | 0.462    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.11e-05 |
|    n_updates              | 2286     |
|    policy_objective       | 2.76e-07 |
|    value_loss             | 0.188    |
----------------------------------------
Ep done - 428000.
Ep done - 429000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.06      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1910      |
|    time_elapsed           | 49095     |
|    total_timesteps        | 11735040  |
| train/                    |           |
|    explained_variance     | 0.336     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.09e-05  |
|    n_updates              | 2294      |
|    policy_objective       | -3.73e-07 |
|    value_loss             | 0.205     |
-----------------------------------------
Eval num_timesteps=11746500, episode_reward=0.14 +/- 0.98
Episode length: 29.97 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 11746500 |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.09e-05 |
|    n_updates              | 2296     |
|    policy_objective       | 2.66e-07 |
|    value_loss             | 0.185    |
----------------------------------------
Ep done - 430000.
Ep done - 431000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.13      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1920      |
|    time_elapsed           | 49353     |
|    total_timesteps        | 11796480  |
| train/                    |           |
|    explained_variance     | 0.408     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.07e-05  |
|    n_updates              | 2304      |
|    policy_objective       | -1.02e-07 |
|    value_loss             | 0.188     |
-----------------------------------------
Ep done - 432000.
Eval num_timesteps=11808000, episode_reward=0.15 +/- 0.96
Episode length: 30.10 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 11808000 |
| train/                    |          |
|    explained_variance     | 0.401    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.07e-05 |
|    n_updates              | 2306     |
|    policy_objective       | 1.42e-07 |
|    value_loss             | 0.194    |
----------------------------------------
Ep done - 433000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.8      |
|    ep_rew_mean            | -0.04     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1930      |
|    time_elapsed           | 49606     |
|    total_timesteps        | 11857920  |
| train/                    |           |
|    explained_variance     | 0.428     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.05e-05  |
|    n_updates              | 2314      |
|    policy_objective       | -1.49e-08 |
|    value_loss             | 0.19      |
-----------------------------------------
Ep done - 434000.
Eval num_timesteps=11869500, episode_reward=0.14 +/- 0.97
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.145     |
| time/                     |           |
|    total_timesteps        | 11869500  |
| train/                    |           |
|    explained_variance     | 0.371     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.05e-05  |
|    n_updates              | 2316      |
|    policy_objective       | -1.36e-07 |
|    value_loss             | 0.207     |
-----------------------------------------
Ep done - 435000.
Ep done - 436000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | 0.02     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1940     |
|    time_elapsed           | 49865    |
|    total_timesteps        | 11919360 |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.03e-05 |
|    n_updates              | 2324     |
|    policy_objective       | 5.29e-07 |
|    value_loss             | 0.185    |
----------------------------------------
Eval num_timesteps=11931000, episode_reward=0.18 +/- 0.97
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 11931000 |
| train/                    |          |
|    explained_variance     | 0.463    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.02e-05 |
|    n_updates              | 2326     |
|    policy_objective       | 3.1e-08  |
|    value_loss             | 0.182    |
----------------------------------------
Ep done - 437000.
Ep done - 438000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 1950     |
|    time_elapsed           | 50122    |
|    total_timesteps        | 11980800 |
| train/                    |          |
|    explained_variance     | 0.503    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.01e-05 |
|    n_updates              | 2334     |
|    policy_objective       | 1.13e-07 |
|    value_loss             | 0.174    |
----------------------------------------
Eval num_timesteps=11992500, episode_reward=0.13 +/- 0.97
Episode length: 30.05 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.13      |
| time/                     |           |
|    total_timesteps        | 11992500  |
| train/                    |           |
|    explained_variance     | 0.365     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6e-05     |
|    n_updates              | 2336      |
|    policy_objective       | -5.59e-09 |
|    value_loss             | 0.203     |
-----------------------------------------
Ep done - 439000.
Ep done - 440000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.04     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1960      |
|    time_elapsed           | 50381     |
|    total_timesteps        | 12042240  |
| train/                    |           |
|    explained_variance     | 0.394     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.99e-05  |
|    n_updates              | 2344      |
|    policy_objective       | -8.01e-07 |
|    value_loss             | 0.193     |
-----------------------------------------
Ep done - 441000.
Eval num_timesteps=12054000, episode_reward=0.21 +/- 0.96
Episode length: 30.06 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.21      |
| time/                     |           |
|    total_timesteps        | 12054000  |
| train/                    |           |
|    explained_variance     | 0.438     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.98e-05  |
|    n_updates              | 2346      |
|    policy_objective       | -1.85e-07 |
|    value_loss             | 0.195     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.21
SELFPLAY: new best model, bumping up generation to 48
Ep done - 442000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.09      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1970      |
|    time_elapsed           | 50634     |
|    total_timesteps        | 12103680  |
| train/                    |           |
|    explained_variance     | 0.403     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.97e-05  |
|    n_updates              | 2354      |
|    policy_objective       | -2.95e-08 |
|    value_loss             | 0.189     |
-----------------------------------------
Ep done - 443000.
Eval num_timesteps=12115500, episode_reward=0.14 +/- 0.98
Episode length: 30.06 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 12115500 |
| train/                    |          |
|    explained_variance     | 0.356    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.96e-05 |
|    n_updates              | 2356     |
|    policy_objective       | 3.31e-07 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 444000.
Ep done - 445000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.11      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1980      |
|    time_elapsed           | 50893     |
|    total_timesteps        | 12165120  |
| train/                    |           |
|    explained_variance     | 0.434     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.95e-05  |
|    n_updates              | 2364      |
|    policy_objective       | -1.45e-07 |
|    value_loss             | 0.196     |
-----------------------------------------
Eval num_timesteps=12177000, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 12177000 |
| train/                    |          |
|    explained_variance     | 0.391    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.94e-05 |
|    n_updates              | 2366     |
|    policy_objective       | 6.02e-07 |
|    value_loss             | 0.187    |
----------------------------------------
Ep done - 446000.
Ep done - 447000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.02      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 1990      |
|    time_elapsed           | 51151     |
|    total_timesteps        | 12226560  |
| train/                    |           |
|    explained_variance     | 0.409     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.93e-05  |
|    n_updates              | 2374      |
|    policy_objective       | -1.26e-07 |
|    value_loss             | 0.198     |
-----------------------------------------
Eval num_timesteps=12238500, episode_reward=0.10 +/- 0.98
Episode length: 30.04 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.105     |
| time/                     |           |
|    total_timesteps        | 12238500  |
| train/                    |           |
|    explained_variance     | 0.413     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.92e-05  |
|    n_updates              | 2376      |
|    policy_objective       | -2.49e-07 |
|    value_loss             | 0.183     |
-----------------------------------------
Ep done - 448000.
Ep done - 449000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.2     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2000     |
|    time_elapsed           | 51405    |
|    total_timesteps        | 12288000 |
| train/                    |          |
|    explained_variance     | 0.446    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.91e-05 |
|    n_updates              | 2384     |
|    policy_objective       | 3.96e-07 |
|    value_loss             | 0.176    |
----------------------------------------
Ep done - 450000.
Eval num_timesteps=12300000, episode_reward=0.11 +/- 0.96
Episode length: 29.95 +/- 1.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 12300000 |
| train/                    |          |
|    explained_variance     | 0.453    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.9e-05  |
|    n_updates              | 2386     |
|    policy_objective       | 6.57e-07 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 451000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.08     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2010      |
|    time_elapsed           | 51662     |
|    total_timesteps        | 12349440  |
| train/                    |           |
|    explained_variance     | 0.433     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.89e-05  |
|    n_updates              | 2394      |
|    policy_objective       | -2.13e-07 |
|    value_loss             | 0.196     |
-----------------------------------------
Ep done - 452000.
Eval num_timesteps=12361500, episode_reward=0.18 +/- 0.96
Episode length: 30.09 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.185    |
| time/                     |          |
|    total_timesteps        | 12361500 |
| train/                    |          |
|    explained_variance     | 0.544    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.88e-05 |
|    n_updates              | 2396     |
|    policy_objective       | 8.01e-08 |
|    value_loss             | 0.168    |
----------------------------------------
Ep done - 453000.
Ep done - 454000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.08     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2020      |
|    time_elapsed           | 51921     |
|    total_timesteps        | 12410880  |
| train/                    |           |
|    explained_variance     | 0.417     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.87e-05  |
|    n_updates              | 2404      |
|    policy_objective       | -1.86e-08 |
|    value_loss             | 0.19      |
-----------------------------------------
Eval num_timesteps=12423000, episode_reward=0.20 +/- 0.96
Episode length: 30.05 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.195     |
| time/                     |           |
|    total_timesteps        | 12423000  |
| train/                    |           |
|    explained_variance     | 0.388     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.86e-05  |
|    n_updates              | 2406      |
|    policy_objective       | -3.29e-07 |
|    value_loss             | 0.197     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.195
SELFPLAY: new best model, bumping up generation to 49
Ep done - 455000.
Ep done - 456000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.02     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2030     |
|    time_elapsed           | 52179    |
|    total_timesteps        | 12472320 |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.84e-05 |
|    n_updates              | 2414     |
|    policy_objective       | 1.75e-07 |
|    value_loss             | 0.2      |
----------------------------------------
Eval num_timesteps=12484500, episode_reward=0.03 +/- 0.97
Episode length: 30.04 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.025     |
| time/                     |           |
|    total_timesteps        | 12484500  |
| train/                    |           |
|    explained_variance     | 0.387     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.84e-05  |
|    n_updates              | 2416      |
|    policy_objective       | -1.79e-07 |
|    value_loss             | 0.193     |
-----------------------------------------
Ep done - 457000.
Ep done - 458000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2040     |
|    time_elapsed           | 52432    |
|    total_timesteps        | 12533760 |
| train/                    |          |
|    explained_variance     | 0.384    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.82e-05 |
|    n_updates              | 2424     |
|    policy_objective       | 5.54e-07 |
|    value_loss             | 0.2      |
----------------------------------------
Ep done - 459000.
Eval num_timesteps=12546000, episode_reward=0.15 +/- 0.97
Episode length: 30.06 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.15      |
| time/                     |           |
|    total_timesteps        | 12546000  |
| train/                    |           |
|    explained_variance     | 0.475     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.82e-05  |
|    n_updates              | 2426      |
|    policy_objective       | -1.37e-07 |
|    value_loss             | 0.179     |
-----------------------------------------
Ep done - 460000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2050      |
|    time_elapsed           | 52689     |
|    total_timesteps        | 12595200  |
| train/                    |           |
|    explained_variance     | 0.338     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.8e-05   |
|    n_updates              | 2434      |
|    policy_objective       | -5.63e-07 |
|    value_loss             | 0.214     |
-----------------------------------------
Ep done - 461000.
Eval num_timesteps=12607500, episode_reward=0.12 +/- 0.98
Episode length: 30.07 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 12607500 |
| train/                    |          |
|    explained_variance     | 0.357    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.8e-05  |
|    n_updates              | 2437     |
|    policy_objective       | 2.86e-08 |
|    value_loss             | 0.203    |
----------------------------------------
Ep done - 462000.
Ep done - 463000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.11     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2060     |
|    time_elapsed           | 52946    |
|    total_timesteps        | 12656640 |
| train/                    |          |
|    explained_variance     | 0.381    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.78e-05 |
|    n_updates              | 2444     |
|    policy_objective       | 9.14e-07 |
|    value_loss             | 0.199    |
----------------------------------------
Eval num_timesteps=12669000, episode_reward=0.04 +/- 0.98
Episode length: 30.04 +/- 0.50
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.035     |
| time/                     |           |
|    total_timesteps        | 12669000  |
| train/                    |           |
|    explained_variance     | 0.395     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.78e-05  |
|    n_updates              | 2447      |
|    policy_objective       | -2.72e-07 |
|    value_loss             | 0.2       |
-----------------------------------------
Ep done - 464000.
Ep done - 465000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.12     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2070      |
|    time_elapsed           | 53205     |
|    total_timesteps        | 12718080  |
| train/                    |           |
|    explained_variance     | 0.402     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.76e-05  |
|    n_updates              | 2454      |
|    policy_objective       | -2.42e-07 |
|    value_loss             | 0.206     |
-----------------------------------------
Eval num_timesteps=12730500, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.68
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 12730500 |
| train/                    |          |
|    explained_variance     | 0.431    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.76e-05 |
|    n_updates              | 2457     |
|    policy_objective       | -6.8e-07 |
|    value_loss             | 0.198    |
----------------------------------------
Ep done - 466000.
Ep done - 467000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2080     |
|    time_elapsed           | 53458    |
|    total_timesteps        | 12779520 |
| train/                    |          |
|    explained_variance     | 0.431    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.74e-05 |
|    n_updates              | 2464     |
|    policy_objective       | 1.96e-07 |
|    value_loss             | 0.195    |
----------------------------------------
Ep done - 468000.
Eval num_timesteps=12792000, episode_reward=0.32 +/- 0.93
Episode length: 30.07 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.32     |
| time/                     |          |
|    total_timesteps        | 12792000 |
| train/                    |          |
|    explained_variance     | 0.431    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.74e-05 |
|    n_updates              | 2467     |
|    policy_objective       | 5.27e-07 |
|    value_loss             | 0.192    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.32
SELFPLAY: new best model, bumping up generation to 50
Ep done - 469000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2090     |
|    time_elapsed           | 53716    |
|    total_timesteps        | 12840960 |
| train/                    |          |
|    explained_variance     | 0.4      |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.72e-05 |
|    n_updates              | 2474     |
|    policy_objective       | 1.99e-07 |
|    value_loss             | 0.194    |
----------------------------------------
Ep done - 470000.
Eval num_timesteps=12853500, episode_reward=0.09 +/- 0.99
Episode length: 29.98 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 12853500 |
| train/                    |          |
|    explained_variance     | 0.419    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.72e-05 |
|    n_updates              | 2477     |
|    policy_objective       | 2.92e-07 |
|    value_loss             | 0.199    |
----------------------------------------
Ep done - 471000.
Ep done - 472000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2100     |
|    time_elapsed           | 53973    |
|    total_timesteps        | 12902400 |
| train/                    |          |
|    explained_variance     | 0.455    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.7e-05  |
|    n_updates              | 2484     |
|    policy_objective       | 4.72e-08 |
|    value_loss             | 0.197    |
----------------------------------------
Eval num_timesteps=12915000, episode_reward=0.17 +/- 0.98
Episode length: 30.08 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 12915000 |
| train/                    |          |
|    explained_variance     | 0.377    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.7e-05  |
|    n_updates              | 2487     |
|    policy_objective       | 6.83e-09 |
|    value_loss             | 0.205    |
----------------------------------------
Ep done - 473000.
Ep done - 474000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2110     |
|    time_elapsed           | 54228    |
|    total_timesteps        | 12963840 |
| train/                    |          |
|    explained_variance     | 0.385    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.68e-05 |
|    n_updates              | 2494     |
|    policy_objective       | 2.3e-08  |
|    value_loss             | 0.202    |
----------------------------------------
Eval num_timesteps=12976500, episode_reward=0.15 +/- 0.98
Episode length: 30.02 +/- 0.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.155     |
| time/                     |           |
|    total_timesteps        | 12976500  |
| train/                    |           |
|    explained_variance     | 0.449     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.67e-05  |
|    n_updates              | 2497      |
|    policy_objective       | -1.27e-07 |
|    value_loss             | 0.19      |
-----------------------------------------
Ep done - 475000.
Ep done - 476000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.11      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2120      |
|    time_elapsed           | 54485     |
|    total_timesteps        | 13025280  |
| train/                    |           |
|    explained_variance     | 0.405     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.66e-05  |
|    n_updates              | 2504      |
|    policy_objective       | -8.48e-08 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 477000.
Eval num_timesteps=13038000, episode_reward=0.07 +/- 0.97
Episode length: 29.94 +/- 1.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.075    |
| time/                     |          |
|    total_timesteps        | 13038000 |
| train/                    |          |
|    explained_variance     | 0.343    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.65e-05 |
|    n_updates              | 2507     |
|    policy_objective       | 4.53e-07 |
|    value_loss             | 0.205    |
----------------------------------------
Ep done - 478000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2130     |
|    time_elapsed           | 54744    |
|    total_timesteps        | 13086720 |
| train/                    |          |
|    explained_variance     | 0.41     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.64e-05 |
|    n_updates              | 2514     |
|    policy_objective       | 6.21e-09 |
|    value_loss             | 0.208    |
----------------------------------------
Ep done - 479000.
Eval num_timesteps=13099500, episode_reward=0.12 +/- 0.98
Episode length: 29.95 +/- 0.68
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 13099500 |
| train/                    |          |
|    explained_variance     | 0.451    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.63e-05 |
|    n_updates              | 2517     |
|    policy_objective       | 8.35e-08 |
|    value_loss             | 0.187    |
----------------------------------------
Ep done - 480000.
Ep done - 481000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2140     |
|    time_elapsed           | 55003    |
|    total_timesteps        | 13148160 |
| train/                    |          |
|    explained_variance     | 0.42     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.62e-05 |
|    n_updates              | 2524     |
|    policy_objective       | 2.11e-08 |
|    value_loss             | 0.195    |
----------------------------------------
Eval num_timesteps=13161000, episode_reward=0.13 +/- 0.97
Episode length: 30.00 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.13      |
| time/                     |           |
|    total_timesteps        | 13161000  |
| train/                    |           |
|    explained_variance     | 0.446     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.61e-05  |
|    n_updates              | 2527      |
|    policy_objective       | -3.58e-07 |
|    value_loss             | 0.183     |
-----------------------------------------
Ep done - 482000.
Ep done - 483000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2150     |
|    time_elapsed           | 55256    |
|    total_timesteps        | 13209600 |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.6e-05  |
|    n_updates              | 2534     |
|    policy_objective       | 3.99e-08 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 484000.
Eval num_timesteps=13222500, episode_reward=0.06 +/- 0.99
Episode length: 29.94 +/- 0.62
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.055     |
| time/                     |           |
|    total_timesteps        | 13222500  |
| train/                    |           |
|    explained_variance     | 0.456     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.59e-05  |
|    n_updates              | 2537      |
|    policy_objective       | -1.27e-07 |
|    value_loss             | 0.184     |
-----------------------------------------
Ep done - 485000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.12     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2160      |
|    time_elapsed           | 55514     |
|    total_timesteps        | 13271040  |
| train/                    |           |
|    explained_variance     | 0.423     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.58e-05  |
|    n_updates              | 2544      |
|    policy_objective       | -2.26e-07 |
|    value_loss             | 0.199     |
-----------------------------------------
Ep done - 486000.
Eval num_timesteps=13284000, episode_reward=0.20 +/- 0.95
Episode length: 30.09 +/- 0.61
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.195     |
| time/                     |           |
|    total_timesteps        | 13284000  |
| train/                    |           |
|    explained_variance     | 0.388     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.57e-05  |
|    n_updates              | 2547      |
|    policy_objective       | -1.35e-07 |
|    value_loss             | 0.204     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.195
SELFPLAY: new best model, bumping up generation to 51
Ep done - 487000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | -0.07     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2170      |
|    time_elapsed           | 55772     |
|    total_timesteps        | 13332480  |
| train/                    |           |
|    explained_variance     | 0.462     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.56e-05  |
|    n_updates              | 2554      |
|    policy_objective       | -1.54e-07 |
|    value_loss             | 0.178     |
-----------------------------------------
Ep done - 488000.
Eval num_timesteps=13345500, episode_reward=0.20 +/- 0.96
Episode length: 30.09 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 13345500 |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.55e-05 |
|    n_updates              | 2557     |
|    policy_objective       | -6.1e-07 |
|    value_loss             | 0.2      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.205
SELFPLAY: new best model, bumping up generation to 52
Ep done - 489000.
Ep done - 490000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | -0.09     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2180      |
|    time_elapsed           | 56030     |
|    total_timesteps        | 13393920  |
| train/                    |           |
|    explained_variance     | 0.465     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.54e-05  |
|    n_updates              | 2564      |
|    policy_objective       | -1.06e-07 |
|    value_loss             | 0.183     |
-----------------------------------------
Eval num_timesteps=13407000, episode_reward=0.16 +/- 0.97
Episode length: 29.98 +/- 1.29
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 13407000 |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.53e-05 |
|    n_updates              | 2567     |
|    policy_objective       | 7.82e-08 |
|    value_loss             | 0.191    |
----------------------------------------
Ep done - 491000.
Ep done - 492000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.07     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2190     |
|    time_elapsed           | 56283    |
|    total_timesteps        | 13455360 |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.52e-05 |
|    n_updates              | 2574     |
|    policy_objective       | 3.55e-07 |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 493000.
Eval num_timesteps=13468500, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.53
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.08      |
| time/                     |           |
|    total_timesteps        | 13468500  |
| train/                    |           |
|    explained_variance     | 0.384     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.51e-05  |
|    n_updates              | 2577      |
|    policy_objective       | -4.78e-07 |
|    value_loss             | 0.203     |
-----------------------------------------
Ep done - 494000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2200     |
|    time_elapsed           | 56541    |
|    total_timesteps        | 13516800 |
| train/                    |          |
|    explained_variance     | 0.382    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.5e-05  |
|    n_updates              | 2584     |
|    policy_objective       | 1.21e-07 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 495000.
Eval num_timesteps=13530000, episode_reward=0.06 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.06      |
| time/                     |           |
|    total_timesteps        | 13530000  |
| train/                    |           |
|    explained_variance     | 0.374     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.49e-05  |
|    n_updates              | 2587      |
|    policy_objective       | -1.31e-07 |
|    value_loss             | 0.202     |
-----------------------------------------
Ep done - 496000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.01    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2210     |
|    time_elapsed           | 56798    |
|    total_timesteps        | 13578240 |
| train/                    |          |
|    explained_variance     | 0.381    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.48e-05 |
|    n_updates              | 2594     |
|    policy_objective       | 3.44e-07 |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 497000.
Eval num_timesteps=13591500, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.04      |
| time/                     |           |
|    total_timesteps        | 13591500  |
| train/                    |           |
|    explained_variance     | 0.401     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.47e-05  |
|    n_updates              | 2597      |
|    policy_objective       | -5.01e-07 |
|    value_loss             | 0.2       |
-----------------------------------------
Ep done - 498000.
Ep done - 499000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.02      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2220      |
|    time_elapsed           | 57057     |
|    total_timesteps        | 13639680  |
| train/                    |           |
|    explained_variance     | 0.424     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.46e-05  |
|    n_updates              | 2604      |
|    policy_objective       | -6.77e-07 |
|    value_loss             | 0.19      |
-----------------------------------------
Eval num_timesteps=13653000, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.105     |
| time/                     |           |
|    total_timesteps        | 13653000  |
| train/                    |           |
|    explained_variance     | 0.406     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.45e-05  |
|    n_updates              | 2607      |
|    policy_objective       | -6.16e-07 |
|    value_loss             | 0.188     |
-----------------------------------------
Ep done - 500000.
Ep done - 501000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2230      |
|    time_elapsed           | 57310     |
|    total_timesteps        | 13701120  |
| train/                    |           |
|    explained_variance     | 0.334     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.44e-05  |
|    n_updates              | 2614      |
|    policy_objective       | -3.27e-07 |
|    value_loss             | 0.215     |
-----------------------------------------
Ep done - 502000.
Eval num_timesteps=13714500, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.105    |
| time/                     |          |
|    total_timesteps        | 13714500 |
| train/                    |          |
|    explained_variance     | 0.383    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.43e-05 |
|    n_updates              | 2617     |
|    policy_objective       | 5.95e-07 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 503000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2240     |
|    time_elapsed           | 57568    |
|    total_timesteps        | 13762560 |
| train/                    |          |
|    explained_variance     | 0.447    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.41e-05 |
|    n_updates              | 2624     |
|    policy_objective       | 2.61e-08 |
|    value_loss             | 0.188    |
----------------------------------------
Ep done - 504000.
Eval num_timesteps=13776000, episode_reward=0.07 +/- 0.97
Episode length: 29.95 +/- 0.65
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.075    |
| time/                     |          |
|    total_timesteps        | 13776000 |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.41e-05 |
|    n_updates              | 2627     |
|    policy_objective       | 4.78e-08 |
|    value_loss             | 0.201    |
----------------------------------------
Ep done - 505000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.02     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2250     |
|    time_elapsed           | 57826    |
|    total_timesteps        | 13824000 |
| train/                    |          |
|    explained_variance     | 0.373    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.39e-05 |
|    n_updates              | 2634     |
|    policy_objective       | 8.99e-08 |
|    value_loss             | 0.2      |
----------------------------------------
Ep done - 506000.
Eval num_timesteps=13837500, episode_reward=0.23 +/- 0.96
Episode length: 30.04 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 13837500 |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.39e-05 |
|    n_updates              | 2637     |
|    policy_objective       | 3.68e-07 |
|    value_loss             | 0.192    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 53
Ep done - 507000.
Ep done - 508000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2260      |
|    time_elapsed           | 58080     |
|    total_timesteps        | 13885440  |
| train/                    |           |
|    explained_variance     | 0.395     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.37e-05  |
|    n_updates              | 2644      |
|    policy_objective       | -4.35e-07 |
|    value_loss             | 0.203     |
-----------------------------------------
Eval num_timesteps=13899000, episode_reward=0.10 +/- 0.98
Episode length: 30.06 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.095     |
| time/                     |           |
|    total_timesteps        | 13899000  |
| train/                    |           |
|    explained_variance     | 0.365     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.37e-05  |
|    n_updates              | 2647      |
|    policy_objective       | -1.87e-07 |
|    value_loss             | 0.217     |
-----------------------------------------
Ep done - 509000.
Ep done - 510000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.16      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2270      |
|    time_elapsed           | 58339     |
|    total_timesteps        | 13946880  |
| train/                    |           |
|    explained_variance     | 0.433     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.35e-05  |
|    n_updates              | 2654      |
|    policy_objective       | -2.52e-07 |
|    value_loss             | 0.199     |
-----------------------------------------
Ep done - 511000.
Eval num_timesteps=13960500, episode_reward=0.20 +/- 0.97
Episode length: 30.04 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.205    |
| time/                     |          |
|    total_timesteps        | 13960500 |
| train/                    |          |
|    explained_variance     | 0.401    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.35e-05 |
|    n_updates              | 2657     |
|    policy_objective       | 1.53e-07 |
|    value_loss             | 0.201    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.205
SELFPLAY: new best model, bumping up generation to 54
Ep done - 512000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.1       |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2280      |
|    time_elapsed           | 58597     |
|    total_timesteps        | 14008320  |
| train/                    |           |
|    explained_variance     | 0.408     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.33e-05  |
|    n_updates              | 2664      |
|    policy_objective       | -1.02e-07 |
|    value_loss             | 0.2       |
-----------------------------------------
Ep done - 513000.
Eval num_timesteps=14022000, episode_reward=0.12 +/- 0.96
Episode length: 29.93 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 14022000 |
| train/                    |          |
|    explained_variance     | 0.445    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.33e-05 |
|    n_updates              | 2667     |
|    policy_objective       | -3.2e-08 |
|    value_loss             | 0.192    |
----------------------------------------
Ep done - 514000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.13     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2290      |
|    time_elapsed           | 58856     |
|    total_timesteps        | 14069760  |
| train/                    |           |
|    explained_variance     | 0.427     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.31e-05  |
|    n_updates              | 2674      |
|    policy_objective       | -5.65e-08 |
|    value_loss             | 0.203     |
-----------------------------------------
Ep done - 515000.
Eval num_timesteps=14083500, episode_reward=0.07 +/- 0.98
Episode length: 29.95 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.065    |
| time/                     |          |
|    total_timesteps        | 14083500 |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.31e-05 |
|    n_updates              | 2677     |
|    policy_objective       | 1.75e-07 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 516000.
Ep done - 517000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2300     |
|    time_elapsed           | 59109    |
|    total_timesteps        | 14131200 |
| train/                    |          |
|    explained_variance     | 0.362    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.29e-05 |
|    n_updates              | 2684     |
|    policy_objective       | 8.07e-09 |
|    value_loss             | 0.202    |
----------------------------------------
Eval num_timesteps=14145000, episode_reward=0.12 +/- 0.97
Episode length: 30.03 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 14145000 |
| train/                    |          |
|    explained_variance     | 0.435    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.29e-05 |
|    n_updates              | 2687     |
|    policy_objective       | 2.24e-08 |
|    value_loss             | 0.19     |
----------------------------------------
Ep done - 518000.
Ep done - 519000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2310      |
|    time_elapsed           | 59367     |
|    total_timesteps        | 14192640  |
| train/                    |           |
|    explained_variance     | 0.404     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.27e-05  |
|    n_updates              | 2694      |
|    policy_objective       | -3.05e-07 |
|    value_loss             | 0.198     |
-----------------------------------------
Ep done - 520000.
Eval num_timesteps=14206500, episode_reward=-0.01 +/- 0.99
Episode length: 29.95 +/- 1.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | -0.005    |
| time/                     |           |
|    total_timesteps        | 14206500  |
| train/                    |           |
|    explained_variance     | 0.375     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.27e-05  |
|    n_updates              | 2697      |
|    policy_objective       | -1.45e-07 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 521000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2320     |
|    time_elapsed           | 59626    |
|    total_timesteps        | 14254080 |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.25e-05 |
|    n_updates              | 2704     |
|    policy_objective       | 9.87e-08 |
|    value_loss             | 0.192    |
----------------------------------------
Ep done - 522000.
Eval num_timesteps=14268000, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.07      |
| time/                     |           |
|    total_timesteps        | 14268000  |
| train/                    |           |
|    explained_variance     | 0.355     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.24e-05  |
|    n_updates              | 2707      |
|    policy_objective       | -5.44e-07 |
|    value_loss             | 0.207     |
-----------------------------------------
Ep done - 523000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2330     |
|    time_elapsed           | 59885    |
|    total_timesteps        | 14315520 |
| train/                    |          |
|    explained_variance     | 0.408    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.23e-05 |
|    n_updates              | 2714     |
|    policy_objective       | 2.51e-08 |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 524000.
Eval num_timesteps=14329500, episode_reward=0.23 +/- 0.97
Episode length: 30.02 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.225     |
| time/                     |           |
|    total_timesteps        | 14329500  |
| train/                    |           |
|    explained_variance     | 0.534     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.22e-05  |
|    n_updates              | 2717      |
|    policy_objective       | -4.63e-08 |
|    value_loss             | 0.163     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 55
Ep done - 525000.
Ep done - 526000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.12    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2340     |
|    time_elapsed           | 60137    |
|    total_timesteps        | 14376960 |
| train/                    |          |
|    explained_variance     | 0.404    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.21e-05 |
|    n_updates              | 2724     |
|    policy_objective       | 8.82e-08 |
|    value_loss             | 0.204    |
----------------------------------------
Eval num_timesteps=14391000, episode_reward=0.07 +/- 0.97
Episode length: 29.93 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.07      |
| time/                     |           |
|    total_timesteps        | 14391000  |
| train/                    |           |
|    explained_variance     | 0.43      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.2e-05   |
|    n_updates              | 2727      |
|    policy_objective       | -1.77e-07 |
|    value_loss             | 0.186     |
-----------------------------------------
Ep done - 527000.
Ep done - 528000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.12    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2350     |
|    time_elapsed           | 60396    |
|    total_timesteps        | 14438400 |
| train/                    |          |
|    explained_variance     | 0.413    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.19e-05 |
|    n_updates              | 2734     |
|    policy_objective       | 7.45e-09 |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 529000.
Eval num_timesteps=14452500, episode_reward=0.06 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.055     |
| time/                     |           |
|    total_timesteps        | 14452500  |
| train/                    |           |
|    explained_variance     | 0.386     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.18e-05  |
|    n_updates              | 2737      |
|    policy_objective       | -3.81e-07 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 530000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.05      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2360      |
|    time_elapsed           | 60654     |
|    total_timesteps        | 14499840  |
| train/                    |           |
|    explained_variance     | 0.413     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.17e-05  |
|    n_updates              | 2744      |
|    policy_objective       | -1.35e-07 |
|    value_loss             | 0.192     |
-----------------------------------------
Ep done - 531000.
Eval num_timesteps=14514000, episode_reward=0.13 +/- 0.97
Episode length: 30.03 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 14514000 |
| train/                    |          |
|    explained_variance     | 0.452    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.16e-05 |
|    n_updates              | 2747     |
|    policy_objective       | 4.04e-07 |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 532000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2370     |
|    time_elapsed           | 60911    |
|    total_timesteps        | 14561280 |
| train/                    |          |
|    explained_variance     | 0.492    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.15e-05 |
|    n_updates              | 2754     |
|    policy_objective       | 3.56e-07 |
|    value_loss             | 0.188    |
----------------------------------------
Ep done - 533000.
Eval num_timesteps=14575500, episode_reward=0.00 +/- 0.98
Episode length: 30.07 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0        |
| time/                     |          |
|    total_timesteps        | 14575500 |
| train/                    |          |
|    explained_variance     | 0.504    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000179 |
|    learning_rate          | 5.14e-05 |
|    n_updates              | 2757     |
|    policy_objective       | 4.23     |
|    value_loss             | 0.18     |
----------------------------------------
Ep done - 534000.
Ep done - 535000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.11      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2380      |
|    time_elapsed           | 61165     |
|    total_timesteps        | 14622720  |
| train/                    |           |
|    explained_variance     | 0.427     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.13e-05  |
|    n_updates              | 2764      |
|    policy_objective       | -4.31e-07 |
|    value_loss             | 0.21      |
-----------------------------------------
Eval num_timesteps=14637000, episode_reward=0.06 +/- 0.97
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 14637000 |
| train/                    |          |
|    explained_variance     | 0.431    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.12e-05 |
|    n_updates              | 2767     |
|    policy_objective       | 3.84e-07 |
|    value_loss             | 0.194    |
----------------------------------------
Ep done - 536000.
Ep done - 537000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.02     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2390      |
|    time_elapsed           | 61423     |
|    total_timesteps        | 14684160  |
| train/                    |           |
|    explained_variance     | 0.488     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.11e-05  |
|    n_updates              | 2774      |
|    policy_objective       | -3.51e-07 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 538000.
Eval num_timesteps=14698500, episode_reward=0.21 +/- 0.97
Episode length: 30.04 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 14698500 |
| train/                    |          |
|    explained_variance     | 0.471    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.1e-05  |
|    n_updates              | 2777     |
|    policy_objective       | 7.26e-08 |
|    value_loss             | 0.179    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.215
SELFPLAY: new best model, bumping up generation to 56
Ep done - 539000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2400     |
|    time_elapsed           | 61681    |
|    total_timesteps        | 14745600 |
| train/                    |          |
|    explained_variance     | 0.445    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.09e-05 |
|    n_updates              | 2784     |
|    policy_objective       | 3.23e-08 |
|    value_loss             | 0.199    |
----------------------------------------
Ep done - 540000.
Eval num_timesteps=14760000, episode_reward=0.17 +/- 0.96
Episode length: 30.00 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.175     |
| time/                     |           |
|    total_timesteps        | 14760000  |
| train/                    |           |
|    explained_variance     | 0.465     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.08e-05  |
|    n_updates              | 2787      |
|    policy_objective       | -3.46e-07 |
|    value_loss             | 0.193     |
-----------------------------------------
Ep done - 541000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.13      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2410      |
|    time_elapsed           | 61934     |
|    total_timesteps        | 14807040  |
| train/                    |           |
|    explained_variance     | 0.485     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.07e-05  |
|    n_updates              | 2794      |
|    policy_objective       | -3.88e-07 |
|    value_loss             | 0.183     |
-----------------------------------------
Ep done - 542000.
Eval num_timesteps=14821500, episode_reward=0.14 +/- 0.97
Episode length: 30.04 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 14821500 |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.06e-05 |
|    n_updates              | 2797     |
|    policy_objective       | 2.57e-07 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 543000.
Ep done - 544000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2420     |
|    time_elapsed           | 62192    |
|    total_timesteps        | 14868480 |
| train/                    |          |
|    explained_variance     | 0.396    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.05e-05 |
|    n_updates              | 2804     |
|    policy_objective       | -4.9e-08 |
|    value_loss             | 0.2      |
----------------------------------------
Eval num_timesteps=14883000, episode_reward=0.15 +/- 0.96
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 14883000 |
| train/                    |          |
|    explained_variance     | 0.42     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.04e-05 |
|    n_updates              | 2807     |
|    policy_objective       | -1.4e-07 |
|    value_loss             | 0.198    |
----------------------------------------
Ep done - 545000.
Ep done - 546000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.19     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2430     |
|    time_elapsed           | 62450    |
|    total_timesteps        | 14929920 |
| train/                    |          |
|    explained_variance     | 0.391    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.03e-05 |
|    n_updates              | 2814     |
|    policy_objective       | 2.05e-07 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 547000.
Eval num_timesteps=14944500, episode_reward=0.21 +/- 0.97
Episode length: 30.08 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 14944500 |
| train/                    |          |
|    explained_variance     | 0.471    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.02e-05 |
|    n_updates              | 2817     |
|    policy_objective       | 1.43e-07 |
|    value_loss             | 0.192    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.215
SELFPLAY: new best model, bumping up generation to 57
Ep done - 548000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2440      |
|    time_elapsed           | 62708     |
|    total_timesteps        | 14991360  |
| train/                    |           |
|    explained_variance     | 0.486     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5e-05     |
|    n_updates              | 2824      |
|    policy_objective       | -3.43e-07 |
|    value_loss             | 0.195     |
-----------------------------------------
Ep done - 549000.
Eval num_timesteps=15006000, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.58
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.095     |
| time/                     |           |
|    total_timesteps        | 15006000  |
| train/                    |           |
|    explained_variance     | 0.364     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5e-05     |
|    n_updates              | 2827      |
|    policy_objective       | -5.91e-07 |
|    value_loss             | 0.216     |
-----------------------------------------
Ep done - 550000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.18     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2450      |
|    time_elapsed           | 62961     |
|    total_timesteps        | 15052800  |
| train/                    |           |
|    explained_variance     | 0.356     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.98e-05  |
|    n_updates              | 2834      |
|    policy_objective       | -5.28e-07 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 551000.
Eval num_timesteps=15067500, episode_reward=-0.04 +/- 0.98
Episode length: 29.96 +/- 0.89
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | -0.045    |
| time/                     |           |
|    total_timesteps        | 15067500  |
| train/                    |           |
|    explained_variance     | 0.449     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.98e-05  |
|    n_updates              | 2837      |
|    policy_objective       | -2.89e-07 |
|    value_loss             | 0.189     |
-----------------------------------------
Ep done - 552000.
Ep done - 553000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.04     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2460      |
|    time_elapsed           | 63219     |
|    total_timesteps        | 15114240  |
| train/                    |           |
|    explained_variance     | 0.38      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.96e-05  |
|    n_updates              | 2844      |
|    policy_objective       | -2.75e-07 |
|    value_loss             | 0.206     |
-----------------------------------------
Eval num_timesteps=15129000, episode_reward=0.16 +/- 0.97
Episode length: 30.01 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.16      |
| time/                     |           |
|    total_timesteps        | 15129000  |
| train/                    |           |
|    explained_variance     | 0.379     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.96e-05  |
|    n_updates              | 2847      |
|    policy_objective       | -9.25e-08 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 554000.
Ep done - 555000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2470     |
|    time_elapsed           | 63477    |
|    total_timesteps        | 15175680 |
| train/                    |          |
|    explained_variance     | 0.411    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.94e-05 |
|    n_updates              | 2854     |
|    policy_objective       | 4.81e-08 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 556000.
Eval num_timesteps=15190500, episode_reward=0.10 +/- 0.99
Episode length: 29.98 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.105    |
| time/                     |          |
|    total_timesteps        | 15190500 |
| train/                    |          |
|    explained_variance     | 0.343    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.94e-05 |
|    n_updates              | 2857     |
|    policy_objective       | 2.44e-07 |
|    value_loss             | 0.214    |
----------------------------------------
Ep done - 557000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2480     |
|    time_elapsed           | 63735    |
|    total_timesteps        | 15237120 |
| train/                    |          |
|    explained_variance     | 0.422    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000659 |
|    learning_rate          | 4.92e-05 |
|    n_updates              | 2864     |
|    policy_objective       | 1.3e+04  |
|    value_loss             | 0.198    |
----------------------------------------
Ep done - 558000.
Eval num_timesteps=15252000, episode_reward=0.14 +/- 0.97
Episode length: 29.90 +/- 1.32
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 15252000 |
| train/                    |          |
|    explained_variance     | 0.424    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.92e-05 |
|    n_updates              | 2867     |
|    policy_objective       | -2.2e-07 |
|    value_loss             | 0.203    |
----------------------------------------
Ep done - 559000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.15     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2490      |
|    time_elapsed           | 63989     |
|    total_timesteps        | 15298560  |
| train/                    |           |
|    explained_variance     | 0.429     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.9e-05   |
|    n_updates              | 2874      |
|    policy_objective       | -4.22e-07 |
|    value_loss             | 0.193     |
-----------------------------------------
Ep done - 560000.
Eval num_timesteps=15313500, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.105     |
| time/                     |           |
|    total_timesteps        | 15313500  |
| train/                    |           |
|    explained_variance     | 0.41      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.9e-05   |
|    n_updates              | 2877      |
|    policy_objective       | -2.11e-07 |
|    value_loss             | 0.206     |
-----------------------------------------
Ep done - 561000.
Ep done - 562000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.17     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2500      |
|    time_elapsed           | 64246     |
|    total_timesteps        | 15360000  |
| train/                    |           |
|    explained_variance     | 0.408     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.88e-05  |
|    n_updates              | 2884      |
|    policy_objective       | -3.14e-07 |
|    value_loss             | 0.191     |
-----------------------------------------
Eval num_timesteps=15375000, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.115     |
| time/                     |           |
|    total_timesteps        | 15375000  |
| train/                    |           |
|    explained_variance     | 0.346     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.88e-05  |
|    n_updates              | 2887      |
|    policy_objective       | -4.07e-07 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 563000.
Ep done - 564000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.17     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2510     |
|    time_elapsed           | 64504    |
|    total_timesteps        | 15421440 |
| train/                    |          |
|    explained_variance     | 0.378    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.86e-05 |
|    n_updates              | 2894     |
|    policy_objective       | 1.43e-07 |
|    value_loss             | 0.205    |
----------------------------------------
Ep done - 565000.
Eval num_timesteps=15436500, episode_reward=0.12 +/- 0.98
Episode length: 30.06 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 15436500 |
| train/                    |          |
|    explained_variance     | 0.45     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.86e-05 |
|    n_updates              | 2897     |
|    policy_objective       | -1.9e-07 |
|    value_loss             | 0.185    |
----------------------------------------
Ep done - 566000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2520      |
|    time_elapsed           | 64758     |
|    total_timesteps        | 15482880  |
| train/                    |           |
|    explained_variance     | 0.42      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.84e-05  |
|    n_updates              | 2904      |
|    policy_objective       | -4.91e-07 |
|    value_loss             | 0.204     |
-----------------------------------------
Ep done - 567000.
Eval num_timesteps=15498000, episode_reward=0.12 +/- 0.97
Episode length: 30.01 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 15498000 |
| train/                    |          |
|    explained_variance     | 0.411    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.83e-05 |
|    n_updates              | 2907     |
|    policy_objective       | 5.35e-07 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 568000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.23    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2530     |
|    time_elapsed           | 65016    |
|    total_timesteps        | 15544320 |
| train/                    |          |
|    explained_variance     | 0.469    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.82e-05 |
|    n_updates              | 2914     |
|    policy_objective       | 4.28e-07 |
|    value_loss             | 0.176    |
----------------------------------------
Ep done - 569000.
Eval num_timesteps=15559500, episode_reward=0.12 +/- 0.97
Episode length: 30.04 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 15559500 |
| train/                    |          |
|    explained_variance     | 0.449    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.81e-05 |
|    n_updates              | 2917     |
|    policy_objective       | 7.98e-08 |
|    value_loss             | 0.19     |
----------------------------------------
Ep done - 570000.
Ep done - 571000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.19      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2540      |
|    time_elapsed           | 65275     |
|    total_timesteps        | 15605760  |
| train/                    |           |
|    explained_variance     | 0.411     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.8e-05   |
|    n_updates              | 2924      |
|    policy_objective       | -1.39e-07 |
|    value_loss             | 0.203     |
-----------------------------------------
Eval num_timesteps=15621000, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 1.07
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.015    |
| time/                     |          |
|    total_timesteps        | 15621000 |
| train/                    |          |
|    explained_variance     | 0.48     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.79e-05 |
|    n_updates              | 2927     |
|    policy_objective       | 8.48e-08 |
|    value_loss             | 0.188    |
----------------------------------------
Ep done - 572000.
Ep done - 573000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2550      |
|    time_elapsed           | 65533     |
|    total_timesteps        | 15667200  |
| train/                    |           |
|    explained_variance     | 0.425     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.78e-05  |
|    n_updates              | 2934      |
|    policy_objective       | -5.71e-08 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 574000.
Eval num_timesteps=15682500, episode_reward=0.08 +/- 0.99
Episode length: 30.00 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 15682500 |
| train/                    |          |
|    explained_variance     | 0.452    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.77e-05 |
|    n_updates              | 2937     |
|    policy_objective       | 5.03e-08 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 575000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2560     |
|    time_elapsed           | 65786    |
|    total_timesteps        | 15728640 |
| train/                    |          |
|    explained_variance     | 0.387    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.76e-05 |
|    n_updates              | 2944     |
|    policy_objective       | 4.12e-07 |
|    value_loss             | 0.204    |
----------------------------------------
Ep done - 576000.
Eval num_timesteps=15744000, episode_reward=0.10 +/- 0.98
Episode length: 29.93 +/- 1.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.105     |
| time/                     |           |
|    total_timesteps        | 15744000  |
| train/                    |           |
|    explained_variance     | 0.356     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.75e-05  |
|    n_updates              | 2947      |
|    policy_objective       | -3.59e-07 |
|    value_loss             | 0.21      |
-----------------------------------------
Ep done - 577000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.14     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2570      |
|    time_elapsed           | 66044     |
|    total_timesteps        | 15790080  |
| train/                    |           |
|    explained_variance     | 0.432     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.74e-05  |
|    n_updates              | 2954      |
|    policy_objective       | -1.21e-08 |
|    value_loss             | 0.203     |
-----------------------------------------
Ep done - 578000.
Eval num_timesteps=15805500, episode_reward=0.20 +/- 0.96
Episode length: 30.09 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 15805500 |
| train/                    |          |
|    explained_variance     | 0.42     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.73e-05 |
|    n_updates              | 2957     |
|    policy_objective       | 4.23e-07 |
|    value_loss             | 0.201    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 58
Ep done - 579000.
Ep done - 580000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.7      |
|    ep_rew_mean            | -0.05     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2580      |
|    time_elapsed           | 66302     |
|    total_timesteps        | 15851520  |
| train/                    |           |
|    explained_variance     | 0.434     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.72e-05  |
|    n_updates              | 2964      |
|    policy_objective       | -1.12e-07 |
|    value_loss             | 0.202     |
-----------------------------------------
Eval num_timesteps=15867000, episode_reward=0.19 +/- 0.98
Episode length: 30.00 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.19      |
| time/                     |           |
|    total_timesteps        | 15867000  |
| train/                    |           |
|    explained_variance     | 0.341     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.71e-05  |
|    n_updates              | 2967      |
|    policy_objective       | -5.62e-08 |
|    value_loss             | 0.215     |
-----------------------------------------
Ep done - 581000.
Ep done - 582000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.03     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2590      |
|    time_elapsed           | 66561     |
|    total_timesteps        | 15912960  |
| train/                    |           |
|    explained_variance     | 0.379     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.7e-05   |
|    n_updates              | 2974      |
|    policy_objective       | -3.61e-07 |
|    value_loss             | 0.22      |
-----------------------------------------
Ep done - 583000.
Eval num_timesteps=15928500, episode_reward=0.07 +/- 0.98
Episode length: 29.94 +/- 1.19
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.07      |
| time/                     |           |
|    total_timesteps        | 15928500  |
| train/                    |           |
|    explained_variance     | 0.371     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.69e-05  |
|    n_updates              | 2977      |
|    policy_objective       | -2.67e-08 |
|    value_loss             | 0.219     |
-----------------------------------------
Ep done - 584000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.02      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2600      |
|    time_elapsed           | 66815     |
|    total_timesteps        | 15974400  |
| train/                    |           |
|    explained_variance     | 0.351     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.68e-05  |
|    n_updates              | 2984      |
|    policy_objective       | -3.19e-07 |
|    value_loss             | 0.207     |
-----------------------------------------
Ep done - 585000.
Eval num_timesteps=15990000, episode_reward=0.16 +/- 0.97
Episode length: 30.00 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 15990000 |
| train/                    |          |
|    explained_variance     | 0.451    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.67e-05 |
|    n_updates              | 2987     |
|    policy_objective       | 1.24e-07 |
|    value_loss             | 0.192    |
----------------------------------------
Ep done - 586000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2610     |
|    time_elapsed           | 67075    |
|    total_timesteps        | 16035840 |
| train/                    |          |
|    explained_variance     | 0.456    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.66e-05 |
|    n_updates              | 2994     |
|    policy_objective       | 6.08e-08 |
|    value_loss             | 0.198    |
----------------------------------------
Ep done - 587000.
Eval num_timesteps=16051500, episode_reward=0.17 +/- 0.98
Episode length: 30.02 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 16051500 |
| train/                    |          |
|    explained_variance     | 0.49     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.65e-05 |
|    n_updates              | 2997     |
|    policy_objective       | 1.53e-07 |
|    value_loss             | 0.186    |
----------------------------------------
Ep done - 588000.
Ep done - 589000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.13      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2620      |
|    time_elapsed           | 67333     |
|    total_timesteps        | 16097280  |
| train/                    |           |
|    explained_variance     | 0.364     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.64e-05  |
|    n_updates              | 3004      |
|    policy_objective       | -2.02e-07 |
|    value_loss             | 0.218     |
-----------------------------------------
Eval num_timesteps=16113000, episode_reward=0.30 +/- 0.93
Episode length: 30.03 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.305    |
| time/                     |          |
|    total_timesteps        | 16113000 |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.63e-05 |
|    n_updates              | 3007     |
|    policy_objective       | 1.68e-07 |
|    value_loss             | 0.193    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.305
SELFPLAY: new best model, bumping up generation to 59
Ep done - 590000.
Ep done - 591000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2630     |
|    time_elapsed           | 67589    |
|    total_timesteps        | 16158720 |
| train/                    |          |
|    explained_variance     | 0.365    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.62e-05 |
|    n_updates              | 3014     |
|    policy_objective       | -1.4e-07 |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 592000.
Eval num_timesteps=16174500, episode_reward=0.23 +/- 0.96
Episode length: 30.07 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 16174500 |
| train/                    |          |
|    explained_variance     | 0.379    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.61e-05 |
|    n_updates              | 3017     |
|    policy_objective       | -1.4e-07 |
|    value_loss             | 0.218    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.23
SELFPLAY: new best model, bumping up generation to 60
Ep done - 593000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2640     |
|    time_elapsed           | 67844    |
|    total_timesteps        | 16220160 |
| train/                    |          |
|    explained_variance     | 0.381    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.6e-05  |
|    n_updates              | 3024     |
|    policy_objective       | 3.29e-07 |
|    value_loss             | 0.208    |
----------------------------------------
Ep done - 594000.
Eval num_timesteps=16236000, episode_reward=0.26 +/- 0.95
Episode length: 30.08 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.255     |
| time/                     |           |
|    total_timesteps        | 16236000  |
| train/                    |           |
|    explained_variance     | 0.395     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.59e-05  |
|    n_updates              | 3027      |
|    policy_objective       | -1.72e-07 |
|    value_loss             | 0.213     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.255
SELFPLAY: new best model, bumping up generation to 61
Ep done - 595000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2650     |
|    time_elapsed           | 68102    |
|    total_timesteps        | 16281600 |
| train/                    |          |
|    explained_variance     | 0.376    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.57e-05 |
|    n_updates              | 3034     |
|    policy_objective       | 1.07e-07 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 596000.
Eval num_timesteps=16297500, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.115     |
| time/                     |           |
|    total_timesteps        | 16297500  |
| train/                    |           |
|    explained_variance     | 0.393     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.57e-05  |
|    n_updates              | 3037      |
|    policy_objective       | -2.62e-08 |
|    value_loss             | 0.204     |
-----------------------------------------
Ep done - 597000.
Ep done - 598000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.12     |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2660      |
|    time_elapsed           | 68360     |
|    total_timesteps        | 16343040  |
| train/                    |           |
|    explained_variance     | 0.388     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.55e-05  |
|    n_updates              | 3044      |
|    policy_objective       | -8.26e-08 |
|    value_loss             | 0.217     |
-----------------------------------------
Eval num_timesteps=16359000, episode_reward=0.02 +/- 0.98
Episode length: 29.95 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.02      |
| time/                     |           |
|    total_timesteps        | 16359000  |
| train/                    |           |
|    explained_variance     | 0.479     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.55e-05  |
|    n_updates              | 3047      |
|    policy_objective       | -1.55e-07 |
|    value_loss             | 0.192     |
-----------------------------------------
Ep done - 599000.
Ep done - 600000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2670     |
|    time_elapsed           | 68613    |
|    total_timesteps        | 16404480 |
| train/                    |          |
|    explained_variance     | 0.348    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.53e-05 |
|    n_updates              | 3054     |
|    policy_objective       | 1.18e-07 |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 601000.
Eval num_timesteps=16420500, episode_reward=-0.04 +/- 0.98
Episode length: 30.00 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | -0.045    |
| time/                     |           |
|    total_timesteps        | 16420500  |
| train/                    |           |
|    explained_variance     | 0.46      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.53e-05  |
|    n_updates              | 3057      |
|    policy_objective       | -2.63e-07 |
|    value_loss             | 0.196     |
-----------------------------------------
Ep done - 602000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2680     |
|    time_elapsed           | 68808    |
|    total_timesteps        | 16465920 |
| train/                    |          |
|    explained_variance     | 0.415    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.51e-05 |
|    n_updates              | 3064     |
|    policy_objective       | 1.14e-07 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 603000.
Eval num_timesteps=16482000, episode_reward=0.15 +/- 0.97
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.155     |
| time/                     |           |
|    total_timesteps        | 16482000  |
| train/                    |           |
|    explained_variance     | 0.398     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.51e-05  |
|    n_updates              | 3067      |
|    policy_objective       | -9.41e-08 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 604000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.02     |
| time/                     |          |
|    fps                    | 239      |
|    iterations             | 2690     |
|    time_elapsed           | 68989    |
|    total_timesteps        | 16527360 |
| train/                    |          |
|    explained_variance     | 0.361    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.49e-05 |
|    n_updates              | 3074     |
|    policy_objective       | 3.76e-07 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 605000.
Eval num_timesteps=16543500, episode_reward=0.11 +/- 0.98
Episode length: 29.96 +/- 0.73
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.11      |
| time/                     |           |
|    total_timesteps        | 16543500  |
| train/                    |           |
|    explained_variance     | 0.405     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.49e-05  |
|    n_updates              | 3077      |
|    policy_objective       | -3.17e-07 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 606000.
Ep done - 607000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.06      |
| time/                     |           |
|    fps                    | 239       |
|    iterations             | 2700      |
|    time_elapsed           | 69169     |
|    total_timesteps        | 16588800  |
| train/                    |           |
|    explained_variance     | 0.387     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.47e-05  |
|    n_updates              | 3084      |
|    policy_objective       | -2.32e-07 |
|    value_loss             | 0.209     |
-----------------------------------------
Eval num_timesteps=16605000, episode_reward=0.14 +/- 0.99
Episode length: 30.03 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.145     |
| time/                     |           |
|    total_timesteps        | 16605000  |
| train/                    |           |
|    explained_variance     | 0.387     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.47e-05  |
|    n_updates              | 3087      |
|    policy_objective       | -2.42e-08 |
|    value_loss             | 0.213     |
-----------------------------------------
Ep done - 608000.
Ep done - 609000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 240      |
|    iterations             | 2710     |
|    time_elapsed           | 69350    |
|    total_timesteps        | 16650240 |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000734 |
|    learning_rate          | 4.45e-05 |
|    n_updates              | 3094     |
|    policy_objective       | 28.1     |
|    value_loss             | 0.206    |
----------------------------------------
Ep done - 610000.
Eval num_timesteps=16666500, episode_reward=0.07 +/- 0.97
Episode length: 29.96 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.065    |
| time/                     |          |
|    total_timesteps        | 16666500 |
| train/                    |          |
|    explained_variance     | 0.449    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.45e-05 |
|    n_updates              | 3097     |
|    policy_objective       | -6.3e-07 |
|    value_loss             | 0.208    |
----------------------------------------
Ep done - 611000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.16      |
| time/                     |           |
|    fps                    | 240       |
|    iterations             | 2720      |
|    time_elapsed           | 69531     |
|    total_timesteps        | 16711680  |
| train/                    |           |
|    explained_variance     | 0.433     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.43e-05  |
|    n_updates              | 3104      |
|    policy_objective       | -1.55e-08 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 612000.
Eval num_timesteps=16728000, episode_reward=0.16 +/- 0.96
Episode length: 29.98 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.16      |
| time/                     |           |
|    total_timesteps        | 16728000  |
| train/                    |           |
|    explained_variance     | 0.442     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.43e-05  |
|    n_updates              | 3107      |
|    policy_objective       | -2.39e-07 |
|    value_loss             | 0.206     |
-----------------------------------------
Ep done - 613000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.22     |
| time/                     |          |
|    fps                    | 240      |
|    iterations             | 2730     |
|    time_elapsed           | 69712    |
|    total_timesteps        | 16773120 |
| train/                    |          |
|    explained_variance     | 0.422    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.41e-05 |
|    n_updates              | 3114     |
|    policy_objective       | 4.22e-07 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 614000.
Eval num_timesteps=16789500, episode_reward=0.15 +/- 0.98
Episode length: 30.02 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.15      |
| time/                     |           |
|    total_timesteps        | 16789500  |
| train/                    |           |
|    explained_variance     | 0.454     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.4e-05   |
|    n_updates              | 3117      |
|    policy_objective       | -2.39e-07 |
|    value_loss             | 0.202     |
-----------------------------------------
Ep done - 615000.
Ep done - 616000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 240      |
|    iterations             | 2740     |
|    time_elapsed           | 69893    |
|    total_timesteps        | 16834560 |
| train/                    |          |
|    explained_variance     | 0.436    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.39e-05 |
|    n_updates              | 3124     |
|    policy_objective       | 2.15e-07 |
|    value_loss             | 0.192    |
----------------------------------------
Eval num_timesteps=16851000, episode_reward=0.07 +/- 0.98
Episode length: 29.95 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.065    |
| time/                     |          |
|    total_timesteps        | 16851000 |
| train/                    |          |
|    explained_variance     | 0.382    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.38e-05 |
|    n_updates              | 3127     |
|    policy_objective       | 5.28e-08 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 617000.
Ep done - 618000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 241      |
|    iterations             | 2750     |
|    time_elapsed           | 70074    |
|    total_timesteps        | 16896000 |
| train/                    |          |
|    explained_variance     | 0.428    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.37e-05 |
|    n_updates              | 3134     |
|    policy_objective       | 5.87e-08 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 619000.
Eval num_timesteps=16912500, episode_reward=0.20 +/- 0.97
Episode length: 29.99 +/- 0.50
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.195     |
| time/                     |           |
|    total_timesteps        | 16912500  |
| train/                    |           |
|    explained_variance     | 0.38      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.36e-05  |
|    n_updates              | 3137      |
|    policy_objective       | -2.36e-08 |
|    value_loss             | 0.206     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.195
SELFPLAY: new best model, bumping up generation to 62
Ep done - 620000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.07     |
| time/                     |          |
|    fps                    | 241      |
|    iterations             | 2760     |
|    time_elapsed           | 70256    |
|    total_timesteps        | 16957440 |
| train/                    |          |
|    explained_variance     | 0.461    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.35e-05 |
|    n_updates              | 3144     |
|    policy_objective       | 4.66e-07 |
|    value_loss             | 0.2      |
----------------------------------------
Ep done - 621000.
Eval num_timesteps=16974000, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 0.66
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 16974000 |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.34e-05 |
|    n_updates              | 3147     |
|    policy_objective       | 2.89e-07 |
|    value_loss             | 0.198    |
----------------------------------------
Ep done - 622000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.08     |
| time/                     |           |
|    fps                    | 241       |
|    iterations             | 2770      |
|    time_elapsed           | 70437     |
|    total_timesteps        | 17018880  |
| train/                    |           |
|    explained_variance     | 0.423     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.33e-05  |
|    n_updates              | 3154      |
|    policy_objective       | -3.98e-07 |
|    value_loss             | 0.202     |
-----------------------------------------
Ep done - 623000.
Eval num_timesteps=17035500, episode_reward=0.15 +/- 0.96
Episode length: 30.06 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 17035500 |
| train/                    |          |
|    explained_variance     | 0.383    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.32e-05 |
|    n_updates              | 3157     |
|    policy_objective       | 2.17e-07 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 624000.
Ep done - 625000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 241      |
|    iterations             | 2780     |
|    time_elapsed           | 70619    |
|    total_timesteps        | 17080320 |
| train/                    |          |
|    explained_variance     | 0.475    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.31e-05 |
|    n_updates              | 3164     |
|    policy_objective       | 3.24e-07 |
|    value_loss             | 0.197    |
----------------------------------------
Eval num_timesteps=17097000, episode_reward=0.07 +/- 0.98
Episode length: 29.91 +/- 1.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.065     |
| time/                     |           |
|    total_timesteps        | 17097000  |
| train/                    |           |
|    explained_variance     | 0.349     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.3e-05   |
|    n_updates              | 3167      |
|    policy_objective       | -2.45e-07 |
|    value_loss             | 0.213     |
-----------------------------------------
Ep done - 626000.
Ep done - 627000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.12      |
| time/                     |           |
|    fps                    | 242       |
|    iterations             | 2790      |
|    time_elapsed           | 70801     |
|    total_timesteps        | 17141760  |
| train/                    |           |
|    explained_variance     | 0.429     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.29e-05  |
|    n_updates              | 3174      |
|    policy_objective       | -8.51e-08 |
|    value_loss             | 0.196     |
-----------------------------------------
Ep done - 628000.
Eval num_timesteps=17158500, episode_reward=0.11 +/- 0.98
Episode length: 30.05 +/- 0.63
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 17158500 |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.28e-05 |
|    n_updates              | 3177     |
|    policy_objective       | 2.94e-07 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 629000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 242      |
|    iterations             | 2800     |
|    time_elapsed           | 70982    |
|    total_timesteps        | 17203200 |
| train/                    |          |
|    explained_variance     | 0.439    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.27e-05 |
|    n_updates              | 3184     |
|    policy_objective       | 6.77e-08 |
|    value_loss             | 0.2      |
----------------------------------------
Ep done - 630000.
Eval num_timesteps=17220000, episode_reward=0.09 +/- 0.99
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.09      |
| time/                     |           |
|    total_timesteps        | 17220000  |
| train/                    |           |
|    explained_variance     | 0.48      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.26e-05  |
|    n_updates              | 3187      |
|    policy_objective       | -1.41e-07 |
|    value_loss             | 0.191     |
-----------------------------------------
Ep done - 631000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.8      |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 242       |
|    iterations             | 2810      |
|    time_elapsed           | 71164     |
|    total_timesteps        | 17264640  |
| train/                    |           |
|    explained_variance     | 0.438     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.25e-05  |
|    n_updates              | 3194      |
|    policy_objective       | -2.48e-07 |
|    value_loss             | 0.216     |
-----------------------------------------
Ep done - 632000.
Eval num_timesteps=17281500, episode_reward=0.29 +/- 0.94
Episode length: 30.07 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.29     |
| time/                     |          |
|    total_timesteps        | 17281500 |
| train/                    |          |
|    explained_variance     | 0.48     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.24e-05 |
|    n_updates              | 3197     |
|    policy_objective       | 1.2e-07  |
|    value_loss             | 0.192    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.29
SELFPLAY: new best model, bumping up generation to 63
Ep done - 633000.
Ep done - 634000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.13     |
| time/                     |           |
|    fps                    | 242       |
|    iterations             | 2820      |
|    time_elapsed           | 71346     |
|    total_timesteps        | 17326080  |
| train/                    |           |
|    explained_variance     | 0.381     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.23e-05  |
|    n_updates              | 3204      |
|    policy_objective       | -3.21e-07 |
|    value_loss             | 0.203     |
-----------------------------------------
Eval num_timesteps=17343000, episode_reward=0.23 +/- 0.95
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.23      |
| time/                     |           |
|    total_timesteps        | 17343000  |
| train/                    |           |
|    explained_variance     | 0.363     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.22e-05  |
|    n_updates              | 3207      |
|    policy_objective       | -3.22e-07 |
|    value_loss             | 0.21      |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.23
SELFPLAY: new best model, bumping up generation to 64
Ep done - 635000.
Ep done - 636000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.15     |
| time/                     |           |
|    fps                    | 243       |
|    iterations             | 2830      |
|    time_elapsed           | 71527     |
|    total_timesteps        | 17387520  |
| train/                    |           |
|    explained_variance     | 0.397     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.21e-05  |
|    n_updates              | 3214      |
|    policy_objective       | -3.56e-07 |
|    value_loss             | 0.195     |
-----------------------------------------
Ep done - 637000.
Eval num_timesteps=17404500, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.025    |
| time/                     |          |
|    total_timesteps        | 17404500 |
| train/                    |          |
|    explained_variance     | 0.391    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.2e-05  |
|    n_updates              | 3217     |
|    policy_objective       | 1.12e-08 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 638000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.11     |
| time/                     |           |
|    fps                    | 243       |
|    iterations             | 2840      |
|    time_elapsed           | 71709     |
|    total_timesteps        | 17448960  |
| train/                    |           |
|    explained_variance     | 0.428     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.19e-05  |
|    n_updates              | 3224      |
|    policy_objective       | -1.37e-07 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 639000.
Eval num_timesteps=17466000, episode_reward=0.14 +/- 0.97
Episode length: 29.93 +/- 1.25
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 17466000 |
| train/                    |          |
|    explained_variance     | 0.421    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.18e-05 |
|    n_updates              | 3227     |
|    policy_objective       | 8.51e-08 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 640000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.23      |
| time/                     |           |
|    fps                    | 243       |
|    iterations             | 2850      |
|    time_elapsed           | 71891     |
|    total_timesteps        | 17510400  |
| train/                    |           |
|    explained_variance     | 0.452     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.17e-05  |
|    n_updates              | 3234      |
|    policy_objective       | -2.33e-07 |
|    value_loss             | 0.198     |
-----------------------------------------
Ep done - 641000.
Eval num_timesteps=17527500, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.035     |
| time/                     |           |
|    total_timesteps        | 17527500  |
| train/                    |           |
|    explained_variance     | 0.498     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.16e-05  |
|    n_updates              | 3237      |
|    policy_objective       | -2.19e-07 |
|    value_loss             | 0.182     |
-----------------------------------------
Ep done - 642000.
Ep done - 643000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.18      |
| time/                     |           |
|    fps                    | 243       |
|    iterations             | 2860      |
|    time_elapsed           | 72072     |
|    total_timesteps        | 17571840  |
| train/                    |           |
|    explained_variance     | 0.435     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.14e-05  |
|    n_updates              | 3244      |
|    policy_objective       | -5.59e-08 |
|    value_loss             | 0.212     |
-----------------------------------------
Eval num_timesteps=17589000, episode_reward=0.17 +/- 0.97
Episode length: 29.98 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.175     |
| time/                     |           |
|    total_timesteps        | 17589000  |
| train/                    |           |
|    explained_variance     | 0.51      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.14e-05  |
|    n_updates              | 3247      |
|    policy_objective       | -1.39e-07 |
|    value_loss             | 0.189     |
-----------------------------------------
Ep done - 644000.
Ep done - 645000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.12     |
| time/                     |           |
|    fps                    | 244       |
|    iterations             | 2870      |
|    time_elapsed           | 72254     |
|    total_timesteps        | 17633280  |
| train/                    |           |
|    explained_variance     | 0.367     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.12e-05  |
|    n_updates              | 3254      |
|    policy_objective       | -5.29e-07 |
|    value_loss             | 0.214     |
-----------------------------------------
Ep done - 646000.
Eval num_timesteps=17650500, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.105     |
| time/                     |           |
|    total_timesteps        | 17650500  |
| train/                    |           |
|    explained_variance     | 0.488     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.12e-05  |
|    n_updates              | 3257      |
|    policy_objective       | -5.96e-08 |
|    value_loss             | 0.185     |
-----------------------------------------
Ep done - 647000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.04    |
| time/                     |          |
|    fps                    | 244      |
|    iterations             | 2880     |
|    time_elapsed           | 72436    |
|    total_timesteps        | 17694720 |
| train/                    |          |
|    explained_variance     | 0.409    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.1e-05  |
|    n_updates              | 3264     |
|    policy_objective       | 1.15e-07 |
|    value_loss             | 0.203    |
----------------------------------------
Ep done - 648000.
Eval num_timesteps=17712000, episode_reward=0.23 +/- 0.96
Episode length: 30.03 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.225     |
| time/                     |           |
|    total_timesteps        | 17712000  |
| train/                    |           |
|    explained_variance     | 0.472     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.1e-05   |
|    n_updates              | 3267      |
|    policy_objective       | -2.45e-07 |
|    value_loss             | 0.2       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 65
Ep done - 649000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.8      |
|    ep_rew_mean            | 0.11      |
| time/                     |           |
|    fps                    | 244       |
|    iterations             | 2890      |
|    time_elapsed           | 72617     |
|    total_timesteps        | 17756160  |
| train/                    |           |
|    explained_variance     | 0.366     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.08e-05  |
|    n_updates              | 3274      |
|    policy_objective       | -3.46e-07 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 650000.
Eval num_timesteps=17773500, episode_reward=0.06 +/- 0.99
Episode length: 29.96 +/- 0.65
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.06     |
| time/                     |          |
|    total_timesteps        | 17773500 |
| train/                    |          |
|    explained_variance     | 0.425    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.08e-05 |
|    n_updates              | 3277     |
|    policy_objective       | 2.3e-07  |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 651000.
Ep done - 652000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 244      |
|    iterations             | 2900     |
|    time_elapsed           | 72798    |
|    total_timesteps        | 17817600 |
| train/                    |          |
|    explained_variance     | 0.443    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.06e-05 |
|    n_updates              | 3284     |
|    policy_objective       | 2.88e-07 |
|    value_loss             | 0.197    |
----------------------------------------
Eval num_timesteps=17835000, episode_reward=0.10 +/- 0.96
Episode length: 29.93 +/- 1.23
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.095     |
| time/                     |           |
|    total_timesteps        | 17835000  |
| train/                    |           |
|    explained_variance     | 0.485     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.06e-05  |
|    n_updates              | 3287      |
|    policy_objective       | -2.02e-07 |
|    value_loss             | 0.191     |
-----------------------------------------
Ep done - 653000.
Ep done - 654000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 244      |
|    iterations             | 2910     |
|    time_elapsed           | 72979    |
|    total_timesteps        | 17879040 |
| train/                    |          |
|    explained_variance     | 0.386    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.04e-05 |
|    n_updates              | 3294     |
|    policy_objective       | 1.13e-07 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 655000.
Eval num_timesteps=17896500, episode_reward=0.09 +/- 0.98
Episode length: 30.03 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 17896500 |
| train/                    |          |
|    explained_variance     | 0.506    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.04e-05 |
|    n_updates              | 3297     |
|    policy_objective       | 1.99e-08 |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 656000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.05     |
| time/                     |           |
|    fps                    | 245       |
|    iterations             | 2920      |
|    time_elapsed           | 73160     |
|    total_timesteps        | 17940480  |
| train/                    |           |
|    explained_variance     | 0.418     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.02e-05  |
|    n_updates              | 3304      |
|    policy_objective       | -1.95e-07 |
|    value_loss             | 0.198     |
-----------------------------------------
Ep done - 657000.
Eval num_timesteps=17958000, episode_reward=0.21 +/- 0.97
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.215    |
| time/                     |          |
|    total_timesteps        | 17958000 |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.02e-05 |
|    n_updates              | 3307     |
|    policy_objective       | 5.9e-09  |
|    value_loss             | 0.204    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.215
SELFPLAY: new best model, bumping up generation to 66
Ep done - 658000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.7      |
|    ep_rew_mean            | -0.09     |
| time/                     |           |
|    fps                    | 245       |
|    iterations             | 2930      |
|    time_elapsed           | 73341     |
|    total_timesteps        | 18001920  |
| train/                    |           |
|    explained_variance     | 0.386     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4e-05     |
|    n_updates              | 3314      |
|    policy_objective       | -2.69e-07 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 659000.
Eval num_timesteps=18019500, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.13      |
| time/                     |           |
|    total_timesteps        | 18019500  |
| train/                    |           |
|    explained_variance     | 0.4       |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4e-05     |
|    n_updates              | 3317      |
|    policy_objective       | -1.94e-07 |
|    value_loss             | 0.206     |
-----------------------------------------
Ep done - 660000.
Ep done - 661000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.28      |
| time/                     |           |
|    fps                    | 245       |
|    iterations             | 2940      |
|    time_elapsed           | 73522     |
|    total_timesteps        | 18063360  |
| train/                    |           |
|    explained_variance     | 0.384     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.98e-05  |
|    n_updates              | 3324      |
|    policy_objective       | -3.64e-07 |
|    value_loss             | 0.204     |
-----------------------------------------
Eval num_timesteps=18081000, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.085     |
| time/                     |           |
|    total_timesteps        | 18081000  |
| train/                    |           |
|    explained_variance     | 0.452     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.97e-05  |
|    n_updates              | 3327      |
|    policy_objective       | -1.61e-08 |
|    value_loss             | 0.192     |
-----------------------------------------
Ep done - 662000.
Ep done - 663000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 245      |
|    iterations             | 2950     |
|    time_elapsed           | 73704    |
|    total_timesteps        | 18124800 |
| train/                    |          |
|    explained_variance     | 0.403    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.96e-05 |
|    n_updates              | 3334     |
|    policy_objective       | 1.19e-07 |
|    value_loss             | 0.205    |
----------------------------------------
Ep done - 664000.
Eval num_timesteps=18142500, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.50
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.135     |
| time/                     |           |
|    total_timesteps        | 18142500  |
| train/                    |           |
|    explained_variance     | 0.514     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.95e-05  |
|    n_updates              | 3337      |
|    policy_objective       | -2.92e-08 |
|    value_loss             | 0.188     |
-----------------------------------------
Ep done - 665000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.02     |
| time/                     |          |
|    fps                    | 246      |
|    iterations             | 2960     |
|    time_elapsed           | 73885    |
|    total_timesteps        | 18186240 |
| train/                    |          |
|    explained_variance     | 0.389    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.94e-05 |
|    n_updates              | 3344     |
|    policy_objective       | 7.84e-08 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 666000.
Eval num_timesteps=18204000, episode_reward=0.10 +/- 0.98
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 18204000 |
| train/                    |          |
|    explained_variance     | 0.44     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.93e-05 |
|    n_updates              | 3347     |
|    policy_objective       | 4.53e-08 |
|    value_loss             | 0.203    |
----------------------------------------
Ep done - 667000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.19     |
| time/                     |           |
|    fps                    | 246       |
|    iterations             | 2970      |
|    time_elapsed           | 74066     |
|    total_timesteps        | 18247680  |
| train/                    |           |
|    explained_variance     | 0.38      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.92e-05  |
|    n_updates              | 3354      |
|    policy_objective       | -2.11e-08 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 668000.
Eval num_timesteps=18265500, episode_reward=0.17 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.17      |
| time/                     |           |
|    total_timesteps        | 18265500  |
| train/                    |           |
|    explained_variance     | 0.459     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.91e-05  |
|    n_updates              | 3357      |
|    policy_objective       | -2.65e-07 |
|    value_loss             | 0.196     |
-----------------------------------------
Ep done - 669000.
Ep done - 670000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 246      |
|    iterations             | 2980     |
|    time_elapsed           | 74247    |
|    total_timesteps        | 18309120 |
| train/                    |          |
|    explained_variance     | 0.449    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.9e-05  |
|    n_updates              | 3364     |
|    policy_objective       | 2.3e-08  |
|    value_loss             | 0.194    |
----------------------------------------
Eval num_timesteps=18327000, episode_reward=0.22 +/- 0.96
Episode length: 30.04 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.22      |
| time/                     |           |
|    total_timesteps        | 18327000  |
| train/                    |           |
|    explained_variance     | 0.446     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.89e-05  |
|    n_updates              | 3367      |
|    policy_objective       | -6.71e-08 |
|    value_loss             | 0.198     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.22
SELFPLAY: new best model, bumping up generation to 67
Ep done - 671000.
Ep done - 672000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.14    |
| time/                     |          |
|    fps                    | 246      |
|    iterations             | 2990     |
|    time_elapsed           | 74429    |
|    total_timesteps        | 18370560 |
| train/                    |          |
|    explained_variance     | 0.405    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.88e-05 |
|    n_updates              | 3374     |
|    policy_objective       | 2.25e-07 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 673000.
Eval num_timesteps=18388500, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 18388500 |
| train/                    |          |
|    explained_variance     | 0.48     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.87e-05 |
|    n_updates              | 3377     |
|    policy_objective       | 2.39e-08 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 674000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 247      |
|    iterations             | 3000     |
|    time_elapsed           | 74610    |
|    total_timesteps        | 18432000 |
| train/                    |          |
|    explained_variance     | 0.481    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.86e-05 |
|    n_updates              | 3384     |
|    policy_objective       | 1.38e-07 |
|    value_loss             | 0.196    |
----------------------------------------
Ep done - 675000.
Eval num_timesteps=18450000, episode_reward=0.14 +/- 0.96
Episode length: 29.91 +/- 1.25
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 18450000 |
| train/                    |          |
|    explained_variance     | 0.381    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.85e-05 |
|    n_updates              | 3387     |
|    policy_objective       | 1.23e-07 |
|    value_loss             | 0.211    |
----------------------------------------
Ep done - 676000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.07     |
| time/                     |           |
|    fps                    | 247       |
|    iterations             | 3010      |
|    time_elapsed           | 74791     |
|    total_timesteps        | 18493440  |
| train/                    |           |
|    explained_variance     | 0.449     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.84e-05  |
|    n_updates              | 3394      |
|    policy_objective       | -4.83e-08 |
|    value_loss             | 0.198     |
-----------------------------------------
Ep done - 677000.
Eval num_timesteps=18511500, episode_reward=0.14 +/- 0.97
Episode length: 29.99 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.14      |
| time/                     |           |
|    total_timesteps        | 18511500  |
| train/                    |           |
|    explained_variance     | 0.378     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.83e-05  |
|    n_updates              | 3397      |
|    policy_objective       | -1.67e-07 |
|    value_loss             | 0.211     |
-----------------------------------------
Ep done - 678000.
Ep done - 679000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.12    |
| time/                     |          |
|    fps                    | 247      |
|    iterations             | 3020     |
|    time_elapsed           | 74972    |
|    total_timesteps        | 18554880 |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.82e-05 |
|    n_updates              | 3404     |
|    policy_objective       | 1.06e-07 |
|    value_loss             | 0.203    |
----------------------------------------
Eval num_timesteps=18573000, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 18573000 |
| train/                    |          |
|    explained_variance     | 0.464    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.81e-05 |
|    n_updates              | 3407     |
|    policy_objective       | 4.24e-07 |
|    value_loss             | 0.19     |
----------------------------------------
Ep done - 680000.
Ep done - 681000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 247      |
|    iterations             | 3030     |
|    time_elapsed           | 75153    |
|    total_timesteps        | 18616320 |
| train/                    |          |
|    explained_variance     | 0.476    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.8e-05  |
|    n_updates              | 3414     |
|    policy_objective       | 1.97e-07 |
|    value_loss             | 0.184    |
----------------------------------------
Ep done - 682000.
Eval num_timesteps=18634500, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.64
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 18634500 |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.79e-05 |
|    n_updates              | 3417     |
|    policy_objective       | 3.65e-07 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 683000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.01    |
| time/                     |          |
|    fps                    | 247      |
|    iterations             | 3040     |
|    time_elapsed           | 75335    |
|    total_timesteps        | 18677760 |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.78e-05 |
|    n_updates              | 3424     |
|    policy_objective       | 2.37e-07 |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 684000.
Eval num_timesteps=18696000, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 18696000 |
| train/                    |          |
|    explained_variance     | 0.452    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.77e-05 |
|    n_updates              | 3427     |
|    policy_objective       | 0        |
|    value_loss             | 0.198    |
----------------------------------------
Ep done - 685000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 248      |
|    iterations             | 3050     |
|    time_elapsed           | 75516    |
|    total_timesteps        | 18739200 |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.76e-05 |
|    n_updates              | 3434     |
|    policy_objective       | 3.97e-08 |
|    value_loss             | 0.214    |
----------------------------------------
Ep done - 686000.
Eval num_timesteps=18757500, episode_reward=0.10 +/- 0.97
Episode length: 30.03 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.105    |
| time/                     |          |
|    total_timesteps        | 18757500 |
| train/                    |          |
|    explained_variance     | 0.361    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.75e-05 |
|    n_updates              | 3437     |
|    policy_objective       | 4.35e-09 |
|    value_loss             | 0.211    |
----------------------------------------
Ep done - 687000.
Ep done - 688000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 248      |
|    iterations             | 3060     |
|    time_elapsed           | 75697    |
|    total_timesteps        | 18800640 |
| train/                    |          |
|    explained_variance     | 0.451    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.74e-05 |
|    n_updates              | 3444     |
|    policy_objective       | 2.58e-08 |
|    value_loss             | 0.204    |
----------------------------------------
Eval num_timesteps=18819000, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.125     |
| time/                     |           |
|    total_timesteps        | 18819000  |
| train/                    |           |
|    explained_variance     | 0.48      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.73e-05  |
|    n_updates              | 3447      |
|    policy_objective       | -4.97e-08 |
|    value_loss             | 0.196     |
-----------------------------------------
Ep done - 689000.
Ep done - 690000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.06     |
| time/                     |          |
|    fps                    | 248      |
|    iterations             | 3070     |
|    time_elapsed           | 75878    |
|    total_timesteps        | 18862080 |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.71e-05 |
|    n_updates              | 3454     |
|    policy_objective       | 3.37e-07 |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 691000.
Eval num_timesteps=18880500, episode_reward=0.04 +/- 0.99
Episode length: 30.00 +/- 0.46
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.035     |
| time/                     |           |
|    total_timesteps        | 18880500  |
| train/                    |           |
|    explained_variance     | 0.4       |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.71e-05  |
|    n_updates              | 3457      |
|    policy_objective       | -2.15e-07 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 692000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 248      |
|    iterations             | 3080     |
|    time_elapsed           | 76059    |
|    total_timesteps        | 18923520 |
| train/                    |          |
|    explained_variance     | 0.452    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.69e-05 |
|    n_updates              | 3464     |
|    policy_objective       | 7.95e-08 |
|    value_loss             | 0.179    |
----------------------------------------
Ep done - 693000.
Eval num_timesteps=18942000, episode_reward=0.20 +/- 0.95
Episode length: 30.05 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 18942000 |
| train/                    |          |
|    explained_variance     | 0.431    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.69e-05 |
|    n_updates              | 3468     |
|    policy_objective       | 1.53e-07 |
|    value_loss             | 0.211    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 68
Ep done - 694000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 249      |
|    iterations             | 3090     |
|    time_elapsed           | 76241    |
|    total_timesteps        | 18984960 |
| train/                    |          |
|    explained_variance     | 0.464    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.67e-05 |
|    n_updates              | 3474     |
|    policy_objective       | 1.79e-07 |
|    value_loss             | 0.194    |
----------------------------------------
Ep done - 695000.
Eval num_timesteps=19003500, episode_reward=0.15 +/- 0.98
Episode length: 30.03 +/- 0.47
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 19003500 |
| train/                    |          |
|    explained_variance     | 0.476    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.67e-05 |
|    n_updates              | 3478     |
|    policy_objective       | 1.07e-07 |
|    value_loss             | 0.199    |
----------------------------------------
Ep done - 696000.
Ep done - 697000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.09     |
| time/                     |           |
|    fps                    | 249       |
|    iterations             | 3100      |
|    time_elapsed           | 76422     |
|    total_timesteps        | 19046400  |
| train/                    |           |
|    explained_variance     | 0.384     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.65e-05  |
|    n_updates              | 3484      |
|    policy_objective       | -1.73e-07 |
|    value_loss             | 0.215     |
-----------------------------------------
Eval num_timesteps=19065000, episode_reward=0.17 +/- 0.97
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.165     |
| time/                     |           |
|    total_timesteps        | 19065000  |
| train/                    |           |
|    explained_variance     | 0.437     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.65e-05  |
|    n_updates              | 3488      |
|    policy_objective       | -3.82e-08 |
|    value_loss             | 0.2       |
-----------------------------------------
Ep done - 698000.
Ep done - 699000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 249       |
|    iterations             | 3110      |
|    time_elapsed           | 76603     |
|    total_timesteps        | 19107840  |
| train/                    |           |
|    explained_variance     | 0.437     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.63e-05  |
|    n_updates              | 3494      |
|    policy_objective       | -6.95e-08 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 700000.
Eval num_timesteps=19126500, episode_reward=0.04 +/- 0.98
Episode length: 30.04 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.045    |
| time/                     |          |
|    total_timesteps        | 19126500 |
| train/                    |          |
|    explained_variance     | 0.445    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.62e-05 |
|    n_updates              | 3498     |
|    policy_objective       | 1.32e-07 |
|    value_loss             | 0.201    |
----------------------------------------
Ep done - 701000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.01    |
| time/                     |          |
|    fps                    | 249      |
|    iterations             | 3120     |
|    time_elapsed           | 76785    |
|    total_timesteps        | 19169280 |
| train/                    |          |
|    explained_variance     | 0.438    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.61e-05 |
|    n_updates              | 3504     |
|    policy_objective       | 7.85e-08 |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 702000.
Eval num_timesteps=19188000, episode_reward=0.22 +/- 0.97
Episode length: 29.99 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 19188000 |
| train/                    |          |
|    explained_variance     | 0.464    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.6e-05  |
|    n_updates              | 3508     |
|    policy_objective       | 2.52e-07 |
|    value_loss             | 0.196    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.22
SELFPLAY: new best model, bumping up generation to 69
Ep done - 703000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.14     |
| time/                     |           |
|    fps                    | 249       |
|    iterations             | 3130      |
|    time_elapsed           | 76966     |
|    total_timesteps        | 19230720  |
| train/                    |           |
|    explained_variance     | 0.442     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.59e-05  |
|    n_updates              | 3514      |
|    policy_objective       | -1.63e-07 |
|    value_loss             | 0.2       |
-----------------------------------------
Ep done - 704000.
Eval num_timesteps=19249500, episode_reward=0.18 +/- 0.97
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 19249500 |
| train/                    |          |
|    explained_variance     | 0.434    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.58e-05 |
|    n_updates              | 3518     |
|    policy_objective       | 1.13e-07 |
|    value_loss             | 0.211    |
----------------------------------------
Ep done - 705000.
Ep done - 706000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.14      |
| time/                     |           |
|    fps                    | 250       |
|    iterations             | 3140      |
|    time_elapsed           | 77147     |
|    total_timesteps        | 19292160  |
| train/                    |           |
|    explained_variance     | 0.451     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.57e-05  |
|    n_updates              | 3524      |
|    policy_objective       | -1.84e-07 |
|    value_loss             | 0.205     |
-----------------------------------------
Eval num_timesteps=19311000, episode_reward=0.17 +/- 0.95
Episode length: 30.08 +/- 0.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.165     |
| time/                     |           |
|    total_timesteps        | 19311000  |
| train/                    |           |
|    explained_variance     | 0.443     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.56e-05  |
|    n_updates              | 3528      |
|    policy_objective       | -1.24e-07 |
|    value_loss             | 0.21      |
-----------------------------------------
Ep done - 707000.
Ep done - 708000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 250      |
|    iterations             | 3150     |
|    time_elapsed           | 77328    |
|    total_timesteps        | 19353600 |
| train/                    |          |
|    explained_variance     | 0.412    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.55e-05 |
|    n_updates              | 3534     |
|    policy_objective       | 1.04e-07 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 709000.
Eval num_timesteps=19372500, episode_reward=0.06 +/- 0.99
Episode length: 30.06 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.055     |
| time/                     |           |
|    total_timesteps        | 19372500  |
| train/                    |           |
|    explained_variance     | 0.443     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.54e-05  |
|    n_updates              | 3538      |
|    policy_objective       | -4.18e-07 |
|    value_loss             | 0.203     |
-----------------------------------------
Ep done - 710000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 250      |
|    iterations             | 3160     |
|    time_elapsed           | 77510    |
|    total_timesteps        | 19415040 |
| train/                    |          |
|    explained_variance     | 0.365    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.53e-05 |
|    n_updates              | 3544     |
|    policy_objective       | 4.1e-07  |
|    value_loss             | 0.217    |
----------------------------------------
Ep done - 711000.
Eval num_timesteps=19434000, episode_reward=0.13 +/- 0.97
Episode length: 29.87 +/- 1.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 19434000 |
| train/                    |          |
|    explained_variance     | 0.471    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.52e-05 |
|    n_updates              | 3548     |
|    policy_objective       | 5.71e-08 |
|    value_loss             | 0.204    |
----------------------------------------
Ep done - 712000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.03      |
| time/                     |           |
|    fps                    | 250       |
|    iterations             | 3170      |
|    time_elapsed           | 77691     |
|    total_timesteps        | 19476480  |
| train/                    |           |
|    explained_variance     | 0.461     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.51e-05  |
|    n_updates              | 3554      |
|    policy_objective       | -1.41e-08 |
|    value_loss             | 0.207     |
-----------------------------------------
Ep done - 713000.
Eval num_timesteps=19495500, episode_reward=0.15 +/- 0.98
Episode length: 30.05 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 19495500 |
| train/                    |          |
|    explained_variance     | 0.422    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.5e-05  |
|    n_updates              | 3558     |
|    policy_objective       | 8.2e-08  |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 714000.
Ep done - 715000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | -0.06    |
| time/                     |          |
|    fps                    | 250      |
|    iterations             | 3180     |
|    time_elapsed           | 77872    |
|    total_timesteps        | 19537920 |
| train/                    |          |
|    explained_variance     | 0.429    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.49e-05 |
|    n_updates              | 3564     |
|    policy_objective       | 1.17e-07 |
|    value_loss             | 0.209    |
----------------------------------------
Eval num_timesteps=19557000, episode_reward=0.16 +/- 0.98
Episode length: 30.02 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 19557000 |
| train/                    |          |
|    explained_variance     | 0.466    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.48e-05 |
|    n_updates              | 3568     |
|    policy_objective       | 1.06e-07 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 716000.
Ep done - 717000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.15     |
| time/                     |          |
|    fps                    | 251      |
|    iterations             | 3190     |
|    time_elapsed           | 78054    |
|    total_timesteps        | 19599360 |
| train/                    |          |
|    explained_variance     | 0.389    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.47e-05 |
|    n_updates              | 3574     |
|    policy_objective       | 7.79e-08 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 718000.
Eval num_timesteps=19618500, episode_reward=0.28 +/- 0.94
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.275    |
| time/                     |          |
|    total_timesteps        | 19618500 |
| train/                    |          |
|    explained_variance     | 0.397    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.46e-05 |
|    n_updates              | 3578     |
|    policy_objective       | 1.09e-07 |
|    value_loss             | 0.215    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 70
Ep done - 719000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.07      |
| time/                     |           |
|    fps                    | 251       |
|    iterations             | 3200      |
|    time_elapsed           | 78235     |
|    total_timesteps        | 19660800  |
| train/                    |           |
|    explained_variance     | 0.524     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.45e-05  |
|    n_updates              | 3584      |
|    policy_objective       | -3.97e-08 |
|    value_loss             | 0.18      |
-----------------------------------------
Ep done - 720000.
Eval num_timesteps=19680000, episode_reward=0.15 +/- 0.98
Episode length: 30.02 +/- 0.48
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.15      |
| time/                     |           |
|    total_timesteps        | 19680000  |
| train/                    |           |
|    explained_variance     | 0.446     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.44e-05  |
|    n_updates              | 3588      |
|    policy_objective       | -1.24e-08 |
|    value_loss             | 0.204     |
-----------------------------------------
Ep done - 721000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.17      |
| time/                     |           |
|    fps                    | 251       |
|    iterations             | 3210      |
|    time_elapsed           | 78417     |
|    total_timesteps        | 19722240  |
| train/                    |           |
|    explained_variance     | 0.47      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.43e-05  |
|    n_updates              | 3594      |
|    policy_objective       | -8.63e-08 |
|    value_loss             | 0.205     |
-----------------------------------------
Ep done - 722000.
Eval num_timesteps=19741500, episode_reward=0.10 +/- 0.98
Episode length: 29.96 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 19741500 |
| train/                    |          |
|    explained_variance     | 0.452    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.42e-05 |
|    n_updates              | 3598     |
|    policy_objective       | 2.01e-07 |
|    value_loss             | 0.191    |
----------------------------------------
Ep done - 723000.
Ep done - 724000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 251       |
|    iterations             | 3220      |
|    time_elapsed           | 78598     |
|    total_timesteps        | 19783680  |
| train/                    |           |
|    explained_variance     | 0.324     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.41e-05  |
|    n_updates              | 3604      |
|    policy_objective       | -5.29e-07 |
|    value_loss             | 0.232     |
-----------------------------------------
Eval num_timesteps=19803000, episode_reward=0.15 +/- 0.98
Episode length: 30.02 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.155     |
| time/                     |           |
|    total_timesteps        | 19803000  |
| train/                    |           |
|    explained_variance     | 0.476     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.4e-05   |
|    n_updates              | 3608      |
|    policy_objective       | -1.75e-07 |
|    value_loss             | 0.199     |
-----------------------------------------
Ep done - 725000.
Ep done - 726000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.04    |
| time/                     |          |
|    fps                    | 251      |
|    iterations             | 3230     |
|    time_elapsed           | 78779    |
|    total_timesteps        | 19845120 |
| train/                    |          |
|    explained_variance     | 0.454    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.39e-05 |
|    n_updates              | 3614     |
|    policy_objective       | 9.16e-08 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 727000.
Eval num_timesteps=19864500, episode_reward=0.20 +/- 0.97
Episode length: 30.09 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.195    |
| time/                     |          |
|    total_timesteps        | 19864500 |
| train/                    |          |
|    explained_variance     | 0.471    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.38e-05 |
|    n_updates              | 3618     |
|    policy_objective       | 4.07e-07 |
|    value_loss             | 0.2      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.195
SELFPLAY: new best model, bumping up generation to 71
Ep done - 728000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.15    |
| time/                     |          |
|    fps                    | 252      |
|    iterations             | 3240     |
|    time_elapsed           | 78961    |
|    total_timesteps        | 19906560 |
| train/                    |          |
|    explained_variance     | 0.355    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.37e-05 |
|    n_updates              | 3624     |
|    policy_objective       | 2.25e-07 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 729000.
Eval num_timesteps=19926000, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.46
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 19926000 |
| train/                    |          |
|    explained_variance     | 0.43     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.36e-05 |
|    n_updates              | 3628     |
|    policy_objective       | 1.01e-09 |
|    value_loss             | 0.217    |
----------------------------------------
Ep done - 730000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.02      |
| time/                     |           |
|    fps                    | 252       |
|    iterations             | 3250      |
|    time_elapsed           | 79142     |
|    total_timesteps        | 19968000  |
| train/                    |           |
|    explained_variance     | 0.476     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.35e-05  |
|    n_updates              | 3634      |
|    policy_objective       | -5.28e-07 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 731000.
Eval num_timesteps=19987500, episode_reward=0.15 +/- 0.97
Episode length: 29.93 +/- 1.17
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 19987500 |
| train/                    |          |
|    explained_variance     | 0.489    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.34e-05 |
|    n_updates              | 3638     |
|    policy_objective       | 8.88e-08 |
|    value_loss             | 0.192    |
----------------------------------------
Ep done - 732000.
Ep done - 733000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.1     |
| time/                     |          |
|    fps                    | 252      |
|    iterations             | 3260     |
|    time_elapsed           | 79324    |
|    total_timesteps        | 20029440 |
| train/                    |          |
|    explained_variance     | 0.455    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00075  |
|    learning_rate          | 3.33e-05 |
|    n_updates              | 3644     |
|    policy_objective       | 11.8     |
|    value_loss             | 0.194    |
----------------------------------------
Eval num_timesteps=20049000, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.08      |
| time/                     |           |
|    total_timesteps        | 20049000  |
| train/                    |           |
|    explained_variance     | 0.422     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.32e-05  |
|    n_updates              | 3648      |
|    policy_objective       | -1.66e-07 |
|    value_loss             | 0.215     |
-----------------------------------------
Ep done - 734000.
Ep done - 735000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.11    |
| time/                     |          |
|    fps                    | 252      |
|    iterations             | 3270     |
|    time_elapsed           | 79505    |
|    total_timesteps        | 20090880 |
| train/                    |          |
|    explained_variance     | 0.469    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.31e-05 |
|    n_updates              | 3654     |
|    policy_objective       | 5.74e-07 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 736000.
Eval num_timesteps=20110500, episode_reward=0.16 +/- 0.97
Episode length: 29.90 +/- 1.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.16      |
| time/                     |           |
|    total_timesteps        | 20110500  |
| train/                    |           |
|    explained_variance     | 0.369     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.3e-05   |
|    n_updates              | 3658      |
|    policy_objective       | -1.49e-07 |
|    value_loss             | 0.234     |
-----------------------------------------
Ep done - 737000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.16     |
| time/                     |           |
|    fps                    | 252       |
|    iterations             | 3280      |
|    time_elapsed           | 79687     |
|    total_timesteps        | 20152320  |
| train/                    |           |
|    explained_variance     | 0.444     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.28e-05  |
|    n_updates              | 3664      |
|    policy_objective       | -3.91e-08 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 738000.
Eval num_timesteps=20172000, episode_reward=0.12 +/- 0.98
Episode length: 29.94 +/- 1.32
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 20172000 |
| train/                    |          |
|    explained_variance     | 0.45     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.28e-05 |
|    n_updates              | 3668     |
|    policy_objective       | 2.01e-07 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 739000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.03      |
| time/                     |           |
|    fps                    | 253       |
|    iterations             | 3290      |
|    time_elapsed           | 79868     |
|    total_timesteps        | 20213760  |
| train/                    |           |
|    explained_variance     | 0.527     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.26e-05  |
|    n_updates              | 3674      |
|    policy_objective       | -1.35e-07 |
|    value_loss             | 0.196     |
-----------------------------------------
Ep done - 740000.
Eval num_timesteps=20233500, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.07     |
| time/                     |          |
|    total_timesteps        | 20233500 |
| train/                    |          |
|    explained_variance     | 0.389    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.26e-05 |
|    n_updates              | 3678     |
|    policy_objective       | 3.99e-07 |
|    value_loss             | 0.217    |
----------------------------------------
Ep done - 741000.
Ep done - 742000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.13    |
| time/                     |          |
|    fps                    | 253      |
|    iterations             | 3300     |
|    time_elapsed           | 80050    |
|    total_timesteps        | 20275200 |
| train/                    |          |
|    explained_variance     | 0.445    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.24e-05 |
|    n_updates              | 3684     |
|    policy_objective       | 2.12e-07 |
|    value_loss             | 0.198    |
----------------------------------------
Eval num_timesteps=20295000, episode_reward=0.20 +/- 0.96
Episode length: 30.00 +/- 1.05
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.195    |
| time/                     |          |
|    total_timesteps        | 20295000 |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.24e-05 |
|    n_updates              | 3688     |
|    policy_objective       | 1.8e-07  |
|    value_loss             | 0.209    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.195
SELFPLAY: new best model, bumping up generation to 72
Ep done - 743000.
Ep done - 744000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.07     |
| time/                     |          |
|    fps                    | 253      |
|    iterations             | 3310     |
|    time_elapsed           | 80231    |
|    total_timesteps        | 20336640 |
| train/                    |          |
|    explained_variance     | 0.417    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.22e-05 |
|    n_updates              | 3694     |
|    policy_objective       | 1.58e-07 |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 745000.
Eval num_timesteps=20356500, episode_reward=0.10 +/- 0.98
Episode length: 29.95 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.095     |
| time/                     |           |
|    total_timesteps        | 20356500  |
| train/                    |           |
|    explained_variance     | 0.355     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.21e-05  |
|    n_updates              | 3698      |
|    policy_objective       | -1.99e-08 |
|    value_loss             | 0.226     |
-----------------------------------------
Ep done - 746000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.12      |
| time/                     |           |
|    fps                    | 253       |
|    iterations             | 3320      |
|    time_elapsed           | 80412     |
|    total_timesteps        | 20398080  |
| train/                    |           |
|    explained_variance     | 0.471     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.2e-05   |
|    n_updates              | 3704      |
|    policy_objective       | -2.36e-08 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 747000.
Eval num_timesteps=20418000, episode_reward=0.17 +/- 0.97
Episode length: 29.98 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 20418000 |
| train/                    |          |
|    explained_variance     | 0.405    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.19e-05 |
|    n_updates              | 3708     |
|    policy_objective       | 1.21e-07 |
|    value_loss             | 0.217    |
----------------------------------------
Ep done - 748000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.21    |
| time/                     |          |
|    fps                    | 253      |
|    iterations             | 3330     |
|    time_elapsed           | 80593    |
|    total_timesteps        | 20459520 |
| train/                    |          |
|    explained_variance     | 0.487    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.18e-05 |
|    n_updates              | 3714     |
|    policy_objective       | 9.03e-08 |
|    value_loss             | 0.189    |
----------------------------------------
Ep done - 749000.
Eval num_timesteps=20479500, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.125     |
| time/                     |           |
|    total_timesteps        | 20479500  |
| train/                    |           |
|    explained_variance     | 0.426     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.17e-05  |
|    n_updates              | 3718      |
|    policy_objective       | -8.26e-08 |
|    value_loss             | 0.216     |
-----------------------------------------
Ep done - 750000.
Ep done - 751000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.1     |
| time/                     |          |
|    fps                    | 254      |
|    iterations             | 3340     |
|    time_elapsed           | 80775    |
|    total_timesteps        | 20520960 |
| train/                    |          |
|    explained_variance     | 0.457    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.16e-05 |
|    n_updates              | 3724     |
|    policy_objective       | 2.27e-07 |
|    value_loss             | 0.209    |
----------------------------------------
Eval num_timesteps=20541000, episode_reward=0.17 +/- 0.97
Episode length: 29.95 +/- 1.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 20541000 |
| train/                    |          |
|    explained_variance     | 0.464    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.15e-05 |
|    n_updates              | 3728     |
|    policy_objective       | 3.73e-08 |
|    value_loss             | 0.199    |
----------------------------------------
Ep done - 752000.
Ep done - 753000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.05      |
| time/                     |           |
|    fps                    | 254       |
|    iterations             | 3350      |
|    time_elapsed           | 80956     |
|    total_timesteps        | 20582400  |
| train/                    |           |
|    explained_variance     | 0.418     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.14e-05  |
|    n_updates              | 3734      |
|    policy_objective       | -2.03e-07 |
|    value_loss             | 0.214     |
-----------------------------------------
Ep done - 754000.
Eval num_timesteps=20602500, episode_reward=0.17 +/- 0.98
Episode length: 30.05 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 20602500 |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.13e-05 |
|    n_updates              | 3738     |
|    policy_objective       | 5.53e-08 |
|    value_loss             | 0.204    |
----------------------------------------
Ep done - 755000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.15    |
| time/                     |          |
|    fps                    | 254      |
|    iterations             | 3360     |
|    time_elapsed           | 81137    |
|    total_timesteps        | 20643840 |
| train/                    |          |
|    explained_variance     | 0.482    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.12e-05 |
|    n_updates              | 3744     |
|    policy_objective       | 1.33e-07 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 756000.
Eval num_timesteps=20664000, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 20664000 |
| train/                    |          |
|    explained_variance     | 0.421    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.11e-05 |
|    n_updates              | 3748     |
|    policy_objective       | 2.05e-07 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 757000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 254      |
|    iterations             | 3370     |
|    time_elapsed           | 81319    |
|    total_timesteps        | 20705280 |
| train/                    |          |
|    explained_variance     | 0.443    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.1e-05  |
|    n_updates              | 3754     |
|    policy_objective       | 1.06e-08 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 758000.
Eval num_timesteps=20725500, episode_reward=0.12 +/- 0.97
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 20725500 |
| train/                    |          |
|    explained_variance     | 0.322    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.09e-05 |
|    n_updates              | 3758     |
|    policy_objective       | 2.03e-07 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 759000.
Ep done - 760000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.08     |
| time/                     |          |
|    fps                    | 254      |
|    iterations             | 3380     |
|    time_elapsed           | 81500    |
|    total_timesteps        | 20766720 |
| train/                    |          |
|    explained_variance     | 0.423    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.08e-05 |
|    n_updates              | 3764     |
|    policy_objective       | 2.99e-07 |
|    value_loss             | 0.211    |
----------------------------------------
Eval num_timesteps=20787000, episode_reward=0.17 +/- 0.96
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 20787000 |
| train/                    |          |
|    explained_variance     | 0.504    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.07e-05 |
|    n_updates              | 3768     |
|    policy_objective       | 4.27e-07 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 761000.
Ep done - 762000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.25     |
| time/                     |          |
|    fps                    | 254      |
|    iterations             | 3390     |
|    time_elapsed           | 81681    |
|    total_timesteps        | 20828160 |
| train/                    |          |
|    explained_variance     | 0.467    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.06e-05 |
|    n_updates              | 3774     |
|    policy_objective       | 2.73e-08 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 763000.
Eval num_timesteps=20848500, episode_reward=0.19 +/- 0.97
Episode length: 29.93 +/- 1.32
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.19      |
| time/                     |           |
|    total_timesteps        | 20848500  |
| train/                    |           |
|    explained_variance     | 0.379     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.05e-05  |
|    n_updates              | 3778      |
|    policy_objective       | -2.56e-07 |
|    value_loss             | 0.219     |
-----------------------------------------
Ep done - 764000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.05    |
| time/                     |          |
|    fps                    | 255      |
|    iterations             | 3400     |
|    time_elapsed           | 81862    |
|    total_timesteps        | 20889600 |
| train/                    |          |
|    explained_variance     | 0.465    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.04e-05 |
|    n_updates              | 3784     |
|    policy_objective       | 1.4e-07  |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 765000.
Eval num_timesteps=20910000, episode_reward=0.14 +/- 0.97
Episode length: 30.01 +/- 0.66
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.145     |
| time/                     |           |
|    total_timesteps        | 20910000  |
| train/                    |           |
|    explained_variance     | 0.489     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.03e-05  |
|    n_updates              | 3788      |
|    policy_objective       | -1.18e-07 |
|    value_loss             | 0.192     |
-----------------------------------------
Ep done - 766000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.04     |
| time/                     |           |
|    fps                    | 255       |
|    iterations             | 3410      |
|    time_elapsed           | 82044     |
|    total_timesteps        | 20951040  |
| train/                    |           |
|    explained_variance     | 0.405     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.02e-05  |
|    n_updates              | 3794      |
|    policy_objective       | -2.61e-08 |
|    value_loss             | 0.213     |
-----------------------------------------
Ep done - 767000.
Eval num_timesteps=20971500, episode_reward=-0.01 +/- 0.99
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | -0.015   |
| time/                     |          |
|    total_timesteps        | 20971500 |
| train/                    |          |
|    explained_variance     | 0.406    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.01e-05 |
|    n_updates              | 3798     |
|    policy_objective       | 4.19e-08 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 768000.
Ep done - 769000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.06      |
| time/                     |           |
|    fps                    | 255       |
|    iterations             | 3420      |
|    time_elapsed           | 82225     |
|    total_timesteps        | 21012480  |
| train/                    |           |
|    explained_variance     | 0.427     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3e-05     |
|    n_updates              | 3804      |
|    policy_objective       | -1.43e-07 |
|    value_loss             | 0.217     |
-----------------------------------------
Eval num_timesteps=21033000, episode_reward=0.12 +/- 0.97
Episode length: 29.95 +/- 1.24
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.125     |
| time/                     |           |
|    total_timesteps        | 21033000  |
| train/                    |           |
|    explained_variance     | 0.4       |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.99e-05  |
|    n_updates              | 3808      |
|    policy_objective       | -3.65e-07 |
|    value_loss             | 0.226     |
-----------------------------------------
Ep done - 770000.
Ep done - 771000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.04     |
| time/                     |           |
|    fps                    | 255       |
|    iterations             | 3430      |
|    time_elapsed           | 82406     |
|    total_timesteps        | 21073920  |
| train/                    |           |
|    explained_variance     | 0.459     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.98e-05  |
|    n_updates              | 3814      |
|    policy_objective       | -2.31e-07 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 772000.
Eval num_timesteps=21094500, episode_reward=0.28 +/- 0.95
Episode length: 29.98 +/- 0.94
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.275     |
| time/                     |           |
|    total_timesteps        | 21094500  |
| train/                    |           |
|    explained_variance     | 0.493     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.97e-05  |
|    n_updates              | 3818      |
|    policy_objective       | -1.38e-07 |
|    value_loss             | 0.2       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.275
SELFPLAY: new best model, bumping up generation to 73
Ep done - 773000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 255      |
|    iterations             | 3440     |
|    time_elapsed           | 82587    |
|    total_timesteps        | 21135360 |
| train/                    |          |
|    explained_variance     | 0.481    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.96e-05 |
|    n_updates              | 3824     |
|    policy_objective       | -8.2e-08 |
|    value_loss             | 0.198    |
----------------------------------------
Ep done - 774000.
Eval num_timesteps=21156000, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.07      |
| time/                     |           |
|    total_timesteps        | 21156000  |
| train/                    |           |
|    explained_variance     | 0.49      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.95e-05  |
|    n_updates              | 3828      |
|    policy_objective       | -3.66e-07 |
|    value_loss             | 0.194     |
-----------------------------------------
Ep done - 775000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.19    |
| time/                     |          |
|    fps                    | 256      |
|    iterations             | 3450     |
|    time_elapsed           | 82769    |
|    total_timesteps        | 21196800 |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.94e-05 |
|    n_updates              | 3834     |
|    policy_objective       | -1.8e-07 |
|    value_loss             | 0.205    |
----------------------------------------
Ep done - 776000.
Eval num_timesteps=21217500, episode_reward=0.12 +/- 0.98
Episode length: 29.98 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.12      |
| time/                     |           |
|    total_timesteps        | 21217500  |
| train/                    |           |
|    explained_variance     | 0.494     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.93e-05  |
|    n_updates              | 3838      |
|    policy_objective       | -1.89e-08 |
|    value_loss             | 0.198     |
-----------------------------------------
Ep done - 777000.
Ep done - 778000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.04     |
| time/                     |           |
|    fps                    | 256       |
|    iterations             | 3460      |
|    time_elapsed           | 82950     |
|    total_timesteps        | 21258240  |
| train/                    |           |
|    explained_variance     | 0.41      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.92e-05  |
|    n_updates              | 3844      |
|    policy_objective       | -3.23e-07 |
|    value_loss             | 0.219     |
-----------------------------------------
Eval num_timesteps=21279000, episode_reward=0.23 +/- 0.95
Episode length: 30.05 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.235     |
| time/                     |           |
|    total_timesteps        | 21279000  |
| train/                    |           |
|    explained_variance     | 0.497     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.91e-05  |
|    n_updates              | 3848      |
|    policy_objective       | -3.09e-07 |
|    value_loss             | 0.199     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.235
SELFPLAY: new best model, bumping up generation to 74
Ep done - 779000.
Ep done - 780000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 256      |
|    iterations             | 3470     |
|    time_elapsed           | 83132    |
|    total_timesteps        | 21319680 |
| train/                    |          |
|    explained_variance     | 0.477    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.9e-05  |
|    n_updates              | 3854     |
|    policy_objective       | 1.51e-07 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 781000.
Eval num_timesteps=21340500, episode_reward=0.14 +/- 0.98
Episode length: 29.96 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.14      |
| time/                     |           |
|    total_timesteps        | 21340500  |
| train/                    |           |
|    explained_variance     | 0.556     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.89e-05  |
|    n_updates              | 3858      |
|    policy_objective       | -7.76e-08 |
|    value_loss             | 0.172     |
-----------------------------------------
Ep done - 782000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.08      |
| time/                     |           |
|    fps                    | 256       |
|    iterations             | 3480      |
|    time_elapsed           | 83313     |
|    total_timesteps        | 21381120  |
| train/                    |           |
|    explained_variance     | 0.47      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.88e-05  |
|    n_updates              | 3864      |
|    policy_objective       | -2.48e-08 |
|    value_loss             | 0.197     |
-----------------------------------------
Ep done - 783000.
Eval num_timesteps=21402000, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.53
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.105     |
| time/                     |           |
|    total_timesteps        | 21402000  |
| train/                    |           |
|    explained_variance     | 0.426     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.87e-05  |
|    n_updates              | 3868      |
|    policy_objective       | -1.69e-07 |
|    value_loss             | 0.213     |
-----------------------------------------
Ep done - 784000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.05     |
| time/                     |           |
|    fps                    | 256       |
|    iterations             | 3490      |
|    time_elapsed           | 83494     |
|    total_timesteps        | 21442560  |
| train/                    |           |
|    explained_variance     | 0.38      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.85e-05  |
|    n_updates              | 3874      |
|    policy_objective       | -7.45e-09 |
|    value_loss             | 0.239     |
-----------------------------------------
Ep done - 785000.
Eval num_timesteps=21463500, episode_reward=0.13 +/- 0.98
Episode length: 29.97 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 21463500 |
| train/                    |          |
|    explained_variance     | 0.495    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000753 |
|    learning_rate          | 2.85e-05 |
|    n_updates              | 3878     |
|    policy_objective       | 1.32     |
|    value_loss             | 0.191    |
----------------------------------------
Ep done - 786000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.11     |
| time/                     |          |
|    fps                    | 256      |
|    iterations             | 3500     |
|    time_elapsed           | 83676    |
|    total_timesteps        | 21504000 |
| train/                    |          |
|    explained_variance     | 0.432    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.83e-05 |
|    n_updates              | 3884     |
|    policy_objective       | 8.44e-08 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 787000.
Eval num_timesteps=21525000, episode_reward=0.14 +/- 0.98
Episode length: 30.05 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 21525000 |
| train/                    |          |
|    explained_variance     | 0.46     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.83e-05 |
|    n_updates              | 3888     |
|    policy_objective       | 2.96e-07 |
|    value_loss             | 0.208    |
----------------------------------------
Ep done - 788000.
Ep done - 789000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.12     |
| time/                     |          |
|    fps                    | 257      |
|    iterations             | 3510     |
|    time_elapsed           | 83857    |
|    total_timesteps        | 21565440 |
| train/                    |          |
|    explained_variance     | 0.432    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.81e-05 |
|    n_updates              | 3894     |
|    policy_objective       | 6.64e-08 |
|    value_loss             | 0.211    |
----------------------------------------
Ep done - 790000.
Eval num_timesteps=21586500, episode_reward=0.17 +/- 0.97
Episode length: 30.05 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 21586500 |
| train/                    |          |
|    explained_variance     | 0.422    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.81e-05 |
|    n_updates              | 3898     |
|    policy_objective       | -4.9e-08 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 791000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.07     |
| time/                     |          |
|    fps                    | 257      |
|    iterations             | 3520     |
|    time_elapsed           | 84039    |
|    total_timesteps        | 21626880 |
| train/                    |          |
|    explained_variance     | 0.454    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.79e-05 |
|    n_updates              | 3904     |
|    policy_objective       | 1.56e-07 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 792000.
Eval num_timesteps=21648000, episode_reward=0.15 +/- 0.97
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 21648000 |
| train/                    |          |
|    explained_variance     | 0.509    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.78e-05 |
|    n_updates              | 3908     |
|    policy_objective       | 3.41e-07 |
|    value_loss             | 0.192    |
----------------------------------------
Ep done - 793000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.02     |
| time/                     |           |
|    fps                    | 257       |
|    iterations             | 3530      |
|    time_elapsed           | 84220     |
|    total_timesteps        | 21688320  |
| train/                    |           |
|    explained_variance     | 0.465     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.77e-05  |
|    n_updates              | 3914      |
|    policy_objective       | -9.08e-08 |
|    value_loss             | 0.192     |
-----------------------------------------
Ep done - 794000.
Eval num_timesteps=21709500, episode_reward=0.09 +/- 0.97
Episode length: 30.00 +/- 0.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.085     |
| time/                     |           |
|    total_timesteps        | 21709500  |
| train/                    |           |
|    explained_variance     | 0.443     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.76e-05  |
|    n_updates              | 3918      |
|    policy_objective       | -5.49e-08 |
|    value_loss             | 0.218     |
-----------------------------------------
Ep done - 795000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 257      |
|    iterations             | 3540     |
|    time_elapsed           | 84402    |
|    total_timesteps        | 21749760 |
| train/                    |          |
|    explained_variance     | 0.47     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.75e-05 |
|    n_updates              | 3924     |
|    policy_objective       | 3.54e-07 |
|    value_loss             | 0.196    |
----------------------------------------
Ep done - 796000.
Eval num_timesteps=21771000, episode_reward=0.12 +/- 0.98
Episode length: 29.86 +/- 1.25
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.12     |
| time/                     |          |
|    total_timesteps        | 21771000 |
| train/                    |          |
|    explained_variance     | 0.522    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.74e-05 |
|    n_updates              | 3928     |
|    policy_objective       | 3.1e-07  |
|    value_loss             | 0.192    |
----------------------------------------
Ep done - 797000.
Ep done - 798000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 257       |
|    iterations             | 3550      |
|    time_elapsed           | 84583     |
|    total_timesteps        | 21811200  |
| train/                    |           |
|    explained_variance     | 0.503     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.73e-05  |
|    n_updates              | 3934      |
|    policy_objective       | -2.55e-07 |
|    value_loss             | 0.199     |
-----------------------------------------
Ep done - 799000.
Eval num_timesteps=21832500, episode_reward=0.17 +/- 0.96
Episode length: 30.00 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 21832500 |
| train/                    |          |
|    explained_variance     | 0.478    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.72e-05 |
|    n_updates              | 3938     |
|    policy_objective       | 1.07e-07 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 800000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.8     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 258      |
|    iterations             | 3560     |
|    time_elapsed           | 84765    |
|    total_timesteps        | 21872640 |
| train/                    |          |
|    explained_variance     | 0.504    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.71e-05 |
|    n_updates              | 3944     |
|    policy_objective       | 2.06e-07 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 801000.
Eval num_timesteps=21894000, episode_reward=0.06 +/- 0.97
Episode length: 29.93 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.055     |
| time/                     |           |
|    total_timesteps        | 21894000  |
| train/                    |           |
|    explained_variance     | 0.439     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.7e-05   |
|    n_updates              | 3948      |
|    policy_objective       | -1.03e-07 |
|    value_loss             | 0.211     |
-----------------------------------------
Ep done - 802000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 258      |
|    iterations             | 3570     |
|    time_elapsed           | 84946    |
|    total_timesteps        | 21934080 |
| train/                    |          |
|    explained_variance     | 0.495    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.69e-05 |
|    n_updates              | 3954     |
|    policy_objective       | 1.39e-07 |
|    value_loss             | 0.196    |
----------------------------------------
Ep done - 803000.
Eval num_timesteps=21955500, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.09      |
| time/                     |           |
|    total_timesteps        | 21955500  |
| train/                    |           |
|    explained_variance     | 0.446     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.68e-05  |
|    n_updates              | 3958      |
|    policy_objective       | -3.34e-07 |
|    value_loss             | 0.214     |
-----------------------------------------
Ep done - 804000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 258      |
|    iterations             | 3580     |
|    time_elapsed           | 85127    |
|    total_timesteps        | 21995520 |
| train/                    |          |
|    explained_variance     | 0.442    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.67e-05 |
|    n_updates              | 3964     |
|    policy_objective       | 2.83e-07 |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 805000.
Eval num_timesteps=22017000, episode_reward=0.17 +/- 0.96
Episode length: 29.96 +/- 1.34
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 22017000 |
| train/                    |          |
|    explained_variance     | 0.501    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.66e-05 |
|    n_updates              | 3968     |
|    policy_objective       | 1.71e-07 |
|    value_loss             | 0.206    |
----------------------------------------
Ep done - 806000.
Ep done - 807000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 258       |
|    iterations             | 3590      |
|    time_elapsed           | 85309     |
|    total_timesteps        | 22056960  |
| train/                    |           |
|    explained_variance     | 0.429     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.65e-05  |
|    n_updates              | 3974      |
|    policy_objective       | -1.08e-07 |
|    value_loss             | 0.215     |
-----------------------------------------
Ep done - 808000.
Eval num_timesteps=22078500, episode_reward=0.20 +/- 0.96
Episode length: 30.04 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.2      |
| time/                     |          |
|    total_timesteps        | 22078500 |
| train/                    |          |
|    explained_variance     | 0.486    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.64e-05 |
|    n_updates              | 3978     |
|    policy_objective       | 2.67e-07 |
|    value_loss             | 0.195    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 75
Ep done - 809000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.13     |
| time/                     |          |
|    fps                    | 258      |
|    iterations             | 3600     |
|    time_elapsed           | 85490    |
|    total_timesteps        | 22118400 |
| train/                    |          |
|    explained_variance     | 0.447    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.63e-05 |
|    n_updates              | 3984     |
|    policy_objective       | 4.33e-07 |
|    value_loss             | 0.211    |
----------------------------------------
Ep done - 810000.
Eval num_timesteps=22140000, episode_reward=-0.03 +/- 0.98
Episode length: 29.95 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | -0.025   |
| time/                     |          |
|    total_timesteps        | 22140000 |
| train/                    |          |
|    explained_variance     | 0.456    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.62e-05 |
|    n_updates              | 3988     |
|    policy_objective       | 8.26e-08 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 811000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.11    |
| time/                     |          |
|    fps                    | 258      |
|    iterations             | 3610     |
|    time_elapsed           | 85672    |
|    total_timesteps        | 22179840 |
| train/                    |          |
|    explained_variance     | 0.492    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000453 |
|    learning_rate          | 2.61e-05 |
|    n_updates              | 3994     |
|    policy_objective       | 0.88     |
|    value_loss             | 0.208    |
----------------------------------------
Ep done - 812000.
Eval num_timesteps=22201500, episode_reward=0.21 +/- 0.97
Episode length: 29.97 +/- 1.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.21      |
| time/                     |           |
|    total_timesteps        | 22201500  |
| train/                    |           |
|    explained_variance     | 0.42      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.6e-05   |
|    n_updates              | 3998      |
|    policy_objective       | -5.36e-07 |
|    value_loss             | 0.215     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.21
SELFPLAY: new best model, bumping up generation to 76
Ep done - 813000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.11     |
| time/                     |          |
|    fps                    | 259      |
|    iterations             | 3620     |
|    time_elapsed           | 85853    |
|    total_timesteps        | 22241280 |
| train/                    |          |
|    explained_variance     | 0.454    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.59e-05 |
|    n_updates              | 4004     |
|    policy_objective       | 2.11e-08 |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 814000.
Eval num_timesteps=22263000, episode_reward=0.09 +/- 0.97
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.085    |
| time/                     |          |
|    total_timesteps        | 22263000 |
| train/                    |          |
|    explained_variance     | 0.478    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.58e-05 |
|    n_updates              | 4008     |
|    policy_objective       | 3.29e-08 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 815000.
Ep done - 816000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.08    |
| time/                     |          |
|    fps                    | 259      |
|    iterations             | 3630     |
|    time_elapsed           | 86034    |
|    total_timesteps        | 22302720 |
| train/                    |          |
|    explained_variance     | 0.424    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.57e-05 |
|    n_updates              | 4014     |
|    policy_objective       | 4.95e-07 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 817000.
Eval num_timesteps=22324500, episode_reward=0.07 +/- 0.99
Episode length: 29.97 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.07     |
| time/                     |          |
|    total_timesteps        | 22324500 |
| train/                    |          |
|    explained_variance     | 0.48     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.56e-05 |
|    n_updates              | 4018     |
|    policy_objective       | 3.95e-07 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 818000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.07     |
| time/                     |           |
|    fps                    | 259       |
|    iterations             | 3640      |
|    time_elapsed           | 86215     |
|    total_timesteps        | 22364160  |
| train/                    |           |
|    explained_variance     | 0.413     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.55e-05  |
|    n_updates              | 4024      |
|    policy_objective       | -2.62e-07 |
|    value_loss             | 0.224     |
-----------------------------------------
Ep done - 819000.
Eval num_timesteps=22386000, episode_reward=0.12 +/- 0.98
Episode length: 30.01 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 22386000 |
| train/                    |          |
|    explained_variance     | 0.491    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.54e-05 |
|    n_updates              | 4028     |
|    policy_objective       | 2.58e-07 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 820000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.18     |
| time/                     |          |
|    fps                    | 259      |
|    iterations             | 3650     |
|    time_elapsed           | 86396    |
|    total_timesteps        | 22425600 |
| train/                    |          |
|    explained_variance     | 0.433    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.53e-05 |
|    n_updates              | 4034     |
|    policy_objective       | 5.59e-08 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 821000.
Eval num_timesteps=22447500, episode_reward=0.13 +/- 0.98
Episode length: 29.84 +/- 1.19
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.8      |
|    mean_reward            | 0.13      |
| time/                     |           |
|    total_timesteps        | 22447500  |
| train/                    |           |
|    explained_variance     | 0.44      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.52e-05  |
|    n_updates              | 4038      |
|    policy_objective       | -2.78e-07 |
|    value_loss             | 0.213     |
-----------------------------------------
Ep done - 822000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 259      |
|    iterations             | 3660     |
|    time_elapsed           | 86578    |
|    total_timesteps        | 22487040 |
| train/                    |          |
|    explained_variance     | 0.497    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.51e-05 |
|    n_updates              | 4044     |
|    policy_objective       | 1.92e-07 |
|    value_loss             | 0.201    |
----------------------------------------
Ep done - 823000.
Eval num_timesteps=22509000, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.50
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.105     |
| time/                     |           |
|    total_timesteps        | 22509000  |
| train/                    |           |
|    explained_variance     | 0.439     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.5e-05   |
|    n_updates              | 4048      |
|    policy_objective       | -7.54e-08 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 824000.
Ep done - 825000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 259      |
|    iterations             | 3670     |
|    time_elapsed           | 86759    |
|    total_timesteps        | 22548480 |
| train/                    |          |
|    explained_variance     | 0.446    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.49e-05 |
|    n_updates              | 4054     |
|    policy_objective       | 1.48e-07 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 826000.
Eval num_timesteps=22570500, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.105    |
| time/                     |          |
|    total_timesteps        | 22570500 |
| train/                    |          |
|    explained_variance     | 0.327    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000275 |
|    learning_rate          | 2.48e-05 |
|    n_updates              | 4058     |
|    policy_objective       | 1.83     |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 827000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.04    |
| time/                     |          |
|    fps                    | 260      |
|    iterations             | 3680     |
|    time_elapsed           | 86940    |
|    total_timesteps        | 22609920 |
| train/                    |          |
|    explained_variance     | 0.466    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.47e-05 |
|    n_updates              | 4064     |
|    policy_objective       | 4.45e-07 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 828000.
Eval num_timesteps=22632000, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.005    |
| time/                     |          |
|    total_timesteps        | 22632000 |
| train/                    |          |
|    explained_variance     | 0.424    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.46e-05 |
|    n_updates              | 4068     |
|    policy_objective       | 2.48e-08 |
|    value_loss             | 0.211    |
----------------------------------------
Ep done - 829000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 260       |
|    iterations             | 3690      |
|    time_elapsed           | 87121     |
|    total_timesteps        | 22671360  |
| train/                    |           |
|    explained_variance     | 0.487     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.44e-05  |
|    n_updates              | 4074      |
|    policy_objective       | -9.31e-09 |
|    value_loss             | 0.204     |
-----------------------------------------
Ep done - 830000.
Eval num_timesteps=22693500, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.53
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.085     |
| time/                     |           |
|    total_timesteps        | 22693500  |
| train/                    |           |
|    explained_variance     | 0.442     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.44e-05  |
|    n_updates              | 4078      |
|    policy_objective       | -2.69e-07 |
|    value_loss             | 0.207     |
-----------------------------------------
Ep done - 831000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.03      |
| time/                     |           |
|    fps                    | 260       |
|    iterations             | 3700      |
|    time_elapsed           | 87302     |
|    total_timesteps        | 22732800  |
| train/                    |           |
|    explained_variance     | 0.449     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.42e-05  |
|    n_updates              | 4084      |
|    policy_objective       | -2.05e-07 |
|    value_loss             | 0.205     |
-----------------------------------------
Ep done - 832000.
Eval num_timesteps=22755000, episode_reward=0.18 +/- 0.95
Episode length: 30.05 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.185     |
| time/                     |           |
|    total_timesteps        | 22755000  |
| train/                    |           |
|    explained_variance     | 0.44      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.42e-05  |
|    n_updates              | 4088      |
|    policy_objective       | -5.37e-07 |
|    value_loss             | 0.22      |
-----------------------------------------
Ep done - 833000.
Ep done - 834000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 260      |
|    iterations             | 3710     |
|    time_elapsed           | 87483    |
|    total_timesteps        | 22794240 |
| train/                    |          |
|    explained_variance     | 0.449    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.4e-05  |
|    n_updates              | 4094     |
|    policy_objective       | 1.92e-07 |
|    value_loss             | 0.214    |
----------------------------------------
Ep done - 835000.
Eval num_timesteps=22816500, episode_reward=0.24 +/- 0.96
Episode length: 30.05 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.24     |
| time/                     |          |
|    total_timesteps        | 22816500 |
| train/                    |          |
|    explained_variance     | 0.458    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000291 |
|    learning_rate          | 2.4e-05  |
|    n_updates              | 4098     |
|    policy_objective       | 1.77     |
|    value_loss             | 0.21     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.24
SELFPLAY: new best model, bumping up generation to 77
Ep done - 836000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 260      |
|    iterations             | 3720     |
|    time_elapsed           | 87665    |
|    total_timesteps        | 22855680 |
| train/                    |          |
|    explained_variance     | 0.439    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.38e-05 |
|    n_updates              | 4104     |
|    policy_objective       | 1.43e-07 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 837000.
Eval num_timesteps=22878000, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.11      |
| time/                     |           |
|    total_timesteps        | 22878000  |
| train/                    |           |
|    explained_variance     | 0.443     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.38e-05  |
|    n_updates              | 4108      |
|    policy_objective       | -2.08e-08 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 838000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0         |
| time/                     |           |
|    fps                    | 260       |
|    iterations             | 3730      |
|    time_elapsed           | 87847     |
|    total_timesteps        | 22917120  |
| train/                    |           |
|    explained_variance     | 0.429     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.36e-05  |
|    n_updates              | 4114      |
|    policy_objective       | -3.22e-07 |
|    value_loss             | 0.22      |
-----------------------------------------
Ep done - 839000.
Eval num_timesteps=22939500, episode_reward=0.23 +/- 0.96
Episode length: 30.07 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.23     |
| time/                     |          |
|    total_timesteps        | 22939500 |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.35e-05 |
|    n_updates              | 4118     |
|    policy_objective       | 5.23e-07 |
|    value_loss             | 0.217    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.23
SELFPLAY: new best model, bumping up generation to 78
Ep done - 840000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.25      |
| time/                     |           |
|    fps                    | 261       |
|    iterations             | 3740      |
|    time_elapsed           | 88028     |
|    total_timesteps        | 22978560  |
| train/                    |           |
|    explained_variance     | 0.436     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.34e-05  |
|    n_updates              | 4124      |
|    policy_objective       | -5.99e-07 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 841000.
Eval num_timesteps=23001000, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 23001000 |
| train/                    |          |
|    explained_variance     | 0.401    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.33e-05 |
|    n_updates              | 4128     |
|    policy_objective       | 3.63e-08 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 842000.
Ep done - 843000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.08     |
| time/                     |           |
|    fps                    | 261       |
|    iterations             | 3750      |
|    time_elapsed           | 88210     |
|    total_timesteps        | 23040000  |
| train/                    |           |
|    explained_variance     | 0.475     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.32e-05  |
|    n_updates              | 4134      |
|    policy_objective       | -3.56e-07 |
|    value_loss             | 0.202     |
-----------------------------------------
Ep done - 844000.
Eval num_timesteps=23062500, episode_reward=0.15 +/- 0.97
Episode length: 30.04 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.15     |
| time/                     |          |
|    total_timesteps        | 23062500 |
| train/                    |          |
|    explained_variance     | 0.449    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.31e-05 |
|    n_updates              | 4138     |
|    policy_objective       | -4e-07   |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 845000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.6      |
|    ep_rew_mean            | -0.09     |
| time/                     |           |
|    fps                    | 261       |
|    iterations             | 3760      |
|    time_elapsed           | 88391     |
|    total_timesteps        | 23101440  |
| train/                    |           |
|    explained_variance     | 0.436     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.3e-05   |
|    n_updates              | 4144      |
|    policy_objective       | -3.59e-07 |
|    value_loss             | 0.214     |
-----------------------------------------
Ep done - 846000.
Eval num_timesteps=23124000, episode_reward=0.17 +/- 0.98
Episode length: 30.09 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 23124000 |
| train/                    |          |
|    explained_variance     | 0.464    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.29e-05 |
|    n_updates              | 4148     |
|    policy_objective       | -2.3e-08 |
|    value_loss             | 0.211    |
----------------------------------------
Ep done - 847000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | -0.02     |
| time/                     |           |
|    fps                    | 261       |
|    iterations             | 3770      |
|    time_elapsed           | 88573     |
|    total_timesteps        | 23162880  |
| train/                    |           |
|    explained_variance     | 0.503     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.28e-05  |
|    n_updates              | 4154      |
|    policy_objective       | -3.26e-07 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 848000.
Eval num_timesteps=23185500, episode_reward=0.16 +/- 0.98
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.16      |
| time/                     |           |
|    total_timesteps        | 23185500  |
| train/                    |           |
|    explained_variance     | 0.367     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.27e-05  |
|    n_updates              | 4158      |
|    policy_objective       | -3.92e-07 |
|    value_loss             | 0.228     |
-----------------------------------------
Ep done - 849000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.12      |
| time/                     |           |
|    fps                    | 261       |
|    iterations             | 3780      |
|    time_elapsed           | 88754     |
|    total_timesteps        | 23224320  |
| train/                    |           |
|    explained_variance     | 0.405     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.26e-05  |
|    n_updates              | 4164      |
|    policy_objective       | -6.58e-08 |
|    value_loss             | 0.215     |
-----------------------------------------
Ep done - 850000.
Eval num_timesteps=23247000, episode_reward=0.17 +/- 0.97
Episode length: 29.98 +/- 0.53
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.165     |
| time/                     |           |
|    total_timesteps        | 23247000  |
| train/                    |           |
|    explained_variance     | 0.485     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.25e-05  |
|    n_updates              | 4168      |
|    policy_objective       | -2.82e-07 |
|    value_loss             | 0.191     |
-----------------------------------------
Ep done - 851000.
Ep done - 852000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.15     |
| time/                     |          |
|    fps                    | 261      |
|    iterations             | 3790     |
|    time_elapsed           | 88936    |
|    total_timesteps        | 23285760 |
| train/                    |          |
|    explained_variance     | 0.405    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.24e-05 |
|    n_updates              | 4174     |
|    policy_objective       | -4.6e-07 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 853000.
Eval num_timesteps=23308500, episode_reward=0.06 +/- 1.00
Episode length: 29.96 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 23308500 |
| train/                    |          |
|    explained_variance     | 0.436    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.23e-05 |
|    n_updates              | 4178     |
|    policy_objective       | 8.63e-08 |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 854000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 261      |
|    iterations             | 3800     |
|    time_elapsed           | 89117    |
|    total_timesteps        | 23347200 |
| train/                    |          |
|    explained_variance     | 0.458    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.22e-05 |
|    n_updates              | 4184     |
|    policy_objective       | 1.8e-07  |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 855000.
Eval num_timesteps=23370000, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.045    |
| time/                     |          |
|    total_timesteps        | 23370000 |
| train/                    |          |
|    explained_variance     | 0.507    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.21e-05 |
|    n_updates              | 4188     |
|    policy_objective       | -5.1e-07 |
|    value_loss             | 0.201    |
----------------------------------------
Ep done - 856000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 262      |
|    iterations             | 3810     |
|    time_elapsed           | 89299    |
|    total_timesteps        | 23408640 |
| train/                    |          |
|    explained_variance     | 0.449    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.2e-05  |
|    n_updates              | 4194     |
|    policy_objective       | 1.43e-07 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 857000.
Eval num_timesteps=23431500, episode_reward=0.27 +/- 0.95
Episode length: 29.93 +/- 1.26
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.27     |
| time/                     |          |
|    total_timesteps        | 23431500 |
| train/                    |          |
|    explained_variance     | 0.375    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.19e-05 |
|    n_updates              | 4198     |
|    policy_objective       | 1.49e-07 |
|    value_loss             | 0.227    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.27
SELFPLAY: new best model, bumping up generation to 79
Ep done - 858000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.17     |
| time/                     |           |
|    fps                    | 262       |
|    iterations             | 3820      |
|    time_elapsed           | 89480     |
|    total_timesteps        | 23470080  |
| train/                    |           |
|    explained_variance     | 0.524     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.18e-05  |
|    n_updates              | 4204      |
|    policy_objective       | -1.39e-07 |
|    value_loss             | 0.182     |
-----------------------------------------
Ep done - 859000.
Eval num_timesteps=23493000, episode_reward=0.09 +/- 0.99
Episode length: 29.96 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.09      |
| time/                     |           |
|    total_timesteps        | 23493000  |
| train/                    |           |
|    explained_variance     | 0.439     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.17e-05  |
|    n_updates              | 4208      |
|    policy_objective       | -2.67e-07 |
|    value_loss             | 0.216     |
-----------------------------------------
Ep done - 860000.
Ep done - 861000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.15     |
| time/                     |          |
|    fps                    | 262      |
|    iterations             | 3830     |
|    time_elapsed           | 89662    |
|    total_timesteps        | 23531520 |
| train/                    |          |
|    explained_variance     | 0.497    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.16e-05 |
|    n_updates              | 4214     |
|    policy_objective       | 1.45e-07 |
|    value_loss             | 0.194    |
----------------------------------------
Ep done - 862000.
Eval num_timesteps=23554500, episode_reward=0.10 +/- 0.98
Episode length: 30.04 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 23554500 |
| train/                    |          |
|    explained_variance     | 0.461    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.15e-05 |
|    n_updates              | 4218     |
|    policy_objective       | 3.23e-08 |
|    value_loss             | 0.206    |
----------------------------------------
Ep done - 863000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.14     |
| time/                     |           |
|    fps                    | 262       |
|    iterations             | 3840      |
|    time_elapsed           | 89843     |
|    total_timesteps        | 23592960  |
| train/                    |           |
|    explained_variance     | 0.437     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.14e-05  |
|    n_updates              | 4224      |
|    policy_objective       | -2.19e-07 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 864000.
Eval num_timesteps=23616000, episode_reward=0.24 +/- 0.95
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.245    |
| time/                     |          |
|    total_timesteps        | 23616000 |
| train/                    |          |
|    explained_variance     | 0.489    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.13e-05 |
|    n_updates              | 4228     |
|    policy_objective       | 3.97e-08 |
|    value_loss             | 0.195    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.245
SELFPLAY: new best model, bumping up generation to 80
Ep done - 865000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 262      |
|    iterations             | 3850     |
|    time_elapsed           | 90024    |
|    total_timesteps        | 23654400 |
| train/                    |          |
|    explained_variance     | 0.485    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.12e-05 |
|    n_updates              | 4234     |
|    policy_objective       | 2.64e-08 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 866000.
Eval num_timesteps=23677500, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.09      |
| time/                     |           |
|    total_timesteps        | 23677500  |
| train/                    |           |
|    explained_variance     | 0.466     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.11e-05  |
|    n_updates              | 4238      |
|    policy_objective       | -2.64e-07 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 867000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.19     |
| time/                     |          |
|    fps                    | 262      |
|    iterations             | 3860     |
|    time_elapsed           | 90205    |
|    total_timesteps        | 23715840 |
| train/                    |          |
|    explained_variance     | 0.397    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.1e-05  |
|    n_updates              | 4244     |
|    policy_objective       | 9.31e-08 |
|    value_loss             | 0.231    |
----------------------------------------
Ep done - 868000.
Eval num_timesteps=23739000, episode_reward=0.13 +/- 0.96
Episode length: 29.93 +/- 0.91
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.13      |
| time/                     |           |
|    total_timesteps        | 23739000  |
| train/                    |           |
|    explained_variance     | 0.463     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.09e-05  |
|    n_updates              | 4248      |
|    policy_objective       | -1.04e-07 |
|    value_loss             | 0.204     |
-----------------------------------------
Ep done - 869000.
Ep done - 870000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.1      |
| time/                     |           |
|    fps                    | 263       |
|    iterations             | 3870      |
|    time_elapsed           | 90387     |
|    total_timesteps        | 23777280  |
| train/                    |           |
|    explained_variance     | 0.458     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.08e-05  |
|    n_updates              | 4254      |
|    policy_objective       | -9.22e-08 |
|    value_loss             | 0.206     |
-----------------------------------------
Ep done - 871000.
Eval num_timesteps=23800500, episode_reward=0.23 +/- 0.96
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.225     |
| time/                     |           |
|    total_timesteps        | 23800500  |
| train/                    |           |
|    explained_variance     | 0.403     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.07e-05  |
|    n_updates              | 4258      |
|    policy_objective       | -1.13e-07 |
|    value_loss             | 0.22      |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 81
Ep done - 872000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.8      |
|    ep_rew_mean            | 0         |
| time/                     |           |
|    fps                    | 263       |
|    iterations             | 3880      |
|    time_elapsed           | 90568     |
|    total_timesteps        | 23838720  |
| train/                    |           |
|    explained_variance     | 0.482     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.06e-05  |
|    n_updates              | 4264      |
|    policy_objective       | -1.44e-07 |
|    value_loss             | 0.207     |
-----------------------------------------
Ep done - 873000.
Eval num_timesteps=23862000, episode_reward=0.08 +/- 0.98
Episode length: 29.94 +/- 1.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 23862000 |
| train/                    |          |
|    explained_variance     | 0.44     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.05e-05 |
|    n_updates              | 4268     |
|    policy_objective       | 1.29e-07 |
|    value_loss             | 0.203    |
----------------------------------------
Ep done - 874000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.2     |
| time/                     |          |
|    fps                    | 263      |
|    iterations             | 3890     |
|    time_elapsed           | 90750    |
|    total_timesteps        | 23900160 |
| train/                    |          |
|    explained_variance     | 0.395    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.04e-05 |
|    n_updates              | 4274     |
|    policy_objective       | 1.68e-07 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 875000.
Eval num_timesteps=23923500, episode_reward=0.19 +/- 0.97
Episode length: 30.05 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 23923500 |
| train/                    |          |
|    explained_variance     | 0.439    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.03e-05 |
|    n_updates              | 4278     |
|    policy_objective       | 2.89e-07 |
|    value_loss             | 0.214    |
----------------------------------------
Ep done - 876000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.09    |
| time/                     |          |
|    fps                    | 263      |
|    iterations             | 3900     |
|    time_elapsed           | 90931    |
|    total_timesteps        | 23961600 |
| train/                    |          |
|    explained_variance     | 0.47     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.01e-05 |
|    n_updates              | 4284     |
|    policy_objective       | 2.7e-07  |
|    value_loss             | 0.205    |
----------------------------------------
Ep done - 877000.
Eval num_timesteps=23985000, episode_reward=0.13 +/- 0.97
Episode length: 30.02 +/- 0.63
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 23985000 |
| train/                    |          |
|    explained_variance     | 0.414    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.01e-05 |
|    n_updates              | 4288     |
|    policy_objective       | 1.65e-07 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 878000.
Ep done - 879000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 263      |
|    iterations             | 3910     |
|    time_elapsed           | 91113    |
|    total_timesteps        | 24023040 |
| train/                    |          |
|    explained_variance     | 0.476    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.99e-05 |
|    n_updates              | 4294     |
|    policy_objective       | 3.26e-07 |
|    value_loss             | 0.208    |
----------------------------------------
Ep done - 880000.
Eval num_timesteps=24046500, episode_reward=0.08 +/- 0.99
Episode length: 30.05 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 24046500 |
| train/                    |          |
|    explained_variance     | 0.496    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.99e-05 |
|    n_updates              | 4298     |
|    policy_objective       | -8.1e-08 |
|    value_loss             | 0.202    |
----------------------------------------
Ep done - 881000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.06     |
| time/                     |          |
|    fps                    | 263      |
|    iterations             | 3920     |
|    time_elapsed           | 91294    |
|    total_timesteps        | 24084480 |
| train/                    |          |
|    explained_variance     | 0.46     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.97e-05 |
|    n_updates              | 4304     |
|    policy_objective       | 4.22e-07 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 882000.
Eval num_timesteps=24108000, episode_reward=0.26 +/- 0.95
Episode length: 30.05 +/- 0.63
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.26      |
| time/                     |           |
|    total_timesteps        | 24108000  |
| train/                    |           |
|    explained_variance     | 0.43      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.97e-05  |
|    n_updates              | 4308      |
|    policy_objective       | -2.12e-07 |
|    value_loss             | 0.22      |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.26
SELFPLAY: new best model, bumping up generation to 82
Ep done - 883000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.17     |
| time/                     |          |
|    fps                    | 263      |
|    iterations             | 3930     |
|    time_elapsed           | 91476    |
|    total_timesteps        | 24145920 |
| train/                    |          |
|    explained_variance     | 0.455    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000261 |
|    learning_rate          | 1.95e-05 |
|    n_updates              | 4314     |
|    policy_objective       | 2.74     |
|    value_loss             | 0.217    |
----------------------------------------
Ep done - 884000.
Eval num_timesteps=24169500, episode_reward=0.10 +/- 0.97
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.1       |
| time/                     |           |
|    total_timesteps        | 24169500  |
| train/                    |           |
|    explained_variance     | 0.464     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.95e-05  |
|    n_updates              | 4318      |
|    policy_objective       | -1.81e-07 |
|    value_loss             | 0.213     |
-----------------------------------------
Ep done - 885000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.14     |
| time/                     |          |
|    fps                    | 264      |
|    iterations             | 3940     |
|    time_elapsed           | 91658    |
|    total_timesteps        | 24207360 |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.93e-05 |
|    n_updates              | 4324     |
|    policy_objective       | 5.09e-07 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 886000.
Eval num_timesteps=24231000, episode_reward=0.09 +/- 0.99
Episode length: 29.96 +/- 0.53
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.085     |
| time/                     |           |
|    total_timesteps        | 24231000  |
| train/                    |           |
|    explained_variance     | 0.459     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.92e-05  |
|    n_updates              | 4328      |
|    policy_objective       | -5.46e-07 |
|    value_loss             | 0.213     |
-----------------------------------------
Ep done - 887000.
Ep done - 888000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.07     |
| time/                     |           |
|    fps                    | 264       |
|    iterations             | 3950      |
|    time_elapsed           | 91839     |
|    total_timesteps        | 24268800  |
| train/                    |           |
|    explained_variance     | 0.475     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.91e-05  |
|    n_updates              | 4334      |
|    policy_objective       | -3.94e-07 |
|    value_loss             | 0.207     |
-----------------------------------------
Ep done - 889000.
Eval num_timesteps=24292500, episode_reward=0.17 +/- 0.96
Episode length: 30.02 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.175    |
| time/                     |          |
|    total_timesteps        | 24292500 |
| train/                    |          |
|    explained_variance     | 0.533    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.9e-05  |
|    n_updates              | 4338     |
|    policy_objective       | 6.16e-07 |
|    value_loss             | 0.183    |
----------------------------------------
Ep done - 890000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.15     |
| time/                     |          |
|    fps                    | 264      |
|    iterations             | 3960     |
|    time_elapsed           | 92021    |
|    total_timesteps        | 24330240 |
| train/                    |          |
|    explained_variance     | 0.482    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.89e-05 |
|    n_updates              | 4344     |
|    policy_objective       | 1.01e-07 |
|    value_loss             | 0.201    |
----------------------------------------
Ep done - 891000.
Eval num_timesteps=24354000, episode_reward=0.17 +/- 0.96
Episode length: 30.00 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.17     |
| time/                     |          |
|    total_timesteps        | 24354000 |
| train/                    |          |
|    explained_variance     | 0.445    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.88e-05 |
|    n_updates              | 4348     |
|    policy_objective       | 2.64e-08 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 892000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.08     |
| time/                     |           |
|    fps                    | 264       |
|    iterations             | 3970      |
|    time_elapsed           | 92202     |
|    total_timesteps        | 24391680  |
| train/                    |           |
|    explained_variance     | 0.51      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.87e-05  |
|    n_updates              | 4354      |
|    policy_objective       | -5.94e-07 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 893000.
Eval num_timesteps=24415500, episode_reward=0.06 +/- 0.98
Episode length: 30.01 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.055    |
| time/                     |          |
|    total_timesteps        | 24415500 |
| train/                    |          |
|    explained_variance     | 0.452    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.86e-05 |
|    n_updates              | 4358     |
|    policy_objective       | 1.12e-08 |
|    value_loss             | 0.205    |
----------------------------------------
Ep done - 894000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 264      |
|    iterations             | 3980     |
|    time_elapsed           | 92384    |
|    total_timesteps        | 24453120 |
| train/                    |          |
|    explained_variance     | 0.491    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.85e-05 |
|    n_updates              | 4364     |
|    policy_objective       | 1.14e-07 |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 895000.
Eval num_timesteps=24477000, episode_reward=0.17 +/- 0.97
Episode length: 30.00 +/- 1.13
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 24477000 |
| train/                    |          |
|    explained_variance     | 0.511    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.84e-05 |
|    n_updates              | 4368     |
|    policy_objective       | 1.88e-07 |
|    value_loss             | 0.191    |
----------------------------------------
Ep done - 896000.
Ep done - 897000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.24      |
| time/                     |           |
|    fps                    | 264       |
|    iterations             | 3990      |
|    time_elapsed           | 92565     |
|    total_timesteps        | 24514560  |
| train/                    |           |
|    explained_variance     | 0.501     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.83e-05  |
|    n_updates              | 4374      |
|    policy_objective       | -2.58e-08 |
|    value_loss             | 0.204     |
-----------------------------------------
Ep done - 898000.
Eval num_timesteps=24538500, episode_reward=0.12 +/- 0.96
Episode length: 29.93 +/- 1.32
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 24538500 |
| train/                    |          |
|    explained_variance     | 0.409    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.82e-05 |
|    n_updates              | 4378     |
|    policy_objective       | 2.24e-07 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 899000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.08    |
| time/                     |          |
|    fps                    | 264      |
|    iterations             | 4000     |
|    time_elapsed           | 92747    |
|    total_timesteps        | 24576000 |
| train/                    |          |
|    explained_variance     | 0.456    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.81e-05 |
|    n_updates              | 4384     |
|    policy_objective       | 1.35e-07 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 900000.
Eval num_timesteps=24600000, episode_reward=0.30 +/- 0.93
Episode length: 30.01 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.305    |
| time/                     |          |
|    total_timesteps        | 24600000 |
| train/                    |          |
|    explained_variance     | 0.504    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.8e-05  |
|    n_updates              | 4388     |
|    policy_objective       | 2.73e-08 |
|    value_loss             | 0.195    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.305
SELFPLAY: new best model, bumping up generation to 83
Ep done - 901000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.08     |
| time/                     |          |
|    fps                    | 265      |
|    iterations             | 4010     |
|    time_elapsed           | 92928    |
|    total_timesteps        | 24637440 |
| train/                    |          |
|    explained_variance     | 0.46     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.79e-05 |
|    n_updates              | 4394     |
|    policy_objective       | 1.31e-07 |
|    value_loss             | 0.211    |
----------------------------------------
Ep done - 902000.
Eval num_timesteps=24661500, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.115     |
| time/                     |           |
|    total_timesteps        | 24661500  |
| train/                    |           |
|    explained_variance     | 0.429     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.78e-05  |
|    n_updates              | 4398      |
|    policy_objective       | -5.03e-08 |
|    value_loss             | 0.222     |
-----------------------------------------
Ep done - 903000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.06     |
| time/                     |          |
|    fps                    | 265      |
|    iterations             | 4020     |
|    time_elapsed           | 93109    |
|    total_timesteps        | 24698880 |
| train/                    |          |
|    explained_variance     | 0.412    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.77e-05 |
|    n_updates              | 4404     |
|    policy_objective       | 6.66e-08 |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 904000.
Eval num_timesteps=24723000, episode_reward=0.16 +/- 0.98
Episode length: 30.02 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 24723000 |
| train/                    |          |
|    explained_variance     | 0.387    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.76e-05 |
|    n_updates              | 4408     |
|    policy_objective       | 4.17e-07 |
|    value_loss             | 0.235    |
----------------------------------------
Ep done - 905000.
Ep done - 906000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 265      |
|    iterations             | 4030     |
|    time_elapsed           | 93291    |
|    total_timesteps        | 24760320 |
| train/                    |          |
|    explained_variance     | 0.531    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.75e-05 |
|    n_updates              | 4414     |
|    policy_objective       | 7.95e-08 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 907000.
Eval num_timesteps=24784500, episode_reward=0.13 +/- 0.97
Episode length: 30.04 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.13      |
| time/                     |           |
|    total_timesteps        | 24784500  |
| train/                    |           |
|    explained_variance     | 0.525     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.74e-05  |
|    n_updates              | 4418      |
|    policy_objective       | -5.46e-08 |
|    value_loss             | 0.196     |
-----------------------------------------
Ep done - 908000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.13    |
| time/                     |          |
|    fps                    | 265      |
|    iterations             | 4040     |
|    time_elapsed           | 93472    |
|    total_timesteps        | 24821760 |
| train/                    |          |
|    explained_variance     | 0.531    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.73e-05 |
|    n_updates              | 4424     |
|    policy_objective       | -2.2e-07 |
|    value_loss             | 0.196    |
----------------------------------------
Ep done - 909000.
Eval num_timesteps=24846000, episode_reward=0.21 +/- 0.97
Episode length: 29.99 +/- 1.14
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.215     |
| time/                     |           |
|    total_timesteps        | 24846000  |
| train/                    |           |
|    explained_variance     | 0.456     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.72e-05  |
|    n_updates              | 4428      |
|    policy_objective       | -2.22e-07 |
|    value_loss             | 0.218     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.215
SELFPLAY: new best model, bumping up generation to 84
Ep done - 910000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.03     |
| time/                     |           |
|    fps                    | 265       |
|    iterations             | 4050      |
|    time_elapsed           | 93653     |
|    total_timesteps        | 24883200  |
| train/                    |           |
|    explained_variance     | 0.442     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.71e-05  |
|    n_updates              | 4434      |
|    policy_objective       | -2.94e-07 |
|    value_loss             | 0.226     |
-----------------------------------------
Ep done - 911000.
Eval num_timesteps=24907500, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.04     |
| time/                     |          |
|    total_timesteps        | 24907500 |
| train/                    |          |
|    explained_variance     | 0.458    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.7e-05  |
|    n_updates              | 4438     |
|    policy_objective       | 1.4e-07  |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 912000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.07      |
| time/                     |           |
|    fps                    | 265       |
|    iterations             | 4060      |
|    time_elapsed           | 93834     |
|    total_timesteps        | 24944640  |
| train/                    |           |
|    explained_variance     | 0.519     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.69e-05  |
|    n_updates              | 4444      |
|    policy_objective       | -3.12e-07 |
|    value_loss             | 0.198     |
-----------------------------------------
Ep done - 913000.
Eval num_timesteps=24969000, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.56
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.11      |
| time/                     |           |
|    total_timesteps        | 24969000  |
| train/                    |           |
|    explained_variance     | 0.499     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.68e-05  |
|    n_updates              | 4448      |
|    policy_objective       | -1.35e-07 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 914000.
Ep done - 915000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.01     |
| time/                     |           |
|    fps                    | 265       |
|    iterations             | 4070      |
|    time_elapsed           | 94015     |
|    total_timesteps        | 25006080  |
| train/                    |           |
|    explained_variance     | 0.474     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.67e-05  |
|    n_updates              | 4454      |
|    policy_objective       | -2.75e-07 |
|    value_loss             | 0.211     |
-----------------------------------------
Ep done - 916000.
Eval num_timesteps=25030500, episode_reward=0.12 +/- 0.98
Episode length: 30.05 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.125     |
| time/                     |           |
|    total_timesteps        | 25030500  |
| train/                    |           |
|    explained_variance     | 0.509     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.66e-05  |
|    n_updates              | 4458      |
|    policy_objective       | -9.38e-08 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 917000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.02      |
| time/                     |           |
|    fps                    | 266       |
|    iterations             | 4080      |
|    time_elapsed           | 94196     |
|    total_timesteps        | 25067520  |
| train/                    |           |
|    explained_variance     | 0.5       |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.65e-05  |
|    n_updates              | 4464      |
|    policy_objective       | -1.88e-07 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 918000.
Eval num_timesteps=25092000, episode_reward=0.13 +/- 0.98
Episode length: 29.91 +/- 1.21
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 25092000 |
| train/                    |          |
|    explained_variance     | 0.493    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.64e-05 |
|    n_updates              | 4468     |
|    policy_objective       | 1.3e-07  |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 919000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 266      |
|    iterations             | 4090     |
|    time_elapsed           | 94377    |
|    total_timesteps        | 25128960 |
| train/                    |          |
|    explained_variance     | 0.495    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.63e-05 |
|    n_updates              | 4474     |
|    policy_objective       | 8.38e-08 |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 920000.
Eval num_timesteps=25153500, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.03      |
| time/                     |           |
|    total_timesteps        | 25153500  |
| train/                    |           |
|    explained_variance     | 0.417     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.62e-05  |
|    n_updates              | 4478      |
|    policy_objective       | -2.68e-07 |
|    value_loss             | 0.222     |
-----------------------------------------
Ep done - 921000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.17      |
| time/                     |           |
|    fps                    | 266       |
|    iterations             | 4100      |
|    time_elapsed           | 94558     |
|    total_timesteps        | 25190400  |
| train/                    |           |
|    explained_variance     | 0.476     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.61e-05  |
|    n_updates              | 4484      |
|    policy_objective       | -1.09e-07 |
|    value_loss             | 0.21      |
-----------------------------------------
Ep done - 922000.
Eval num_timesteps=25215000, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 25215000 |
| train/                    |          |
|    explained_variance     | 0.395    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.6e-05  |
|    n_updates              | 4489     |
|    policy_objective       | 4.67e-07 |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 923000.
Ep done - 924000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.13      |
| time/                     |           |
|    fps                    | 266       |
|    iterations             | 4110      |
|    time_elapsed           | 94739     |
|    total_timesteps        | 25251840  |
| train/                    |           |
|    explained_variance     | 0.391     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.58e-05  |
|    n_updates              | 4494      |
|    policy_objective       | -2.95e-07 |
|    value_loss             | 0.232     |
-----------------------------------------
Ep done - 925000.
Eval num_timesteps=25276500, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.01     |
| time/                     |          |
|    total_timesteps        | 25276500 |
| train/                    |          |
|    explained_variance     | 0.509    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.57e-05 |
|    n_updates              | 4499     |
|    policy_objective       | 1.68e-08 |
|    value_loss             | 0.196    |
----------------------------------------
Ep done - 926000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.1       |
| time/                     |           |
|    fps                    | 266       |
|    iterations             | 4120      |
|    time_elapsed           | 94920     |
|    total_timesteps        | 25313280  |
| train/                    |           |
|    explained_variance     | 0.445     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.56e-05  |
|    n_updates              | 4504      |
|    policy_objective       | -9.31e-08 |
|    value_loss             | 0.224     |
-----------------------------------------
Ep done - 927000.
Eval num_timesteps=25338000, episode_reward=0.24 +/- 0.96
Episode length: 30.08 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.24      |
| time/                     |           |
|    total_timesteps        | 25338000  |
| train/                    |           |
|    explained_variance     | 0.503     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.55e-05  |
|    n_updates              | 4509      |
|    policy_objective       | -2.57e-07 |
|    value_loss             | 0.198     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.24
SELFPLAY: new best model, bumping up generation to 85
Ep done - 928000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.09     |
| time/                     |           |
|    fps                    | 266       |
|    iterations             | 4130      |
|    time_elapsed           | 95101     |
|    total_timesteps        | 25374720  |
| train/                    |           |
|    explained_variance     | 0.43      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.54e-05  |
|    n_updates              | 4514      |
|    policy_objective       | -4.64e-07 |
|    value_loss             | 0.218     |
-----------------------------------------
Ep done - 929000.
Eval num_timesteps=25399500, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.045    |
| time/                     |          |
|    total_timesteps        | 25399500 |
| train/                    |          |
|    explained_variance     | 0.479    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.53e-05 |
|    n_updates              | 4519     |
|    policy_objective       | 3.41e-07 |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 930000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.12     |
| time/                     |           |
|    fps                    | 266       |
|    iterations             | 4140      |
|    time_elapsed           | 95282     |
|    total_timesteps        | 25436160  |
| train/                    |           |
|    explained_variance     | 0.399     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.52e-05  |
|    n_updates              | 4524      |
|    policy_objective       | -1.46e-07 |
|    value_loss             | 0.232     |
-----------------------------------------
Ep done - 931000.
Eval num_timesteps=25461000, episode_reward=0.15 +/- 0.97
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 25461000 |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000511 |
|    learning_rate          | 1.51e-05 |
|    n_updates              | 4529     |
|    policy_objective       | 1.8      |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 932000.
Ep done - 933000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 267      |
|    iterations             | 4150     |
|    time_elapsed           | 95463    |
|    total_timesteps        | 25497600 |
| train/                    |          |
|    explained_variance     | 0.457    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.5e-05  |
|    n_updates              | 4534     |
|    policy_objective       | -3.1e-07 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 934000.
Eval num_timesteps=25522500, episode_reward=0.12 +/- 0.97
Episode length: 29.95 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 25522500 |
| train/                    |          |
|    explained_variance     | 0.48     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.49e-05 |
|    n_updates              | 4539     |
|    policy_objective       | 2.01e-07 |
|    value_loss             | 0.207    |
----------------------------------------
Ep done - 935000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.05     |
| time/                     |           |
|    fps                    | 267       |
|    iterations             | 4160      |
|    time_elapsed           | 95644     |
|    total_timesteps        | 25559040  |
| train/                    |           |
|    explained_variance     | 0.511     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.48e-05  |
|    n_updates              | 4544      |
|    policy_objective       | -1.05e-07 |
|    value_loss             | 0.193     |
-----------------------------------------
Ep done - 936000.
Eval num_timesteps=25584000, episode_reward=0.08 +/- 0.99
Episode length: 29.95 +/- 1.11
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 25584000 |
| train/                    |          |
|    explained_variance     | 0.508    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.47e-05 |
|    n_updates              | 4549     |
|    policy_objective       | 7.45e-08 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 937000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.06     |
| time/                     |          |
|    fps                    | 267      |
|    iterations             | 4170     |
|    time_elapsed           | 95825    |
|    total_timesteps        | 25620480 |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.46e-05 |
|    n_updates              | 4554     |
|    policy_objective       | 1.06e-08 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 938000.
Eval num_timesteps=25645500, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.59
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 25645500 |
| train/                    |          |
|    explained_variance     | 0.488    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.45e-05 |
|    n_updates              | 4559     |
|    policy_objective       | 4.59e-08 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 939000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.08    |
| time/                     |          |
|    fps                    | 267      |
|    iterations             | 4180     |
|    time_elapsed           | 96007    |
|    total_timesteps        | 25681920 |
| train/                    |          |
|    explained_variance     | 0.529    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.44e-05 |
|    n_updates              | 4564     |
|    policy_objective       | 2.07e-07 |
|    value_loss             | 0.195    |
----------------------------------------
Ep done - 940000.
Eval num_timesteps=25707000, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.11      |
| time/                     |           |
|    total_timesteps        | 25707000  |
| train/                    |           |
|    explained_variance     | 0.416     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.43e-05  |
|    n_updates              | 4569      |
|    policy_objective       | -4.56e-08 |
|    value_loss             | 0.238     |
-----------------------------------------
Ep done - 941000.
Ep done - 942000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.8      |
|    ep_rew_mean            | 0.12      |
| time/                     |           |
|    fps                    | 267       |
|    iterations             | 4190      |
|    time_elapsed           | 96188     |
|    total_timesteps        | 25743360  |
| train/                    |           |
|    explained_variance     | 0.434     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.42e-05  |
|    n_updates              | 4574      |
|    policy_objective       | -5.44e-07 |
|    value_loss             | 0.223     |
-----------------------------------------
Ep done - 943000.
Eval num_timesteps=25768500, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.14     |
| time/                     |          |
|    total_timesteps        | 25768500 |
| train/                    |          |
|    explained_variance     | 0.488    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.41e-05 |
|    n_updates              | 4579     |
|    policy_objective       | 1.86e-08 |
|    value_loss             | 0.201    |
----------------------------------------
Ep done - 944000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.09      |
| time/                     |           |
|    fps                    | 267       |
|    iterations             | 4200      |
|    time_elapsed           | 96369     |
|    total_timesteps        | 25804800  |
| train/                    |           |
|    explained_variance     | 0.477     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.4e-05   |
|    n_updates              | 4584      |
|    policy_objective       | -3.15e-07 |
|    value_loss             | 0.209     |
-----------------------------------------
Ep done - 945000.
Eval num_timesteps=25830000, episode_reward=0.17 +/- 0.96
Episode length: 30.06 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.165    |
| time/                     |          |
|    total_timesteps        | 25830000 |
| train/                    |          |
|    explained_variance     | 0.503    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.39e-05 |
|    n_updates              | 4589     |
|    policy_objective       | 8.46e-07 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 946000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.01      |
| time/                     |           |
|    fps                    | 267       |
|    iterations             | 4210      |
|    time_elapsed           | 96550     |
|    total_timesteps        | 25866240  |
| train/                    |           |
|    explained_variance     | 0.431     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.38e-05  |
|    n_updates              | 4594      |
|    policy_objective       | -1.56e-07 |
|    value_loss             | 0.234     |
-----------------------------------------
Ep done - 947000.
Eval num_timesteps=25891500, episode_reward=0.12 +/- 0.97
Episode length: 29.92 +/- 1.30
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 25891500 |
| train/                    |          |
|    explained_variance     | 0.374    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.37e-05 |
|    n_updates              | 4599     |
|    policy_objective       | 3.89e-07 |
|    value_loss             | 0.243    |
----------------------------------------
Ep done - 948000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.09     |
| time/                     |           |
|    fps                    | 268       |
|    iterations             | 4220      |
|    time_elapsed           | 96731     |
|    total_timesteps        | 25927680  |
| train/                    |           |
|    explained_variance     | 0.433     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.36e-05  |
|    n_updates              | 4604      |
|    policy_objective       | -3.72e-07 |
|    value_loss             | 0.228     |
-----------------------------------------
Ep done - 949000.
Eval num_timesteps=25953000, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.125    |
| time/                     |          |
|    total_timesteps        | 25953000 |
| train/                    |          |
|    explained_variance     | 0.489    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.35e-05 |
|    n_updates              | 4609     |
|    policy_objective       | 3.85e-08 |
|    value_loss             | 0.198    |
----------------------------------------
Ep done - 950000.
Ep done - 951000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.06     |
| time/                     |           |
|    fps                    | 268       |
|    iterations             | 4230      |
|    time_elapsed           | 96912     |
|    total_timesteps        | 25989120  |
| train/                    |           |
|    explained_variance     | 0.461     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.34e-05  |
|    n_updates              | 4614      |
|    policy_objective       | -4.66e-08 |
|    value_loss             | 0.223     |
-----------------------------------------
Ep done - 952000.
Eval num_timesteps=26014500, episode_reward=0.08 +/- 0.98
Episode length: 29.94 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 26014500 |
| train/                    |          |
|    explained_variance     | 0.515    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.33e-05 |
|    n_updates              | 4619     |
|    policy_objective       | 1.29e-07 |
|    value_loss             | 0.186    |
----------------------------------------
Ep done - 953000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 268       |
|    iterations             | 4240      |
|    time_elapsed           | 97093     |
|    total_timesteps        | 26050560  |
| train/                    |           |
|    explained_variance     | 0.508     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.32e-05  |
|    n_updates              | 4624      |
|    policy_objective       | -5.97e-07 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 954000.
Eval num_timesteps=26076000, episode_reward=0.20 +/- 0.96
Episode length: 30.07 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.2       |
| time/                     |           |
|    total_timesteps        | 26076000  |
| train/                    |           |
|    explained_variance     | 0.483     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.31e-05  |
|    n_updates              | 4629      |
|    policy_objective       | -5.85e-08 |
|    value_loss             | 0.213     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 86
Ep done - 955000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.1       |
| time/                     |           |
|    fps                    | 268       |
|    iterations             | 4250      |
|    time_elapsed           | 97274     |
|    total_timesteps        | 26112000  |
| train/                    |           |
|    explained_variance     | 0.483     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.3e-05   |
|    n_updates              | 4634      |
|    policy_objective       | -2.74e-07 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 956000.
Eval num_timesteps=26137500, episode_reward=0.12 +/- 0.98
Episode length: 29.90 +/- 1.53
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.12      |
| time/                     |           |
|    total_timesteps        | 26137500  |
| train/                    |           |
|    explained_variance     | 0.439     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.29e-05  |
|    n_updates              | 4639      |
|    policy_objective       | -1.78e-07 |
|    value_loss             | 0.224     |
-----------------------------------------
Ep done - 957000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.03      |
| time/                     |           |
|    fps                    | 268       |
|    iterations             | 4260      |
|    time_elapsed           | 97455     |
|    total_timesteps        | 26173440  |
| train/                    |           |
|    explained_variance     | 0.47      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.28e-05  |
|    n_updates              | 4644      |
|    policy_objective       | -4.98e-07 |
|    value_loss             | 0.215     |
-----------------------------------------
Ep done - 958000.
Eval num_timesteps=26199000, episode_reward=0.28 +/- 0.93
Episode length: 29.98 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.285    |
| time/                     |          |
|    total_timesteps        | 26199000 |
| train/                    |          |
|    explained_variance     | 0.514    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.27e-05 |
|    n_updates              | 4649     |
|    policy_objective       | 1.63e-07 |
|    value_loss             | 0.192    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 87
Ep done - 959000.
Ep done - 960000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.08     |
| time/                     |           |
|    fps                    | 268       |
|    iterations             | 4270      |
|    time_elapsed           | 97637     |
|    total_timesteps        | 26234880  |
| train/                    |           |
|    explained_variance     | 0.465     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.26e-05  |
|    n_updates              | 4654      |
|    policy_objective       | -2.67e-08 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 961000.
Eval num_timesteps=26260500, episode_reward=0.20 +/- 0.97
Episode length: 29.95 +/- 1.50
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.2       |
| time/                     |           |
|    total_timesteps        | 26260500  |
| train/                    |           |
|    explained_variance     | 0.434     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.25e-05  |
|    n_updates              | 4659      |
|    policy_objective       | -2.73e-07 |
|    value_loss             | 0.227     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.2
SELFPLAY: new best model, bumping up generation to 88
Ep done - 962000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.06      |
| time/                     |           |
|    fps                    | 268       |
|    iterations             | 4280      |
|    time_elapsed           | 97818     |
|    total_timesteps        | 26296320  |
| train/                    |           |
|    explained_variance     | 0.489     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.24e-05  |
|    n_updates              | 4664      |
|    policy_objective       | -3.09e-07 |
|    value_loss             | 0.205     |
-----------------------------------------
Ep done - 963000.
Eval num_timesteps=26322000, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 26322000 |
| train/                    |          |
|    explained_variance     | 0.446    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.23e-05 |
|    n_updates              | 4669     |
|    policy_objective       | -2.7e-07 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 964000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.07      |
| time/                     |           |
|    fps                    | 268       |
|    iterations             | 4290      |
|    time_elapsed           | 97999     |
|    total_timesteps        | 26357760  |
| train/                    |           |
|    explained_variance     | 0.538     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.22e-05  |
|    n_updates              | 4674      |
|    policy_objective       | -2.58e-07 |
|    value_loss             | 0.197     |
-----------------------------------------
Ep done - 965000.
Eval num_timesteps=26383500, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.04     |
| time/                     |          |
|    total_timesteps        | 26383500 |
| train/                    |          |
|    explained_variance     | 0.525    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.21e-05 |
|    n_updates              | 4679     |
|    policy_objective       | 2.3e-07  |
|    value_loss             | 0.19     |
----------------------------------------
Ep done - 966000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 269      |
|    iterations             | 4300     |
|    time_elapsed           | 98181    |
|    total_timesteps        | 26419200 |
| train/                    |          |
|    explained_variance     | 0.47     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.2e-05  |
|    n_updates              | 4684     |
|    policy_objective       | 9.95e-08 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 967000.
Eval num_timesteps=26445000, episode_reward=0.09 +/- 0.97
Episode length: 30.04 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.09      |
| time/                     |           |
|    total_timesteps        | 26445000  |
| train/                    |           |
|    explained_variance     | 0.5       |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.19e-05  |
|    n_updates              | 4689      |
|    policy_objective       | -1.51e-07 |
|    value_loss             | 0.205     |
-----------------------------------------
Ep done - 968000.
Ep done - 969000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 269       |
|    iterations             | 4310      |
|    time_elapsed           | 98362     |
|    total_timesteps        | 26480640  |
| train/                    |           |
|    explained_variance     | 0.51      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.18e-05  |
|    n_updates              | 4694      |
|    policy_objective       | -1.12e-08 |
|    value_loss             | 0.198     |
-----------------------------------------
Ep done - 970000.
Eval num_timesteps=26506500, episode_reward=0.07 +/- 0.99
Episode length: 29.93 +/- 1.25
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.07      |
| time/                     |           |
|    total_timesteps        | 26506500  |
| train/                    |           |
|    explained_variance     | 0.452     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.16e-05  |
|    n_updates              | 4699      |
|    policy_objective       | -5.67e-07 |
|    value_loss             | 0.217     |
-----------------------------------------
Ep done - 971000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 269      |
|    iterations             | 4320     |
|    time_elapsed           | 98544    |
|    total_timesteps        | 26542080 |
| train/                    |          |
|    explained_variance     | 0.444    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.15e-05 |
|    n_updates              | 4704     |
|    policy_objective       | -1.7e-07 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 972000.
Eval num_timesteps=26568000, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.115    |
| time/                     |          |
|    total_timesteps        | 26568000 |
| train/                    |          |
|    explained_variance     | 0.507    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.14e-05 |
|    n_updates              | 4709     |
|    policy_objective       | -5.1e-07 |
|    value_loss             | 0.203    |
----------------------------------------
Ep done - 973000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.08     |
| time/                     |           |
|    fps                    | 269       |
|    iterations             | 4330      |
|    time_elapsed           | 98725     |
|    total_timesteps        | 26603520  |
| train/                    |           |
|    explained_variance     | 0.398     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.13e-05  |
|    n_updates              | 4714      |
|    policy_objective       | -3.73e-07 |
|    value_loss             | 0.232     |
-----------------------------------------
Ep done - 974000.
Eval num_timesteps=26629500, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.53
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 26629500 |
| train/                    |          |
|    explained_variance     | 0.433    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.12e-05 |
|    n_updates              | 4719     |
|    policy_objective       | -2.2e-07 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 975000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.19    |
| time/                     |          |
|    fps                    | 269      |
|    iterations             | 4340     |
|    time_elapsed           | 98907    |
|    total_timesteps        | 26664960 |
| train/                    |          |
|    explained_variance     | 0.519    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.11e-05 |
|    n_updates              | 4724     |
|    policy_objective       | 6.82e-07 |
|    value_loss             | 0.192    |
----------------------------------------
Ep done - 976000.
Eval num_timesteps=26691000, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.47
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.145     |
| time/                     |           |
|    total_timesteps        | 26691000  |
| train/                    |           |
|    explained_variance     | 0.464     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.1e-05   |
|    n_updates              | 4729      |
|    policy_objective       | -1.47e-07 |
|    value_loss             | 0.216     |
-----------------------------------------
Ep done - 977000.
Ep done - 978000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 269      |
|    iterations             | 4350     |
|    time_elapsed           | 99088    |
|    total_timesteps        | 26726400 |
| train/                    |          |
|    explained_variance     | 0.431    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.09e-05 |
|    n_updates              | 4734     |
|    policy_objective       | 4.99e-07 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 979000.
Eval num_timesteps=26752500, episode_reward=0.15 +/- 0.96
Episode length: 30.05 +/- 0.52
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.155    |
| time/                     |          |
|    total_timesteps        | 26752500 |
| train/                    |          |
|    explained_variance     | 0.505    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000603 |
|    learning_rate          | 1.08e-05 |
|    n_updates              | 4739     |
|    policy_objective       | 8.46     |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 980000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.07      |
| time/                     |           |
|    fps                    | 269       |
|    iterations             | 4360      |
|    time_elapsed           | 99269     |
|    total_timesteps        | 26787840  |
| train/                    |           |
|    explained_variance     | 0.41      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.07e-05  |
|    n_updates              | 4744      |
|    policy_objective       | -4.35e-08 |
|    value_loss             | 0.23      |
-----------------------------------------
Ep done - 981000.
Eval num_timesteps=26814000, episode_reward=0.16 +/- 0.97
Episode length: 29.99 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.16     |
| time/                     |          |
|    total_timesteps        | 26814000 |
| train/                    |          |
|    explained_variance     | 0.453    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.06e-05 |
|    n_updates              | 4749     |
|    policy_objective       | 5.14e-07 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 982000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.1      |
| time/                     |           |
|    fps                    | 269       |
|    iterations             | 4370      |
|    time_elapsed           | 99451     |
|    total_timesteps        | 26849280  |
| train/                    |           |
|    explained_variance     | 0.431     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.05e-05  |
|    n_updates              | 4754      |
|    policy_objective       | -4.72e-08 |
|    value_loss             | 0.218     |
-----------------------------------------
Ep done - 983000.
Eval num_timesteps=26875500, episode_reward=0.13 +/- 0.98
Episode length: 29.95 +/- 0.49
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 26875500 |
| train/                    |          |
|    explained_variance     | 0.446    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.04e-05 |
|    n_updates              | 4759     |
|    policy_objective       | 4.37e-07 |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 984000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.03     |
| time/                     |           |
|    fps                    | 270       |
|    iterations             | 4380      |
|    time_elapsed           | 99632     |
|    total_timesteps        | 26910720  |
| train/                    |           |
|    explained_variance     | 0.446     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.03e-05  |
|    n_updates              | 4764      |
|    policy_objective       | -1.15e-07 |
|    value_loss             | 0.214     |
-----------------------------------------
Ep done - 985000.
Ep done - 986000.
Eval num_timesteps=26937000, episode_reward=0.01 +/- 0.98
Episode length: 30.00 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.015     |
| time/                     |           |
|    total_timesteps        | 26937000  |
| train/                    |           |
|    explained_variance     | 0.455     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.02e-05  |
|    n_updates              | 4769      |
|    policy_objective       | -1.49e-08 |
|    value_loss             | 0.225     |
-----------------------------------------
Ep done - 987000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.12     |
| time/                     |          |
|    fps                    | 270      |
|    iterations             | 4390     |
|    time_elapsed           | 99814    |
|    total_timesteps        | 26972160 |
| train/                    |          |
|    explained_variance     | 0.442    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.01e-05 |
|    n_updates              | 4774     |
|    policy_objective       | -2e-07   |
|    value_loss             | 0.223    |
----------------------------------------
Ep done - 988000.
Eval num_timesteps=26998500, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.02     |
| time/                     |          |
|    total_timesteps        | 26998500 |
| train/                    |          |
|    explained_variance     | 0.418    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1e-05    |
|    n_updates              | 4779     |
|    policy_objective       | 6.78e-07 |
|    value_loss             | 0.225    |
----------------------------------------
Ep done - 989000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 270      |
|    iterations             | 4400     |
|    time_elapsed           | 99995    |
|    total_timesteps        | 27033600 |
| train/                    |          |
|    explained_variance     | 0.435    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 9.91e-06 |
|    n_updates              | 4784     |
|    policy_objective       | 3.41e-07 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 990000.
Eval num_timesteps=27060000, episode_reward=0.04 +/- 0.98
Episode length: 30.07 +/- 0.53
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.035     |
| time/                     |           |
|    total_timesteps        | 27060000  |
| train/                    |           |
|    explained_variance     | 0.5       |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 9.81e-06  |
|    n_updates              | 4789      |
|    policy_objective       | -4.21e-07 |
|    value_loss             | 0.211     |
-----------------------------------------
Ep done - 991000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.04     |
| time/                     |           |
|    fps                    | 270       |
|    iterations             | 4410      |
|    time_elapsed           | 100176    |
|    total_timesteps        | 27095040  |
| train/                    |           |
|    explained_variance     | 0.427     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 9.7e-06   |
|    n_updates              | 4794      |
|    policy_objective       | -1.15e-07 |
|    value_loss             | 0.229     |
-----------------------------------------
Ep done - 992000.
Eval num_timesteps=27121500, episode_reward=0.22 +/- 0.96
Episode length: 30.04 +/- 0.65
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 27121500 |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 9.6e-06  |
|    n_updates              | 4799     |
|    policy_objective       | 1.4e-07  |
|    value_loss             | 0.231    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.22
SELFPLAY: new best model, bumping up generation to 89
Ep done - 993000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 270      |
|    iterations             | 4420     |
|    time_elapsed           | 100357   |
|    total_timesteps        | 27156480 |
| train/                    |          |
|    explained_variance     | 0.443    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 9.5e-06  |
|    n_updates              | 4804     |
|    policy_objective       | 5.85e-07 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 994000.
Ep done - 995000.
Eval num_timesteps=27183000, episode_reward=0.10 +/- 0.98
Episode length: 29.87 +/- 1.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.095    |
| time/                     |          |
|    total_timesteps        | 27183000 |
| train/                    |          |
|    explained_variance     | 0.437    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 9.4e-06  |
|    n_updates              | 4809     |
|    policy_objective       | 4.97e-08 |
|    value_loss             | 0.227    |
----------------------------------------
Ep done - 996000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.1      |
| time/                     |          |
|    fps                    | 270      |
|    iterations             | 4430     |
|    time_elapsed           | 100539   |
|    total_timesteps        | 27217920 |
| train/                    |          |
|    explained_variance     | 0.528    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.000275 |
|    learning_rate          | 9.29e-06 |
|    n_updates              | 4814     |
|    policy_objective       | 1.52     |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 997000.
Eval num_timesteps=27244500, episode_reward=0.07 +/- 0.98
Episode length: 29.96 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.07     |
| time/                     |          |
|    total_timesteps        | 27244500 |
| train/                    |          |
|    explained_variance     | 0.49     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 9.19e-06 |
|    n_updates              | 4819     |
|    policy_objective       | 9.24e-08 |
|    value_loss             | 0.203    |
----------------------------------------
Ep done - 998000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.02      |
| time/                     |           |
|    fps                    | 270       |
|    iterations             | 4440      |
|    time_elapsed           | 100720    |
|    total_timesteps        | 27279360  |
| train/                    |           |
|    explained_variance     | 0.459     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 9.09e-06  |
|    n_updates              | 4824      |
|    policy_objective       | -2.15e-07 |
|    value_loss             | 0.218     |
-----------------------------------------
Ep done - 999000.
Eval num_timesteps=27306000, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.11      |
| time/                     |           |
|    total_timesteps        | 27306000  |
| train/                    |           |
|    explained_variance     | 0.428     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.99e-06  |
|    n_updates              | 4829      |
|    policy_objective       | -1.71e-07 |
|    value_loss             | 0.228     |
-----------------------------------------
Ep done - 1000000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | -0.14    |
| time/                     |          |
|    fps                    | 270      |
|    iterations             | 4450     |
|    time_elapsed           | 100901   |
|    total_timesteps        | 27340800 |
| train/                    |          |
|    explained_variance     | 0.507    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.88e-06 |
|    n_updates              | 4834     |
|    policy_objective       | 2.3e-07  |
|    value_loss             | 0.197    |
----------------------------------------
Ep done - 1001000.
Eval num_timesteps=27367500, episode_reward=0.07 +/- 0.97
Episode length: 29.92 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.075    |
| time/                     |          |
|    total_timesteps        | 27367500 |
| train/                    |          |
|    explained_variance     | 0.489    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.78e-06 |
|    n_updates              | 4839     |
|    policy_objective       | 2.52e-07 |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 1002000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.08     |
| time/                     |          |
|    fps                    | 271      |
|    iterations             | 4460     |
|    time_elapsed           | 101082   |
|    total_timesteps        | 27402240 |
| train/                    |          |
|    explained_variance     | 0.436    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.68e-06 |
|    n_updates              | 4844     |
|    policy_objective       | 1.29e-07 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 1003000.
Ep done - 1004000.
Eval num_timesteps=27429000, episode_reward=0.18 +/- 0.96
Episode length: 29.98 +/- 0.54
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.18     |
| time/                     |          |
|    total_timesteps        | 27429000 |
| train/                    |          |
|    explained_variance     | 0.477    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.58e-06 |
|    n_updates              | 4849     |
|    policy_objective       | 5.06e-08 |
|    value_loss             | 0.215    |
----------------------------------------
Ep done - 1005000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.09     |
| time/                     |          |
|    fps                    | 271      |
|    iterations             | 4470     |
|    time_elapsed           | 101264   |
|    total_timesteps        | 27463680 |
| train/                    |          |
|    explained_variance     | 0.423    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.47e-06 |
|    n_updates              | 4854     |
|    policy_objective       | 2.06e-07 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 1006000.
Eval num_timesteps=27490500, episode_reward=0.28 +/- 0.95
Episode length: 30.09 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.285     |
| time/                     |           |
|    total_timesteps        | 27490500  |
| train/                    |           |
|    explained_variance     | 0.457     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.37e-06  |
|    n_updates              | 4859      |
|    policy_objective       | -2.59e-07 |
|    value_loss             | 0.226     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.285
SELFPLAY: new best model, bumping up generation to 90
Ep done - 1007000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.1       |
| time/                     |           |
|    fps                    | 271       |
|    iterations             | 4480      |
|    time_elapsed           | 101445    |
|    total_timesteps        | 27525120  |
| train/                    |           |
|    explained_variance     | 0.395     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.27e-06  |
|    n_updates              | 4864      |
|    policy_objective       | -3.54e-08 |
|    value_loss             | 0.233     |
-----------------------------------------
Ep done - 1008000.
Eval num_timesteps=27552000, episode_reward=0.14 +/- 0.98
Episode length: 30.02 +/- 0.47
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.145     |
| time/                     |           |
|    total_timesteps        | 27552000  |
| train/                    |           |
|    explained_variance     | 0.467     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.17e-06  |
|    n_updates              | 4869      |
|    policy_objective       | -4.72e-08 |
|    value_loss             | 0.217     |
-----------------------------------------
Ep done - 1009000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.06     |
| time/                     |          |
|    fps                    | 271      |
|    iterations             | 4490     |
|    time_elapsed           | 101627   |
|    total_timesteps        | 27586560 |
| train/                    |          |
|    explained_variance     | 0.49     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 8.07e-06 |
|    n_updates              | 4874     |
|    policy_objective       | 2.64e-07 |
|    value_loss             | 0.219    |
----------------------------------------
Ep done - 1010000.
Eval num_timesteps=27613500, episode_reward=0.22 +/- 0.97
Episode length: 30.03 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 27613500 |
| train/                    |          |
|    explained_variance     | 0.478    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.96e-06 |
|    n_updates              | 4879     |
|    policy_objective       | 1.5e-07  |
|    value_loss             | 0.214    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.22
SELFPLAY: new best model, bumping up generation to 91
Ep done - 1011000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.1       |
| time/                     |           |
|    fps                    | 271       |
|    iterations             | 4500      |
|    time_elapsed           | 101809    |
|    total_timesteps        | 27648000  |
| train/                    |           |
|    explained_variance     | 0.41      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.86e-06  |
|    n_updates              | 4884      |
|    policy_objective       | -1.65e-07 |
|    value_loss             | 0.232     |
-----------------------------------------
Ep done - 1012000.
Ep done - 1013000.
Eval num_timesteps=27675000, episode_reward=0.18 +/- 0.97
Episode length: 30.02 +/- 0.46
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.185     |
| time/                     |           |
|    total_timesteps        | 27675000  |
| train/                    |           |
|    explained_variance     | 0.491     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.76e-06  |
|    n_updates              | 4889      |
|    policy_objective       | -4.35e-08 |
|    value_loss             | 0.215     |
-----------------------------------------
Ep done - 1014000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.08    |
| time/                     |          |
|    fps                    | 271      |
|    iterations             | 4510     |
|    time_elapsed           | 101990   |
|    total_timesteps        | 27709440 |
| train/                    |          |
|    explained_variance     | 0.448    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.66e-06 |
|    n_updates              | 4894     |
|    policy_objective       | 1.08e-07 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 1015000.
Eval num_timesteps=27736500, episode_reward=0.17 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.17      |
| time/                     |           |
|    total_timesteps        | 27736500  |
| train/                    |           |
|    explained_variance     | 0.483     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.55e-06  |
|    n_updates              | 4899      |
|    policy_objective       | -1.76e-07 |
|    value_loss             | 0.215     |
-----------------------------------------
Ep done - 1016000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.2       |
| time/                     |           |
|    fps                    | 271       |
|    iterations             | 4520      |
|    time_elapsed           | 102171    |
|    total_timesteps        | 27770880  |
| train/                    |           |
|    explained_variance     | 0.453     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 7.45e-06  |
|    n_updates              | 4904      |
|    policy_objective       | -8.69e-09 |
|    value_loss             | 0.226     |
-----------------------------------------
Ep done - 1017000.
Eval num_timesteps=27798000, episode_reward=0.08 +/- 0.98
Episode length: 29.97 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 27798000 |
| train/                    |          |
|    explained_variance     | 0.488    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.35e-06 |
|    n_updates              | 4909     |
|    policy_objective       | 6.72e-07 |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 1018000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 271      |
|    iterations             | 4530     |
|    time_elapsed           | 102353   |
|    total_timesteps        | 27832320 |
| train/                    |          |
|    explained_variance     | 0.483    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.25e-06 |
|    n_updates              | 4914     |
|    policy_objective       | 9.53e-08 |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 1019000.
Eval num_timesteps=27859500, episode_reward=0.08 +/- 0.99
Episode length: 29.96 +/- 0.58
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 27859500 |
| train/                    |          |
|    explained_variance     | 0.459    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.14e-06 |
|    n_updates              | 4919     |
|    policy_objective       | -1.6e-07 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 1020000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.12     |
| time/                     |          |
|    fps                    | 272      |
|    iterations             | 4540     |
|    time_elapsed           | 102534   |
|    total_timesteps        | 27893760 |
| train/                    |          |
|    explained_variance     | 0.353    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.04e-06 |
|    n_updates              | 4924     |
|    policy_objective       | 3.35e-07 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 1021000.
Ep done - 1022000.
Eval num_timesteps=27921000, episode_reward=-0.05 +/- 0.98
Episode length: 29.96 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | -0.05    |
| time/                     |          |
|    total_timesteps        | 27921000 |
| train/                    |          |
|    explained_variance     | 0.469    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.94e-06 |
|    n_updates              | 4929     |
|    policy_objective       | 5.3e-07  |
|    value_loss             | 0.208    |
----------------------------------------
Ep done - 1023000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.17      |
| time/                     |           |
|    fps                    | 272       |
|    iterations             | 4550      |
|    time_elapsed           | 102716    |
|    total_timesteps        | 27955200  |
| train/                    |           |
|    explained_variance     | 0.484     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.84e-06  |
|    n_updates              | 4934      |
|    policy_objective       | -5.54e-07 |
|    value_loss             | 0.204     |
-----------------------------------------
Ep done - 1024000.
Eval num_timesteps=27982500, episode_reward=0.20 +/- 0.97
Episode length: 29.93 +/- 1.13
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.195     |
| time/                     |           |
|    total_timesteps        | 27982500  |
| train/                    |           |
|    explained_variance     | 0.47      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.73e-06  |
|    n_updates              | 4939      |
|    policy_objective       | -1.07e-07 |
|    value_loss             | 0.216     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.195
SELFPLAY: new best model, bumping up generation to 92
Ep done - 1025000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 272      |
|    iterations             | 4560     |
|    time_elapsed           | 102897   |
|    total_timesteps        | 28016640 |
| train/                    |          |
|    explained_variance     | 0.474    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.63e-06 |
|    n_updates              | 4944     |
|    policy_objective       | 4.07e-08 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 1026000.
Eval num_timesteps=28044000, episode_reward=0.16 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.16      |
| time/                     |           |
|    total_timesteps        | 28044000  |
| train/                    |           |
|    explained_variance     | 0.476     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.53e-06  |
|    n_updates              | 4949      |
|    policy_objective       | -7.92e-08 |
|    value_loss             | 0.213     |
-----------------------------------------
Ep done - 1027000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.15     |
| time/                     |          |
|    fps                    | 272      |
|    iterations             | 4570     |
|    time_elapsed           | 103078   |
|    total_timesteps        | 28078080 |
| train/                    |          |
|    explained_variance     | 0.48     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.43e-06 |
|    n_updates              | 4954     |
|    policy_objective       | -5.2e-08 |
|    value_loss             | 0.214    |
----------------------------------------
Ep done - 1028000.
Eval num_timesteps=28105500, episode_reward=0.24 +/- 0.94
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.24     |
| time/                     |          |
|    total_timesteps        | 28105500 |
| train/                    |          |
|    explained_variance     | 0.493    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.32e-06 |
|    n_updates              | 4959     |
|    policy_objective       | 1.85e-07 |
|    value_loss             | 0.211    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.24
SELFPLAY: new best model, bumping up generation to 93
Ep done - 1029000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 272      |
|    iterations             | 4580     |
|    time_elapsed           | 103260   |
|    total_timesteps        | 28139520 |
| train/                    |          |
|    explained_variance     | 0.46     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.22e-06 |
|    n_updates              | 4964     |
|    policy_objective       | 1.91e-08 |
|    value_loss             | 0.224    |
----------------------------------------
Ep done - 1030000.
Ep done - 1031000.
Eval num_timesteps=28167000, episode_reward=0.15 +/- 0.97
Episode length: 29.88 +/- 1.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.155     |
| time/                     |           |
|    total_timesteps        | 28167000  |
| train/                    |           |
|    explained_variance     | 0.481     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.12e-06  |
|    n_updates              | 4969      |
|    policy_objective       | -3.22e-07 |
|    value_loss             | 0.211     |
-----------------------------------------
Ep done - 1032000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.17    |
| time/                     |          |
|    fps                    | 272      |
|    iterations             | 4590     |
|    time_elapsed           | 103442   |
|    total_timesteps        | 28200960 |
| train/                    |          |
|    explained_variance     | 0.467    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 6.02e-06 |
|    n_updates              | 4974     |
|    policy_objective       | 5.84e-08 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 1033000.
Eval num_timesteps=28228500, episode_reward=0.22 +/- 0.97
Episode length: 30.03 +/- 0.63
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.22      |
| time/                     |           |
|    total_timesteps        | 28228500  |
| train/                    |           |
|    explained_variance     | 0.496     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.91e-06  |
|    n_updates              | 4979      |
|    policy_objective       | -2.11e-08 |
|    value_loss             | 0.205     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.22
SELFPLAY: new best model, bumping up generation to 94
Ep done - 1034000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.05     |
| time/                     |          |
|    fps                    | 272      |
|    iterations             | 4600     |
|    time_elapsed           | 103623   |
|    total_timesteps        | 28262400 |
| train/                    |          |
|    explained_variance     | 0.458    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.81e-06 |
|    n_updates              | 4984     |
|    policy_objective       | 6.46e-08 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 1035000.
Eval num_timesteps=28290000, episode_reward=0.17 +/- 0.97
Episode length: 30.04 +/- 0.57
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.165     |
| time/                     |           |
|    total_timesteps        | 28290000  |
| train/                    |           |
|    explained_variance     | 0.472     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.71e-06  |
|    n_updates              | 4989      |
|    policy_objective       | -3.14e-07 |
|    value_loss             | 0.215     |
-----------------------------------------
Ep done - 1036000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | 0.04     |
| time/                     |          |
|    fps                    | 272      |
|    iterations             | 4610     |
|    time_elapsed           | 103804   |
|    total_timesteps        | 28323840 |
| train/                    |          |
|    explained_variance     | 0.485    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.61e-06 |
|    n_updates              | 4994     |
|    policy_objective       | 6.31e-07 |
|    value_loss             | 0.214    |
----------------------------------------
Ep done - 1037000.
Eval num_timesteps=28351500, episode_reward=0.23 +/- 0.96
Episode length: 29.95 +/- 0.64
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.225    |
| time/                     |          |
|    total_timesteps        | 28351500 |
| train/                    |          |
|    explained_variance     | 0.469    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.51e-06 |
|    n_updates              | 4999     |
|    policy_objective       | 2.94e-07 |
|    value_loss             | 0.214    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.225
SELFPLAY: new best model, bumping up generation to 95
Ep done - 1038000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.11     |
| time/                     |          |
|    fps                    | 272      |
|    iterations             | 4620     |
|    time_elapsed           | 103986   |
|    total_timesteps        | 28385280 |
| train/                    |          |
|    explained_variance     | 0.416    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.4e-06  |
|    n_updates              | 5004     |
|    policy_objective       | 4.56e-08 |
|    value_loss             | 0.232    |
----------------------------------------
Ep done - 1039000.
Ep done - 1040000.
Eval num_timesteps=28413000, episode_reward=0.15 +/- 0.97
Episode length: 29.96 +/- 0.71
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.15      |
| time/                     |           |
|    total_timesteps        | 28413000  |
| train/                    |           |
|    explained_variance     | 0.486     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.3e-06   |
|    n_updates              | 5009      |
|    policy_objective       | -1.96e-08 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 1041000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.08    |
| time/                     |          |
|    fps                    | 273      |
|    iterations             | 4630     |
|    time_elapsed           | 104167   |
|    total_timesteps        | 28446720 |
| train/                    |          |
|    explained_variance     | 0.481    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 5.2e-06  |
|    n_updates              | 5014     |
|    policy_objective       | 4.89e-07 |
|    value_loss             | 0.212    |
----------------------------------------
Ep done - 1042000.
Eval num_timesteps=28474500, episode_reward=0.23 +/- 0.95
Episode length: 30.05 +/- 0.52
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.23      |
| time/                     |           |
|    total_timesteps        | 28474500  |
| train/                    |           |
|    explained_variance     | 0.469     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.1e-06   |
|    n_updates              | 5019      |
|    policy_objective       | -4.53e-08 |
|    value_loss             | 0.21      |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.23
SELFPLAY: new best model, bumping up generation to 96
Ep done - 1043000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.03     |
| time/                     |           |
|    fps                    | 273       |
|    iterations             | 4640      |
|    time_elapsed           | 104348    |
|    total_timesteps        | 28508160  |
| train/                    |           |
|    explained_variance     | 0.518     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.99e-06  |
|    n_updates              | 5024      |
|    policy_objective       | -1.49e-07 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 1044000.
Eval num_timesteps=28536000, episode_reward=0.05 +/- 0.98
Episode length: 30.02 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.05      |
| time/                     |           |
|    total_timesteps        | 28536000  |
| train/                    |           |
|    explained_variance     | 0.478     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.89e-06  |
|    n_updates              | 5029      |
|    policy_objective       | -5.72e-07 |
|    value_loss             | 0.214     |
-----------------------------------------
Ep done - 1045000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.01     |
| time/                     |          |
|    fps                    | 273      |
|    iterations             | 4650     |
|    time_elapsed           | 104529   |
|    total_timesteps        | 28569600 |
| train/                    |          |
|    explained_variance     | 0.517    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.79e-06 |
|    n_updates              | 5034     |
|    policy_objective       | 2.45e-07 |
|    value_loss             | 0.2      |
----------------------------------------
Ep done - 1046000.
Eval num_timesteps=28597500, episode_reward=0.07 +/- 0.98
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.075    |
| time/                     |          |
|    total_timesteps        | 28597500 |
| train/                    |          |
|    explained_variance     | 0.487    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.69e-06 |
|    n_updates              | 5039     |
|    policy_objective       | -1.6e-07 |
|    value_loss             | 0.211    |
----------------------------------------
Ep done - 1047000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.12     |
| time/                     |           |
|    fps                    | 273       |
|    iterations             | 4660      |
|    time_elapsed           | 104710    |
|    total_timesteps        | 28631040  |
| train/                    |           |
|    explained_variance     | 0.478     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.58e-06  |
|    n_updates              | 5044      |
|    policy_objective       | -5.77e-07 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 1048000.
Ep done - 1049000.
Eval num_timesteps=28659000, episode_reward=0.14 +/- 0.98
Episode length: 30.01 +/- 0.48
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.145    |
| time/                     |          |
|    total_timesteps        | 28659000 |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.48e-06 |
|    n_updates              | 5049     |
|    policy_objective       | 3.52e-07 |
|    value_loss             | 0.222    |
----------------------------------------
Ep done - 1050000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 273      |
|    iterations             | 4670     |
|    time_elapsed           | 104892   |
|    total_timesteps        | 28692480 |
| train/                    |          |
|    explained_variance     | 0.505    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.38e-06 |
|    n_updates              | 5054     |
|    policy_objective       | 2.21e-07 |
|    value_loss             | 0.206    |
----------------------------------------
Ep done - 1051000.
Eval num_timesteps=28720500, episode_reward=0.19 +/- 0.97
Episode length: 29.98 +/- 0.51
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.19     |
| time/                     |          |
|    total_timesteps        | 28720500 |
| train/                    |          |
|    explained_variance     | 0.497    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 4.28e-06 |
|    n_updates              | 5059     |
|    policy_objective       | 1.89e-08 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 1052000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | -0.03     |
| time/                     |           |
|    fps                    | 273       |
|    iterations             | 4680      |
|    time_elapsed           | 105073    |
|    total_timesteps        | 28753920  |
| train/                    |           |
|    explained_variance     | 0.455     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.17e-06  |
|    n_updates              | 5064      |
|    policy_objective       | -2.79e-07 |
|    value_loss             | 0.226     |
-----------------------------------------
Ep done - 1053000.
Eval num_timesteps=28782000, episode_reward=0.01 +/- 0.99
Episode length: 29.97 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.005     |
| time/                     |           |
|    total_timesteps        | 28782000  |
| train/                    |           |
|    explained_variance     | 0.497     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.07e-06  |
|    n_updates              | 5069      |
|    policy_objective       | -2.38e-07 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 1054000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.12     |
| time/                     |          |
|    fps                    | 273      |
|    iterations             | 4690     |
|    time_elapsed           | 105254   |
|    total_timesteps        | 28815360 |
| train/                    |          |
|    explained_variance     | 0.507    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.97e-06 |
|    n_updates              | 5074     |
|    policy_objective       | -3.5e-07 |
|    value_loss             | 0.209    |
----------------------------------------
Ep done - 1055000.
Eval num_timesteps=28843500, episode_reward=0.07 +/- 0.99
Episode length: 29.91 +/- 1.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | 0.07      |
| time/                     |           |
|    total_timesteps        | 28843500  |
| train/                    |           |
|    explained_variance     | 0.431     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.87e-06  |
|    n_updates              | 5079      |
|    policy_objective       | -2.32e-07 |
|    value_loss             | 0.223     |
-----------------------------------------
Ep done - 1056000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.11     |
| time/                     |           |
|    fps                    | 273       |
|    iterations             | 4700      |
|    time_elapsed           | 105435    |
|    total_timesteps        | 28876800  |
| train/                    |           |
|    explained_variance     | 0.434     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.76e-06  |
|    n_updates              | 5084      |
|    policy_objective       | -2.64e-07 |
|    value_loss             | 0.223     |
-----------------------------------------
Ep done - 1057000.
Ep done - 1058000.
Eval num_timesteps=28905000, episode_reward=0.14 +/- 0.97
Episode length: 29.98 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.135    |
| time/                     |          |
|    total_timesteps        | 28905000 |
| train/                    |          |
|    explained_variance     | 0.461    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.66e-06 |
|    n_updates              | 5089     |
|    policy_objective       | 1.21e-07 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 1059000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.13      |
| time/                     |           |
|    fps                    | 273       |
|    iterations             | 4710      |
|    time_elapsed           | 105617    |
|    total_timesteps        | 28938240  |
| train/                    |           |
|    explained_variance     | 0.529     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.56e-06  |
|    n_updates              | 5094      |
|    policy_objective       | -1.99e-07 |
|    value_loss             | 0.205     |
-----------------------------------------
Ep done - 1060000.
Eval num_timesteps=28966500, episode_reward=0.20 +/- 0.97
Episode length: 29.98 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.195     |
| time/                     |           |
|    total_timesteps        | 28966500  |
| train/                    |           |
|    explained_variance     | 0.478     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.46e-06  |
|    n_updates              | 5099      |
|    policy_objective       | -1.51e-07 |
|    value_loss             | 0.209     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.195
SELFPLAY: new best model, bumping up generation to 97
Ep done - 1061000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 274      |
|    iterations             | 4720     |
|    time_elapsed           | 105798   |
|    total_timesteps        | 28999680 |
| train/                    |          |
|    explained_variance     | 0.481    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.35e-06 |
|    n_updates              | 5104     |
|    policy_objective       | 3.2e-07  |
|    value_loss             | 0.22     |
----------------------------------------
Ep done - 1062000.
Eval num_timesteps=29028000, episode_reward=0.19 +/- 0.96
Episode length: 30.00 +/- 0.72
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.19      |
| time/                     |           |
|    total_timesteps        | 29028000  |
| train/                    |           |
|    explained_variance     | 0.374     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 3.25e-06  |
|    n_updates              | 5109      |
|    policy_objective       | -2.73e-07 |
|    value_loss             | 0.24      |
-----------------------------------------
Ep done - 1063000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 274      |
|    iterations             | 4730     |
|    time_elapsed           | 105979   |
|    total_timesteps        | 29061120 |
| train/                    |          |
|    explained_variance     | 0.427    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.15e-06 |
|    n_updates              | 5114     |
|    policy_objective       | 2.17e-07 |
|    value_loss             | 0.23     |
----------------------------------------
Ep done - 1064000.
Eval num_timesteps=29089500, episode_reward=0.11 +/- 0.98
Episode length: 29.98 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.11     |
| time/                     |          |
|    total_timesteps        | 29089500 |
| train/                    |          |
|    explained_variance     | 0.481    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.05e-06 |
|    n_updates              | 5119     |
|    policy_objective       | 1.73e-07 |
|    value_loss             | 0.221    |
----------------------------------------
Ep done - 1065000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.07    |
| time/                     |          |
|    fps                    | 274      |
|    iterations             | 4740     |
|    time_elapsed           | 106160   |
|    total_timesteps        | 29122560 |
| train/                    |          |
|    explained_variance     | 0.537    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.95e-06 |
|    n_updates              | 5124     |
|    policy_objective       | 3.12e-07 |
|    value_loss             | 0.193    |
----------------------------------------
Ep done - 1066000.
Ep done - 1067000.
Eval num_timesteps=29151000, episode_reward=0.09 +/- 0.97
Episode length: 30.00 +/- 0.55
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.09     |
| time/                     |          |
|    total_timesteps        | 29151000 |
| train/                    |          |
|    explained_variance     | 0.41     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.84e-06 |
|    n_updates              | 5129     |
|    policy_objective       | 2.62e-07 |
|    value_loss             | 0.234    |
----------------------------------------
Ep done - 1068000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | 0        |
| time/                     |          |
|    fps                    | 274      |
|    iterations             | 4750     |
|    time_elapsed           | 106341   |
|    total_timesteps        | 29184000 |
| train/                    |          |
|    explained_variance     | 0.521    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.74e-06 |
|    n_updates              | 5134     |
|    policy_objective       | 3.37e-07 |
|    value_loss             | 0.2      |
----------------------------------------
Ep done - 1069000.
Eval num_timesteps=29212500, episode_reward=0.08 +/- 0.97
Episode length: 29.92 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.08     |
| time/                     |          |
|    total_timesteps        | 29212500 |
| train/                    |          |
|    explained_variance     | 0.439    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.64e-06 |
|    n_updates              | 5139     |
|    policy_objective       | 2.26e-07 |
|    value_loss             | 0.237    |
----------------------------------------
Ep done - 1070000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | -0.02    |
| time/                     |          |
|    fps                    | 274      |
|    iterations             | 4760     |
|    time_elapsed           | 106522   |
|    total_timesteps        | 29245440 |
| train/                    |          |
|    explained_variance     | 0.473    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.54e-06 |
|    n_updates              | 5144     |
|    policy_objective       | 2.58e-07 |
|    value_loss             | 0.217    |
----------------------------------------
Ep done - 1071000.
Eval num_timesteps=29274000, episode_reward=0.10 +/- 0.97
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 29274000 |
| train/                    |          |
|    explained_variance     | 0.441    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.43e-06 |
|    n_updates              | 5149     |
|    policy_objective       | 1.18e-07 |
|    value_loss             | 0.228    |
----------------------------------------
Ep done - 1072000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.02      |
| time/                     |           |
|    fps                    | 274       |
|    iterations             | 4770      |
|    time_elapsed           | 106704    |
|    total_timesteps        | 29306880  |
| train/                    |           |
|    explained_variance     | 0.493     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.33e-06  |
|    n_updates              | 5154      |
|    policy_objective       | -2.81e-07 |
|    value_loss             | 0.21      |
-----------------------------------------
Ep done - 1073000.
Eval num_timesteps=29335500, episode_reward=0.14 +/- 0.98
Episode length: 30.00 +/- 0.48
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.145     |
| time/                     |           |
|    total_timesteps        | 29335500  |
| train/                    |           |
|    explained_variance     | 0.439     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 2.23e-06  |
|    n_updates              | 5159      |
|    policy_objective       | -3.77e-07 |
|    value_loss             | 0.23      |
-----------------------------------------
Ep done - 1074000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 29.9     |
|    ep_rew_mean            | 0.03     |
| time/                     |          |
|    fps                    | 274      |
|    iterations             | 4780     |
|    time_elapsed           | 106885   |
|    total_timesteps        | 29368320 |
| train/                    |          |
|    explained_variance     | 0.462    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.13e-06 |
|    n_updates              | 5164     |
|    policy_objective       | 2.99e-07 |
|    value_loss             | 0.218    |
----------------------------------------
Ep done - 1075000.
Ep done - 1076000.
Eval num_timesteps=29397000, episode_reward=0.13 +/- 0.97
Episode length: 30.03 +/- 0.62
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.13     |
| time/                     |          |
|    total_timesteps        | 29397000 |
| train/                    |          |
|    explained_variance     | 0.483    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.02e-06 |
|    n_updates              | 5169     |
|    policy_objective       | 4.44e-08 |
|    value_loss             | 0.21     |
----------------------------------------
Ep done - 1077000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.03      |
| time/                     |           |
|    fps                    | 274       |
|    iterations             | 4790      |
|    time_elapsed           | 107066    |
|    total_timesteps        | 29429760  |
| train/                    |           |
|    explained_variance     | 0.494     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.92e-06  |
|    n_updates              | 5174      |
|    policy_objective       | -5.46e-08 |
|    value_loss             | 0.213     |
-----------------------------------------
Ep done - 1078000.
Eval num_timesteps=29458500, episode_reward=0.10 +/- 0.97
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 29458500 |
| train/                    |          |
|    explained_variance     | 0.453    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.82e-06 |
|    n_updates              | 5179     |
|    policy_objective       | 1.14e-07 |
|    value_loss             | 0.233    |
----------------------------------------
Ep done - 1079000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | 0.1       |
| time/                     |           |
|    fps                    | 274       |
|    iterations             | 4800      |
|    time_elapsed           | 107247    |
|    total_timesteps        | 29491200  |
| train/                    |           |
|    explained_variance     | 0.463     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.72e-06  |
|    n_updates              | 5184      |
|    policy_objective       | -1.38e-07 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 1080000.
Eval num_timesteps=29520000, episode_reward=0.18 +/- 0.96
Episode length: 30.08 +/- 0.55
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.185     |
| time/                     |           |
|    total_timesteps        | 29520000  |
| train/                    |           |
|    explained_variance     | 0.496     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.61e-06  |
|    n_updates              | 5189      |
|    policy_objective       | -1.58e-07 |
|    value_loss             | 0.208     |
-----------------------------------------
Ep done - 1081000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.04      |
| time/                     |           |
|    fps                    | 275       |
|    iterations             | 4810      |
|    time_elapsed           | 107428    |
|    total_timesteps        | 29552640  |
| train/                    |           |
|    explained_variance     | 0.417     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.51e-06  |
|    n_updates              | 5194      |
|    policy_objective       | -1.17e-07 |
|    value_loss             | 0.23      |
-----------------------------------------
Ep done - 1082000.
Eval num_timesteps=29581500, episode_reward=-0.01 +/- 0.99
Episode length: 29.95 +/- 0.95
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 29.9      |
|    mean_reward            | -0.005    |
| time/                     |           |
|    total_timesteps        | 29581500  |
| train/                    |           |
|    explained_variance     | 0.457     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.41e-06  |
|    n_updates              | 5199      |
|    policy_objective       | -1.88e-07 |
|    value_loss             | 0.224     |
-----------------------------------------
Ep done - 1083000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30.1      |
|    ep_rew_mean            | 0.07      |
| time/                     |           |
|    fps                    | 275       |
|    iterations             | 4820      |
|    time_elapsed           | 107609    |
|    total_timesteps        | 29614080  |
| train/                    |           |
|    explained_variance     | 0.42      |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.31e-06  |
|    n_updates              | 5204      |
|    policy_objective       | -3.82e-07 |
|    value_loss             | 0.231     |
-----------------------------------------
Ep done - 1084000.
Ep done - 1085000.
Eval num_timesteps=29643000, episode_reward=0.10 +/- 0.98
Episode length: 30.06 +/- 0.50
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 29643000 |
| train/                    |          |
|    explained_variance     | 0.493    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.2e-06  |
|    n_updates              | 5209     |
|    policy_objective       | 1.15e-07 |
|    value_loss             | 0.214    |
----------------------------------------
Ep done - 1086000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30.1     |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 275      |
|    iterations             | 4830     |
|    time_elapsed           | 107790   |
|    total_timesteps        | 29675520 |
| train/                    |          |
|    explained_variance     | 0.497    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 1.1e-06  |
|    n_updates              | 5214     |
|    policy_objective       | 1.89e-07 |
|    value_loss             | 0.216    |
----------------------------------------
Ep done - 1087000.
Eval num_timesteps=29704500, episode_reward=0.15 +/- 0.97
Episode length: 30.00 +/- 0.51
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.155     |
| time/                     |           |
|    total_timesteps        | 29704500  |
| train/                    |           |
|    explained_variance     | 0.468     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1e-06     |
|    n_updates              | 5219      |
|    policy_objective       | -4.78e-08 |
|    value_loss             | 0.216     |
-----------------------------------------
Ep done - 1088000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.03      |
| time/                     |           |
|    fps                    | 275       |
|    iterations             | 4840      |
|    time_elapsed           | 107971    |
|    total_timesteps        | 29736960  |
| train/                    |           |
|    explained_variance     | 0.533     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 8.97e-07  |
|    n_updates              | 5224      |
|    policy_objective       | -2.82e-07 |
|    value_loss             | 0.201     |
-----------------------------------------
Ep done - 1089000.
Eval num_timesteps=29766000, episode_reward=0.21 +/- 0.95
Episode length: 29.95 +/- 1.04
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.9     |
|    mean_reward            | 0.21     |
| time/                     |          |
|    total_timesteps        | 29766000 |
| train/                    |          |
|    explained_variance     | 0.51     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.95e-07 |
|    n_updates              | 5229     |
|    policy_objective       | 1.35e-07 |
|    value_loss             | 0.209    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.21
SELFPLAY: new best model, bumping up generation to 98
Ep done - 1090000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 29.9      |
|    ep_rew_mean            | -0.15     |
| time/                     |           |
|    fps                    | 275       |
|    iterations             | 4850      |
|    time_elapsed           | 108152    |
|    total_timesteps        | 29798400  |
| train/                    |           |
|    explained_variance     | 0.464     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 6.92e-07  |
|    n_updates              | 5234      |
|    policy_objective       | -5.88e-07 |
|    value_loss             | 0.223     |
-----------------------------------------
Ep done - 1091000.
Eval num_timesteps=29827500, episode_reward=-0.01 +/- 0.98
Episode length: 29.96 +/- 0.49
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | -0.01     |
| time/                     |           |
|    total_timesteps        | 29827500  |
| train/                    |           |
|    explained_variance     | 0.403     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 5.9e-07   |
|    n_updates              | 5239      |
|    policy_objective       | -2.04e-07 |
|    value_loss             | 0.24      |
-----------------------------------------
Ep done - 1092000.
-----------------------------------------
| rollout/                  |           |
|    ep_len_mean            | 30        |
|    ep_rew_mean            | 0.01      |
| time/                     |           |
|    fps                    | 275       |
|    iterations             | 4860      |
|    time_elapsed           | 108334    |
|    total_timesteps        | 29859840  |
| train/                    |           |
|    explained_variance     | 0.502     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 4.88e-07  |
|    n_updates              | 5244      |
|    policy_objective       | -2.24e-07 |
|    value_loss             | 0.212     |
-----------------------------------------
Ep done - 1093000.
Ep done - 1094000.
Eval num_timesteps=29889000, episode_reward=0.22 +/- 0.96
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.22     |
| time/                     |          |
|    total_timesteps        | 29889000 |
| train/                    |          |
|    explained_variance     | 0.505    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 3.85e-07 |
|    n_updates              | 5249     |
|    policy_objective       | -9e-09   |
|    value_loss             | 0.208    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.22
SELFPLAY: new best model, bumping up generation to 99
Ep done - 1095000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.03    |
| time/                     |          |
|    fps                    | 275      |
|    iterations             | 4870     |
|    time_elapsed           | 108515   |
|    total_timesteps        | 29921280 |
| train/                    |          |
|    explained_variance     | 0.505    |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 2.83e-07 |
|    n_updates              | 5254     |
|    policy_objective       | 6.55e-08 |
|    value_loss             | 0.213    |
----------------------------------------
Ep done - 1096000.
Eval num_timesteps=29950500, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.54
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30        |
|    mean_reward            | 0.095     |
| time/                     |           |
|    total_timesteps        | 29950500  |
| train/                    |           |
|    explained_variance     | 0.415     |
|    is_line_search_success | 0         |
|    kl_divergence_loss     | 0         |
|    learning_rate          | 1.8e-07   |
|    n_updates              | 5259      |
|    policy_objective       | -5.65e-07 |
|    value_loss             | 0.239     |
-----------------------------------------
Ep done - 1097000.
----------------------------------------
| rollout/                  |          |
|    ep_len_mean            | 30       |
|    ep_rew_mean            | -0.01    |
| time/                     |          |
|    fps                    | 275      |
|    iterations             | 4880     |
|    time_elapsed           | 108696   |
|    total_timesteps        | 29982720 |
| train/                    |          |
|    explained_variance     | 0.47     |
|    is_line_search_success | 0        |
|    kl_divergence_loss     | 0        |
|    learning_rate          | 7.81e-08 |
|    n_updates              | 5264     |
|    policy_objective       | 6.71e-08 |
|    value_loss             | 0.226    |
----------------------------------------
Ep done - 1098000.
Elapsed time: 30h 12m 32s
