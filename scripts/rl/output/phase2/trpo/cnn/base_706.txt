/bin/bash: /opt/miniconda/3.10/lib/libtinfo.so.6: no version information available (required by /bin/bash)
CUDA Available: True
CPU Model: AMD EPYC 7313P 16-Core Processor
GPU Model: Tesla T4
CUDA available: True
seed: 13 
num_timesteps: 30000000 
eval_freq: 21000 
eval_episoded: 200 
best_threshold: 0.25 
logdir: scripts/rl/output/phase2/trpo/cnn/base/ 
cnn_policy: True 
continueFrom_model: None
/home/student/pantrasa/project/venv310/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.[0m
  logger.warn(
/home/student/pantrasa/project/venv310/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.[0m
  logger.warn(
Using cpu device

params: {'learning_rate': 1e-05, 'n_steps': 20480, 'batch_size': 128, 'verbose': 100, 'seed': 13, 'policy_kwargs': {'net_arch': {'pi': [128, 128, 128, 128, 128, 128, 128, 128], 'vf': [64, 64, 64, 64, 64, 64, 64, 64]}}}

Eval num_timesteps=21000, episode_reward=0.55 +/- 0.81
Episode length: 30.09 +/- 0.60
-----------------------------------------
| eval/                     |           |
|    mean_ep_length         | 30.1      |
|    mean_reward            | 0.55      |
| time/                     |           |
|    total_timesteps        | 21000     |
| train/                    |           |
|    explained_variance     | -0.000422 |
|    is_line_search_success | 1         |
|    kl_divergence_loss     | 0.00195   |
|    learning_rate          | 1e-05     |
|    n_updates              | 1         |
|    policy_objective       | 0.0065    |
|    value_loss             | 0.265     |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.55
SELFPLAY: new best model, bumping up generation to 1
Ep done - 1000.
Eval num_timesteps=42000, episode_reward=0.61 +/- 0.77
Episode length: 30.05 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.61     |
| time/                     |          |
|    total_timesteps        | 42000    |
| train/                    |          |
|    explained_variance     | 0.0118   |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00801  |
|    learning_rate          | 1e-05    |
|    n_updates              | 2        |
|    policy_objective       | 0.0125   |
|    value_loss             | 0.27     |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.61
SELFPLAY: new best model, bumping up generation to 2
Ep done - 2000.
Eval num_timesteps=63000, episode_reward=0.74 +/- 0.66
Episode length: 30.06 +/- 0.61
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30.1     |
|    mean_reward            | 0.745    |
| time/                     |          |
|    total_timesteps        | 63000    |
| train/                    |          |
|    explained_variance     | 0.0161   |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00946  |
|    learning_rate          | 1e-05    |
|    n_updates              | 3        |
|    policy_objective       | 0.0115   |
|    value_loss             | 0.268    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.745
SELFPLAY: new best model, bumping up generation to 3
Ep done - 3000.
Eval num_timesteps=84000, episode_reward=0.60 +/- 0.77
Episode length: 29.98 +/- 0.64
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 30       |
|    mean_reward            | 0.605    |
| time/                     |          |
|    total_timesteps        | 84000    |
| train/                    |          |
|    explained_variance     | 0.0215   |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00995  |
|    learning_rate          | 1e-05    |
|    n_updates              | 4        |
|    policy_objective       | 0.0132   |
|    value_loss             | 0.262    |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.605
SELFPLAY: new best model, bumping up generation to 4
Ep done - 4000.
Eval num_timesteps=105000, episode_reward=0.65 +/- 0.74
Episode length: 29.75 +/- 2.42
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 29.8     |
|    mean_reward            | 0.65     |
| time/                     |          |
|    total_timesteps        | 105000   |
| train/                    |          |
|    explained_variance     | 0.0314   |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00861  |
|    learning_rate          | 1e-05    |
|    n_updates              | 5        |
|    policy_objective       | 0.0113   |
|    value_loss             | 0.263    |
----------------------------------------
slurmstepd-n19: error: *** JOB 706 ON n19 CANCELLED AT 2024-06-15T21:35:41 ***
