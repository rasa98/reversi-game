CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
2024-06-09 23:33:56.016716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-09 23:33:56.164374: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-06-09 23:33:57.032355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-09 23:33:57.032466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-09 23:33:57.032478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
CUDA available: True
net architecture - {'net_arch': {'pi': [64, 64, 64, 64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64]}}
params: 
NUM_TIMESTEPS -100000000
EVAL_FREQ=61441
EVAL_EPISODES=500
BEST_THRESHOLD=0.14
LOGDIR=scripts/rl/output/v3v3/
model params: 
 {'learning_rate': <function linear_schedule.<locals>.func at 0x7f09edb3ff70>, 'n_steps': 61440, 'n_epochs': 5, 'batch_size': 64}
Wrapping the env in a DummyVecEnv.
starting model: scripts/rl/output/v3/history_0020
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 1        |
|    time_elapsed    | 288      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61441, episode_reward=0.01 +/- 0.97
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 61441       |
| train/                  |             |
|    approx_kl            | 0.022792924 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.769      |
|    explained_variance   | 0.271       |
|    learning_rate        | 8.99e-05    |
|    loss                 | 0.0824      |
|    n_updates            | 4185        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 183      |
|    iterations      | 2        |
|    time_elapsed    | 668      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=122882, episode_reward=-0.03 +/- 0.98
Episode length: 29.95 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 122882      |
| train/                  |             |
|    approx_kl            | 0.021966577 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.311       |
|    learning_rate        | 8.99e-05    |
|    loss                 | 0.0683      |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 3        |
|    time_elapsed    | 1056     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184323, episode_reward=-0.02 +/- 0.98
Episode length: 29.95 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 184323     |
| train/                  |            |
|    approx_kl            | 0.02211949 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.3        |
|    learning_rate        | 8.98e-05   |
|    loss                 | 0.0885     |
|    n_updates            | 4195       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 4        |
|    time_elapsed    | 1440     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=245764, episode_reward=0.07 +/- 0.98
Episode length: 29.93 +/- 1.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 245764      |
| train/                  |             |
|    approx_kl            | 0.022393577 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.98e-05    |
|    loss                 | 0.0595      |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 5        |
|    time_elapsed    | 1823     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307205, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 307205      |
| train/                  |             |
|    approx_kl            | 0.021617362 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.769      |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.97e-05    |
|    loss                 | 0.111       |
|    n_updates            | 4205        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 6        |
|    time_elapsed    | 2206     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=368646, episode_reward=-0.00 +/- 0.98
Episode length: 29.92 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 368646     |
| train/                  |            |
|    approx_kl            | 0.02150069 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.321      |
|    learning_rate        | 8.97e-05   |
|    loss                 | 0.0901     |
|    n_updates            | 4210       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 7        |
|    time_elapsed    | 2587     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430087, episode_reward=-0.02 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 430087      |
| train/                  |             |
|    approx_kl            | 0.022412479 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.306       |
|    learning_rate        | 8.96e-05    |
|    loss                 | 0.0683      |
|    n_updates            | 4215        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 8        |
|    time_elapsed    | 2959     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=491528, episode_reward=-0.03 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.032      |
| time/                   |             |
|    total_timesteps      | 491528      |
| train/                  |             |
|    approx_kl            | 0.021174219 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.298       |
|    learning_rate        | 8.96e-05    |
|    loss                 | 0.0714      |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 9        |
|    time_elapsed    | 3341     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=552969, episode_reward=-0.02 +/- 0.99
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 552969      |
| train/                  |             |
|    approx_kl            | 0.021263449 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.95e-05    |
|    loss                 | 0.121       |
|    n_updates            | 4225        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 10       |
|    time_elapsed    | 3724     |
|    total_timesteps | 614400   |
---------------------------------
Eval num_timesteps=614410, episode_reward=-0.06 +/- 0.98
Episode length: 29.88 +/- 1.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.056      |
| time/                   |             |
|    total_timesteps      | 614410      |
| train/                  |             |
|    approx_kl            | 0.021307152 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.94e-05    |
|    loss                 | 0.0676      |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 11       |
|    time_elapsed    | 4105     |
|    total_timesteps | 675840   |
---------------------------------
Eval num_timesteps=675851, episode_reward=0.00 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 675851      |
| train/                  |             |
|    approx_kl            | 0.020992203 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.338       |
|    learning_rate        | 8.94e-05    |
|    loss                 | 0.065       |
|    n_updates            | 4235        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 12       |
|    time_elapsed    | 4488     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=737292, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 737292      |
| train/                  |             |
|    approx_kl            | 0.021580892 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.769      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.93e-05    |
|    loss                 | 0.0777      |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 13       |
|    time_elapsed    | 4871     |
|    total_timesteps | 798720   |
---------------------------------
Eval num_timesteps=798733, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 798733      |
| train/                  |             |
|    approx_kl            | 0.021657629 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.93e-05    |
|    loss                 | 0.0655      |
|    n_updates            | 4245        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 14       |
|    time_elapsed    | 5247     |
|    total_timesteps | 860160   |
---------------------------------
Eval num_timesteps=860174, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 860174      |
| train/                  |             |
|    approx_kl            | 0.021254355 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.326       |
|    learning_rate        | 8.92e-05    |
|    loss                 | 0.0648      |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 15       |
|    time_elapsed    | 5627     |
|    total_timesteps | 921600   |
---------------------------------
Eval num_timesteps=921615, episode_reward=0.07 +/- 0.99
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 921615      |
| train/                  |             |
|    approx_kl            | 0.021293871 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.92e-05    |
|    loss                 | 0.0521      |
|    n_updates            | 4255        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 16       |
|    time_elapsed    | 6006     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=983056, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 983056      |
| train/                  |             |
|    approx_kl            | 0.021458734 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.329       |
|    learning_rate        | 8.91e-05    |
|    loss                 | 0.0692      |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 17       |
|    time_elapsed    | 6380     |
|    total_timesteps | 1044480  |
---------------------------------
Eval num_timesteps=1044497, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.088      |
| time/                   |            |
|    total_timesteps      | 1044497    |
| train/                  |            |
|    approx_kl            | 0.02162784 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.307      |
|    learning_rate        | 8.91e-05   |
|    loss                 | 0.109      |
|    n_updates            | 4265       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 18       |
|    time_elapsed    | 6755     |
|    total_timesteps | 1105920  |
---------------------------------
Eval num_timesteps=1105938, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 1105938     |
| train/                  |             |
|    approx_kl            | 0.021085646 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.9e-05     |
|    loss                 | 0.113       |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 19       |
|    time_elapsed    | 7136     |
|    total_timesteps | 1167360  |
---------------------------------
Eval num_timesteps=1167379, episode_reward=0.06 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 1167379     |
| train/                  |             |
|    approx_kl            | 0.021266619 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.89e-05    |
|    loss                 | 0.0816      |
|    n_updates            | 4275        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 20       |
|    time_elapsed    | 7516     |
|    total_timesteps | 1228800  |
---------------------------------
Eval num_timesteps=1228820, episode_reward=0.12 +/- 0.98
Episode length: 30.06 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 1228820     |
| train/                  |             |
|    approx_kl            | 0.021449776 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.315       |
|    learning_rate        | 8.89e-05    |
|    loss                 | 0.0788      |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 21       |
|    time_elapsed    | 7900     |
|    total_timesteps | 1290240  |
---------------------------------
Eval num_timesteps=1290261, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 1290261     |
| train/                  |             |
|    approx_kl            | 0.020866103 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.88e-05    |
|    loss                 | 0.0553      |
|    n_updates            | 4285        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 22       |
|    time_elapsed    | 8273     |
|    total_timesteps | 1351680  |
---------------------------------
Eval num_timesteps=1351702, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 1351702     |
| train/                  |             |
|    approx_kl            | 0.021269482 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.314       |
|    learning_rate        | 8.88e-05    |
|    loss                 | 0.0943      |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 23       |
|    time_elapsed    | 8657     |
|    total_timesteps | 1413120  |
---------------------------------
Eval num_timesteps=1413143, episode_reward=0.03 +/- 0.99
Episode length: 29.89 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 1413143     |
| train/                  |             |
|    approx_kl            | 0.021115707 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.337       |
|    learning_rate        | 8.87e-05    |
|    loss                 | 0.0535      |
|    n_updates            | 4295        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 24       |
|    time_elapsed    | 9041     |
|    total_timesteps | 1474560  |
---------------------------------
Eval num_timesteps=1474584, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 1474584     |
| train/                  |             |
|    approx_kl            | 0.022162955 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.87e-05    |
|    loss                 | 0.11        |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 25       |
|    time_elapsed    | 9420     |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1536025, episode_reward=0.16 +/- 0.98
Episode length: 30.06 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.156       |
| time/                   |             |
|    total_timesteps      | 1536025     |
| train/                  |             |
|    approx_kl            | 0.021092596 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.331       |
|    learning_rate        | 8.86e-05    |
|    loss                 | 0.065       |
|    n_updates            | 4305        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.156
SELFPLAY: new best model, bumping up generation to 1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 26       |
|    time_elapsed    | 9801     |
|    total_timesteps | 1597440  |
---------------------------------
Eval num_timesteps=1597466, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.062      |
| time/                   |            |
|    total_timesteps      | 1597466    |
| train/                  |            |
|    approx_kl            | 0.02127773 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.761     |
|    explained_variance   | 0.327      |
|    learning_rate        | 8.86e-05   |
|    loss                 | 0.0826     |
|    n_updates            | 4310       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 27       |
|    time_elapsed    | 10183    |
|    total_timesteps | 1658880  |
---------------------------------
Eval num_timesteps=1658907, episode_reward=-0.06 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.064      |
| time/                   |             |
|    total_timesteps      | 1658907     |
| train/                  |             |
|    approx_kl            | 0.020882858 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.761      |
|    explained_variance   | 0.294       |
|    learning_rate        | 8.85e-05    |
|    loss                 | 0.0784      |
|    n_updates            | 4315        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.246       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 28       |
|    time_elapsed    | 10556    |
|    total_timesteps | 1720320  |
---------------------------------
Eval num_timesteps=1720348, episode_reward=-0.03 +/- 0.98
Episode length: 29.94 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 1720348    |
| train/                  |            |
|    approx_kl            | 0.02107683 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.311      |
|    learning_rate        | 8.85e-05   |
|    loss                 | 0.0546     |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 29       |
|    time_elapsed    | 10938    |
|    total_timesteps | 1781760  |
---------------------------------
Eval num_timesteps=1781789, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 1781789     |
| train/                  |             |
|    approx_kl            | 0.021095494 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.337       |
|    learning_rate        | 8.84e-05    |
|    loss                 | 0.0866      |
|    n_updates            | 4325        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 30       |
|    time_elapsed    | 11320    |
|    total_timesteps | 1843200  |
---------------------------------
Eval num_timesteps=1843230, episode_reward=-0.03 +/- 0.99
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.034      |
| time/                   |             |
|    total_timesteps      | 1843230     |
| train/                  |             |
|    approx_kl            | 0.020307107 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.83e-05    |
|    loss                 | 0.0563      |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 31       |
|    time_elapsed    | 11701    |
|    total_timesteps | 1904640  |
---------------------------------
Eval num_timesteps=1904671, episode_reward=-0.03 +/- 0.99
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 1904671     |
| train/                  |             |
|    approx_kl            | 0.020866137 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.83e-05    |
|    loss                 | 0.0785      |
|    n_updates            | 4335        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 32       |
|    time_elapsed    | 12075    |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1966112, episode_reward=0.06 +/- 0.99
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 1966112     |
| train/                  |             |
|    approx_kl            | 0.020979516 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.82e-05    |
|    loss                 | 0.124       |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 33       |
|    time_elapsed    | 12456    |
|    total_timesteps | 2027520  |
---------------------------------
Eval num_timesteps=2027553, episode_reward=0.03 +/- 0.98
Episode length: 30.04 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 2027553     |
| train/                  |             |
|    approx_kl            | 0.020871881 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.306       |
|    learning_rate        | 8.82e-05    |
|    loss                 | 0.073       |
|    n_updates            | 4345        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 34       |
|    time_elapsed    | 12830    |
|    total_timesteps | 2088960  |
---------------------------------
Eval num_timesteps=2088994, episode_reward=0.12 +/- 0.97
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 2088994     |
| train/                  |             |
|    approx_kl            | 0.020371385 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.334       |
|    learning_rate        | 8.81e-05    |
|    loss                 | 0.096       |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 35       |
|    time_elapsed    | 13212    |
|    total_timesteps | 2150400  |
---------------------------------
Eval num_timesteps=2150435, episode_reward=0.01 +/- 0.98
Episode length: 30.01 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.008      |
| time/                   |            |
|    total_timesteps      | 2150435    |
| train/                  |            |
|    approx_kl            | 0.02079815 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.762     |
|    explained_variance   | 0.299      |
|    learning_rate        | 8.81e-05   |
|    loss                 | 0.0368     |
|    n_updates            | 4355       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 36       |
|    time_elapsed    | 13593    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2211876, episode_reward=0.04 +/- 0.98
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 2211876    |
| train/                  |            |
|    approx_kl            | 0.02035747 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.334      |
|    learning_rate        | 8.8e-05    |
|    loss                 | 0.0661     |
|    n_updates            | 4360       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 37       |
|    time_elapsed    | 13964    |
|    total_timesteps | 2273280  |
---------------------------------
Eval num_timesteps=2273317, episode_reward=-0.02 +/- 0.99
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 2273317     |
| train/                  |             |
|    approx_kl            | 0.020630274 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.8e-05     |
|    loss                 | 0.0718      |
|    n_updates            | 4365        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 38       |
|    time_elapsed    | 14344    |
|    total_timesteps | 2334720  |
---------------------------------
Eval num_timesteps=2334758, episode_reward=0.04 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 2334758     |
| train/                  |             |
|    approx_kl            | 0.020539582 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.298       |
|    learning_rate        | 8.79e-05    |
|    loss                 | 0.0426      |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 39       |
|    time_elapsed    | 14725    |
|    total_timesteps | 2396160  |
---------------------------------
Eval num_timesteps=2396199, episode_reward=0.05 +/- 0.98
Episode length: 30.05 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 2396199    |
| train/                  |            |
|    approx_kl            | 0.02034525 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.33       |
|    learning_rate        | 8.78e-05   |
|    loss                 | 0.0624     |
|    n_updates            | 4375       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 40       |
|    time_elapsed    | 15107    |
|    total_timesteps | 2457600  |
---------------------------------
Eval num_timesteps=2457640, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.01       |
| time/                   |            |
|    total_timesteps      | 2457640    |
| train/                  |            |
|    approx_kl            | 0.02060372 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.308      |
|    learning_rate        | 8.78e-05   |
|    loss                 | 0.0482     |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 41       |
|    time_elapsed    | 15465    |
|    total_timesteps | 2519040  |
---------------------------------
Eval num_timesteps=2519081, episode_reward=0.10 +/- 0.98
Episode length: 30.06 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 2519081    |
| train/                  |            |
|    approx_kl            | 0.02071855 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.754     |
|    explained_variance   | 0.338      |
|    learning_rate        | 8.77e-05   |
|    loss                 | 0.0777     |
|    n_updates            | 4385       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 42       |
|    time_elapsed    | 15807    |
|    total_timesteps | 2580480  |
---------------------------------
Eval num_timesteps=2580522, episode_reward=0.10 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 2580522     |
| train/                  |             |
|    approx_kl            | 0.020295104 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.751      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.77e-05    |
|    loss                 | 0.0871      |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 163      |
|    iterations      | 43       |
|    time_elapsed    | 16151    |
|    total_timesteps | 2641920  |
---------------------------------
Eval num_timesteps=2641963, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 2641963     |
| train/                  |             |
|    approx_kl            | 0.020365536 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.321       |
|    learning_rate        | 8.76e-05    |
|    loss                 | 0.0835      |
|    n_updates            | 4395        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 44       |
|    time_elapsed    | 16501    |
|    total_timesteps | 2703360  |
---------------------------------
Eval num_timesteps=2703404, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 2703404     |
| train/                  |             |
|    approx_kl            | 0.020729624 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.293       |
|    learning_rate        | 8.76e-05    |
|    loss                 | 0.101       |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 45       |
|    time_elapsed    | 16850    |
|    total_timesteps | 2764800  |
---------------------------------
Eval num_timesteps=2764845, episode_reward=0.09 +/- 0.97
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 2764845     |
| train/                  |             |
|    approx_kl            | 0.020412052 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.317       |
|    learning_rate        | 8.75e-05    |
|    loss                 | 0.0999      |
|    n_updates            | 4405        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 46       |
|    time_elapsed    | 17199    |
|    total_timesteps | 2826240  |
---------------------------------
Eval num_timesteps=2826286, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 2826286     |
| train/                  |             |
|    approx_kl            | 0.020367507 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.326       |
|    learning_rate        | 8.75e-05    |
|    loss                 | 0.0944      |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 47       |
|    time_elapsed    | 17547    |
|    total_timesteps | 2887680  |
---------------------------------
Eval num_timesteps=2887727, episode_reward=0.07 +/- 0.98
Episode length: 30.04 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 2887727     |
| train/                  |             |
|    approx_kl            | 0.020090152 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.74e-05    |
|    loss                 | 0.0665      |
|    n_updates            | 4415        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 48       |
|    time_elapsed    | 17894    |
|    total_timesteps | 2949120  |
---------------------------------
Eval num_timesteps=2949168, episode_reward=0.23 +/- 0.96
Episode length: 30.01 +/- 0.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.226      |
| time/                   |            |
|    total_timesteps      | 2949168    |
| train/                  |            |
|    approx_kl            | 0.01984762 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.334      |
|    learning_rate        | 8.73e-05   |
|    loss                 | 0.0663     |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.236      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.226
SELFPLAY: new best model, bumping up generation to 2
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 49       |
|    time_elapsed    | 18232    |
|    total_timesteps | 3010560  |
---------------------------------
Eval num_timesteps=3010609, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 3010609     |
| train/                  |             |
|    approx_kl            | 0.020330714 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.746      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.73e-05    |
|    loss                 | 0.0569      |
|    n_updates            | 4425        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 50       |
|    time_elapsed    | 18580    |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3072050, episode_reward=0.02 +/- 0.99
Episode length: 29.99 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 3072050    |
| train/                  |            |
|    approx_kl            | 0.02024635 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.749     |
|    explained_variance   | 0.315      |
|    learning_rate        | 8.72e-05   |
|    loss                 | 0.0739     |
|    n_updates            | 4430       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 51       |
|    time_elapsed    | 18917    |
|    total_timesteps | 3133440  |
---------------------------------
Eval num_timesteps=3133491, episode_reward=-0.05 +/- 0.99
Episode length: 29.96 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.052      |
| time/                   |             |
|    total_timesteps      | 3133491     |
| train/                  |             |
|    approx_kl            | 0.020162333 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.306       |
|    learning_rate        | 8.72e-05    |
|    loss                 | 0.142       |
|    n_updates            | 4435        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 52       |
|    time_elapsed    | 19265    |
|    total_timesteps | 3194880  |
---------------------------------
Eval num_timesteps=3194932, episode_reward=0.04 +/- 0.98
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 3194932     |
| train/                  |             |
|    approx_kl            | 0.020505702 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.326       |
|    learning_rate        | 8.71e-05    |
|    loss                 | 0.091       |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 53       |
|    time_elapsed    | 19613    |
|    total_timesteps | 3256320  |
---------------------------------
Eval num_timesteps=3256373, episode_reward=0.01 +/- 0.98
Episode length: 30.04 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 3256373     |
| train/                  |             |
|    approx_kl            | 0.019883566 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.315       |
|    learning_rate        | 8.71e-05    |
|    loss                 | 0.0925      |
|    n_updates            | 4445        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 54       |
|    time_elapsed    | 19953    |
|    total_timesteps | 3317760  |
---------------------------------
Eval num_timesteps=3317814, episode_reward=-0.00 +/- 0.98
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 3317814     |
| train/                  |             |
|    approx_kl            | 0.019796094 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.7e-05     |
|    loss                 | 0.0479      |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 55       |
|    time_elapsed    | 20303    |
|    total_timesteps | 3379200  |
---------------------------------
Eval num_timesteps=3379255, episode_reward=0.01 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 3379255     |
| train/                  |             |
|    approx_kl            | 0.019702436 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.308       |
|    learning_rate        | 8.7e-05     |
|    loss                 | 0.118       |
|    n_updates            | 4455        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 56       |
|    time_elapsed    | 20653    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3440696, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 3440696     |
| train/                  |             |
|    approx_kl            | 0.019617235 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.69e-05    |
|    loss                 | 0.0787      |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 57       |
|    time_elapsed    | 20997    |
|    total_timesteps | 3502080  |
---------------------------------
Eval num_timesteps=3502137, episode_reward=-0.04 +/- 0.98
Episode length: 29.97 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.036     |
| time/                   |            |
|    total_timesteps      | 3502137    |
| train/                  |            |
|    approx_kl            | 0.01998671 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.747     |
|    explained_variance   | 0.321      |
|    learning_rate        | 8.68e-05   |
|    loss                 | 0.106      |
|    n_updates            | 4465       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 58       |
|    time_elapsed    | 21341    |
|    total_timesteps | 3563520  |
---------------------------------
Eval num_timesteps=3563578, episode_reward=-0.04 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.038      |
| time/                   |             |
|    total_timesteps      | 3563578     |
| train/                  |             |
|    approx_kl            | 0.020668339 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.68e-05    |
|    loss                 | 0.0555      |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 59       |
|    time_elapsed    | 21682    |
|    total_timesteps | 3624960  |
---------------------------------
Eval num_timesteps=3625019, episode_reward=0.16 +/- 0.97
Episode length: 30.00 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.164     |
| time/                   |           |
|    total_timesteps      | 3625019   |
| train/                  |           |
|    approx_kl            | 0.0198542 |
|    clip_fraction        | 0.214     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.747    |
|    explained_variance   | 0.292     |
|    learning_rate        | 8.67e-05  |
|    loss                 | 0.104     |
|    n_updates            | 4475      |
|    policy_gradient_loss | -0.0172   |
|    value_loss           | 0.245     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.164
SELFPLAY: new best model, bumping up generation to 3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 60       |
|    time_elapsed    | 22026    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3686460, episode_reward=-0.04 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 3686460     |
| train/                  |             |
|    approx_kl            | 0.020452352 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.305       |
|    learning_rate        | 8.67e-05    |
|    loss                 | 0.105       |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 61       |
|    time_elapsed    | 22367    |
|    total_timesteps | 3747840  |
---------------------------------
Eval num_timesteps=3747901, episode_reward=-0.04 +/- 0.99
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 3747901     |
| train/                  |             |
|    approx_kl            | 0.020168344 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.66e-05    |
|    loss                 | 0.0503      |
|    n_updates            | 4485        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 62       |
|    time_elapsed    | 22711    |
|    total_timesteps | 3809280  |
---------------------------------
Eval num_timesteps=3809342, episode_reward=-0.03 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 3809342     |
| train/                  |             |
|    approx_kl            | 0.020055562 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.753      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.66e-05    |
|    loss                 | 0.0832      |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 63       |
|    time_elapsed    | 23052    |
|    total_timesteps | 3870720  |
---------------------------------
Eval num_timesteps=3870783, episode_reward=0.07 +/- 0.97
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 3870783     |
| train/                  |             |
|    approx_kl            | 0.020008603 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.336       |
|    learning_rate        | 8.65e-05    |
|    loss                 | 0.0915      |
|    n_updates            | 4495        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 64       |
|    time_elapsed    | 23396    |
|    total_timesteps | 3932160  |
---------------------------------
Eval num_timesteps=3932224, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 3932224     |
| train/                  |             |
|    approx_kl            | 0.019599298 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.65e-05    |
|    loss                 | 0.0532      |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 65       |
|    time_elapsed    | 23741    |
|    total_timesteps | 3993600  |
---------------------------------
Eval num_timesteps=3993665, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 3993665     |
| train/                  |             |
|    approx_kl            | 0.019597221 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.64e-05    |
|    loss                 | 0.0528      |
|    n_updates            | 4505        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 66       |
|    time_elapsed    | 24093    |
|    total_timesteps | 4055040  |
---------------------------------
Eval num_timesteps=4055106, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 4055106     |
| train/                  |             |
|    approx_kl            | 0.019592626 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.64e-05    |
|    loss                 | 0.0585      |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 67       |
|    time_elapsed    | 24438    |
|    total_timesteps | 4116480  |
---------------------------------
Eval num_timesteps=4116547, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 4116547     |
| train/                  |             |
|    approx_kl            | 0.019324092 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.29        |
|    learning_rate        | 8.63e-05    |
|    loss                 | 0.0908      |
|    n_updates            | 4515        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 68       |
|    time_elapsed    | 24784    |
|    total_timesteps | 4177920  |
---------------------------------
Eval num_timesteps=4177988, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 4177988     |
| train/                  |             |
|    approx_kl            | 0.019241739 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.62e-05    |
|    loss                 | 0.102       |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 69       |
|    time_elapsed    | 25126    |
|    total_timesteps | 4239360  |
---------------------------------
Eval num_timesteps=4239429, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 4239429    |
| train/                  |            |
|    approx_kl            | 0.01922259 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.748     |
|    explained_variance   | 0.312      |
|    learning_rate        | 8.62e-05   |
|    loss                 | 0.0544     |
|    n_updates            | 4525       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 70       |
|    time_elapsed    | 25470    |
|    total_timesteps | 4300800  |
---------------------------------
Eval num_timesteps=4300870, episode_reward=-0.03 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 4300870     |
| train/                  |             |
|    approx_kl            | 0.018953362 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.746      |
|    explained_variance   | 0.317       |
|    learning_rate        | 8.61e-05    |
|    loss                 | 0.104       |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 71       |
|    time_elapsed    | 25811    |
|    total_timesteps | 4362240  |
---------------------------------
Eval num_timesteps=4362311, episode_reward=-0.07 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.074      |
| time/                   |             |
|    total_timesteps      | 4362311     |
| train/                  |             |
|    approx_kl            | 0.019844143 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.321       |
|    learning_rate        | 8.61e-05    |
|    loss                 | 0.0637      |
|    n_updates            | 4535        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 72       |
|    time_elapsed    | 26155    |
|    total_timesteps | 4423680  |
---------------------------------
Eval num_timesteps=4423752, episode_reward=0.06 +/- 0.97
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 4423752     |
| train/                  |             |
|    approx_kl            | 0.019322308 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.314       |
|    learning_rate        | 8.6e-05     |
|    loss                 | 0.059       |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 73       |
|    time_elapsed    | 26497    |
|    total_timesteps | 4485120  |
---------------------------------
Eval num_timesteps=4485193, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 0.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 4485193     |
| train/                  |             |
|    approx_kl            | 0.019290026 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.6e-05     |
|    loss                 | 0.0708      |
|    n_updates            | 4545        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 74       |
|    time_elapsed    | 26828    |
|    total_timesteps | 4546560  |
---------------------------------
Eval num_timesteps=4546634, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 4546634     |
| train/                  |             |
|    approx_kl            | 0.019619914 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.59e-05    |
|    loss                 | 0.118       |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 75       |
|    time_elapsed    | 27146    |
|    total_timesteps | 4608000  |
---------------------------------
Eval num_timesteps=4608075, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 4608075     |
| train/                  |             |
|    approx_kl            | 0.019235427 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.59e-05    |
|    loss                 | 0.0683      |
|    n_updates            | 4555        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 76       |
|    time_elapsed    | 27466    |
|    total_timesteps | 4669440  |
---------------------------------
Eval num_timesteps=4669516, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 4669516     |
| train/                  |             |
|    approx_kl            | 0.019372748 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.58e-05    |
|    loss                 | 0.064       |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 77       |
|    time_elapsed    | 27787    |
|    total_timesteps | 4730880  |
---------------------------------
Eval num_timesteps=4730957, episode_reward=0.19 +/- 0.97
Episode length: 30.06 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.194       |
| time/                   |             |
|    total_timesteps      | 4730957     |
| train/                  |             |
|    approx_kl            | 0.019504452 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.57e-05    |
|    loss                 | 0.0755      |
|    n_updates            | 4565        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.194
SELFPLAY: new best model, bumping up generation to 4
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 78       |
|    time_elapsed    | 28109    |
|    total_timesteps | 4792320  |
---------------------------------
Eval num_timesteps=4792398, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 4792398     |
| train/                  |             |
|    approx_kl            | 0.019629804 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.57e-05    |
|    loss                 | 0.0375      |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 79       |
|    time_elapsed    | 28431    |
|    total_timesteps | 4853760  |
---------------------------------
Eval num_timesteps=4853839, episode_reward=-0.02 +/- 0.99
Episode length: 29.99 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.024     |
| time/                   |            |
|    total_timesteps      | 4853839    |
| train/                  |            |
|    approx_kl            | 0.01936958 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.745     |
|    explained_variance   | 0.315      |
|    learning_rate        | 8.56e-05   |
|    loss                 | 0.0783     |
|    n_updates            | 4575       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 80       |
|    time_elapsed    | 28751    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4915280, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 4915280     |
| train/                  |             |
|    approx_kl            | 0.019639881 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.33        |
|    learning_rate        | 8.56e-05    |
|    loss                 | 0.0633      |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 81       |
|    time_elapsed    | 29068    |
|    total_timesteps | 4976640  |
---------------------------------
Eval num_timesteps=4976721, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 4976721     |
| train/                  |             |
|    approx_kl            | 0.019401932 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.746      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.55e-05    |
|    loss                 | 0.0704      |
|    n_updates            | 4585        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 82       |
|    time_elapsed    | 29386    |
|    total_timesteps | 5038080  |
---------------------------------
Eval num_timesteps=5038162, episode_reward=0.01 +/- 0.99
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 5038162     |
| train/                  |             |
|    approx_kl            | 0.019606046 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.314       |
|    learning_rate        | 8.55e-05    |
|    loss                 | 0.103       |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 83       |
|    time_elapsed    | 29703    |
|    total_timesteps | 5099520  |
---------------------------------
Eval num_timesteps=5099603, episode_reward=0.01 +/- 0.98
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 5099603     |
| train/                  |             |
|    approx_kl            | 0.019176692 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.342       |
|    learning_rate        | 8.54e-05    |
|    loss                 | 0.0835      |
|    n_updates            | 4595        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 84       |
|    time_elapsed    | 30021    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5161044, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 5161044    |
| train/                  |            |
|    approx_kl            | 0.01919082 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.306      |
|    learning_rate        | 8.54e-05   |
|    loss                 | 0.0723     |
|    n_updates            | 4600       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 85       |
|    time_elapsed    | 30338    |
|    total_timesteps | 5222400  |
---------------------------------
Eval num_timesteps=5222485, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 5222485     |
| train/                  |             |
|    approx_kl            | 0.018697377 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.296       |
|    learning_rate        | 8.53e-05    |
|    loss                 | 0.077       |
|    n_updates            | 4605        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 86       |
|    time_elapsed    | 30655    |
|    total_timesteps | 5283840  |
---------------------------------
Eval num_timesteps=5283926, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.022      |
| time/                   |            |
|    total_timesteps      | 5283926    |
| train/                  |            |
|    approx_kl            | 0.01973129 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.325      |
|    learning_rate        | 8.52e-05   |
|    loss                 | 0.0454     |
|    n_updates            | 4610       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 87       |
|    time_elapsed    | 30974    |
|    total_timesteps | 5345280  |
---------------------------------
Eval num_timesteps=5345367, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 5345367     |
| train/                  |             |
|    approx_kl            | 0.019317053 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.52e-05    |
|    loss                 | 0.105       |
|    n_updates            | 4615        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 88       |
|    time_elapsed    | 31294    |
|    total_timesteps | 5406720  |
---------------------------------
Eval num_timesteps=5406808, episode_reward=0.02 +/- 0.98
Episode length: 29.88 +/- 1.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 5406808     |
| train/                  |             |
|    approx_kl            | 0.019018346 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.51e-05    |
|    loss                 | 0.0603      |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 89       |
|    time_elapsed    | 31615    |
|    total_timesteps | 5468160  |
---------------------------------
Eval num_timesteps=5468249, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.01      |
| time/                   |           |
|    total_timesteps      | 5468249   |
| train/                  |           |
|    approx_kl            | 0.0195026 |
|    clip_fraction        | 0.207     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.74     |
|    explained_variance   | 0.31      |
|    learning_rate        | 8.51e-05  |
|    loss                 | 0.111     |
|    n_updates            | 4625      |
|    policy_gradient_loss | -0.0175   |
|    value_loss           | 0.237     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 90       |
|    time_elapsed    | 31937    |
|    total_timesteps | 5529600  |
---------------------------------
Eval num_timesteps=5529690, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 5529690     |
| train/                  |             |
|    approx_kl            | 0.018892242 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.742      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.112       |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 91       |
|    time_elapsed    | 32258    |
|    total_timesteps | 5591040  |
---------------------------------
Eval num_timesteps=5591131, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 5591131    |
| train/                  |            |
|    approx_kl            | 0.01923114 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.326      |
|    learning_rate        | 8.5e-05    |
|    loss                 | 0.0673     |
|    n_updates            | 4635       |
|    policy_gradient_loss | -0.018     |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 92       |
|    time_elapsed    | 32576    |
|    total_timesteps | 5652480  |
---------------------------------
Eval num_timesteps=5652572, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 5652572     |
| train/                  |             |
|    approx_kl            | 0.018542886 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.49e-05    |
|    loss                 | 0.0706      |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 93       |
|    time_elapsed    | 32893    |
|    total_timesteps | 5713920  |
---------------------------------
Eval num_timesteps=5714013, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.008     |
| time/                   |            |
|    total_timesteps      | 5714013    |
| train/                  |            |
|    approx_kl            | 0.01917092 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.314      |
|    learning_rate        | 8.49e-05   |
|    loss                 | 0.0835     |
|    n_updates            | 4645       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 94       |
|    time_elapsed    | 33210    |
|    total_timesteps | 5775360  |
---------------------------------
Eval num_timesteps=5775454, episode_reward=-0.01 +/- 0.98
Episode length: 29.93 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 5775454     |
| train/                  |             |
|    approx_kl            | 0.018662462 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.48e-05    |
|    loss                 | 0.067       |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 95       |
|    time_elapsed    | 33527    |
|    total_timesteps | 5836800  |
---------------------------------
Eval num_timesteps=5836895, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 5836895     |
| train/                  |             |
|    approx_kl            | 0.018683437 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.732      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.47e-05    |
|    loss                 | 0.0824      |
|    n_updates            | 4655        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 96       |
|    time_elapsed    | 33844    |
|    total_timesteps | 5898240  |
---------------------------------
Eval num_timesteps=5898336, episode_reward=-0.09 +/- 0.97
Episode length: 29.89 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.088      |
| time/                   |             |
|    total_timesteps      | 5898336     |
| train/                  |             |
|    approx_kl            | 0.018733233 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.317       |
|    learning_rate        | 8.47e-05    |
|    loss                 | 0.0799      |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 97       |
|    time_elapsed    | 34161    |
|    total_timesteps | 5959680  |
---------------------------------
Eval num_timesteps=5959777, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 5959777     |
| train/                  |             |
|    approx_kl            | 0.018951198 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.736      |
|    explained_variance   | 0.308       |
|    learning_rate        | 8.46e-05    |
|    loss                 | 0.0496      |
|    n_updates            | 4665        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 98       |
|    time_elapsed    | 34478    |
|    total_timesteps | 6021120  |
---------------------------------
Eval num_timesteps=6021218, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 6021218     |
| train/                  |             |
|    approx_kl            | 0.018309517 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.298       |
|    learning_rate        | 8.46e-05    |
|    loss                 | 0.105       |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 99       |
|    time_elapsed    | 34797    |
|    total_timesteps | 6082560  |
---------------------------------
Eval num_timesteps=6082659, episode_reward=0.06 +/- 0.98
Episode length: 30.05 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 6082659     |
| train/                  |             |
|    approx_kl            | 0.018623741 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.731      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.45e-05    |
|    loss                 | 0.0756      |
|    n_updates            | 4675        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 100      |
|    time_elapsed    | 35117    |
|    total_timesteps | 6144000  |
---------------------------------
Eval num_timesteps=6144100, episode_reward=0.04 +/- 0.98
Episode length: 29.95 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 6144100     |
| train/                  |             |
|    approx_kl            | 0.019260736 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.731      |
|    explained_variance   | 0.324       |
|    learning_rate        | 8.45e-05    |
|    loss                 | 0.0838      |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 101      |
|    time_elapsed    | 35439    |
|    total_timesteps | 6205440  |
---------------------------------
Eval num_timesteps=6205541, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 6205541     |
| train/                  |             |
|    approx_kl            | 0.019017534 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.0628      |
|    n_updates            | 4685        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 102      |
|    time_elapsed    | 35760    |
|    total_timesteps | 6266880  |
---------------------------------
Eval num_timesteps=6266982, episode_reward=-0.05 +/- 0.98
Episode length: 29.97 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.046     |
| time/                   |            |
|    total_timesteps      | 6266982    |
| train/                  |            |
|    approx_kl            | 0.01847287 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.31       |
|    learning_rate        | 8.44e-05   |
|    loss                 | 0.0755     |
|    n_updates            | 4690       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 103      |
|    time_elapsed    | 36080    |
|    total_timesteps | 6328320  |
---------------------------------
Eval num_timesteps=6328423, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 6328423     |
| train/                  |             |
|    approx_kl            | 0.018578133 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.43e-05    |
|    loss                 | 0.0838      |
|    n_updates            | 4695        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 104      |
|    time_elapsed    | 36397    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6389864, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 6389864     |
| train/                  |             |
|    approx_kl            | 0.018597163 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.42e-05    |
|    loss                 | 0.0717      |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 105      |
|    time_elapsed    | 36714    |
|    total_timesteps | 6451200  |
---------------------------------
Eval num_timesteps=6451305, episode_reward=-0.01 +/- 0.98
Episode length: 29.95 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 6451305     |
| train/                  |             |
|    approx_kl            | 0.018056631 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.324       |
|    learning_rate        | 8.42e-05    |
|    loss                 | 0.0661      |
|    n_updates            | 4705        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 106      |
|    time_elapsed    | 37031    |
|    total_timesteps | 6512640  |
---------------------------------
Eval num_timesteps=6512746, episode_reward=0.11 +/- 0.98
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 6512746     |
| train/                  |             |
|    approx_kl            | 0.018626135 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.41e-05    |
|    loss                 | 0.0606      |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 107      |
|    time_elapsed    | 37349    |
|    total_timesteps | 6574080  |
---------------------------------
Eval num_timesteps=6574187, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 6574187     |
| train/                  |             |
|    approx_kl            | 0.017904732 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.41e-05    |
|    loss                 | 0.0659      |
|    n_updates            | 4715        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 108      |
|    time_elapsed    | 37665    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6635628, episode_reward=0.01 +/- 0.98
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 6635628     |
| train/                  |             |
|    approx_kl            | 0.018181821 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.4e-05     |
|    loss                 | 0.0396      |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 109      |
|    time_elapsed    | 37983    |
|    total_timesteps | 6696960  |
---------------------------------
Eval num_timesteps=6697069, episode_reward=-0.02 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 6697069     |
| train/                  |             |
|    approx_kl            | 0.018397411 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.4e-05     |
|    loss                 | 0.081       |
|    n_updates            | 4725        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 110      |
|    time_elapsed    | 38301    |
|    total_timesteps | 6758400  |
---------------------------------
Eval num_timesteps=6758510, episode_reward=0.06 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 6758510     |
| train/                  |             |
|    approx_kl            | 0.018284826 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.39e-05    |
|    loss                 | 0.122       |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 111      |
|    time_elapsed    | 38621    |
|    total_timesteps | 6819840  |
---------------------------------
Eval num_timesteps=6819951, episode_reward=0.02 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 6819951     |
| train/                  |             |
|    approx_kl            | 0.018171556 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.331       |
|    learning_rate        | 8.39e-05    |
|    loss                 | 0.0627      |
|    n_updates            | 4735        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 112      |
|    time_elapsed    | 38941    |
|    total_timesteps | 6881280  |
---------------------------------
Eval num_timesteps=6881392, episode_reward=0.03 +/- 0.98
Episode length: 30.05 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 6881392     |
| train/                  |             |
|    approx_kl            | 0.018549003 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.38e-05    |
|    loss                 | 0.0947      |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 176      |
|    iterations      | 113      |
|    time_elapsed    | 39263    |
|    total_timesteps | 6942720  |
---------------------------------
Eval num_timesteps=6942833, episode_reward=0.09 +/- 0.97
Episode length: 30.05 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 6942833     |
| train/                  |             |
|    approx_kl            | 0.018627478 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.303       |
|    learning_rate        | 8.38e-05    |
|    loss                 | 0.0876      |
|    n_updates            | 4745        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 114      |
|    time_elapsed    | 39585    |
|    total_timesteps | 7004160  |
---------------------------------
Eval num_timesteps=7004274, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 7004274    |
| train/                  |            |
|    approx_kl            | 0.01830752 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.705     |
|    explained_variance   | 0.314      |
|    learning_rate        | 8.37e-05   |
|    loss                 | 0.0564     |
|    n_updates            | 4750       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 115      |
|    time_elapsed    | 39904    |
|    total_timesteps | 7065600  |
---------------------------------
Eval num_timesteps=7065715, episode_reward=0.08 +/- 0.98
Episode length: 29.94 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 7065715     |
| train/                  |             |
|    approx_kl            | 0.018104954 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.329       |
|    learning_rate        | 8.36e-05    |
|    loss                 | 0.0593      |
|    n_updates            | 4755        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 116      |
|    time_elapsed    | 40222    |
|    total_timesteps | 7127040  |
---------------------------------
Eval num_timesteps=7127156, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.078      |
| time/                   |            |
|    total_timesteps      | 7127156    |
| train/                  |            |
|    approx_kl            | 0.01782045 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.324      |
|    learning_rate        | 8.36e-05   |
|    loss                 | 0.0681     |
|    n_updates            | 4760       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 117      |
|    time_elapsed    | 40539    |
|    total_timesteps | 7188480  |
---------------------------------
Eval num_timesteps=7188597, episode_reward=0.21 +/- 0.96
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.21        |
| time/                   |             |
|    total_timesteps      | 7188597     |
| train/                  |             |
|    approx_kl            | 0.018219313 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.327       |
|    learning_rate        | 8.35e-05    |
|    loss                 | 0.0804      |
|    n_updates            | 4765        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.21
SELFPLAY: new best model, bumping up generation to 5
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 118      |
|    time_elapsed    | 40857    |
|    total_timesteps | 7249920  |
---------------------------------
Eval num_timesteps=7250038, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 7250038     |
| train/                  |             |
|    approx_kl            | 0.018564073 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.35e-05    |
|    loss                 | 0.0924      |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 119      |
|    time_elapsed    | 41175    |
|    total_timesteps | 7311360  |
---------------------------------
Eval num_timesteps=7311479, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 7311479     |
| train/                  |             |
|    approx_kl            | 0.017995143 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.335       |
|    learning_rate        | 8.34e-05    |
|    loss                 | 0.0473      |
|    n_updates            | 4775        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 120      |
|    time_elapsed    | 41492    |
|    total_timesteps | 7372800  |
---------------------------------
Eval num_timesteps=7372920, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 7372920     |
| train/                  |             |
|    approx_kl            | 0.017546065 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.329       |
|    learning_rate        | 8.34e-05    |
|    loss                 | 0.0762      |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 121      |
|    time_elapsed    | 41809    |
|    total_timesteps | 7434240  |
---------------------------------
Eval num_timesteps=7434361, episode_reward=-0.02 +/- 0.98
Episode length: 29.96 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 7434361    |
| train/                  |            |
|    approx_kl            | 0.01779716 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.709     |
|    explained_variance   | 0.326      |
|    learning_rate        | 8.33e-05   |
|    loss                 | 0.0923     |
|    n_updates            | 4785       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 122      |
|    time_elapsed    | 42128    |
|    total_timesteps | 7495680  |
---------------------------------
Eval num_timesteps=7495802, episode_reward=0.00 +/- 0.99
Episode length: 29.95 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.002      |
| time/                   |            |
|    total_timesteps      | 7495802    |
| train/                  |            |
|    approx_kl            | 0.01793491 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.32       |
|    learning_rate        | 8.33e-05   |
|    loss                 | 0.0442     |
|    n_updates            | 4790       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 123      |
|    time_elapsed    | 42449    |
|    total_timesteps | 7557120  |
---------------------------------
Eval num_timesteps=7557243, episode_reward=-0.02 +/- 0.99
Episode length: 29.95 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 7557243     |
| train/                  |             |
|    approx_kl            | 0.017894156 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.303       |
|    learning_rate        | 8.32e-05    |
|    loss                 | 0.0715      |
|    n_updates            | 4795        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 124      |
|    time_elapsed    | 42770    |
|    total_timesteps | 7618560  |
---------------------------------
Eval num_timesteps=7618684, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 7618684     |
| train/                  |             |
|    approx_kl            | 0.017480906 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.31e-05    |
|    loss                 | 0.0695      |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 125      |
|    time_elapsed    | 43094    |
|    total_timesteps | 7680000  |
---------------------------------
Eval num_timesteps=7680125, episode_reward=0.05 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 7680125     |
| train/                  |             |
|    approx_kl            | 0.017738339 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.709      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.31e-05    |
|    loss                 | 0.0983      |
|    n_updates            | 4805        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 126      |
|    time_elapsed    | 43414    |
|    total_timesteps | 7741440  |
---------------------------------
Eval num_timesteps=7741566, episode_reward=0.01 +/- 0.98
Episode length: 30.03 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 7741566     |
| train/                  |             |
|    approx_kl            | 0.018013712 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.318       |
|    learning_rate        | 8.3e-05     |
|    loss                 | 0.061       |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 127      |
|    time_elapsed    | 43732    |
|    total_timesteps | 7802880  |
---------------------------------
Eval num_timesteps=7803007, episode_reward=0.03 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 7803007     |
| train/                  |             |
|    approx_kl            | 0.018030994 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.303       |
|    learning_rate        | 8.3e-05     |
|    loss                 | 0.0809      |
|    n_updates            | 4815        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 128      |
|    time_elapsed    | 44050    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7864448, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 7864448     |
| train/                  |             |
|    approx_kl            | 0.018215578 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.29e-05    |
|    loss                 | 0.0769      |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 129      |
|    time_elapsed    | 44366    |
|    total_timesteps | 7925760  |
---------------------------------
Eval num_timesteps=7925889, episode_reward=-0.01 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 7925889     |
| train/                  |             |
|    approx_kl            | 0.017565824 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.29e-05    |
|    loss                 | 0.0817      |
|    n_updates            | 4825        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 130      |
|    time_elapsed    | 44684    |
|    total_timesteps | 7987200  |
---------------------------------
Eval num_timesteps=7987330, episode_reward=0.08 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 7987330    |
| train/                  |            |
|    approx_kl            | 0.01748898 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.308      |
|    learning_rate        | 8.28e-05   |
|    loss                 | 0.0822     |
|    n_updates            | 4830       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 131      |
|    time_elapsed    | 45001    |
|    total_timesteps | 8048640  |
---------------------------------
Eval num_timesteps=8048771, episode_reward=0.08 +/- 0.98
Episode length: 30.05 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 8048771    |
| train/                  |            |
|    approx_kl            | 0.01795539 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.321      |
|    learning_rate        | 8.28e-05   |
|    loss                 | 0.104      |
|    n_updates            | 4835       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 132      |
|    time_elapsed    | 45319    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8110212, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 8110212     |
| train/                  |             |
|    approx_kl            | 0.017706493 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.27e-05    |
|    loss                 | 0.0967      |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 133      |
|    time_elapsed    | 45636    |
|    total_timesteps | 8171520  |
---------------------------------
Eval num_timesteps=8171653, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 8171653     |
| train/                  |             |
|    approx_kl            | 0.017217766 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.31        |
|    learning_rate        | 8.26e-05    |
|    loss                 | 0.0883      |
|    n_updates            | 4845        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 134      |
|    time_elapsed    | 45956    |
|    total_timesteps | 8232960  |
---------------------------------
Eval num_timesteps=8233094, episode_reward=0.10 +/- 0.98
Episode length: 30.07 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 8233094     |
| train/                  |             |
|    approx_kl            | 0.017139263 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.26e-05    |
|    loss                 | 0.0949      |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 135      |
|    time_elapsed    | 46276    |
|    total_timesteps | 8294400  |
---------------------------------
Eval num_timesteps=8294535, episode_reward=0.04 +/- 0.98
Episode length: 29.92 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 8294535     |
| train/                  |             |
|    approx_kl            | 0.018485116 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.25e-05    |
|    loss                 | 0.048       |
|    n_updates            | 4855        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 136      |
|    time_elapsed    | 46597    |
|    total_timesteps | 8355840  |
---------------------------------
Eval num_timesteps=8355976, episode_reward=0.02 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 8355976     |
| train/                  |             |
|    approx_kl            | 0.017295027 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.25e-05    |
|    loss                 | 0.0831      |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 137      |
|    time_elapsed    | 46919    |
|    total_timesteps | 8417280  |
---------------------------------
Eval num_timesteps=8417417, episode_reward=0.07 +/- 0.99
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 8417417     |
| train/                  |             |
|    approx_kl            | 0.017685981 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.315       |
|    learning_rate        | 8.24e-05    |
|    loss                 | 0.128       |
|    n_updates            | 4865        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 138      |
|    time_elapsed    | 47239    |
|    total_timesteps | 8478720  |
---------------------------------
Eval num_timesteps=8478858, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 8478858     |
| train/                  |             |
|    approx_kl            | 0.017661806 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.24e-05    |
|    loss                 | 0.0549      |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 139      |
|    time_elapsed    | 47557    |
|    total_timesteps | 8540160  |
---------------------------------
Eval num_timesteps=8540299, episode_reward=0.02 +/- 0.99
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 8540299     |
| train/                  |             |
|    approx_kl            | 0.017523713 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.324       |
|    learning_rate        | 8.23e-05    |
|    loss                 | 0.0531      |
|    n_updates            | 4875        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 140      |
|    time_elapsed    | 47874    |
|    total_timesteps | 8601600  |
---------------------------------
Eval num_timesteps=8601740, episode_reward=-0.01 +/- 0.99
Episode length: 29.99 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 8601740     |
| train/                  |             |
|    approx_kl            | 0.017866341 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.301       |
|    learning_rate        | 8.23e-05    |
|    loss                 | 0.0523      |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 141      |
|    time_elapsed    | 48191    |
|    total_timesteps | 8663040  |
---------------------------------
Eval num_timesteps=8663181, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 8663181     |
| train/                  |             |
|    approx_kl            | 0.017023286 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.703      |
|    explained_variance   | 0.329       |
|    learning_rate        | 8.22e-05    |
|    loss                 | 0.0953      |
|    n_updates            | 4885        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 142      |
|    time_elapsed    | 48508    |
|    total_timesteps | 8724480  |
---------------------------------
Eval num_timesteps=8724622, episode_reward=0.12 +/- 0.97
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 8724622     |
| train/                  |             |
|    approx_kl            | 0.017582063 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.21e-05    |
|    loss                 | 0.0812      |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 143      |
|    time_elapsed    | 48825    |
|    total_timesteps | 8785920  |
---------------------------------
Eval num_timesteps=8786063, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 8786063     |
| train/                  |             |
|    approx_kl            | 0.016488828 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.339       |
|    learning_rate        | 8.21e-05    |
|    loss                 | 0.072       |
|    n_updates            | 4895        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.234       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.142
SELFPLAY: new best model, bumping up generation to 6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 144      |
|    time_elapsed    | 49142    |
|    total_timesteps | 8847360  |
---------------------------------
Eval num_timesteps=8847504, episode_reward=-0.03 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 8847504     |
| train/                  |             |
|    approx_kl            | 0.017101271 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.334       |
|    learning_rate        | 8.2e-05     |
|    loss                 | 0.104       |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 145      |
|    time_elapsed    | 49461    |
|    total_timesteps | 8908800  |
---------------------------------
Eval num_timesteps=8908945, episode_reward=-0.06 +/- 0.98
Episode length: 29.93 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.064      |
| time/                   |             |
|    total_timesteps      | 8908945     |
| train/                  |             |
|    approx_kl            | 0.017491838 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.2e-05     |
|    loss                 | 0.0631      |
|    n_updates            | 4905        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.19    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 146      |
|    time_elapsed    | 49781    |
|    total_timesteps | 8970240  |
---------------------------------
Eval num_timesteps=8970386, episode_reward=-0.01 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 8970386     |
| train/                  |             |
|    approx_kl            | 0.018288044 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.19e-05    |
|    loss                 | 0.124       |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 147      |
|    time_elapsed    | 50102    |
|    total_timesteps | 9031680  |
---------------------------------
Eval num_timesteps=9031827, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 9031827     |
| train/                  |             |
|    approx_kl            | 0.017775487 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.19e-05    |
|    loss                 | 0.0654      |
|    n_updates            | 4915        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 148      |
|    time_elapsed    | 50424    |
|    total_timesteps | 9093120  |
---------------------------------
Eval num_timesteps=9093268, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 9093268    |
| train/                  |            |
|    approx_kl            | 0.01771807 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.314      |
|    learning_rate        | 8.18e-05   |
|    loss                 | 0.0892     |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 180      |
|    iterations      | 149      |
|    time_elapsed    | 50745    |
|    total_timesteps | 9154560  |
---------------------------------
Eval num_timesteps=9154709, episode_reward=-0.03 +/- 0.99
Episode length: 29.95 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.028     |
| time/                   |            |
|    total_timesteps      | 9154709    |
| train/                  |            |
|    approx_kl            | 0.01750758 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.328      |
|    learning_rate        | 8.18e-05   |
|    loss                 | 0.0985     |
|    n_updates            | 4925       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 150      |
|    time_elapsed    | 51063    |
|    total_timesteps | 9216000  |
---------------------------------
Eval num_timesteps=9216150, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 9216150     |
| train/                  |             |
|    approx_kl            | 0.017444933 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.301       |
|    learning_rate        | 8.17e-05    |
|    loss                 | 0.108       |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 180      |
|    iterations      | 151      |
|    time_elapsed    | 51380    |
|    total_timesteps | 9277440  |
---------------------------------
Eval num_timesteps=9277591, episode_reward=-0.04 +/- 0.98
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.044     |
| time/                   |            |
|    total_timesteps      | 9277591    |
| train/                  |            |
|    approx_kl            | 0.01708657 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.316      |
|    learning_rate        | 8.17e-05   |
|    loss                 | 0.0699     |
|    n_updates            | 4935       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 152      |
|    time_elapsed    | 51698    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9339032, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 9339032     |
| train/                  |             |
|    approx_kl            | 0.017359884 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.16e-05    |
|    loss                 | 0.0779      |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 153      |
|    time_elapsed    | 52015    |
|    total_timesteps | 9400320  |
---------------------------------
Eval num_timesteps=9400473, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 9400473     |
| train/                  |             |
|    approx_kl            | 0.017730536 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.15e-05    |
|    loss                 | 0.105       |
|    n_updates            | 4945        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 154      |
|    time_elapsed    | 52332    |
|    total_timesteps | 9461760  |
---------------------------------
Eval num_timesteps=9461914, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 1.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 9461914     |
| train/                  |             |
|    approx_kl            | 0.017184565 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.15e-05    |
|    loss                 | 0.0964      |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 155      |
|    time_elapsed    | 52649    |
|    total_timesteps | 9523200  |
---------------------------------
Eval num_timesteps=9523355, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 9523355     |
| train/                  |             |
|    approx_kl            | 0.017334336 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.14e-05    |
|    loss                 | 0.0453      |
|    n_updates            | 4955        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 156      |
|    time_elapsed    | 52967    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9584796, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 9584796     |
| train/                  |             |
|    approx_kl            | 0.017365614 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.304       |
|    learning_rate        | 8.14e-05    |
|    loss                 | 0.0905      |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 157      |
|    time_elapsed    | 53286    |
|    total_timesteps | 9646080  |
---------------------------------
Eval num_timesteps=9646237, episode_reward=-0.00 +/- 0.98
Episode length: 29.96 +/- 1.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 9646237    |
| train/                  |            |
|    approx_kl            | 0.01742645 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.312      |
|    learning_rate        | 8.13e-05   |
|    loss                 | 0.0893     |
|    n_updates            | 4965       |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 158      |
|    time_elapsed    | 53606    |
|    total_timesteps | 9707520  |
---------------------------------
Eval num_timesteps=9707678, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 9707678    |
| train/                  |            |
|    approx_kl            | 0.01698091 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.319      |
|    learning_rate        | 8.13e-05   |
|    loss                 | 0.106      |
|    n_updates            | 4970       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 159      |
|    time_elapsed    | 53927    |
|    total_timesteps | 9768960  |
---------------------------------
Eval num_timesteps=9769119, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 9769119     |
| train/                  |             |
|    approx_kl            | 0.017332833 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.12e-05    |
|    loss                 | 0.0574      |
|    n_updates            | 4975        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 160      |
|    time_elapsed    | 54249    |
|    total_timesteps | 9830400  |
---------------------------------
Eval num_timesteps=9830560, episode_reward=0.03 +/- 0.98
Episode length: 29.95 +/- 1.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 9830560     |
| train/                  |             |
|    approx_kl            | 0.016994582 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.317       |
|    learning_rate        | 8.12e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 161      |
|    time_elapsed    | 54569    |
|    total_timesteps | 9891840  |
---------------------------------
Eval num_timesteps=9892001, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.026     |
| time/                   |           |
|    total_timesteps      | 9892001   |
| train/                  |           |
|    approx_kl            | 0.0173416 |
|    clip_fraction        | 0.191     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.722    |
|    explained_variance   | 0.305     |
|    learning_rate        | 8.11e-05  |
|    loss                 | 0.0746    |
|    n_updates            | 4985      |
|    policy_gradient_loss | -0.0174   |
|    value_loss           | 0.239     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 181      |
|    iterations      | 162      |
|    time_elapsed    | 54887    |
|    total_timesteps | 9953280  |
---------------------------------
Eval num_timesteps=9953442, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 9953442     |
| train/                  |             |
|    approx_kl            | 0.017290099 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.311       |
|    learning_rate        | 8.1e-05     |
|    loss                 | 0.0397      |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 163      |
|    time_elapsed    | 55204    |
|    total_timesteps | 10014720 |
---------------------------------
Eval num_timesteps=10014883, episode_reward=-0.07 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.068      |
| time/                   |             |
|    total_timesteps      | 10014883    |
| train/                  |             |
|    approx_kl            | 0.017015181 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.1e-05     |
|    loss                 | 0.0361      |
|    n_updates            | 4995        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 164      |
|    time_elapsed    | 55520    |
|    total_timesteps | 10076160 |
---------------------------------
Eval num_timesteps=10076324, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 10076324    |
| train/                  |             |
|    approx_kl            | 0.017514149 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.09e-05    |
|    loss                 | 0.0618      |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 165      |
|    time_elapsed    | 55837    |
|    total_timesteps | 10137600 |
---------------------------------
Eval num_timesteps=10137765, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 10137765    |
| train/                  |             |
|    approx_kl            | 0.016861923 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.315       |
|    learning_rate        | 8.09e-05    |
|    loss                 | 0.104       |
|    n_updates            | 5005        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 166      |
|    time_elapsed    | 56154    |
|    total_timesteps | 10199040 |
---------------------------------
Eval num_timesteps=10199206, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 10199206    |
| train/                  |             |
|    approx_kl            | 0.017236773 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.08e-05    |
|    loss                 | 0.104       |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 167      |
|    time_elapsed    | 56471    |
|    total_timesteps | 10260480 |
---------------------------------
Eval num_timesteps=10260647, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 10260647    |
| train/                  |             |
|    approx_kl            | 0.018084887 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.31        |
|    learning_rate        | 8.08e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 5015        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 168      |
|    time_elapsed    | 56789    |
|    total_timesteps | 10321920 |
---------------------------------
Eval num_timesteps=10322088, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 10322088    |
| train/                  |             |
|    approx_kl            | 0.017508456 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.304       |
|    learning_rate        | 8.07e-05    |
|    loss                 | 0.0777      |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 169      |
|    time_elapsed    | 57109    |
|    total_timesteps | 10383360 |
---------------------------------
Eval num_timesteps=10383529, episode_reward=-0.04 +/- 0.98
Episode length: 29.98 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 10383529    |
| train/                  |             |
|    approx_kl            | 0.017127255 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.308       |
|    learning_rate        | 8.07e-05    |
|    loss                 | 0.0691      |
|    n_updates            | 5025        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 170      |
|    time_elapsed    | 57429    |
|    total_timesteps | 10444800 |
---------------------------------
Eval num_timesteps=10444970, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 10444970    |
| train/                  |             |
|    approx_kl            | 0.017494457 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.06e-05    |
|    loss                 | 0.0549      |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 171      |
|    time_elapsed    | 57751    |
|    total_timesteps | 10506240 |
---------------------------------
Eval num_timesteps=10506411, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 10506411    |
| train/                  |             |
|    approx_kl            | 0.017111413 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.05e-05    |
|    loss                 | 0.106       |
|    n_updates            | 5035        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 172      |
|    time_elapsed    | 58072    |
|    total_timesteps | 10567680 |
---------------------------------
Eval num_timesteps=10567852, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 10567852    |
| train/                  |             |
|    approx_kl            | 0.017441908 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.305       |
|    learning_rate        | 8.05e-05    |
|    loss                 | 0.122       |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 173      |
|    time_elapsed    | 58392    |
|    total_timesteps | 10629120 |
---------------------------------
Eval num_timesteps=10629293, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 10629293    |
| train/                  |             |
|    approx_kl            | 0.017163651 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.31        |
|    learning_rate        | 8.04e-05    |
|    loss                 | 0.0907      |
|    n_updates            | 5045        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 174      |
|    time_elapsed    | 58708    |
|    total_timesteps | 10690560 |
---------------------------------
Eval num_timesteps=10690734, episode_reward=-0.00 +/- 0.98
Episode length: 29.94 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 10690734    |
| train/                  |             |
|    approx_kl            | 0.016990645 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.302       |
|    learning_rate        | 8.04e-05    |
|    loss                 | 0.08        |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 182      |
|    iterations      | 175      |
|    time_elapsed    | 59025    |
|    total_timesteps | 10752000 |
---------------------------------
Eval num_timesteps=10752175, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 10752175    |
| train/                  |             |
|    approx_kl            | 0.017308788 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.314       |
|    learning_rate        | 8.03e-05    |
|    loss                 | 0.081       |
|    n_updates            | 5055        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 176      |
|    time_elapsed    | 59342    |
|    total_timesteps | 10813440 |
---------------------------------
Eval num_timesteps=10813616, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 10813616    |
| train/                  |             |
|    approx_kl            | 0.017618103 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.299       |
|    learning_rate        | 8.03e-05    |
|    loss                 | 0.0646      |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 177      |
|    time_elapsed    | 59660    |
|    total_timesteps | 10874880 |
---------------------------------
Eval num_timesteps=10875057, episode_reward=0.02 +/- 0.98
Episode length: 29.97 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 10875057    |
| train/                  |             |
|    approx_kl            | 0.017357111 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.02e-05    |
|    loss                 | 0.0749      |
|    n_updates            | 5065        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 178      |
|    time_elapsed    | 59976    |
|    total_timesteps | 10936320 |
---------------------------------
Eval num_timesteps=10936498, episode_reward=0.05 +/- 0.99
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 10936498    |
| train/                  |             |
|    approx_kl            | 0.017567521 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.02e-05    |
|    loss                 | 0.0639      |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 179      |
|    time_elapsed    | 60294    |
|    total_timesteps | 10997760 |
---------------------------------
Eval num_timesteps=10997939, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 10997939    |
| train/                  |             |
|    approx_kl            | 0.016770227 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.311       |
|    learning_rate        | 8.01e-05    |
|    loss                 | 0.0992      |
|    n_updates            | 5075        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 180      |
|    time_elapsed    | 60612    |
|    total_timesteps | 11059200 |
---------------------------------
Eval num_timesteps=11059380, episode_reward=0.04 +/- 0.99
Episode length: 30.04 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 11059380    |
| train/                  |             |
|    approx_kl            | 0.017649146 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.088       |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 181      |
|    time_elapsed    | 60932    |
|    total_timesteps | 11120640 |
---------------------------------
Eval num_timesteps=11120821, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 11120821    |
| train/                  |             |
|    approx_kl            | 0.017215911 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0979      |
|    n_updates            | 5085        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 182      |
|    time_elapsed    | 61253    |
|    total_timesteps | 11182080 |
---------------------------------
Eval num_timesteps=11182262, episode_reward=-0.03 +/- 0.98
Episode length: 29.94 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 11182262    |
| train/                  |             |
|    approx_kl            | 0.017068116 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.99e-05    |
|    loss                 | 0.0658      |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 183      |
|    time_elapsed    | 61575    |
|    total_timesteps | 11243520 |
---------------------------------
Eval num_timesteps=11243703, episode_reward=0.02 +/- 0.99
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 11243703    |
| train/                  |             |
|    approx_kl            | 0.017016836 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.31        |
|    learning_rate        | 7.99e-05    |
|    loss                 | 0.0926      |
|    n_updates            | 5095        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 184      |
|    time_elapsed    | 61895    |
|    total_timesteps | 11304960 |
---------------------------------
Eval num_timesteps=11305144, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 11305144    |
| train/                  |             |
|    approx_kl            | 0.017353557 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.321       |
|    learning_rate        | 7.98e-05    |
|    loss                 | 0.0833      |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 185      |
|    time_elapsed    | 62213    |
|    total_timesteps | 11366400 |
---------------------------------
Eval num_timesteps=11366585, episode_reward=0.02 +/- 0.98
Episode length: 29.95 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 11366585    |
| train/                  |             |
|    approx_kl            | 0.016641563 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.326       |
|    learning_rate        | 7.98e-05    |
|    loss                 | 0.0417      |
|    n_updates            | 5105        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 186      |
|    time_elapsed    | 62530    |
|    total_timesteps | 11427840 |
---------------------------------
Eval num_timesteps=11428026, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 11428026    |
| train/                  |             |
|    approx_kl            | 0.016939878 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.321       |
|    learning_rate        | 7.97e-05    |
|    loss                 | 0.0879      |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 187      |
|    time_elapsed    | 62847    |
|    total_timesteps | 11489280 |
---------------------------------
Eval num_timesteps=11489467, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 11489467    |
| train/                  |             |
|    approx_kl            | 0.016703213 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.306       |
|    learning_rate        | 7.97e-05    |
|    loss                 | 0.0759      |
|    n_updates            | 5115        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 188      |
|    time_elapsed    | 63165    |
|    total_timesteps | 11550720 |
---------------------------------
Eval num_timesteps=11550908, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 11550908    |
| train/                  |             |
|    approx_kl            | 0.016877966 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.96e-05    |
|    loss                 | 0.0908      |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 189      |
|    time_elapsed    | 63481    |
|    total_timesteps | 11612160 |
---------------------------------
Eval num_timesteps=11612349, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 11612349   |
| train/                  |            |
|    approx_kl            | 0.01702377 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.315      |
|    learning_rate        | 7.95e-05   |
|    loss                 | 0.0305     |
|    n_updates            | 5125       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 190      |
|    time_elapsed    | 63803    |
|    total_timesteps | 11673600 |
---------------------------------
Eval num_timesteps=11673790, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 11673790    |
| train/                  |             |
|    approx_kl            | 0.016949024 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.95e-05    |
|    loss                 | 0.0703      |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 191      |
|    time_elapsed    | 64128    |
|    total_timesteps | 11735040 |
---------------------------------
Eval num_timesteps=11735231, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 11735231    |
| train/                  |             |
|    approx_kl            | 0.017374424 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.317       |
|    learning_rate        | 7.94e-05    |
|    loss                 | 0.114       |
|    n_updates            | 5135        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 192      |
|    time_elapsed    | 64473    |
|    total_timesteps | 11796480 |
---------------------------------
Eval num_timesteps=11796672, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 11796672    |
| train/                  |             |
|    approx_kl            | 0.016969329 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.94e-05    |
|    loss                 | 0.0505      |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 193      |
|    time_elapsed    | 64811    |
|    total_timesteps | 11857920 |
---------------------------------
Eval num_timesteps=11858113, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 11858113   |
| train/                  |            |
|    approx_kl            | 0.01660774 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.717     |
|    explained_variance   | 0.299      |
|    learning_rate        | 7.93e-05   |
|    loss                 | 0.0654     |
|    n_updates            | 5145       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 194      |
|    time_elapsed    | 65148    |
|    total_timesteps | 11919360 |
---------------------------------
Eval num_timesteps=11919554, episode_reward=0.09 +/- 0.99
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 11919554    |
| train/                  |             |
|    approx_kl            | 0.016907278 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.326       |
|    learning_rate        | 7.93e-05    |
|    loss                 | 0.0781      |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 195      |
|    time_elapsed    | 65485    |
|    total_timesteps | 11980800 |
---------------------------------
Eval num_timesteps=11980995, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 11980995    |
| train/                  |             |
|    approx_kl            | 0.017232196 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.92e-05    |
|    loss                 | 0.0556      |
|    n_updates            | 5155        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.32     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 196      |
|    time_elapsed    | 65825    |
|    total_timesteps | 12042240 |
---------------------------------
Eval num_timesteps=12042436, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 12042436    |
| train/                  |             |
|    approx_kl            | 0.016886117 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.339       |
|    learning_rate        | 7.92e-05    |
|    loss                 | 0.0457      |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 197      |
|    time_elapsed    | 66176    |
|    total_timesteps | 12103680 |
---------------------------------
Eval num_timesteps=12103877, episode_reward=0.03 +/- 0.98
Episode length: 30.07 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 12103877    |
| train/                  |             |
|    approx_kl            | 0.017367505 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.91e-05    |
|    loss                 | 0.0706      |
|    n_updates            | 5165        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 182      |
|    iterations      | 198      |
|    time_elapsed    | 66538    |
|    total_timesteps | 12165120 |
---------------------------------
Eval num_timesteps=12165318, episode_reward=0.00 +/- 0.98
Episode length: 29.93 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 12165318   |
| train/                  |            |
|    approx_kl            | 0.01706206 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.719     |
|    explained_variance   | 0.297      |
|    learning_rate        | 7.91e-05   |
|    loss                 | 0.0716     |
|    n_updates            | 5170       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 199      |
|    time_elapsed    | 66913    |
|    total_timesteps | 12226560 |
---------------------------------
Eval num_timesteps=12226759, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 12226759    |
| train/                  |             |
|    approx_kl            | 0.017088873 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.299       |
|    learning_rate        | 7.9e-05     |
|    loss                 | 0.0935      |
|    n_updates            | 5175        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 200      |
|    time_elapsed    | 67289    |
|    total_timesteps | 12288000 |
---------------------------------
Eval num_timesteps=12288200, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.126      |
| time/                   |            |
|    total_timesteps      | 12288200   |
| train/                  |            |
|    approx_kl            | 0.01670417 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.329      |
|    learning_rate        | 7.89e-05   |
|    loss                 | 0.0723     |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 201      |
|    time_elapsed    | 67670    |
|    total_timesteps | 12349440 |
---------------------------------
Eval num_timesteps=12349641, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 12349641    |
| train/                  |             |
|    approx_kl            | 0.016799442 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.89e-05    |
|    loss                 | 0.047       |
|    n_updates            | 5185        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 202      |
|    time_elapsed    | 68044    |
|    total_timesteps | 12410880 |
---------------------------------
Eval num_timesteps=12411082, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.112      |
| time/                   |            |
|    total_timesteps      | 12411082   |
| train/                  |            |
|    approx_kl            | 0.01666797 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.318      |
|    learning_rate        | 7.88e-05   |
|    loss                 | 0.089      |
|    n_updates            | 5190       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 203      |
|    time_elapsed    | 68426    |
|    total_timesteps | 12472320 |
---------------------------------
Eval num_timesteps=12472523, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 12472523    |
| train/                  |             |
|    approx_kl            | 0.017033877 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.322       |
|    learning_rate        | 7.88e-05    |
|    loss                 | 0.06        |
|    n_updates            | 5195        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 204      |
|    time_elapsed    | 68805    |
|    total_timesteps | 12533760 |
---------------------------------
Eval num_timesteps=12533964, episode_reward=0.06 +/- 0.98
Episode length: 30.04 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 12533964    |
| train/                  |             |
|    approx_kl            | 0.017323399 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.87e-05    |
|    loss                 | 0.0657      |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 205      |
|    time_elapsed    | 69187    |
|    total_timesteps | 12595200 |
---------------------------------
Eval num_timesteps=12595405, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 12595405    |
| train/                  |             |
|    approx_kl            | 0.016528646 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.31        |
|    learning_rate        | 7.87e-05    |
|    loss                 | 0.0457      |
|    n_updates            | 5205        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 181      |
|    iterations      | 206      |
|    time_elapsed    | 69569    |
|    total_timesteps | 12656640 |
---------------------------------
Eval num_timesteps=12656846, episode_reward=0.08 +/- 0.98
Episode length: 30.07 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 12656846    |
| train/                  |             |
|    approx_kl            | 0.016590774 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.86e-05    |
|    loss                 | 0.0557      |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 207      |
|    time_elapsed    | 69942    |
|    total_timesteps | 12718080 |
---------------------------------
Eval num_timesteps=12718287, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 12718287   |
| train/                  |            |
|    approx_kl            | 0.01663589 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.327      |
|    learning_rate        | 7.86e-05   |
|    loss                 | 0.0582     |
|    n_updates            | 5215       |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 208      |
|    time_elapsed    | 70323    |
|    total_timesteps | 12779520 |
---------------------------------
Eval num_timesteps=12779728, episode_reward=0.05 +/- 0.97
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 12779728    |
| train/                  |             |
|    approx_kl            | 0.016939156 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.303       |
|    learning_rate        | 7.85e-05    |
|    loss                 | 0.0681      |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 209      |
|    time_elapsed    | 70696    |
|    total_timesteps | 12840960 |
---------------------------------
Eval num_timesteps=12841169, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 12841169    |
| train/                  |             |
|    approx_kl            | 0.016539915 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.84e-05    |
|    loss                 | 0.0849      |
|    n_updates            | 5225        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 210      |
|    time_elapsed    | 71078    |
|    total_timesteps | 12902400 |
---------------------------------
Eval num_timesteps=12902610, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 12902610    |
| train/                  |             |
|    approx_kl            | 0.016831426 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.84e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 211      |
|    time_elapsed    | 71450    |
|    total_timesteps | 12963840 |
---------------------------------
Eval num_timesteps=12964051, episode_reward=0.01 +/- 0.98
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.012      |
| time/                   |            |
|    total_timesteps      | 12964051   |
| train/                  |            |
|    approx_kl            | 0.01688428 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.317      |
|    learning_rate        | 7.83e-05   |
|    loss                 | 0.0691     |
|    n_updates            | 5235       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 212      |
|    time_elapsed    | 71832    |
|    total_timesteps | 13025280 |
---------------------------------
Eval num_timesteps=13025492, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 13025492   |
| train/                  |            |
|    approx_kl            | 0.01651804 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.304      |
|    learning_rate        | 7.83e-05   |
|    loss                 | 0.0634     |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 213      |
|    time_elapsed    | 72205    |
|    total_timesteps | 13086720 |
---------------------------------
Eval num_timesteps=13086933, episode_reward=0.05 +/- 0.99
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 13086933    |
| train/                  |             |
|    approx_kl            | 0.017542768 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.317       |
|    learning_rate        | 7.82e-05    |
|    loss                 | 0.112       |
|    n_updates            | 5245        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 214      |
|    time_elapsed    | 72589    |
|    total_timesteps | 13148160 |
---------------------------------
Eval num_timesteps=13148374, episode_reward=0.13 +/- 0.98
Episode length: 30.05 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.134       |
| time/                   |             |
|    total_timesteps      | 13148374    |
| train/                  |             |
|    approx_kl            | 0.016149469 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.33        |
|    learning_rate        | 7.82e-05    |
|    loss                 | 0.0781      |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 215      |
|    time_elapsed    | 73089    |
|    total_timesteps | 13209600 |
---------------------------------
Eval num_timesteps=13209815, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 13209815    |
| train/                  |             |
|    approx_kl            | 0.016774887 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.81e-05    |
|    loss                 | 0.0639      |
|    n_updates            | 5255        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.237       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 216      |
|    time_elapsed    | 73514    |
|    total_timesteps | 13271040 |
---------------------------------
Eval num_timesteps=13271256, episode_reward=0.04 +/- 0.99
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 13271256    |
| train/                  |             |
|    approx_kl            | 0.016647771 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.81e-05    |
|    loss                 | 0.0813      |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 217      |
|    time_elapsed    | 73894    |
|    total_timesteps | 13332480 |
---------------------------------
Eval num_timesteps=13332697, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 13332697    |
| train/                  |             |
|    approx_kl            | 0.016434094 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.8e-05     |
|    loss                 | 0.0892      |
|    n_updates            | 5265        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 218      |
|    time_elapsed    | 74271    |
|    total_timesteps | 13393920 |
---------------------------------
Eval num_timesteps=13394138, episode_reward=0.07 +/- 0.97
Episode length: 29.99 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 13394138   |
| train/                  |            |
|    approx_kl            | 0.01627668 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.313      |
|    learning_rate        | 7.79e-05   |
|    loss                 | 0.0731     |
|    n_updates            | 5270       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 219      |
|    time_elapsed    | 74651    |
|    total_timesteps | 13455360 |
---------------------------------
Eval num_timesteps=13455579, episode_reward=-0.00 +/- 0.98
Episode length: 29.95 +/- 1.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 13455579    |
| train/                  |             |
|    approx_kl            | 0.016582675 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.308       |
|    learning_rate        | 7.79e-05    |
|    loss                 | 0.0801      |
|    n_updates            | 5275        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 220      |
|    time_elapsed    | 75025    |
|    total_timesteps | 13516800 |
---------------------------------
Eval num_timesteps=13517020, episode_reward=-0.04 +/- 0.98
Episode length: 29.97 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.038     |
| time/                   |            |
|    total_timesteps      | 13517020   |
| train/                  |            |
|    approx_kl            | 0.01663043 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.328      |
|    learning_rate        | 7.78e-05   |
|    loss                 | 0.0476     |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 221      |
|    time_elapsed    | 75405    |
|    total_timesteps | 13578240 |
---------------------------------
Eval num_timesteps=13578461, episode_reward=0.02 +/- 0.99
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 13578461   |
| train/                  |            |
|    approx_kl            | 0.01630614 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.309      |
|    learning_rate        | 7.78e-05   |
|    loss                 | 0.132      |
|    n_updates            | 5285       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 222      |
|    time_elapsed    | 75783    |
|    total_timesteps | 13639680 |
---------------------------------
Eval num_timesteps=13639902, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 13639902    |
| train/                  |             |
|    approx_kl            | 0.016839776 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.77e-05    |
|    loss                 | 0.0627      |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 223      |
|    time_elapsed    | 76158    |
|    total_timesteps | 13701120 |
---------------------------------
Eval num_timesteps=13701343, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 13701343   |
| train/                  |            |
|    approx_kl            | 0.01659941 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.319      |
|    learning_rate        | 7.77e-05   |
|    loss                 | 0.0818     |
|    n_updates            | 5295       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 224      |
|    time_elapsed    | 76545    |
|    total_timesteps | 13762560 |
---------------------------------
Eval num_timesteps=13762784, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 13762784    |
| train/                  |             |
|    approx_kl            | 0.016228355 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.76e-05    |
|    loss                 | 0.0653      |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 225      |
|    time_elapsed    | 76918    |
|    total_timesteps | 13824000 |
---------------------------------
Eval num_timesteps=13824225, episode_reward=-0.03 +/- 0.98
Episode length: 30.01 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 13824225    |
| train/                  |             |
|    approx_kl            | 0.016833914 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.287       |
|    learning_rate        | 7.76e-05    |
|    loss                 | 0.03        |
|    n_updates            | 5305        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 226      |
|    time_elapsed    | 77307    |
|    total_timesteps | 13885440 |
---------------------------------
Eval num_timesteps=13885666, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 13885666    |
| train/                  |             |
|    approx_kl            | 0.016867897 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.75e-05    |
|    loss                 | 0.0781      |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 227      |
|    time_elapsed    | 77679    |
|    total_timesteps | 13946880 |
---------------------------------
Eval num_timesteps=13947107, episode_reward=-0.01 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 13947107    |
| train/                  |             |
|    approx_kl            | 0.016610181 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.74e-05    |
|    loss                 | 0.0422      |
|    n_updates            | 5315        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 228      |
|    time_elapsed    | 78063    |
|    total_timesteps | 14008320 |
---------------------------------
Eval num_timesteps=14008548, episode_reward=-0.03 +/- 0.99
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.034      |
| time/                   |             |
|    total_timesteps      | 14008548    |
| train/                  |             |
|    approx_kl            | 0.016118152 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.317       |
|    learning_rate        | 7.74e-05    |
|    loss                 | 0.105       |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 229      |
|    time_elapsed    | 78437    |
|    total_timesteps | 14069760 |
---------------------------------
Eval num_timesteps=14069989, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.032      |
| time/                   |            |
|    total_timesteps      | 14069989   |
| train/                  |            |
|    approx_kl            | 0.01636862 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.314      |
|    learning_rate        | 7.73e-05   |
|    loss                 | 0.0318     |
|    n_updates            | 5325       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 230      |
|    time_elapsed    | 78816    |
|    total_timesteps | 14131200 |
---------------------------------
Eval num_timesteps=14131430, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 14131430    |
| train/                  |             |
|    approx_kl            | 0.016344046 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.73e-05    |
|    loss                 | 0.0832      |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 231      |
|    time_elapsed    | 79194    |
|    total_timesteps | 14192640 |
---------------------------------
Eval num_timesteps=14192871, episode_reward=0.01 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 14192871    |
| train/                  |             |
|    approx_kl            | 0.016493073 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.72e-05    |
|    loss                 | 0.119       |
|    n_updates            | 5335        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 232      |
|    time_elapsed    | 79571    |
|    total_timesteps | 14254080 |
---------------------------------
Eval num_timesteps=14254312, episode_reward=-0.00 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 14254312    |
| train/                  |             |
|    approx_kl            | 0.016694251 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.72e-05    |
|    loss                 | 0.101       |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 233      |
|    time_elapsed    | 80083    |
|    total_timesteps | 14315520 |
---------------------------------
Eval num_timesteps=14315753, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 14315753    |
| train/                  |             |
|    approx_kl            | 0.016296398 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.303       |
|    learning_rate        | 7.71e-05    |
|    loss                 | 0.109       |
|    n_updates            | 5345        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 234      |
|    time_elapsed    | 80496    |
|    total_timesteps | 14376960 |
---------------------------------
Eval num_timesteps=14377194, episode_reward=-0.06 +/- 0.98
Episode length: 29.97 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.058      |
| time/                   |             |
|    total_timesteps      | 14377194    |
| train/                  |             |
|    approx_kl            | 0.016329778 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.317       |
|    learning_rate        | 7.71e-05    |
|    loss                 | 0.112       |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 235      |
|    time_elapsed    | 80874    |
|    total_timesteps | 14438400 |
---------------------------------
Eval num_timesteps=14438635, episode_reward=-0.06 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.062      |
| time/                   |             |
|    total_timesteps      | 14438635    |
| train/                  |             |
|    approx_kl            | 0.016102323 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.7e-05     |
|    loss                 | 0.046       |
|    n_updates            | 5355        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 236      |
|    time_elapsed    | 81258    |
|    total_timesteps | 14499840 |
---------------------------------
Eval num_timesteps=14500076, episode_reward=0.03 +/- 0.98
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 14500076    |
| train/                  |             |
|    approx_kl            | 0.016117835 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.7e-05     |
|    loss                 | 0.0664      |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 237      |
|    time_elapsed    | 81637    |
|    total_timesteps | 14561280 |
---------------------------------
Eval num_timesteps=14561517, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 14561517   |
| train/                  |            |
|    approx_kl            | 0.01612157 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.717     |
|    explained_variance   | 0.327      |
|    learning_rate        | 7.69e-05   |
|    loss                 | 0.0457     |
|    n_updates            | 5365       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 238      |
|    time_elapsed    | 82019    |
|    total_timesteps | 14622720 |
---------------------------------
Eval num_timesteps=14622958, episode_reward=-0.04 +/- 0.97
Episode length: 30.01 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.038      |
| time/                   |             |
|    total_timesteps      | 14622958    |
| train/                  |             |
|    approx_kl            | 0.016012326 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.325       |
|    learning_rate        | 7.68e-05    |
|    loss                 | 0.0863      |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 239      |
|    time_elapsed    | 82396    |
|    total_timesteps | 14684160 |
---------------------------------
Eval num_timesteps=14684399, episode_reward=-0.03 +/- 0.98
Episode length: 29.99 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 14684399   |
| train/                  |            |
|    approx_kl            | 0.01648122 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.717     |
|    explained_variance   | 0.315      |
|    learning_rate        | 7.68e-05   |
|    loss                 | 0.106      |
|    n_updates            | 5375       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 240      |
|    time_elapsed    | 82776    |
|    total_timesteps | 14745600 |
---------------------------------
Eval num_timesteps=14745840, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 14745840    |
| train/                  |             |
|    approx_kl            | 0.016728071 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.67e-05    |
|    loss                 | 0.0943      |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 241      |
|    time_elapsed    | 83149    |
|    total_timesteps | 14807040 |
---------------------------------
Eval num_timesteps=14807281, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 14807281    |
| train/                  |             |
|    approx_kl            | 0.016255958 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.304       |
|    learning_rate        | 7.67e-05    |
|    loss                 | 0.0843      |
|    n_updates            | 5385        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 242      |
|    time_elapsed    | 83531    |
|    total_timesteps | 14868480 |
---------------------------------
Eval num_timesteps=14868722, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 14868722    |
| train/                  |             |
|    approx_kl            | 0.015984626 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.66e-05    |
|    loss                 | 0.111       |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 243      |
|    time_elapsed    | 83903    |
|    total_timesteps | 14929920 |
---------------------------------
Eval num_timesteps=14930163, episode_reward=0.03 +/- 0.99
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 14930163    |
| train/                  |             |
|    approx_kl            | 0.016456338 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.66e-05    |
|    loss                 | 0.104       |
|    n_updates            | 5395        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 244      |
|    time_elapsed    | 84285    |
|    total_timesteps | 14991360 |
---------------------------------
Eval num_timesteps=14991604, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 14991604    |
| train/                  |             |
|    approx_kl            | 0.015956901 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.333       |
|    learning_rate        | 7.65e-05    |
|    loss                 | 0.115       |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 245      |
|    time_elapsed    | 84661    |
|    total_timesteps | 15052800 |
---------------------------------
Eval num_timesteps=15053045, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 15053045   |
| train/                  |            |
|    approx_kl            | 0.01624627 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.307      |
|    learning_rate        | 7.65e-05   |
|    loss                 | 0.0742     |
|    n_updates            | 5405       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 246      |
|    time_elapsed    | 85042    |
|    total_timesteps | 15114240 |
---------------------------------
Eval num_timesteps=15114486, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 15114486    |
| train/                  |             |
|    approx_kl            | 0.016145399 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.64e-05    |
|    loss                 | 0.0818      |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 247      |
|    time_elapsed    | 85426    |
|    total_timesteps | 15175680 |
---------------------------------
Eval num_timesteps=15175927, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 15175927   |
| train/                  |            |
|    approx_kl            | 0.01602823 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.328      |
|    learning_rate        | 7.63e-05   |
|    loss                 | 0.0822     |
|    n_updates            | 5415       |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 248      |
|    time_elapsed    | 85755    |
|    total_timesteps | 15237120 |
---------------------------------
Eval num_timesteps=15237368, episode_reward=-0.03 +/- 0.99
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 15237368    |
| train/                  |             |
|    approx_kl            | 0.016351407 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7.63e-05    |
|    loss                 | 0.0569      |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 249      |
|    time_elapsed    | 86122    |
|    total_timesteps | 15298560 |
---------------------------------
Eval num_timesteps=15298809, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 15298809    |
| train/                  |             |
|    approx_kl            | 0.016153278 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.304       |
|    learning_rate        | 7.62e-05    |
|    loss                 | 0.0746      |
|    n_updates            | 5425        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 250      |
|    time_elapsed    | 86493    |
|    total_timesteps | 15360000 |
---------------------------------
Eval num_timesteps=15360250, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 15360250    |
| train/                  |             |
|    approx_kl            | 0.016100701 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.62e-05    |
|    loss                 | 0.0659      |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 251      |
|    time_elapsed    | 86859    |
|    total_timesteps | 15421440 |
---------------------------------
Eval num_timesteps=15421691, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 15421691    |
| train/                  |             |
|    approx_kl            | 0.016296167 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.61e-05    |
|    loss                 | 0.0739      |
|    n_updates            | 5435        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 252      |
|    time_elapsed    | 87231    |
|    total_timesteps | 15482880 |
---------------------------------
Eval num_timesteps=15483132, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 15483132   |
| train/                  |            |
|    approx_kl            | 0.01623659 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.707     |
|    explained_variance   | 0.325      |
|    learning_rate        | 7.61e-05   |
|    loss                 | 0.0741     |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 253      |
|    time_elapsed    | 87596    |
|    total_timesteps | 15544320 |
---------------------------------
Eval num_timesteps=15544573, episode_reward=0.05 +/- 0.98
Episode length: 29.96 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 15544573    |
| train/                  |             |
|    approx_kl            | 0.016572902 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.6e-05     |
|    loss                 | 0.098       |
|    n_updates            | 5445        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 177      |
|    iterations      | 254      |
|    time_elapsed    | 87971    |
|    total_timesteps | 15605760 |
---------------------------------
Eval num_timesteps=15606014, episode_reward=-0.02 +/- 0.98
Episode length: 29.94 +/- 0.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 15606014   |
| train/                  |            |
|    approx_kl            | 0.01636534 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.307      |
|    learning_rate        | 7.6e-05    |
|    loss                 | 0.0438     |
|    n_updates            | 5450       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 255      |
|    time_elapsed    | 88365    |
|    total_timesteps | 15667200 |
---------------------------------
Eval num_timesteps=15667455, episode_reward=0.07 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 15667455    |
| train/                  |             |
|    approx_kl            | 0.016229996 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.59e-05    |
|    loss                 | 0.072       |
|    n_updates            | 5455        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 256      |
|    time_elapsed    | 88852    |
|    total_timesteps | 15728640 |
---------------------------------
Eval num_timesteps=15728896, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 15728896    |
| train/                  |             |
|    approx_kl            | 0.016634485 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.0623      |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 257      |
|    time_elapsed    | 89232    |
|    total_timesteps | 15790080 |
---------------------------------
Eval num_timesteps=15790337, episode_reward=0.09 +/- 0.97
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 15790337    |
| train/                  |             |
|    approx_kl            | 0.016802164 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.0792      |
|    n_updates            | 5465        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 258      |
|    time_elapsed    | 89610    |
|    total_timesteps | 15851520 |
---------------------------------
Eval num_timesteps=15851778, episode_reward=0.06 +/- 0.97
Episode length: 30.01 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 15851778   |
| train/                  |            |
|    approx_kl            | 0.01663903 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.707     |
|    explained_variance   | 0.296      |
|    learning_rate        | 7.57e-05   |
|    loss                 | 0.0738     |
|    n_updates            | 5470       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 259      |
|    time_elapsed    | 89985    |
|    total_timesteps | 15912960 |
---------------------------------
Eval num_timesteps=15913219, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 15913219    |
| train/                  |             |
|    approx_kl            | 0.016075326 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.57e-05    |
|    loss                 | 0.105       |
|    n_updates            | 5475        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 260      |
|    time_elapsed    | 90360    |
|    total_timesteps | 15974400 |
---------------------------------
Eval num_timesteps=15974660, episode_reward=0.05 +/- 0.99
Episode length: 29.96 +/- 1.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.046     |
| time/                   |           |
|    total_timesteps      | 15974660  |
| train/                  |           |
|    approx_kl            | 0.0161785 |
|    clip_fraction        | 0.181     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.705    |
|    explained_variance   | 0.314     |
|    learning_rate        | 7.56e-05  |
|    loss                 | 0.0916    |
|    n_updates            | 5480      |
|    policy_gradient_loss | -0.0172   |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 261      |
|    time_elapsed    | 90732    |
|    total_timesteps | 16035840 |
---------------------------------
Eval num_timesteps=16036101, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 16036101    |
| train/                  |             |
|    approx_kl            | 0.016173184 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.56e-05    |
|    loss                 | 0.0694      |
|    n_updates            | 5485        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 262      |
|    time_elapsed    | 91106    |
|    total_timesteps | 16097280 |
---------------------------------
Eval num_timesteps=16097542, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total_timesteps      | 16097542   |
| train/                  |            |
|    approx_kl            | 0.01567343 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.705     |
|    explained_variance   | 0.312      |
|    learning_rate        | 7.55e-05   |
|    loss                 | 0.0593     |
|    n_updates            | 5490       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 263      |
|    time_elapsed    | 91556    |
|    total_timesteps | 16158720 |
---------------------------------
Eval num_timesteps=16158983, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 16158983    |
| train/                  |             |
|    approx_kl            | 0.015885886 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.326       |
|    learning_rate        | 7.55e-05    |
|    loss                 | 0.1         |
|    n_updates            | 5495        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 264      |
|    time_elapsed    | 91996    |
|    total_timesteps | 16220160 |
---------------------------------
Eval num_timesteps=16220424, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 16220424    |
| train/                  |             |
|    approx_kl            | 0.016281392 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.295       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0527      |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 265      |
|    time_elapsed    | 92369    |
|    total_timesteps | 16281600 |
---------------------------------
Eval num_timesteps=16281865, episode_reward=0.04 +/- 0.98
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 16281865    |
| train/                  |             |
|    approx_kl            | 0.016428642 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.298       |
|    learning_rate        | 7.53e-05    |
|    loss                 | 0.0879      |
|    n_updates            | 5505        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 266      |
|    time_elapsed    | 92744    |
|    total_timesteps | 16343040 |
---------------------------------
Eval num_timesteps=16343306, episode_reward=0.05 +/- 0.99
Episode length: 30.04 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 16343306   |
| train/                  |            |
|    approx_kl            | 0.01624669 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.699     |
|    explained_variance   | 0.313      |
|    learning_rate        | 7.53e-05   |
|    loss                 | 0.065      |
|    n_updates            | 5510       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 267      |
|    time_elapsed    | 93116    |
|    total_timesteps | 16404480 |
---------------------------------
Eval num_timesteps=16404747, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.102      |
| time/                   |            |
|    total_timesteps      | 16404747   |
| train/                  |            |
|    approx_kl            | 0.01579856 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.694     |
|    explained_variance   | 0.325      |
|    learning_rate        | 7.52e-05   |
|    loss                 | 0.0654     |
|    n_updates            | 5515       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 268      |
|    time_elapsed    | 93492    |
|    total_timesteps | 16465920 |
---------------------------------
Eval num_timesteps=16466188, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 16466188   |
| train/                  |            |
|    approx_kl            | 0.01614362 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.302      |
|    learning_rate        | 7.52e-05   |
|    loss                 | 0.0991     |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 269      |
|    time_elapsed    | 93862    |
|    total_timesteps | 16527360 |
---------------------------------
Eval num_timesteps=16527629, episode_reward=0.04 +/- 0.97
Episode length: 29.96 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 16527629    |
| train/                  |             |
|    approx_kl            | 0.016161168 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.51e-05    |
|    loss                 | 0.0737      |
|    n_updates            | 5525        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 270      |
|    time_elapsed    | 94277    |
|    total_timesteps | 16588800 |
---------------------------------
Eval num_timesteps=16589070, episode_reward=0.09 +/- 0.98
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 16589070    |
| train/                  |             |
|    approx_kl            | 0.016031047 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.51e-05    |
|    loss                 | 0.0784      |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 271      |
|    time_elapsed    | 94703    |
|    total_timesteps | 16650240 |
---------------------------------
Eval num_timesteps=16650511, episode_reward=0.15 +/- 0.97
Episode length: 30.05 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.148       |
| time/                   |             |
|    total_timesteps      | 16650511    |
| train/                  |             |
|    approx_kl            | 0.015817897 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.691      |
|    explained_variance   | 0.308       |
|    learning_rate        | 7.5e-05     |
|    loss                 | 0.0793      |
|    n_updates            | 5535        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 8
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 272      |
|    time_elapsed    | 95132    |
|    total_timesteps | 16711680 |
---------------------------------
Eval num_timesteps=16711952, episode_reward=-0.04 +/- 0.98
Episode length: 29.93 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.04      |
| time/                   |            |
|    total_timesteps      | 16711952   |
| train/                  |            |
|    approx_kl            | 0.01597316 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.695     |
|    explained_variance   | 0.292      |
|    learning_rate        | 7.5e-05    |
|    loss                 | 0.0551     |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 273      |
|    time_elapsed    | 95497    |
|    total_timesteps | 16773120 |
---------------------------------
Eval num_timesteps=16773393, episode_reward=-0.04 +/- 0.99
Episode length: 29.99 +/- 0.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 16773393    |
| train/                  |             |
|    approx_kl            | 0.016012449 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.49e-05    |
|    loss                 | 0.0615      |
|    n_updates            | 5545        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 274      |
|    time_elapsed    | 95869    |
|    total_timesteps | 16834560 |
---------------------------------
Eval num_timesteps=16834834, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 16834834    |
| train/                  |             |
|    approx_kl            | 0.016243335 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.31        |
|    learning_rate        | 7.48e-05    |
|    loss                 | 0.0748      |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 275      |
|    time_elapsed    | 96235    |
|    total_timesteps | 16896000 |
---------------------------------
Eval num_timesteps=16896275, episode_reward=0.11 +/- 0.97
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 16896275    |
| train/                  |             |
|    approx_kl            | 0.016060468 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.48e-05    |
|    loss                 | 0.0772      |
|    n_updates            | 5555        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 276      |
|    time_elapsed    | 96610    |
|    total_timesteps | 16957440 |
---------------------------------
Eval num_timesteps=16957716, episode_reward=-0.05 +/- 0.99
Episode length: 29.95 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.048     |
| time/                   |            |
|    total_timesteps      | 16957716   |
| train/                  |            |
|    approx_kl            | 0.01651083 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.699     |
|    explained_variance   | 0.315      |
|    learning_rate        | 7.47e-05   |
|    loss                 | 0.0471     |
|    n_updates            | 5560       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 277      |
|    time_elapsed    | 97003    |
|    total_timesteps | 17018880 |
---------------------------------
Eval num_timesteps=17019157, episode_reward=-0.04 +/- 0.99
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 17019157    |
| train/                  |             |
|    approx_kl            | 0.015482637 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.325       |
|    learning_rate        | 7.47e-05    |
|    loss                 | 0.0691      |
|    n_updates            | 5565        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 278      |
|    time_elapsed    | 97412    |
|    total_timesteps | 17080320 |
---------------------------------
Eval num_timesteps=17080598, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 17080598    |
| train/                  |             |
|    approx_kl            | 0.015627708 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.46e-05    |
|    loss                 | 0.0268      |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 279      |
|    time_elapsed    | 97832    |
|    total_timesteps | 17141760 |
---------------------------------
Eval num_timesteps=17142039, episode_reward=-0.03 +/- 0.98
Episode length: 30.01 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 17142039   |
| train/                  |            |
|    approx_kl            | 0.01547621 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.696     |
|    explained_variance   | 0.32       |
|    learning_rate        | 7.46e-05   |
|    loss                 | 0.103      |
|    n_updates            | 5575       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 280      |
|    time_elapsed    | 98251    |
|    total_timesteps | 17203200 |
---------------------------------
Eval num_timesteps=17203480, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 17203480    |
| train/                  |             |
|    approx_kl            | 0.015761148 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.45e-05    |
|    loss                 | 0.0521      |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 281      |
|    time_elapsed    | 98626    |
|    total_timesteps | 17264640 |
---------------------------------
Eval num_timesteps=17264921, episode_reward=-0.09 +/- 0.98
Episode length: 29.93 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.086      |
| time/                   |             |
|    total_timesteps      | 17264921    |
| train/                  |             |
|    approx_kl            | 0.016140299 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.31        |
|    learning_rate        | 7.45e-05    |
|    loss                 | 0.0785      |
|    n_updates            | 5585        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 282      |
|    time_elapsed    | 99001    |
|    total_timesteps | 17326080 |
---------------------------------
Eval num_timesteps=17326362, episode_reward=-0.01 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 17326362    |
| train/                  |             |
|    approx_kl            | 0.016043423 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.304       |
|    learning_rate        | 7.44e-05    |
|    loss                 | 0.0548      |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 283      |
|    time_elapsed    | 99376    |
|    total_timesteps | 17387520 |
---------------------------------
Eval num_timesteps=17387803, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 17387803   |
| train/                  |            |
|    approx_kl            | 0.01586615 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.697     |
|    explained_variance   | 0.325      |
|    learning_rate        | 7.44e-05   |
|    loss                 | 0.0703     |
|    n_updates            | 5595       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 174      |
|    iterations      | 284      |
|    time_elapsed    | 99748    |
|    total_timesteps | 17448960 |
---------------------------------
Eval num_timesteps=17449244, episode_reward=0.01 +/- 0.98
Episode length: 30.03 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 17449244    |
| train/                  |             |
|    approx_kl            | 0.016004955 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.297       |
|    learning_rate        | 7.43e-05    |
|    loss                 | 0.055       |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 285      |
|    time_elapsed    | 100178   |
|    total_timesteps | 17510400 |
---------------------------------
Eval num_timesteps=17510685, episode_reward=-0.05 +/- 0.98
Episode length: 29.96 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | -0.046    |
| time/                   |           |
|    total_timesteps      | 17510685  |
| train/                  |           |
|    approx_kl            | 0.0158278 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.696    |
|    explained_variance   | 0.321     |
|    learning_rate        | 7.42e-05  |
|    loss                 | 0.0633    |
|    n_updates            | 5605      |
|    policy_gradient_loss | -0.0177   |
|    value_loss           | 0.235     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 286      |
|    time_elapsed    | 100550   |
|    total_timesteps | 17571840 |
---------------------------------
Eval num_timesteps=17572126, episode_reward=0.00 +/- 0.98
Episode length: 29.98 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 17572126    |
| train/                  |             |
|    approx_kl            | 0.016030189 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.322       |
|    learning_rate        | 7.42e-05    |
|    loss                 | 0.0621      |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 287      |
|    time_elapsed    | 100989   |
|    total_timesteps | 17633280 |
---------------------------------
Eval num_timesteps=17633567, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 17633567    |
| train/                  |             |
|    approx_kl            | 0.015604302 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.41e-05    |
|    loss                 | 0.0707      |
|    n_updates            | 5615        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 288      |
|    time_elapsed    | 101388   |
|    total_timesteps | 17694720 |
---------------------------------
Eval num_timesteps=17695008, episode_reward=0.04 +/- 0.99
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 17695008    |
| train/                  |             |
|    approx_kl            | 0.015746374 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.41e-05    |
|    loss                 | 0.15        |
|    n_updates            | 5620        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 289      |
|    time_elapsed    | 101761   |
|    total_timesteps | 17756160 |
---------------------------------
Eval num_timesteps=17756449, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.06      |
| time/                   |           |
|    total_timesteps      | 17756449  |
| train/                  |           |
|    approx_kl            | 0.0158723 |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.696    |
|    explained_variance   | 0.316     |
|    learning_rate        | 7.4e-05   |
|    loss                 | 0.075     |
|    n_updates            | 5625      |
|    policy_gradient_loss | -0.0177   |
|    value_loss           | 0.24      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 174      |
|    iterations      | 290      |
|    time_elapsed    | 102138   |
|    total_timesteps | 17817600 |
---------------------------------
Eval num_timesteps=17817890, episode_reward=0.00 +/- 0.99
Episode length: 29.95 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 17817890    |
| train/                  |             |
|    approx_kl            | 0.015607874 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.4e-05     |
|    loss                 | 0.121       |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 291      |
|    time_elapsed    | 102513   |
|    total_timesteps | 17879040 |
---------------------------------
Eval num_timesteps=17879331, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 17879331    |
| train/                  |             |
|    approx_kl            | 0.015795987 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.329       |
|    learning_rate        | 7.39e-05    |
|    loss                 | 0.093       |
|    n_updates            | 5635        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 292      |
|    time_elapsed    | 102941   |
|    total_timesteps | 17940480 |
---------------------------------
Eval num_timesteps=17940772, episode_reward=0.06 +/- 0.98
Episode length: 30.06 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 17940772    |
| train/                  |             |
|    approx_kl            | 0.015915819 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.695      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.39e-05    |
|    loss                 | 0.0509      |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 293      |
|    time_elapsed    | 103313   |
|    total_timesteps | 18001920 |
---------------------------------
Eval num_timesteps=18002213, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 18002213    |
| train/                  |             |
|    approx_kl            | 0.015448594 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.38e-05    |
|    loss                 | 0.0682      |
|    n_updates            | 5645        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 294      |
|    time_elapsed    | 103698   |
|    total_timesteps | 18063360 |
---------------------------------
Eval num_timesteps=18063654, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 18063654    |
| train/                  |             |
|    approx_kl            | 0.015862694 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.326       |
|    learning_rate        | 7.37e-05    |
|    loss                 | 0.101       |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 174      |
|    iterations      | 295      |
|    time_elapsed    | 104146   |
|    total_timesteps | 18124800 |
---------------------------------
Eval num_timesteps=18125095, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 18125095    |
| train/                  |             |
|    approx_kl            | 0.015442976 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.695      |
|    explained_variance   | 0.302       |
|    learning_rate        | 7.37e-05    |
|    loss                 | 0.0844      |
|    n_updates            | 5655        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 296      |
|    time_elapsed    | 104515   |
|    total_timesteps | 18186240 |
---------------------------------
Eval num_timesteps=18186536, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 18186536    |
| train/                  |             |
|    approx_kl            | 0.015906697 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.331       |
|    learning_rate        | 7.36e-05    |
|    loss                 | 0.064       |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 297      |
|    time_elapsed    | 104887   |
|    total_timesteps | 18247680 |
---------------------------------
Eval num_timesteps=18247977, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 18247977    |
| train/                  |             |
|    approx_kl            | 0.015980856 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.306       |
|    learning_rate        | 7.36e-05    |
|    loss                 | 0.0963      |
|    n_updates            | 5665        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 298      |
|    time_elapsed    | 105253   |
|    total_timesteps | 18309120 |
---------------------------------
Eval num_timesteps=18309418, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 18309418    |
| train/                  |             |
|    approx_kl            | 0.015660703 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.337       |
|    learning_rate        | 7.35e-05    |
|    loss                 | 0.0396      |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 173      |
|    iterations      | 299      |
|    time_elapsed    | 105672   |
|    total_timesteps | 18370560 |
---------------------------------
Eval num_timesteps=18370859, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.072      |
| time/                   |            |
|    total_timesteps      | 18370859   |
| train/                  |            |
|    approx_kl            | 0.01563114 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.312      |
|    learning_rate        | 7.35e-05   |
|    loss                 | 0.0849     |
|    n_updates            | 5675       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 300      |
|    time_elapsed    | 106058   |
|    total_timesteps | 18432000 |
---------------------------------
Eval num_timesteps=18432300, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 18432300    |
| train/                  |             |
|    approx_kl            | 0.015763378 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.34e-05    |
|    loss                 | 0.102       |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 301      |
|    time_elapsed    | 106432   |
|    total_timesteps | 18493440 |
---------------------------------
Eval num_timesteps=18493741, episode_reward=0.03 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 18493741    |
| train/                  |             |
|    approx_kl            | 0.015307063 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.34e-05    |
|    loss                 | 0.0773      |
|    n_updates            | 5685        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 302      |
|    time_elapsed    | 106839   |
|    total_timesteps | 18554880 |
---------------------------------
Eval num_timesteps=18555182, episode_reward=0.06 +/- 0.99
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 18555182    |
| train/                  |             |
|    approx_kl            | 0.015765237 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.322       |
|    learning_rate        | 7.33e-05    |
|    loss                 | 0.0986      |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 303      |
|    time_elapsed    | 107273   |
|    total_timesteps | 18616320 |
---------------------------------
Eval num_timesteps=18616623, episode_reward=0.06 +/- 0.98
Episode length: 30.05 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 18616623    |
| train/                  |             |
|    approx_kl            | 0.015922565 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.32e-05    |
|    loss                 | 0.0722      |
|    n_updates            | 5695        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 304      |
|    time_elapsed    | 107646   |
|    total_timesteps | 18677760 |
---------------------------------
Eval num_timesteps=18678064, episode_reward=-0.03 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.032      |
| time/                   |             |
|    total_timesteps      | 18678064    |
| train/                  |             |
|    approx_kl            | 0.015514074 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.32e-05    |
|    loss                 | 0.0722      |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 305      |
|    time_elapsed    | 108021   |
|    total_timesteps | 18739200 |
---------------------------------
Eval num_timesteps=18739505, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 18739505    |
| train/                  |             |
|    approx_kl            | 0.015475394 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.31e-05    |
|    loss                 | 0.0887      |
|    n_updates            | 5705        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 306      |
|    time_elapsed    | 108426   |
|    total_timesteps | 18800640 |
---------------------------------
Eval num_timesteps=18800946, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.062        |
| time/                   |              |
|    total_timesteps      | 18800946     |
| train/                  |              |
|    approx_kl            | 0.0153832855 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.69        |
|    explained_variance   | 0.317        |
|    learning_rate        | 7.31e-05     |
|    loss                 | 0.0739       |
|    n_updates            | 5710         |
|    policy_gradient_loss | -0.0175      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 307      |
|    time_elapsed    | 108822   |
|    total_timesteps | 18862080 |
---------------------------------
Eval num_timesteps=18862387, episode_reward=-0.03 +/- 0.99
Episode length: 29.96 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.028       |
| time/                   |              |
|    total_timesteps      | 18862387     |
| train/                  |              |
|    approx_kl            | 0.0152323805 |
|    clip_fraction        | 0.175        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.689       |
|    explained_variance   | 0.33         |
|    learning_rate        | 7.3e-05      |
|    loss                 | 0.0846       |
|    n_updates            | 5715         |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.236        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 308      |
|    time_elapsed    | 109197   |
|    total_timesteps | 18923520 |
---------------------------------
Eval num_timesteps=18923828, episode_reward=0.07 +/- 0.99
Episode length: 30.04 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.07         |
| time/                   |              |
|    total_timesteps      | 18923828     |
| train/                  |              |
|    approx_kl            | 0.0156245865 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.692       |
|    explained_variance   | 0.32         |
|    learning_rate        | 7.3e-05      |
|    loss                 | 0.0338       |
|    n_updates            | 5720         |
|    policy_gradient_loss | -0.0179      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 309      |
|    time_elapsed    | 109567   |
|    total_timesteps | 18984960 |
---------------------------------
Eval num_timesteps=18985269, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 18985269    |
| train/                  |             |
|    approx_kl            | 0.015163052 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.323       |
|    learning_rate        | 7.29e-05    |
|    loss                 | 0.0857      |
|    n_updates            | 5725        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 310      |
|    time_elapsed    | 109996   |
|    total_timesteps | 19046400 |
---------------------------------
Eval num_timesteps=19046710, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 19046710    |
| train/                  |             |
|    approx_kl            | 0.015550792 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7.29e-05    |
|    loss                 | 0.0785      |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 311      |
|    time_elapsed    | 110406   |
|    total_timesteps | 19107840 |
---------------------------------
Eval num_timesteps=19108151, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 19108151    |
| train/                  |             |
|    approx_kl            | 0.015385026 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.688      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.28e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 5735        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 312      |
|    time_elapsed    | 110780   |
|    total_timesteps | 19169280 |
---------------------------------
Eval num_timesteps=19169592, episode_reward=-0.04 +/- 0.98
Episode length: 30.00 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.038       |
| time/                   |              |
|    total_timesteps      | 19169592     |
| train/                  |              |
|    approx_kl            | 0.0153350085 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.687       |
|    explained_variance   | 0.326        |
|    learning_rate        | 7.27e-05     |
|    loss                 | 0.0415       |
|    n_updates            | 5740         |
|    policy_gradient_loss | -0.0175      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 313      |
|    time_elapsed    | 111176   |
|    total_timesteps | 19230720 |
---------------------------------
Eval num_timesteps=19231033, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19231033    |
| train/                  |             |
|    approx_kl            | 0.015406829 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.27e-05    |
|    loss                 | 0.0658      |
|    n_updates            | 5745        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 314      |
|    time_elapsed    | 111585   |
|    total_timesteps | 19292160 |
---------------------------------
Eval num_timesteps=19292474, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 19292474    |
| train/                  |             |
|    approx_kl            | 0.015493517 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.26e-05    |
|    loss                 | 0.0878      |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 315      |
|    time_elapsed    | 111952   |
|    total_timesteps | 19353600 |
---------------------------------
Eval num_timesteps=19353915, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 19353915    |
| train/                  |             |
|    approx_kl            | 0.015578709 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.686      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.26e-05    |
|    loss                 | 0.0876      |
|    n_updates            | 5755        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 316      |
|    time_elapsed    | 112324   |
|    total_timesteps | 19415040 |
---------------------------------
Eval num_timesteps=19415356, episode_reward=0.05 +/- 0.98
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 19415356   |
| train/                  |            |
|    approx_kl            | 0.01523244 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.685     |
|    explained_variance   | 0.301      |
|    learning_rate        | 7.25e-05   |
|    loss                 | 0.0867     |
|    n_updates            | 5760       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 317      |
|    time_elapsed    | 112691   |
|    total_timesteps | 19476480 |
---------------------------------
Eval num_timesteps=19476797, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 19476797    |
| train/                  |             |
|    approx_kl            | 0.015416462 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.322       |
|    learning_rate        | 7.25e-05    |
|    loss                 | 0.0756      |
|    n_updates            | 5765        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 318      |
|    time_elapsed    | 113136   |
|    total_timesteps | 19537920 |
---------------------------------
Eval num_timesteps=19538238, episode_reward=-0.02 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 19538238    |
| train/                  |             |
|    approx_kl            | 0.015193894 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.321       |
|    learning_rate        | 7.24e-05    |
|    loss                 | 0.0765      |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 319      |
|    time_elapsed    | 113527   |
|    total_timesteps | 19599360 |
---------------------------------
Eval num_timesteps=19599679, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.05         |
| time/                   |              |
|    total_timesteps      | 19599679     |
| train/                  |              |
|    approx_kl            | 0.0153015265 |
|    clip_fraction        | 0.173        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.683       |
|    explained_variance   | 0.298        |
|    learning_rate        | 7.24e-05     |
|    loss                 | 0.0853       |
|    n_updates            | 5775         |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 320      |
|    time_elapsed    | 113907   |
|    total_timesteps | 19660800 |
---------------------------------
Eval num_timesteps=19661120, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 19661120   |
| train/                  |            |
|    approx_kl            | 0.01515793 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.681     |
|    explained_variance   | 0.319      |
|    learning_rate        | 7.23e-05   |
|    loss                 | 0.0988     |
|    n_updates            | 5780       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 321      |
|    time_elapsed    | 114326   |
|    total_timesteps | 19722240 |
---------------------------------
Eval num_timesteps=19722561, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 19722561    |
| train/                  |             |
|    approx_kl            | 0.015240681 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.22e-05    |
|    loss                 | 0.0792      |
|    n_updates            | 5785        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 322      |
|    time_elapsed    | 114700   |
|    total_timesteps | 19783680 |
---------------------------------
Eval num_timesteps=19784002, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.11         |
| time/                   |              |
|    total_timesteps      | 19784002     |
| train/                  |              |
|    approx_kl            | 0.0155166825 |
|    clip_fraction        | 0.173        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.681       |
|    explained_variance   | 0.32         |
|    learning_rate        | 7.22e-05     |
|    loss                 | 0.0681       |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.0177      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 323      |
|    time_elapsed    | 115072   |
|    total_timesteps | 19845120 |
---------------------------------
Eval num_timesteps=19845443, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 19845443    |
| train/                  |             |
|    approx_kl            | 0.015085061 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.682      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.21e-05    |
|    loss                 | 0.0604      |
|    n_updates            | 5795        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 324      |
|    time_elapsed    | 115448   |
|    total_timesteps | 19906560 |
---------------------------------
Eval num_timesteps=19906884, episode_reward=0.10 +/- 0.98
Episode length: 30.07 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 19906884    |
| train/                  |             |
|    approx_kl            | 0.014889121 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7.21e-05    |
|    loss                 | 0.105       |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 325      |
|    time_elapsed    | 115834   |
|    total_timesteps | 19968000 |
---------------------------------
Eval num_timesteps=19968325, episode_reward=0.16 +/- 0.97
Episode length: 30.04 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.158       |
| time/                   |             |
|    total_timesteps      | 19968325    |
| train/                  |             |
|    approx_kl            | 0.015056029 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.683      |
|    explained_variance   | 0.323       |
|    learning_rate        | 7.2e-05     |
|    loss                 | 0.0503      |
|    n_updates            | 5805        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.158
SELFPLAY: new best model, bumping up generation to 9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 326      |
|    time_elapsed    | 116280   |
|    total_timesteps | 20029440 |
---------------------------------
Eval num_timesteps=20029766, episode_reward=-0.05 +/- 0.98
Episode length: 29.94 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.048      |
| time/                   |             |
|    total_timesteps      | 20029766    |
| train/                  |             |
|    approx_kl            | 0.015157403 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.2e-05     |
|    loss                 | 0.0617      |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 327      |
|    time_elapsed    | 116655   |
|    total_timesteps | 20090880 |
---------------------------------
Eval num_timesteps=20091207, episode_reward=-0.04 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.044      |
| time/                   |             |
|    total_timesteps      | 20091207    |
| train/                  |             |
|    approx_kl            | 0.015086828 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.674      |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.19e-05    |
|    loss                 | 0.0791      |
|    n_updates            | 5815        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 328      |
|    time_elapsed    | 117086   |
|    total_timesteps | 20152320 |
---------------------------------
Eval num_timesteps=20152648, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 20152648    |
| train/                  |             |
|    approx_kl            | 0.015067237 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.672      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.19e-05    |
|    loss                 | 0.0694      |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 329      |
|    time_elapsed    | 117455   |
|    total_timesteps | 20213760 |
---------------------------------
Eval num_timesteps=20214089, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 20214089    |
| train/                  |             |
|    approx_kl            | 0.014592668 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.323       |
|    learning_rate        | 7.18e-05    |
|    loss                 | 0.0537      |
|    n_updates            | 5825        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 330      |
|    time_elapsed    | 117830   |
|    total_timesteps | 20275200 |
---------------------------------
Eval num_timesteps=20275530, episode_reward=0.12 +/- 0.98
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 20275530    |
| train/                  |             |
|    approx_kl            | 0.015021382 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.332       |
|    learning_rate        | 7.18e-05    |
|    loss                 | 0.073       |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 331      |
|    time_elapsed    | 118202   |
|    total_timesteps | 20336640 |
---------------------------------
Eval num_timesteps=20336971, episode_reward=-0.03 +/- 0.98
Episode length: 29.91 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 20336971    |
| train/                  |             |
|    approx_kl            | 0.015195944 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.17e-05    |
|    loss                 | 0.0687      |
|    n_updates            | 5835        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 332      |
|    time_elapsed    | 118579   |
|    total_timesteps | 20398080 |
---------------------------------
Eval num_timesteps=20398412, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 20398412    |
| train/                  |             |
|    approx_kl            | 0.015046009 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.668      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.16e-05    |
|    loss                 | 0.119       |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 333      |
|    time_elapsed    | 118982   |
|    total_timesteps | 20459520 |
---------------------------------
Eval num_timesteps=20459853, episode_reward=-0.01 +/- 0.98
Episode length: 30.01 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 20459853    |
| train/                  |             |
|    approx_kl            | 0.015251636 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.16e-05    |
|    loss                 | 0.051       |
|    n_updates            | 5845        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 334      |
|    time_elapsed    | 119419   |
|    total_timesteps | 20520960 |
---------------------------------
Eval num_timesteps=20521294, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 20521294    |
| train/                  |             |
|    approx_kl            | 0.015302369 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.15e-05    |
|    loss                 | 0.0846      |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 171      |
|    iterations      | 335      |
|    time_elapsed    | 119839   |
|    total_timesteps | 20582400 |
---------------------------------
Eval num_timesteps=20582735, episode_reward=0.03 +/- 0.97
Episode length: 30.02 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 20582735    |
| train/                  |             |
|    approx_kl            | 0.015404095 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.661      |
|    explained_variance   | 0.325       |
|    learning_rate        | 7.15e-05    |
|    loss                 | 0.0895      |
|    n_updates            | 5855        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 336      |
|    time_elapsed    | 120219   |
|    total_timesteps | 20643840 |
---------------------------------
Eval num_timesteps=20644176, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 20644176    |
| train/                  |             |
|    approx_kl            | 0.015382199 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.14e-05    |
|    loss                 | 0.0836      |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 337      |
|    time_elapsed    | 120591   |
|    total_timesteps | 20705280 |
---------------------------------
Eval num_timesteps=20705617, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 20705617   |
| train/                  |            |
|    approx_kl            | 0.01504013 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.662     |
|    explained_variance   | 0.313      |
|    learning_rate        | 7.14e-05   |
|    loss                 | 0.0621     |
|    n_updates            | 5865       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 338      |
|    time_elapsed    | 120963   |
|    total_timesteps | 20766720 |
---------------------------------
Eval num_timesteps=20767058, episode_reward=0.01 +/- 0.98
Episode length: 30.03 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 20767058    |
| train/                  |             |
|    approx_kl            | 0.015071299 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.13e-05    |
|    loss                 | 0.0677      |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 339      |
|    time_elapsed    | 121337   |
|    total_timesteps | 20828160 |
---------------------------------
Eval num_timesteps=20828499, episode_reward=0.00 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 20828499    |
| train/                  |             |
|    approx_kl            | 0.014437424 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.334       |
|    learning_rate        | 7.13e-05    |
|    loss                 | 0.0791      |
|    n_updates            | 5875        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 340      |
|    time_elapsed    | 121704   |
|    total_timesteps | 20889600 |
---------------------------------
Eval num_timesteps=20889940, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 1.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 20889940    |
| train/                  |             |
|    approx_kl            | 0.015074795 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.306       |
|    learning_rate        | 7.12e-05    |
|    loss                 | 0.119       |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 341      |
|    time_elapsed    | 122095   |
|    total_timesteps | 20951040 |
---------------------------------
Eval num_timesteps=20951381, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 20951381    |
| train/                  |             |
|    approx_kl            | 0.014812457 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.11e-05    |
|    loss                 | 0.102       |
|    n_updates            | 5885        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 342      |
|    time_elapsed    | 122414   |
|    total_timesteps | 21012480 |
---------------------------------
Eval num_timesteps=21012822, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 21012822    |
| train/                  |             |
|    approx_kl            | 0.014978526 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.11e-05    |
|    loss                 | 0.0762      |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 343      |
|    time_elapsed    | 122735   |
|    total_timesteps | 21073920 |
---------------------------------
Eval num_timesteps=21074263, episode_reward=-0.04 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.04       |
| time/                   |             |
|    total_timesteps      | 21074263    |
| train/                  |             |
|    approx_kl            | 0.014676273 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.315       |
|    learning_rate        | 7.1e-05     |
|    loss                 | 0.0802      |
|    n_updates            | 5895        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 344      |
|    time_elapsed    | 123056   |
|    total_timesteps | 21135360 |
---------------------------------
Eval num_timesteps=21135704, episode_reward=-0.00 +/- 0.98
Episode length: 30.01 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 21135704    |
| train/                  |             |
|    approx_kl            | 0.014875488 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.304       |
|    learning_rate        | 7.1e-05     |
|    loss                 | 0.0984      |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 345      |
|    time_elapsed    | 123379   |
|    total_timesteps | 21196800 |
---------------------------------
Eval num_timesteps=21197145, episode_reward=-0.05 +/- 0.98
Episode length: 30.02 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.048      |
| time/                   |             |
|    total_timesteps      | 21197145    |
| train/                  |             |
|    approx_kl            | 0.014978672 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.09e-05    |
|    loss                 | 0.0825      |
|    n_updates            | 5905        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 346      |
|    time_elapsed    | 123698   |
|    total_timesteps | 21258240 |
---------------------------------
Eval num_timesteps=21258586, episode_reward=-0.08 +/- 0.98
Episode length: 29.99 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.076      |
| time/                   |             |
|    total_timesteps      | 21258586    |
| train/                  |             |
|    approx_kl            | 0.014922309 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7.09e-05    |
|    loss                 | 0.0726      |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 347      |
|    time_elapsed    | 124016   |
|    total_timesteps | 21319680 |
---------------------------------
Eval num_timesteps=21320027, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 21320027    |
| train/                  |             |
|    approx_kl            | 0.014794475 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.08e-05    |
|    loss                 | 0.077       |
|    n_updates            | 5915        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 348      |
|    time_elapsed    | 124333   |
|    total_timesteps | 21381120 |
---------------------------------
Eval num_timesteps=21381468, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 21381468    |
| train/                  |             |
|    approx_kl            | 0.014702203 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.661      |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.08e-05    |
|    loss                 | 0.0794      |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 349      |
|    time_elapsed    | 124650   |
|    total_timesteps | 21442560 |
---------------------------------
Eval num_timesteps=21442909, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 21442909    |
| train/                  |             |
|    approx_kl            | 0.014870254 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.07e-05    |
|    loss                 | 0.133       |
|    n_updates            | 5925        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 350      |
|    time_elapsed    | 124967   |
|    total_timesteps | 21504000 |
---------------------------------
Eval num_timesteps=21504350, episode_reward=0.11 +/- 0.98
Episode length: 29.94 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 21504350    |
| train/                  |             |
|    approx_kl            | 0.015278561 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.06e-05    |
|    loss                 | 0.157       |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 351      |
|    time_elapsed    | 125284   |
|    total_timesteps | 21565440 |
---------------------------------
Eval num_timesteps=21565791, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 21565791    |
| train/                  |             |
|    approx_kl            | 0.014660213 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.06e-05    |
|    loss                 | 0.0427      |
|    n_updates            | 5935        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 352      |
|    time_elapsed    | 125601   |
|    total_timesteps | 21626880 |
---------------------------------
Eval num_timesteps=21627232, episode_reward=0.09 +/- 0.97
Episode length: 30.01 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.088        |
| time/                   |              |
|    total_timesteps      | 21627232     |
| train/                  |              |
|    approx_kl            | 0.0147427665 |
|    clip_fraction        | 0.171        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.664       |
|    explained_variance   | 0.301        |
|    learning_rate        | 7.05e-05     |
|    loss                 | 0.0474       |
|    n_updates            | 5940         |
|    policy_gradient_loss | -0.0177      |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 353      |
|    time_elapsed    | 125920   |
|    total_timesteps | 21688320 |
---------------------------------
Eval num_timesteps=21688673, episode_reward=0.01 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 21688673    |
| train/                  |             |
|    approx_kl            | 0.014473651 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.323       |
|    learning_rate        | 7.05e-05    |
|    loss                 | 0.118       |
|    n_updates            | 5945        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 354      |
|    time_elapsed    | 126239   |
|    total_timesteps | 21749760 |
---------------------------------
Eval num_timesteps=21750114, episode_reward=0.10 +/- 0.97
Episode length: 29.94 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 21750114    |
| train/                  |             |
|    approx_kl            | 0.015007009 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.661      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.04e-05    |
|    loss                 | 0.132       |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 355      |
|    time_elapsed    | 126560   |
|    total_timesteps | 21811200 |
---------------------------------
Eval num_timesteps=21811555, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 21811555    |
| train/                  |             |
|    approx_kl            | 0.014706322 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.04e-05    |
|    loss                 | 0.102       |
|    n_updates            | 5955        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 356      |
|    time_elapsed    | 126882   |
|    total_timesteps | 21872640 |
---------------------------------
Eval num_timesteps=21872996, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 21872996    |
| train/                  |             |
|    approx_kl            | 0.014675486 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.652      |
|    explained_variance   | 0.3         |
|    learning_rate        | 7.03e-05    |
|    loss                 | 0.119       |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 357      |
|    time_elapsed    | 127203   |
|    total_timesteps | 21934080 |
---------------------------------
Eval num_timesteps=21934437, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 21934437    |
| train/                  |             |
|    approx_kl            | 0.014994641 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.03e-05    |
|    loss                 | 0.0642      |
|    n_updates            | 5965        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 358      |
|    time_elapsed    | 127520   |
|    total_timesteps | 21995520 |
---------------------------------
Eval num_timesteps=21995878, episode_reward=-0.05 +/- 0.98
Episode length: 29.89 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.054      |
| time/                   |             |
|    total_timesteps      | 21995878    |
| train/                  |             |
|    approx_kl            | 0.014904498 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.02e-05    |
|    loss                 | 0.0657      |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 359      |
|    time_elapsed    | 127838   |
|    total_timesteps | 22056960 |
---------------------------------
Eval num_timesteps=22057319, episode_reward=0.03 +/- 0.98
Episode length: 30.03 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 22057319    |
| train/                  |             |
|    approx_kl            | 0.014503286 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.315       |
|    learning_rate        | 7.01e-05    |
|    loss                 | 0.104       |
|    n_updates            | 5975        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 360      |
|    time_elapsed    | 128155   |
|    total_timesteps | 22118400 |
---------------------------------
Eval num_timesteps=22118760, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.078      |
| time/                   |            |
|    total_timesteps      | 22118760   |
| train/                  |            |
|    approx_kl            | 0.01456124 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.654     |
|    explained_variance   | 0.305      |
|    learning_rate        | 7.01e-05   |
|    loss                 | 0.0883     |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 361      |
|    time_elapsed    | 128472   |
|    total_timesteps | 22179840 |
---------------------------------
Eval num_timesteps=22180201, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 1.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.094      |
| time/                   |            |
|    total_timesteps      | 22180201   |
| train/                  |            |
|    approx_kl            | 0.01473254 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.658     |
|    explained_variance   | 0.308      |
|    learning_rate        | 7e-05      |
|    loss                 | 0.0609     |
|    n_updates            | 5985       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 362      |
|    time_elapsed    | 128788   |
|    total_timesteps | 22241280 |
---------------------------------
Eval num_timesteps=22241642, episode_reward=0.07 +/- 0.98
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 22241642    |
| train/                  |             |
|    approx_kl            | 0.014895331 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7e-05       |
|    loss                 | 0.045       |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 363      |
|    time_elapsed    | 129105   |
|    total_timesteps | 22302720 |
---------------------------------
Eval num_timesteps=22303083, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 22303083    |
| train/                  |             |
|    approx_kl            | 0.014553682 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.653      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.99e-05    |
|    loss                 | 0.0581      |
|    n_updates            | 5995        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 364      |
|    time_elapsed    | 129426   |
|    total_timesteps | 22364160 |
---------------------------------
Eval num_timesteps=22364524, episode_reward=0.14 +/- 0.97
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 22364524    |
| train/                  |             |
|    approx_kl            | 0.014773343 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.318       |
|    learning_rate        | 6.99e-05    |
|    loss                 | 0.0856      |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 365      |
|    time_elapsed    | 129784   |
|    total_timesteps | 22425600 |
---------------------------------
Eval num_timesteps=22425965, episode_reward=-0.06 +/- 0.98
Episode length: 29.86 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.064      |
| time/                   |             |
|    total_timesteps      | 22425965    |
| train/                  |             |
|    approx_kl            | 0.014774977 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.307       |
|    learning_rate        | 6.98e-05    |
|    loss                 | 0.0688      |
|    n_updates            | 6005        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 366      |
|    time_elapsed    | 130199   |
|    total_timesteps | 22487040 |
---------------------------------
Eval num_timesteps=22487406, episode_reward=-0.02 +/- 0.98
Episode length: 29.88 +/- 1.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 22487406    |
| train/                  |             |
|    approx_kl            | 0.014741447 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.306       |
|    learning_rate        | 6.98e-05    |
|    loss                 | 0.0739      |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 367      |
|    time_elapsed    | 130564   |
|    total_timesteps | 22548480 |
---------------------------------
Eval num_timesteps=22548847, episode_reward=-0.05 +/- 0.97
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.052      |
| time/                   |             |
|    total_timesteps      | 22548847    |
| train/                  |             |
|    approx_kl            | 0.014708121 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.314       |
|    learning_rate        | 6.97e-05    |
|    loss                 | 0.0886      |
|    n_updates            | 6015        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 368      |
|    time_elapsed    | 130975   |
|    total_timesteps | 22609920 |
---------------------------------
Eval num_timesteps=22610288, episode_reward=-0.09 +/- 0.98
Episode length: 29.87 +/- 1.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.088     |
| time/                   |            |
|    total_timesteps      | 22610288   |
| train/                  |            |
|    approx_kl            | 0.01478655 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.324      |
|    learning_rate        | 6.97e-05   |
|    loss                 | 0.0867     |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 369      |
|    time_elapsed    | 131340   |
|    total_timesteps | 22671360 |
---------------------------------
Eval num_timesteps=22671729, episode_reward=-0.03 +/- 0.98
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 22671729    |
| train/                  |             |
|    approx_kl            | 0.014446701 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.324       |
|    learning_rate        | 6.96e-05    |
|    loss                 | 0.134       |
|    n_updates            | 6025        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 370      |
|    time_elapsed    | 131773   |
|    total_timesteps | 22732800 |
---------------------------------
Eval num_timesteps=22733170, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 22733170    |
| train/                  |             |
|    approx_kl            | 0.014967029 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.95e-05    |
|    loss                 | 0.0836      |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 371      |
|    time_elapsed    | 132196   |
|    total_timesteps | 22794240 |
---------------------------------
Eval num_timesteps=22794611, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 1.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 22794611    |
| train/                  |             |
|    approx_kl            | 0.014599546 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.312       |
|    learning_rate        | 6.95e-05    |
|    loss                 | 0.114       |
|    n_updates            | 6035        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 372      |
|    time_elapsed    | 132647   |
|    total_timesteps | 22855680 |
---------------------------------
Eval num_timesteps=22856052, episode_reward=0.00 +/- 0.99
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 22856052    |
| train/                  |             |
|    approx_kl            | 0.014296573 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.327       |
|    learning_rate        | 6.94e-05    |
|    loss                 | 0.0865      |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 373      |
|    time_elapsed    | 133054   |
|    total_timesteps | 22917120 |
---------------------------------
Eval num_timesteps=22917493, episode_reward=-0.00 +/- 0.97
Episode length: 29.96 +/- 1.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.004       |
| time/                   |              |
|    total_timesteps      | 22917493     |
| train/                  |              |
|    approx_kl            | 0.0145158265 |
|    clip_fraction        | 0.165        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.658       |
|    explained_variance   | 0.316        |
|    learning_rate        | 6.94e-05     |
|    loss                 | 0.0694       |
|    n_updates            | 6045         |
|    policy_gradient_loss | -0.0172      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 374      |
|    time_elapsed    | 133525   |
|    total_timesteps | 22978560 |
---------------------------------
Eval num_timesteps=22978934, episode_reward=-0.06 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.058      |
| time/                   |             |
|    total_timesteps      | 22978934    |
| train/                  |             |
|    approx_kl            | 0.015041472 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.319       |
|    learning_rate        | 6.93e-05    |
|    loss                 | 0.0922      |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 375      |
|    time_elapsed    | 133927   |
|    total_timesteps | 23040000 |
---------------------------------
Eval num_timesteps=23040375, episode_reward=0.04 +/- 0.99
Episode length: 29.95 +/- 1.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 23040375    |
| train/                  |             |
|    approx_kl            | 0.014385251 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.93e-05    |
|    loss                 | 0.0798      |
|    n_updates            | 6055        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 376      |
|    time_elapsed    | 134405   |
|    total_timesteps | 23101440 |
---------------------------------
Eval num_timesteps=23101816, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 23101816    |
| train/                  |             |
|    approx_kl            | 0.014703153 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.317       |
|    learning_rate        | 6.92e-05    |
|    loss                 | 0.0418      |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 377      |
|    time_elapsed    | 134820   |
|    total_timesteps | 23162880 |
---------------------------------
Eval num_timesteps=23163257, episode_reward=0.02 +/- 0.98
Episode length: 29.95 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 23163257    |
| train/                  |             |
|    approx_kl            | 0.014742373 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.32        |
|    learning_rate        | 6.92e-05    |
|    loss                 | 0.0829      |
|    n_updates            | 6065        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 378      |
|    time_elapsed    | 135288   |
|    total_timesteps | 23224320 |
---------------------------------
Eval num_timesteps=23224698, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 23224698    |
| train/                  |             |
|    approx_kl            | 0.014685605 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.324       |
|    learning_rate        | 6.91e-05    |
|    loss                 | 0.0743      |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 379      |
|    time_elapsed    | 135706   |
|    total_timesteps | 23285760 |
---------------------------------
Eval num_timesteps=23286139, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.086      |
| time/                   |            |
|    total_timesteps      | 23286139   |
| train/                  |            |
|    approx_kl            | 0.01460619 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.656     |
|    explained_variance   | 0.325      |
|    learning_rate        | 6.9e-05    |
|    loss                 | 0.112      |
|    n_updates            | 6075       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 380      |
|    time_elapsed    | 136154   |
|    total_timesteps | 23347200 |
---------------------------------
Eval num_timesteps=23347580, episode_reward=0.10 +/- 0.98
Episode length: 30.04 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.096        |
| time/                   |              |
|    total_timesteps      | 23347580     |
| train/                  |              |
|    approx_kl            | 0.0147876525 |
|    clip_fraction        | 0.168        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.657       |
|    explained_variance   | 0.316        |
|    learning_rate        | 6.9e-05      |
|    loss                 | 0.0606       |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.0173      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 171      |
|    iterations      | 381      |
|    time_elapsed    | 136579   |
|    total_timesteps | 23408640 |
---------------------------------
Eval num_timesteps=23409021, episode_reward=-0.03 +/- 0.99
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 23409021   |
| train/                  |            |
|    approx_kl            | 0.01456916 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.648     |
|    explained_variance   | 0.305      |
|    learning_rate        | 6.89e-05   |
|    loss                 | 0.0545     |
|    n_updates            | 6085       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 382      |
|    time_elapsed    | 137021   |
|    total_timesteps | 23470080 |
---------------------------------
Eval num_timesteps=23470462, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 1.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 23470462    |
| train/                  |             |
|    approx_kl            | 0.014174852 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.331       |
|    learning_rate        | 6.89e-05    |
|    loss                 | 0.126       |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 383      |
|    time_elapsed    | 137456   |
|    total_timesteps | 23531520 |
---------------------------------
Eval num_timesteps=23531903, episode_reward=0.03 +/- 0.98
Episode length: 29.98 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 23531903    |
| train/                  |             |
|    approx_kl            | 0.014534267 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.88e-05    |
|    loss                 | 0.0506      |
|    n_updates            | 6095        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 384      |
|    time_elapsed    | 137882   |
|    total_timesteps | 23592960 |
---------------------------------
Eval num_timesteps=23593344, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 23593344    |
| train/                  |             |
|    approx_kl            | 0.014517859 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.316       |
|    learning_rate        | 6.88e-05    |
|    loss                 | 0.0541      |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 385      |
|    time_elapsed    | 138329   |
|    total_timesteps | 23654400 |
---------------------------------
Eval num_timesteps=23654785, episode_reward=-0.00 +/- 0.98
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 23654785    |
| train/                  |             |
|    approx_kl            | 0.014168364 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.642      |
|    explained_variance   | 0.316       |
|    learning_rate        | 6.87e-05    |
|    loss                 | 0.112       |
|    n_updates            | 6105        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 386      |
|    time_elapsed    | 138770   |
|    total_timesteps | 23715840 |
---------------------------------
Eval num_timesteps=23716226, episode_reward=0.04 +/- 0.97
Episode length: 29.96 +/- 1.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.038        |
| time/                   |              |
|    total_timesteps      | 23716226     |
| train/                  |              |
|    approx_kl            | 0.0140652545 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.324        |
|    learning_rate        | 6.87e-05     |
|    loss                 | 0.0626       |
|    n_updates            | 6110         |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 387      |
|    time_elapsed    | 139212   |
|    total_timesteps | 23777280 |
---------------------------------
Eval num_timesteps=23777667, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.028        |
| time/                   |              |
|    total_timesteps      | 23777667     |
| train/                  |              |
|    approx_kl            | 0.0142207295 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.341        |
|    learning_rate        | 6.86e-05     |
|    loss                 | 0.0578       |
|    n_updates            | 6115         |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 0.233        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 388      |
|    time_elapsed    | 139661   |
|    total_timesteps | 23838720 |
---------------------------------
Eval num_timesteps=23839108, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 23839108    |
| train/                  |             |
|    approx_kl            | 0.014004255 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.639      |
|    explained_variance   | 0.321       |
|    learning_rate        | 6.85e-05    |
|    loss                 | 0.061       |
|    n_updates            | 6120        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 389      |
|    time_elapsed    | 140089   |
|    total_timesteps | 23900160 |
---------------------------------
Eval num_timesteps=23900549, episode_reward=0.14 +/- 0.98
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 23900549    |
| train/                  |             |
|    approx_kl            | 0.014472932 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.639      |
|    explained_variance   | 0.329       |
|    learning_rate        | 6.85e-05    |
|    loss                 | 0.105       |
|    n_updates            | 6125        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 11
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 390      |
|    time_elapsed    | 140537   |
|    total_timesteps | 23961600 |
---------------------------------
Eval num_timesteps=23961990, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.002        |
| time/                   |              |
|    total_timesteps      | 23961990     |
| train/                  |              |
|    approx_kl            | 0.0146327475 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.647       |
|    explained_variance   | 0.325        |
|    learning_rate        | 6.84e-05     |
|    loss                 | 0.0679       |
|    n_updates            | 6130         |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 391      |
|    time_elapsed    | 140956   |
|    total_timesteps | 24023040 |
---------------------------------
Eval num_timesteps=24023431, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 24023431    |
| train/                  |             |
|    approx_kl            | 0.013946053 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.309       |
|    learning_rate        | 6.84e-05    |
|    loss                 | 0.0914      |
|    n_updates            | 6135        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 392      |
|    time_elapsed    | 141408   |
|    total_timesteps | 24084480 |
---------------------------------
Eval num_timesteps=24084872, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 24084872    |
| train/                  |             |
|    approx_kl            | 0.014057559 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.331       |
|    learning_rate        | 6.83e-05    |
|    loss                 | 0.0646      |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 393      |
|    time_elapsed    | 141806   |
|    total_timesteps | 24145920 |
---------------------------------
Eval num_timesteps=24146313, episode_reward=0.06 +/- 0.99
Episode length: 30.03 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 24146313    |
| train/                  |             |
|    approx_kl            | 0.014082391 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.338       |
|    learning_rate        | 6.83e-05    |
|    loss                 | 0.0751      |
|    n_updates            | 6145        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 394      |
|    time_elapsed    | 142280   |
|    total_timesteps | 24207360 |
---------------------------------
Eval num_timesteps=24207754, episode_reward=-0.06 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.06       |
| time/                   |             |
|    total_timesteps      | 24207754    |
| train/                  |             |
|    approx_kl            | 0.014121844 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.312       |
|    learning_rate        | 6.82e-05    |
|    loss                 | 0.058       |
|    n_updates            | 6150        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 395      |
|    time_elapsed    | 142704   |
|    total_timesteps | 24268800 |
---------------------------------
Eval num_timesteps=24269195, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 24269195    |
| train/                  |             |
|    approx_kl            | 0.014075895 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.315       |
|    learning_rate        | 6.82e-05    |
|    loss                 | 0.0368      |
|    n_updates            | 6155        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 396      |
|    time_elapsed    | 143171   |
|    total_timesteps | 24330240 |
---------------------------------
Eval num_timesteps=24330636, episode_reward=0.07 +/- 0.97
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 24330636    |
| train/                  |             |
|    approx_kl            | 0.014331766 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.81e-05    |
|    loss                 | 0.0837      |
|    n_updates            | 6160        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 397      |
|    time_elapsed    | 143598   |
|    total_timesteps | 24391680 |
---------------------------------
Eval num_timesteps=24392077, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.008     |
| time/                   |            |
|    total_timesteps      | 24392077   |
| train/                  |            |
|    approx_kl            | 0.01448957 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.335      |
|    learning_rate        | 6.8e-05    |
|    loss                 | 0.086      |
|    n_updates            | 6165       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 398      |
|    time_elapsed    | 144057   |
|    total_timesteps | 24453120 |
---------------------------------
Eval num_timesteps=24453518, episode_reward=-0.01 +/- 0.98
Episode length: 29.99 +/- 0.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.006       |
| time/                   |              |
|    total_timesteps      | 24453518     |
| train/                  |              |
|    approx_kl            | 0.0142895095 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.654       |
|    explained_variance   | 0.337        |
|    learning_rate        | 6.8e-05      |
|    loss                 | 0.0952       |
|    n_updates            | 6170         |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 399      |
|    time_elapsed    | 144486   |
|    total_timesteps | 24514560 |
---------------------------------
Eval num_timesteps=24514959, episode_reward=-0.01 +/- 0.99
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 24514959    |
| train/                  |             |
|    approx_kl            | 0.013856142 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.336       |
|    learning_rate        | 6.79e-05    |
|    loss                 | 0.0953      |
|    n_updates            | 6175        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 400      |
|    time_elapsed    | 144920   |
|    total_timesteps | 24576000 |
---------------------------------
Eval num_timesteps=24576400, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 24576400    |
| train/                  |             |
|    approx_kl            | 0.013906792 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.336       |
|    learning_rate        | 6.79e-05    |
|    loss                 | 0.0697      |
|    n_updates            | 6180        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 401      |
|    time_elapsed    | 145366   |
|    total_timesteps | 24637440 |
---------------------------------
Eval num_timesteps=24637841, episode_reward=-0.07 +/- 0.98
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.07       |
| time/                   |             |
|    total_timesteps      | 24637841    |
| train/                  |             |
|    approx_kl            | 0.014482032 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.321       |
|    learning_rate        | 6.78e-05    |
|    loss                 | 0.107       |
|    n_updates            | 6185        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 402      |
|    time_elapsed    | 145784   |
|    total_timesteps | 24698880 |
---------------------------------
Eval num_timesteps=24699282, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 24699282    |
| train/                  |             |
|    approx_kl            | 0.014364659 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.78e-05    |
|    loss                 | 0.109       |
|    n_updates            | 6190        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 403      |
|    time_elapsed    | 146254   |
|    total_timesteps | 24760320 |
---------------------------------
Eval num_timesteps=24760723, episode_reward=0.00 +/- 0.99
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 24760723    |
| train/                  |             |
|    approx_kl            | 0.013792067 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.333       |
|    learning_rate        | 6.77e-05    |
|    loss                 | 0.0376      |
|    n_updates            | 6195        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 404      |
|    time_elapsed    | 146648   |
|    total_timesteps | 24821760 |
---------------------------------
Eval num_timesteps=24822164, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 24822164    |
| train/                  |             |
|    approx_kl            | 0.014355223 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.77e-05    |
|    loss                 | 0.0767      |
|    n_updates            | 6200        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 405      |
|    time_elapsed    | 147136   |
|    total_timesteps | 24883200 |
---------------------------------
Eval num_timesteps=24883605, episode_reward=0.16 +/- 0.97
Episode length: 30.02 +/- 1.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.16        |
| time/                   |             |
|    total_timesteps      | 24883605    |
| train/                  |             |
|    approx_kl            | 0.013608497 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.333       |
|    learning_rate        | 6.76e-05    |
|    loss                 | 0.0795      |
|    n_updates            | 6205        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.238       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.16
SELFPLAY: new best model, bumping up generation to 12
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 406      |
|    time_elapsed    | 147468   |
|    total_timesteps | 24944640 |
---------------------------------
Eval num_timesteps=24945046, episode_reward=-0.01 +/- 0.99
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 24945046    |
| train/                  |             |
|    approx_kl            | 0.014124066 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.318       |
|    learning_rate        | 6.75e-05    |
|    loss                 | 0.0542      |
|    n_updates            | 6210        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 407      |
|    time_elapsed    | 147789   |
|    total_timesteps | 25006080 |
---------------------------------
Eval num_timesteps=25006487, episode_reward=-0.08 +/- 0.97
Episode length: 29.98 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.082      |
| time/                   |             |
|    total_timesteps      | 25006487    |
| train/                  |             |
|    approx_kl            | 0.014099171 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.31        |
|    learning_rate        | 6.75e-05    |
|    loss                 | 0.0739      |
|    n_updates            | 6215        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 408      |
|    time_elapsed    | 148111   |
|    total_timesteps | 25067520 |
---------------------------------
Eval num_timesteps=25067928, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 25067928    |
| train/                  |             |
|    approx_kl            | 0.013853958 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.341       |
|    learning_rate        | 6.74e-05    |
|    loss                 | 0.0479      |
|    n_updates            | 6220        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 409      |
|    time_elapsed    | 148431   |
|    total_timesteps | 25128960 |
---------------------------------
Eval num_timesteps=25129369, episode_reward=-0.05 +/- 0.99
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.054      |
| time/                   |             |
|    total_timesteps      | 25129369    |
| train/                  |             |
|    approx_kl            | 0.013900071 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.335       |
|    learning_rate        | 6.74e-05    |
|    loss                 | 0.131       |
|    n_updates            | 6225        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 410      |
|    time_elapsed    | 148749   |
|    total_timesteps | 25190400 |
---------------------------------
Eval num_timesteps=25190810, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 25190810    |
| train/                  |             |
|    approx_kl            | 0.013872037 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.73e-05    |
|    loss                 | 0.0814      |
|    n_updates            | 6230        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 411      |
|    time_elapsed    | 149065   |
|    total_timesteps | 25251840 |
---------------------------------
Eval num_timesteps=25252251, episode_reward=0.11 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 25252251    |
| train/                  |             |
|    approx_kl            | 0.013968857 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.322       |
|    learning_rate        | 6.73e-05    |
|    loss                 | 0.0813      |
|    n_updates            | 6235        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 412      |
|    time_elapsed    | 149382   |
|    total_timesteps | 25313280 |
---------------------------------
Eval num_timesteps=25313692, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.018     |
| time/                   |            |
|    total_timesteps      | 25313692   |
| train/                  |            |
|    approx_kl            | 0.01364722 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.639     |
|    explained_variance   | 0.318      |
|    learning_rate        | 6.72e-05   |
|    loss                 | 0.064      |
|    n_updates            | 6240       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 413      |
|    time_elapsed    | 149699   |
|    total_timesteps | 25374720 |
---------------------------------
Eval num_timesteps=25375133, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 25375133    |
| train/                  |             |
|    approx_kl            | 0.013912547 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.72e-05    |
|    loss                 | 0.0597      |
|    n_updates            | 6245        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 414      |
|    time_elapsed    | 150016   |
|    total_timesteps | 25436160 |
---------------------------------
Eval num_timesteps=25436574, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.048        |
| time/                   |              |
|    total_timesteps      | 25436574     |
| train/                  |              |
|    approx_kl            | 0.0141609665 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.634       |
|    explained_variance   | 0.325        |
|    learning_rate        | 6.71e-05     |
|    loss                 | 0.0428       |
|    n_updates            | 6250         |
|    policy_gradient_loss | -0.0172      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 415      |
|    time_elapsed    | 150333   |
|    total_timesteps | 25497600 |
---------------------------------
Eval num_timesteps=25498015, episode_reward=-0.02 +/- 0.99
Episode length: 29.96 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 25498015    |
| train/                  |             |
|    approx_kl            | 0.014288875 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.635      |
|    explained_variance   | 0.33        |
|    learning_rate        | 6.71e-05    |
|    loss                 | 0.119       |
|    n_updates            | 6255        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 416      |
|    time_elapsed    | 150651   |
|    total_timesteps | 25559040 |
---------------------------------
Eval num_timesteps=25559456, episode_reward=-0.00 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.004     |
| time/                   |            |
|    total_timesteps      | 25559456   |
| train/                  |            |
|    approx_kl            | 0.01416348 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.324      |
|    learning_rate        | 6.7e-05    |
|    loss                 | 0.084      |
|    n_updates            | 6260       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 417      |
|    time_elapsed    | 150970   |
|    total_timesteps | 25620480 |
---------------------------------
Eval num_timesteps=25620897, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 25620897    |
| train/                  |             |
|    approx_kl            | 0.013585289 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.632      |
|    explained_variance   | 0.322       |
|    learning_rate        | 6.69e-05    |
|    loss                 | 0.0643      |
|    n_updates            | 6265        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 169      |
|    iterations      | 418      |
|    time_elapsed    | 151291   |
|    total_timesteps | 25681920 |
---------------------------------
Eval num_timesteps=25682338, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 25682338    |
| train/                  |             |
|    approx_kl            | 0.014293085 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.69e-05    |
|    loss                 | 0.0564      |
|    n_updates            | 6270        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 419      |
|    time_elapsed    | 151613   |
|    total_timesteps | 25743360 |
---------------------------------
Eval num_timesteps=25743779, episode_reward=0.07 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 25743779    |
| train/                  |             |
|    approx_kl            | 0.013686349 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.329       |
|    learning_rate        | 6.68e-05    |
|    loss                 | 0.0443      |
|    n_updates            | 6275        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 420      |
|    time_elapsed    | 151934   |
|    total_timesteps | 25804800 |
---------------------------------
Eval num_timesteps=25805220, episode_reward=0.02 +/- 0.97
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 25805220    |
| train/                  |             |
|    approx_kl            | 0.013897182 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.626      |
|    explained_variance   | 0.33        |
|    learning_rate        | 6.68e-05    |
|    loss                 | 0.0654      |
|    n_updates            | 6280        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 421      |
|    time_elapsed    | 152252   |
|    total_timesteps | 25866240 |
---------------------------------
Eval num_timesteps=25866661, episode_reward=-0.01 +/- 0.98
Episode length: 29.95 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 25866661    |
| train/                  |             |
|    approx_kl            | 0.013798132 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.626      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.67e-05    |
|    loss                 | 0.113       |
|    n_updates            | 6285        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 422      |
|    time_elapsed    | 152569   |
|    total_timesteps | 25927680 |
---------------------------------
Eval num_timesteps=25928102, episode_reward=0.11 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 25928102    |
| train/                  |             |
|    approx_kl            | 0.014326821 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.331       |
|    learning_rate        | 6.67e-05    |
|    loss                 | 0.0895      |
|    n_updates            | 6290        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 423      |
|    time_elapsed    | 152886   |
|    total_timesteps | 25989120 |
---------------------------------
Eval num_timesteps=25989543, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 25989543    |
| train/                  |             |
|    approx_kl            | 0.013792106 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.621      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.66e-05    |
|    loss                 | 0.0654      |
|    n_updates            | 6295        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 424      |
|    time_elapsed    | 153223   |
|    total_timesteps | 26050560 |
---------------------------------
Eval num_timesteps=26050984, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 26050984    |
| train/                  |             |
|    approx_kl            | 0.013739972 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.622      |
|    explained_variance   | 0.341       |
|    learning_rate        | 6.66e-05    |
|    loss                 | 0.0525      |
|    n_updates            | 6300        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 425      |
|    time_elapsed    | 153582   |
|    total_timesteps | 26112000 |
---------------------------------
Eval num_timesteps=26112425, episode_reward=-0.00 +/- 0.98
Episode length: 29.99 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 26112425    |
| train/                  |             |
|    approx_kl            | 0.013811716 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.62       |
|    explained_variance   | 0.333       |
|    learning_rate        | 6.65e-05    |
|    loss                 | 0.0599      |
|    n_updates            | 6305        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 426      |
|    time_elapsed    | 153947   |
|    total_timesteps | 26173440 |
---------------------------------
Eval num_timesteps=26173866, episode_reward=-0.00 +/- 0.98
Episode length: 29.93 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 26173866    |
| train/                  |             |
|    approx_kl            | 0.013717163 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.62       |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.64e-05    |
|    loss                 | 0.11        |
|    n_updates            | 6310        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 427      |
|    time_elapsed    | 154368   |
|    total_timesteps | 26234880 |
---------------------------------
Eval num_timesteps=26235307, episode_reward=-0.02 +/- 0.99
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 26235307    |
| train/                  |             |
|    approx_kl            | 0.013393154 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.62       |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.64e-05    |
|    loss                 | 0.0843      |
|    n_updates            | 6315        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 428      |
|    time_elapsed    | 154866   |
|    total_timesteps | 26296320 |
---------------------------------
Eval num_timesteps=26296748, episode_reward=0.09 +/- 0.98
Episode length: 30.08 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 26296748    |
| train/                  |             |
|    approx_kl            | 0.013766497 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.617      |
|    explained_variance   | 0.317       |
|    learning_rate        | 6.63e-05    |
|    loss                 | 0.0607      |
|    n_updates            | 6320        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 429      |
|    time_elapsed    | 155229   |
|    total_timesteps | 26357760 |
---------------------------------
Eval num_timesteps=26358189, episode_reward=0.01 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 26358189    |
| train/                  |             |
|    approx_kl            | 0.013119827 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.337       |
|    learning_rate        | 6.63e-05    |
|    loss                 | 0.0925      |
|    n_updates            | 6325        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 430      |
|    time_elapsed    | 155597   |
|    total_timesteps | 26419200 |
---------------------------------
Eval num_timesteps=26419630, episode_reward=0.09 +/- 0.98
Episode length: 29.95 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 26419630    |
| train/                  |             |
|    approx_kl            | 0.013706463 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.62e-05    |
|    loss                 | 0.0653      |
|    n_updates            | 6330        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 431      |
|    time_elapsed    | 156013   |
|    total_timesteps | 26480640 |
---------------------------------
Eval num_timesteps=26481071, episode_reward=0.11 +/- 0.97
Episode length: 30.04 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 26481071    |
| train/                  |             |
|    approx_kl            | 0.013440312 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.317       |
|    learning_rate        | 6.62e-05    |
|    loss                 | 0.068       |
|    n_updates            | 6335        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 432      |
|    time_elapsed    | 156505   |
|    total_timesteps | 26542080 |
---------------------------------
Eval num_timesteps=26542512, episode_reward=0.03 +/- 0.99
Episode length: 29.94 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 26542512    |
| train/                  |             |
|    approx_kl            | 0.013759387 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.32        |
|    learning_rate        | 6.61e-05    |
|    loss                 | 0.0685      |
|    n_updates            | 6340        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 433      |
|    time_elapsed    | 156880   |
|    total_timesteps | 26603520 |
---------------------------------
Eval num_timesteps=26603953, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 26603953    |
| train/                  |             |
|    approx_kl            | 0.013561507 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.335       |
|    learning_rate        | 6.61e-05    |
|    loss                 | 0.0945      |
|    n_updates            | 6345        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 434      |
|    time_elapsed    | 157240   |
|    total_timesteps | 26664960 |
---------------------------------
Eval num_timesteps=26665394, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 26665394    |
| train/                  |             |
|    approx_kl            | 0.013345009 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.6e-05     |
|    loss                 | 0.0652      |
|    n_updates            | 6350        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 435      |
|    time_elapsed    | 157678   |
|    total_timesteps | 26726400 |
---------------------------------
Eval num_timesteps=26726835, episode_reward=0.10 +/- 0.98
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 26726835    |
| train/                  |             |
|    approx_kl            | 0.013684318 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.337       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0935      |
|    n_updates            | 6355        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 436      |
|    time_elapsed    | 158129   |
|    total_timesteps | 26787840 |
---------------------------------
Eval num_timesteps=26788276, episode_reward=0.05 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 26788276    |
| train/                  |             |
|    approx_kl            | 0.013620437 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0729      |
|    n_updates            | 6360        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 437      |
|    time_elapsed    | 158526   |
|    total_timesteps | 26849280 |
---------------------------------
Eval num_timesteps=26849717, episode_reward=-0.01 +/- 0.99
Episode length: 30.01 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.008     |
| time/                   |            |
|    total_timesteps      | 26849717   |
| train/                  |            |
|    approx_kl            | 0.01369102 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.605     |
|    explained_variance   | 0.335      |
|    learning_rate        | 6.58e-05   |
|    loss                 | 0.0655     |
|    n_updates            | 6365       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 438      |
|    time_elapsed    | 158884   |
|    total_timesteps | 26910720 |
---------------------------------
Eval num_timesteps=26911158, episode_reward=0.05 +/- 0.99
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 26911158    |
| train/                  |             |
|    approx_kl            | 0.013741513 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.58e-05    |
|    loss                 | 0.0929      |
|    n_updates            | 6370        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 439      |
|    time_elapsed    | 159351   |
|    total_timesteps | 26972160 |
---------------------------------
Eval num_timesteps=26972599, episode_reward=0.05 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 26972599    |
| train/                  |             |
|    approx_kl            | 0.013882697 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.345       |
|    learning_rate        | 6.57e-05    |
|    loss                 | 0.0944      |
|    n_updates            | 6375        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 440      |
|    time_elapsed    | 159744   |
|    total_timesteps | 27033600 |
---------------------------------
Eval num_timesteps=27034040, episode_reward=0.01 +/- 0.99
Episode length: 29.99 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 27034040    |
| train/                  |             |
|    approx_kl            | 0.013005615 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.334       |
|    learning_rate        | 6.57e-05    |
|    loss                 | 0.093       |
|    n_updates            | 6380        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 441      |
|    time_elapsed    | 160177   |
|    total_timesteps | 27095040 |
---------------------------------
Eval num_timesteps=27095481, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.114      |
| time/                   |            |
|    total_timesteps      | 27095481   |
| train/                  |            |
|    approx_kl            | 0.01343287 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.602     |
|    explained_variance   | 0.341      |
|    learning_rate        | 6.56e-05   |
|    loss                 | 0.0683     |
|    n_updates            | 6385       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 169      |
|    iterations      | 442      |
|    time_elapsed    | 160548   |
|    total_timesteps | 27156480 |
---------------------------------
Eval num_timesteps=27156922, episode_reward=0.06 +/- 0.99
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 27156922    |
| train/                  |             |
|    approx_kl            | 0.013393337 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.343       |
|    learning_rate        | 6.56e-05    |
|    loss                 | 0.0733      |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 443      |
|    time_elapsed    | 161014   |
|    total_timesteps | 27217920 |
---------------------------------
Eval num_timesteps=27218363, episode_reward=0.05 +/- 0.98
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 27218363    |
| train/                  |             |
|    approx_kl            | 0.013499535 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.339       |
|    learning_rate        | 6.55e-05    |
|    loss                 | 0.0672      |
|    n_updates            | 6395        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 444      |
|    time_elapsed    | 161375   |
|    total_timesteps | 27279360 |
---------------------------------
Eval num_timesteps=27279804, episode_reward=0.09 +/- 0.97
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 27279804    |
| train/                  |             |
|    approx_kl            | 0.014016219 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.32        |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0816      |
|    n_updates            | 6400        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 445      |
|    time_elapsed    | 161809   |
|    total_timesteps | 27340800 |
---------------------------------
Eval num_timesteps=27341245, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 27341245    |
| train/                  |             |
|    approx_kl            | 0.013287387 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.343       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0736      |
|    n_updates            | 6405        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 446      |
|    time_elapsed    | 162223   |
|    total_timesteps | 27402240 |
---------------------------------
Eval num_timesteps=27402686, episode_reward=-0.00 +/- 0.98
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 27402686    |
| train/                  |             |
|    approx_kl            | 0.013342118 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.344       |
|    learning_rate        | 6.53e-05    |
|    loss                 | 0.0546      |
|    n_updates            | 6410        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 447      |
|    time_elapsed    | 162667   |
|    total_timesteps | 27463680 |
---------------------------------
Eval num_timesteps=27464127, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.084      |
| time/                   |            |
|    total_timesteps      | 27464127   |
| train/                  |            |
|    approx_kl            | 0.01398552 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.588     |
|    explained_variance   | 0.352      |
|    learning_rate        | 6.53e-05   |
|    loss                 | 0.0882     |
|    n_updates            | 6415       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.23       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 448      |
|    time_elapsed    | 163028   |
|    total_timesteps | 27525120 |
---------------------------------
Eval num_timesteps=27525568, episode_reward=0.17 +/- 0.97
Episode length: 30.06 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.168       |
| time/                   |             |
|    total_timesteps      | 27525568    |
| train/                  |             |
|    approx_kl            | 0.013466728 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.353       |
|    learning_rate        | 6.52e-05    |
|    loss                 | 0.085       |
|    n_updates            | 6420        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.234       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.168
SELFPLAY: new best model, bumping up generation to 13
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.19    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 449      |
|    time_elapsed    | 163446   |
|    total_timesteps | 27586560 |
---------------------------------
Eval num_timesteps=27587009, episode_reward=0.00 +/- 0.99
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 27587009    |
| train/                  |             |
|    approx_kl            | 0.013181149 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.332       |
|    learning_rate        | 6.52e-05    |
|    loss                 | 0.0766      |
|    n_updates            | 6425        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 450      |
|    time_elapsed    | 163906   |
|    total_timesteps | 27648000 |
---------------------------------
Eval num_timesteps=27648450, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 27648450    |
| train/                  |             |
|    approx_kl            | 0.012936885 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.329       |
|    learning_rate        | 6.51e-05    |
|    loss                 | 0.0875      |
|    n_updates            | 6430        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 451      |
|    time_elapsed    | 164319   |
|    total_timesteps | 27709440 |
---------------------------------
Eval num_timesteps=27709891, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 27709891    |
| train/                  |             |
|    approx_kl            | 0.013302078 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.347       |
|    learning_rate        | 6.51e-05    |
|    loss                 | 0.099       |
|    n_updates            | 6435        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 452      |
|    time_elapsed    | 164686   |
|    total_timesteps | 27770880 |
---------------------------------
Eval num_timesteps=27771332, episode_reward=-0.03 +/- 0.97
Episode length: 29.96 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 27771332    |
| train/                  |             |
|    approx_kl            | 0.013158834 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.54       |
|    explained_variance   | 0.348       |
|    learning_rate        | 6.5e-05     |
|    loss                 | 0.0599      |
|    n_updates            | 6440        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 453      |
|    time_elapsed    | 165098   |
|    total_timesteps | 27832320 |
---------------------------------
Eval num_timesteps=27832773, episode_reward=-0.02 +/- 0.98
Episode length: 29.94 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.016     |
| time/                   |            |
|    total_timesteps      | 27832773   |
| train/                  |            |
|    approx_kl            | 0.01307785 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.536     |
|    explained_variance   | 0.33       |
|    learning_rate        | 6.5e-05    |
|    loss                 | 0.126      |
|    n_updates            | 6445       |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 454      |
|    time_elapsed    | 165586   |
|    total_timesteps | 27893760 |
---------------------------------
Eval num_timesteps=27894214, episode_reward=-0.05 +/- 0.99
Episode length: 30.00 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.052      |
| time/                   |             |
|    total_timesteps      | 27894214    |
| train/                  |             |
|    approx_kl            | 0.012989178 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.49e-05    |
|    loss                 | 0.0888      |
|    n_updates            | 6450        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 455      |
|    time_elapsed    | 165976   |
|    total_timesteps | 27955200 |
---------------------------------
Eval num_timesteps=27955655, episode_reward=-0.03 +/- 0.98
Episode length: 29.96 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.032      |
| time/                   |             |
|    total_timesteps      | 27955655    |
| train/                  |             |
|    approx_kl            | 0.012960286 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.48e-05    |
|    loss                 | 0.0958      |
|    n_updates            | 6455        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 456      |
|    time_elapsed    | 166338   |
|    total_timesteps | 28016640 |
---------------------------------
Eval num_timesteps=28017096, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 28017096    |
| train/                  |             |
|    approx_kl            | 0.012720474 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.348       |
|    learning_rate        | 6.48e-05    |
|    loss                 | 0.106       |
|    n_updates            | 6460        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 457      |
|    time_elapsed    | 166730   |
|    total_timesteps | 28078080 |
---------------------------------
Eval num_timesteps=28078537, episode_reward=-0.01 +/- 0.99
Episode length: 29.92 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | -0.006       |
| time/                   |              |
|    total_timesteps      | 28078537     |
| train/                  |              |
|    approx_kl            | 0.0131989615 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.539       |
|    explained_variance   | 0.329        |
|    learning_rate        | 6.47e-05     |
|    loss                 | 0.0883       |
|    n_updates            | 6465         |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 458      |
|    time_elapsed    | 167241   |
|    total_timesteps | 28139520 |
---------------------------------
Eval num_timesteps=28139978, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 28139978    |
| train/                  |             |
|    approx_kl            | 0.012880055 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.546      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.47e-05    |
|    loss                 | 0.0741      |
|    n_updates            | 6470        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 459      |
|    time_elapsed    | 167619   |
|    total_timesteps | 28200960 |
---------------------------------
Eval num_timesteps=28201419, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.064        |
| time/                   |              |
|    total_timesteps      | 28201419     |
| train/                  |              |
|    approx_kl            | 0.0130375745 |
|    clip_fraction        | 0.144        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.542       |
|    explained_variance   | 0.347        |
|    learning_rate        | 6.46e-05     |
|    loss                 | 0.118        |
|    n_updates            | 6475         |
|    policy_gradient_loss | -0.0164      |
|    value_loss           | 0.231        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 460      |
|    time_elapsed    | 167985   |
|    total_timesteps | 28262400 |
---------------------------------
Eval num_timesteps=28262860, episode_reward=-0.02 +/- 0.98
Episode length: 29.98 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 28262860    |
| train/                  |             |
|    approx_kl            | 0.012566639 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.544      |
|    explained_variance   | 0.335       |
|    learning_rate        | 6.46e-05    |
|    loss                 | 0.0799      |
|    n_updates            | 6480        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 461      |
|    time_elapsed    | 168356   |
|    total_timesteps | 28323840 |
---------------------------------
Eval num_timesteps=28324301, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 28324301    |
| train/                  |             |
|    approx_kl            | 0.013394005 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.544      |
|    explained_variance   | 0.312       |
|    learning_rate        | 6.45e-05    |
|    loss                 | 0.0882      |
|    n_updates            | 6485        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 462      |
|    time_elapsed    | 168892   |
|    total_timesteps | 28385280 |
---------------------------------
Eval num_timesteps=28385742, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 28385742    |
| train/                  |             |
|    approx_kl            | 0.012797837 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.33        |
|    learning_rate        | 6.45e-05    |
|    loss                 | 0.0945      |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 463      |
|    time_elapsed    | 169271   |
|    total_timesteps | 28446720 |
---------------------------------
Eval num_timesteps=28447183, episode_reward=0.01 +/- 0.99
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 28447183    |
| train/                  |             |
|    approx_kl            | 0.012911525 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.44e-05    |
|    loss                 | 0.0678      |
|    n_updates            | 6495        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 168      |
|    iterations      | 464      |
|    time_elapsed    | 169641   |
|    total_timesteps | 28508160 |
---------------------------------
Eval num_timesteps=28508624, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 1.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 28508624    |
| train/                  |             |
|    approx_kl            | 0.012700494 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.547      |
|    explained_variance   | 0.324       |
|    learning_rate        | 6.43e-05    |
|    loss                 | 0.0935      |
|    n_updates            | 6500        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 465      |
|    time_elapsed    | 170015   |
|    total_timesteps | 28569600 |
---------------------------------
Eval num_timesteps=28570065, episode_reward=0.08 +/- 0.98
Episode length: 30.06 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 28570065    |
| train/                  |             |
|    approx_kl            | 0.013160622 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.341       |
|    learning_rate        | 6.43e-05    |
|    loss                 | 0.0863      |
|    n_updates            | 6505        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 466      |
|    time_elapsed    | 170553   |
|    total_timesteps | 28631040 |
---------------------------------
Eval num_timesteps=28631506, episode_reward=0.14 +/- 0.97
Episode length: 30.01 +/- 0.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.142        |
| time/                   |              |
|    total_timesteps      | 28631506     |
| train/                  |              |
|    approx_kl            | 0.0128219845 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.542       |
|    explained_variance   | 0.332        |
|    learning_rate        | 6.42e-05     |
|    loss                 | 0.125        |
|    n_updates            | 6510         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.237        |
------------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.142
SELFPLAY: new best model, bumping up generation to 14
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 467      |
|    time_elapsed    | 170920   |
|    total_timesteps | 28692480 |
---------------------------------
Eval num_timesteps=28692947, episode_reward=0.04 +/- 0.99
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 28692947    |
| train/                  |             |
|    approx_kl            | 0.012551004 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.555      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.42e-05    |
|    loss                 | 0.0479      |
|    n_updates            | 6515        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 468      |
|    time_elapsed    | 171286   |
|    total_timesteps | 28753920 |
---------------------------------
Eval num_timesteps=28754388, episode_reward=0.02 +/- 0.97
Episode length: 29.96 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 28754388    |
| train/                  |             |
|    approx_kl            | 0.013296706 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.339       |
|    learning_rate        | 6.41e-05    |
|    loss                 | 0.0413      |
|    n_updates            | 6520        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 469      |
|    time_elapsed    | 171668   |
|    total_timesteps | 28815360 |
---------------------------------
Eval num_timesteps=28815829, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 28815829    |
| train/                  |             |
|    approx_kl            | 0.013286577 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.331       |
|    learning_rate        | 6.41e-05    |
|    loss                 | 0.0619      |
|    n_updates            | 6525        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 470      |
|    time_elapsed    | 172181   |
|    total_timesteps | 28876800 |
---------------------------------
Eval num_timesteps=28877270, episode_reward=0.05 +/- 0.98
Episode length: 30.02 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.048        |
| time/                   |              |
|    total_timesteps      | 28877270     |
| train/                  |              |
|    approx_kl            | 0.0127788745 |
|    clip_fraction        | 0.148        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.572       |
|    explained_variance   | 0.33         |
|    learning_rate        | 6.4e-05      |
|    loss                 | 0.0703       |
|    n_updates            | 6530         |
|    policy_gradient_loss | -0.0164      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 471      |
|    time_elapsed    | 172565   |
|    total_timesteps | 28938240 |
---------------------------------
Eval num_timesteps=28938711, episode_reward=0.02 +/- 0.99
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 28938711    |
| train/                  |             |
|    approx_kl            | 0.013334819 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.4e-05     |
|    loss                 | 0.0915      |
|    n_updates            | 6535        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 167      |
|    iterations      | 472      |
|    time_elapsed    | 172931   |
|    total_timesteps | 28999680 |
---------------------------------
Eval num_timesteps=29000152, episode_reward=-0.02 +/- 0.99
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 29000152    |
| train/                  |             |
|    approx_kl            | 0.012572644 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.566      |
|    explained_variance   | 0.348       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 0.0803      |
|    n_updates            | 6540        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 473      |
|    time_elapsed    | 173322   |
|    total_timesteps | 29061120 |
---------------------------------
Eval num_timesteps=29061593, episode_reward=0.00 +/- 0.98
Episode length: 29.94 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 29061593    |
| train/                  |             |
|    approx_kl            | 0.012987747 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.333       |
|    learning_rate        | 6.38e-05    |
|    loss                 | 0.12        |
|    n_updates            | 6545        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 167      |
|    iterations      | 474      |
|    time_elapsed    | 173801   |
|    total_timesteps | 29122560 |
---------------------------------
Eval num_timesteps=29123034, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 29123034    |
| train/                  |             |
|    approx_kl            | 0.012962451 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.329       |
|    learning_rate        | 6.38e-05    |
|    loss                 | 0.0774      |
|    n_updates            | 6550        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 475      |
|    time_elapsed    | 174224   |
|    total_timesteps | 29184000 |
---------------------------------
Eval num_timesteps=29184475, episode_reward=0.02 +/- 0.98
Episode length: 29.93 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 29184475   |
| train/                  |            |
|    approx_kl            | 0.01283014 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.577     |
|    explained_variance   | 0.32       |
|    learning_rate        | 6.37e-05   |
|    loss                 | 0.0561     |
|    n_updates            | 6555       |
|    policy_gradient_loss | -0.0164    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 476      |
|    time_elapsed    | 174586   |
|    total_timesteps | 29245440 |
---------------------------------
Eval num_timesteps=29245916, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 1.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 29245916    |
| train/                  |             |
|    approx_kl            | 0.013370734 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.315       |
|    learning_rate        | 6.37e-05    |
|    loss                 | 0.0641      |
|    n_updates            | 6560        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 477      |
|    time_elapsed    | 174976   |
|    total_timesteps | 29306880 |
---------------------------------
Eval num_timesteps=29307357, episode_reward=0.03 +/- 0.99
Episode length: 30.03 +/- 0.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 29307357    |
| train/                  |             |
|    approx_kl            | 0.013160374 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.36e-05    |
|    loss                 | 0.105       |
|    n_updates            | 6565        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 478      |
|    time_elapsed    | 175442   |
|    total_timesteps | 29368320 |
---------------------------------
Eval num_timesteps=29368798, episode_reward=0.03 +/- 0.99
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.032      |
| time/                   |            |
|    total_timesteps      | 29368798   |
| train/                  |            |
|    approx_kl            | 0.01251001 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.588     |
|    explained_variance   | 0.33       |
|    learning_rate        | 6.36e-05   |
|    loss                 | 0.07       |
|    n_updates            | 6570       |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 479      |
|    time_elapsed    | 175822   |
|    total_timesteps | 29429760 |
---------------------------------
Eval num_timesteps=29430239, episode_reward=-0.01 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 29430239    |
| train/                  |             |
|    approx_kl            | 0.013064878 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.583      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.35e-05    |
|    loss                 | 0.0688      |
|    n_updates            | 6575        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 167      |
|    iterations      | 480      |
|    time_elapsed    | 176140   |
|    total_timesteps | 29491200 |
---------------------------------
Eval num_timesteps=29491680, episode_reward=-0.02 +/- 0.98
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 29491680    |
| train/                  |             |
|    approx_kl            | 0.013614644 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.311       |
|    learning_rate        | 6.35e-05    |
|    loss                 | 0.0639      |
|    n_updates            | 6580        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 481      |
|    time_elapsed    | 176458   |
|    total_timesteps | 29552640 |
---------------------------------
Eval num_timesteps=29553121, episode_reward=0.01 +/- 0.98
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 29553121    |
| train/                  |             |
|    approx_kl            | 0.012987697 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.34e-05    |
|    loss                 | 0.048       |
|    n_updates            | 6585        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 482      |
|    time_elapsed    | 176777   |
|    total_timesteps | 29614080 |
---------------------------------
Eval num_timesteps=29614562, episode_reward=0.00 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 29614562    |
| train/                  |             |
|    approx_kl            | 0.013607457 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.318       |
|    learning_rate        | 6.33e-05    |
|    loss                 | 0.0633      |
|    n_updates            | 6590        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 167      |
|    iterations      | 483      |
|    time_elapsed    | 177098   |
|    total_timesteps | 29675520 |
---------------------------------
Eval num_timesteps=29676003, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 29676003    |
| train/                  |             |
|    approx_kl            | 0.013295816 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.322       |
|    learning_rate        | 6.33e-05    |
|    loss                 | 0.093       |
|    n_updates            | 6595        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 484      |
|    time_elapsed    | 177420   |
|    total_timesteps | 29736960 |
---------------------------------
Eval num_timesteps=29737444, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 29737444    |
| train/                  |             |
|    approx_kl            | 0.012942392 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.571      |
|    explained_variance   | 0.341       |
|    learning_rate        | 6.32e-05    |
|    loss                 | 0.0345      |
|    n_updates            | 6600        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 485      |
|    time_elapsed    | 177743   |
|    total_timesteps | 29798400 |
---------------------------------
Eval num_timesteps=29798885, episode_reward=-0.01 +/- 0.98
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 29798885    |
| train/                  |             |
|    approx_kl            | 0.013019248 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.343       |
|    learning_rate        | 6.32e-05    |
|    loss                 | 0.0788      |
|    n_updates            | 6605        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 486      |
|    time_elapsed    | 178066   |
|    total_timesteps | 29859840 |
---------------------------------
Eval num_timesteps=29860326, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 29860326    |
| train/                  |             |
|    approx_kl            | 0.013101735 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.574      |
|    explained_variance   | 0.336       |
|    learning_rate        | 6.31e-05    |
|    loss                 | 0.0914      |
|    n_updates            | 6610        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 487      |
|    time_elapsed    | 178386   |
|    total_timesteps | 29921280 |
---------------------------------
Eval num_timesteps=29921767, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 29921767    |
| train/                  |             |
|    approx_kl            | 0.012891298 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.574      |
|    explained_variance   | 0.34        |
|    learning_rate        | 6.31e-05    |
|    loss                 | 0.0683      |
|    n_updates            | 6615        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 488      |
|    time_elapsed    | 178705   |
|    total_timesteps | 29982720 |
---------------------------------
Eval num_timesteps=29983208, episode_reward=-0.00 +/- 0.97
Episode length: 29.97 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 29983208   |
| train/                  |            |
|    approx_kl            | 0.01303931 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.581     |
|    explained_variance   | 0.338      |
|    learning_rate        | 6.3e-05    |
|    loss                 | 0.105      |
|    n_updates            | 6620       |
|    policy_gradient_loss | -0.0165    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 167      |
|    iterations      | 489      |
|    time_elapsed    | 179022   |
|    total_timesteps | 30044160 |
---------------------------------
Eval num_timesteps=30044649, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.006     |
| time/                   |            |
|    total_timesteps      | 30044649   |
| train/                  |            |
|    approx_kl            | 0.01305023 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.576     |
|    explained_variance   | 0.33       |
|    learning_rate        | 6.3e-05    |
|    loss                 | 0.0556     |
|    n_updates            | 6625       |
|    policy_gradient_loss | -0.0165    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 490      |
|    time_elapsed    | 179340   |
|    total_timesteps | 30105600 |
---------------------------------
Eval num_timesteps=30106090, episode_reward=0.05 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 30106090    |
| train/                  |             |
|    approx_kl            | 0.013325051 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.341       |
|    learning_rate        | 6.29e-05    |
|    loss                 | 0.0784      |
|    n_updates            | 6630        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 491      |
|    time_elapsed    | 179657   |
|    total_timesteps | 30167040 |
---------------------------------
Eval num_timesteps=30167531, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.034        |
| time/                   |              |
|    total_timesteps      | 30167531     |
| train/                  |              |
|    approx_kl            | 0.0128066195 |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.577       |
|    explained_variance   | 0.326        |
|    learning_rate        | 6.28e-05     |
|    loss                 | 0.0575       |
|    n_updates            | 6635         |
|    policy_gradient_loss | -0.0166      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 492      |
|    time_elapsed    | 179975   |
|    total_timesteps | 30228480 |
---------------------------------
Eval num_timesteps=30228972, episode_reward=-0.06 +/- 0.98
Episode length: 29.93 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.06       |
| time/                   |             |
|    total_timesteps      | 30228972    |
| train/                  |             |
|    approx_kl            | 0.012723009 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.321       |
|    learning_rate        | 6.28e-05    |
|    loss                 | 0.0654      |
|    n_updates            | 6640        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 493      |
|    time_elapsed    | 180293   |
|    total_timesteps | 30289920 |
---------------------------------
Eval num_timesteps=30290413, episode_reward=0.14 +/- 0.97
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 30290413    |
| train/                  |             |
|    approx_kl            | 0.013077549 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.27e-05    |
|    loss                 | 0.0868      |
|    n_updates            | 6645        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.241       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 15
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 494      |
|    time_elapsed    | 180613   |
|    total_timesteps | 30351360 |
---------------------------------
Eval num_timesteps=30351854, episode_reward=-0.02 +/- 0.99
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 30351854    |
| train/                  |             |
|    approx_kl            | 0.012748318 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.335       |
|    learning_rate        | 6.27e-05    |
|    loss                 | 0.0502      |
|    n_updates            | 6650        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 495      |
|    time_elapsed    | 180935   |
|    total_timesteps | 30412800 |
---------------------------------
Eval num_timesteps=30413295, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 30413295    |
| train/                  |             |
|    approx_kl            | 0.012835762 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.315       |
|    learning_rate        | 6.26e-05    |
|    loss                 | 0.0596      |
|    n_updates            | 6655        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 496      |
|    time_elapsed    | 181257   |
|    total_timesteps | 30474240 |
---------------------------------
Eval num_timesteps=30474736, episode_reward=0.04 +/- 0.98
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 30474736    |
| train/                  |             |
|    approx_kl            | 0.012525286 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.342       |
|    learning_rate        | 6.26e-05    |
|    loss                 | 0.0686      |
|    n_updates            | 6660        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 497      |
|    time_elapsed    | 181582   |
|    total_timesteps | 30535680 |
---------------------------------
Eval num_timesteps=30536177, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 30536177    |
| train/                  |             |
|    approx_kl            | 0.012824555 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.33        |
|    learning_rate        | 6.25e-05    |
|    loss                 | 0.0521      |
|    n_updates            | 6665        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 168      |
|    iterations      | 498      |
|    time_elapsed    | 181904   |
|    total_timesteps | 30597120 |
---------------------------------
Eval num_timesteps=30597618, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 30597618    |
| train/                  |             |
|    approx_kl            | 0.012725503 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.324       |
|    learning_rate        | 6.25e-05    |
|    loss                 | 0.0821      |
|    n_updates            | 6670        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.19    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 499      |
|    time_elapsed    | 182223   |
|    total_timesteps | 30658560 |
---------------------------------
Eval num_timesteps=30659059, episode_reward=-0.03 +/- 0.98
Episode length: 29.95 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.034      |
| time/                   |             |
|    total_timesteps      | 30659059    |
| train/                  |             |
|    approx_kl            | 0.012877548 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.24e-05    |
|    loss                 | 0.0992      |
|    n_updates            | 6675        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 500      |
|    time_elapsed    | 182541   |
|    total_timesteps | 30720000 |
---------------------------------
Eval num_timesteps=30720500, episode_reward=0.01 +/- 0.99
Episode length: 29.94 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 30720500    |
| train/                  |             |
|    approx_kl            | 0.013239233 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.24e-05    |
|    loss                 | 0.074       |
|    n_updates            | 6680        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 501      |
|    time_elapsed    | 182859   |
|    total_timesteps | 30781440 |
---------------------------------
Eval num_timesteps=30781941, episode_reward=-0.04 +/- 0.99
Episode length: 30.02 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 30781941    |
| train/                  |             |
|    approx_kl            | 0.013030204 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.322       |
|    learning_rate        | 6.23e-05    |
|    loss                 | 0.0985      |
|    n_updates            | 6685        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 502      |
|    time_elapsed    | 183177   |
|    total_timesteps | 30842880 |
---------------------------------
Eval num_timesteps=30843382, episode_reward=0.07 +/- 0.98
Episode length: 29.89 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 30843382    |
| train/                  |             |
|    approx_kl            | 0.012763756 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.335       |
|    learning_rate        | 6.22e-05    |
|    loss                 | 0.0763      |
|    n_updates            | 6690        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 503      |
|    time_elapsed    | 183495   |
|    total_timesteps | 30904320 |
---------------------------------
Eval num_timesteps=30904823, episode_reward=0.03 +/- 0.99
Episode length: 29.99 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 30904823    |
| train/                  |             |
|    approx_kl            | 0.012796477 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.329       |
|    learning_rate        | 6.22e-05    |
|    loss                 | 0.0596      |
|    n_updates            | 6695        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 504      |
|    time_elapsed    | 183813   |
|    total_timesteps | 30965760 |
---------------------------------
Eval num_timesteps=30966264, episode_reward=0.04 +/- 0.99
Episode length: 30.02 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 30966264    |
| train/                  |             |
|    approx_kl            | 0.013101882 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.315       |
|    learning_rate        | 6.21e-05    |
|    loss                 | 0.0608      |
|    n_updates            | 6700        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 505      |
|    time_elapsed    | 184132   |
|    total_timesteps | 31027200 |
---------------------------------
Eval num_timesteps=31027705, episode_reward=0.10 +/- 0.98
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 31027705    |
| train/                  |             |
|    approx_kl            | 0.013237284 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.315       |
|    learning_rate        | 6.21e-05    |
|    loss                 | 0.0762      |
|    n_updates            | 6705        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 506      |
|    time_elapsed    | 184452   |
|    total_timesteps | 31088640 |
---------------------------------
Eval num_timesteps=31089146, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 31089146    |
| train/                  |             |
|    approx_kl            | 0.012771689 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.327       |
|    learning_rate        | 6.2e-05     |
|    loss                 | 0.0821      |
|    n_updates            | 6710        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 507      |
|    time_elapsed    | 184774   |
|    total_timesteps | 31150080 |
---------------------------------
Eval num_timesteps=31150587, episode_reward=-0.03 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 31150587    |
| train/                  |             |
|    approx_kl            | 0.013150237 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.348       |
|    learning_rate        | 6.2e-05     |
|    loss                 | 0.0916      |
|    n_updates            | 6715        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 508      |
|    time_elapsed    | 185097   |
|    total_timesteps | 31211520 |
---------------------------------
Eval num_timesteps=31212028, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 31212028    |
| train/                  |             |
|    approx_kl            | 0.012848658 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.613      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.19e-05    |
|    loss                 | 0.0745      |
|    n_updates            | 6720        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 509      |
|    time_elapsed    | 185420   |
|    total_timesteps | 31272960 |
---------------------------------
Eval num_timesteps=31273469, episode_reward=0.01 +/- 0.99
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 31273469    |
| train/                  |             |
|    approx_kl            | 0.013321349 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.321       |
|    learning_rate        | 6.19e-05    |
|    loss                 | 0.0397      |
|    n_updates            | 6725        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 510      |
|    time_elapsed    | 185741   |
|    total_timesteps | 31334400 |
---------------------------------
Eval num_timesteps=31334910, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.024      |
| time/                   |             |
|    total_timesteps      | 31334910    |
| train/                  |             |
|    approx_kl            | 0.013012493 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.18e-05    |
|    loss                 | 0.0759      |
|    n_updates            | 6730        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 511      |
|    time_elapsed    | 186061   |
|    total_timesteps | 31395840 |
---------------------------------
Eval num_timesteps=31396351, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 31396351    |
| train/                  |             |
|    approx_kl            | 0.013230761 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.319       |
|    learning_rate        | 6.17e-05    |
|    loss                 | 0.0871      |
|    n_updates            | 6735        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 512      |
|    time_elapsed    | 186379   |
|    total_timesteps | 31457280 |
---------------------------------
Eval num_timesteps=31457792, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 31457792    |
| train/                  |             |
|    approx_kl            | 0.012641293 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.334       |
|    learning_rate        | 6.17e-05    |
|    loss                 | 0.073       |
|    n_updates            | 6740        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 513      |
|    time_elapsed    | 186697   |
|    total_timesteps | 31518720 |
---------------------------------
Eval num_timesteps=31519233, episode_reward=0.07 +/- 0.98
Episode length: 30.03 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 31519233    |
| train/                  |             |
|    approx_kl            | 0.012903963 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.16e-05    |
|    loss                 | 0.0877      |
|    n_updates            | 6745        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 514      |
|    time_elapsed    | 187015   |
|    total_timesteps | 31580160 |
---------------------------------
Eval num_timesteps=31580674, episode_reward=-0.00 +/- 0.98
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 31580674    |
| train/                  |             |
|    approx_kl            | 0.012758728 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.317       |
|    learning_rate        | 6.16e-05    |
|    loss                 | 0.0343      |
|    n_updates            | 6750        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 515      |
|    time_elapsed    | 187333   |
|    total_timesteps | 31641600 |
---------------------------------
Eval num_timesteps=31642115, episode_reward=-0.02 +/- 0.98
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 31642115    |
| train/                  |             |
|    approx_kl            | 0.013081485 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.337       |
|    learning_rate        | 6.15e-05    |
|    loss                 | 0.0597      |
|    n_updates            | 6755        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 516      |
|    time_elapsed    | 187651   |
|    total_timesteps | 31703040 |
---------------------------------
Eval num_timesteps=31703556, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 31703556    |
| train/                  |             |
|    approx_kl            | 0.012834464 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.15e-05    |
|    loss                 | 0.123       |
|    n_updates            | 6760        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 517      |
|    time_elapsed    | 187970   |
|    total_timesteps | 31764480 |
---------------------------------
Eval num_timesteps=31764997, episode_reward=-0.00 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 31764997    |
| train/                  |             |
|    approx_kl            | 0.012806362 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.335       |
|    learning_rate        | 6.14e-05    |
|    loss                 | 0.112       |
|    n_updates            | 6765        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 518      |
|    time_elapsed    | 188291   |
|    total_timesteps | 31825920 |
---------------------------------
Eval num_timesteps=31826438, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.004        |
| time/                   |              |
|    total_timesteps      | 31826438     |
| train/                  |              |
|    approx_kl            | 0.0130905425 |
|    clip_fraction        | 0.151        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.612       |
|    explained_variance   | 0.32         |
|    learning_rate        | 6.14e-05     |
|    loss                 | 0.103        |
|    n_updates            | 6770         |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 0.242        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 519      |
|    time_elapsed    | 188613   |
|    total_timesteps | 31887360 |
---------------------------------
Eval num_timesteps=31887879, episode_reward=0.03 +/- 0.98
Episode length: 30.01 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 31887879   |
| train/                  |            |
|    approx_kl            | 0.01293536 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.62      |
|    explained_variance   | 0.33       |
|    learning_rate        | 6.13e-05   |
|    loss                 | 0.055      |
|    n_updates            | 6775       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 520      |
|    time_elapsed    | 188936   |
|    total_timesteps | 31948800 |
---------------------------------
Eval num_timesteps=31949320, episode_reward=0.00 +/- 0.99
Episode length: 29.97 +/- 0.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.004      |
| time/                   |            |
|    total_timesteps      | 31949320   |
| train/                  |            |
|    approx_kl            | 0.01259941 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.613     |
|    explained_variance   | 0.331      |
|    learning_rate        | 6.12e-05   |
|    loss                 | 0.129      |
|    n_updates            | 6780       |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 521      |
|    time_elapsed    | 189259   |
|    total_timesteps | 32010240 |
---------------------------------
Eval num_timesteps=32010761, episode_reward=0.02 +/- 0.98
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 32010761    |
| train/                  |             |
|    approx_kl            | 0.013029786 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.338       |
|    learning_rate        | 6.12e-05    |
|    loss                 | 0.051       |
|    n_updates            | 6785        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 522      |
|    time_elapsed    | 189580   |
|    total_timesteps | 32071680 |
---------------------------------
Eval num_timesteps=32072202, episode_reward=0.13 +/- 0.97
Episode length: 30.01 +/- 0.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.13         |
| time/                   |              |
|    total_timesteps      | 32072202     |
| train/                  |              |
|    approx_kl            | 0.0129217515 |
|    clip_fraction        | 0.151        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.613       |
|    explained_variance   | 0.342        |
|    learning_rate        | 6.11e-05     |
|    loss                 | 0.066        |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.0175      |
|    value_loss           | 0.233        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 523      |
|    time_elapsed    | 189898   |
|    total_timesteps | 32133120 |
---------------------------------
Eval num_timesteps=32133643, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 32133643    |
| train/                  |             |
|    approx_kl            | 0.012771059 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.333       |
|    learning_rate        | 6.11e-05    |
|    loss                 | 0.0688      |
|    n_updates            | 6795        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 524      |
|    time_elapsed    | 190217   |
|    total_timesteps | 32194560 |
---------------------------------
Eval num_timesteps=32195084, episode_reward=0.09 +/- 0.98
Episode length: 30.05 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 32195084    |
| train/                  |             |
|    approx_kl            | 0.012473354 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.1e-05     |
|    loss                 | 0.0655      |
|    n_updates            | 6800        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 525      |
|    time_elapsed    | 190534   |
|    total_timesteps | 32256000 |
---------------------------------
Eval num_timesteps=32256525, episode_reward=0.08 +/- 0.98
Episode length: 30.05 +/- 0.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.078        |
| time/                   |              |
|    total_timesteps      | 32256525     |
| train/                  |              |
|    approx_kl            | 0.0134919835 |
|    clip_fraction        | 0.152        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.608       |
|    explained_variance   | 0.34         |
|    learning_rate        | 6.1e-05      |
|    loss                 | 0.08         |
|    n_updates            | 6805         |
|    policy_gradient_loss | -0.0173      |
|    value_loss           | 0.234        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 526      |
|    time_elapsed    | 190852   |
|    total_timesteps | 32317440 |
---------------------------------
Eval num_timesteps=32317966, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 32317966    |
| train/                  |             |
|    approx_kl            | 0.012751727 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.329       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0893      |
|    n_updates            | 6810        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 527      |
|    time_elapsed    | 191169   |
|    total_timesteps | 32378880 |
---------------------------------
Eval num_timesteps=32379407, episode_reward=-0.04 +/- 0.98
Episode length: 29.96 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.042      |
| time/                   |             |
|    total_timesteps      | 32379407    |
| train/                  |             |
|    approx_kl            | 0.012894892 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.108       |
|    n_updates            | 6815        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 528      |
|    time_elapsed    | 191488   |
|    total_timesteps | 32440320 |
---------------------------------
Eval num_timesteps=32440848, episode_reward=-0.07 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.066      |
| time/                   |             |
|    total_timesteps      | 32440848    |
| train/                  |             |
|    approx_kl            | 0.012713541 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.316       |
|    learning_rate        | 6.08e-05    |
|    loss                 | 0.0537      |
|    n_updates            | 6820        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 529      |
|    time_elapsed    | 191808   |
|    total_timesteps | 32501760 |
---------------------------------
Eval num_timesteps=32502289, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 32502289    |
| train/                  |             |
|    approx_kl            | 0.012768453 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.34        |
|    learning_rate        | 6.07e-05    |
|    loss                 | 0.0634      |
|    n_updates            | 6825        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 530      |
|    time_elapsed    | 192129   |
|    total_timesteps | 32563200 |
---------------------------------
Eval num_timesteps=32563730, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 32563730    |
| train/                  |             |
|    approx_kl            | 0.012857294 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.07e-05    |
|    loss                 | 0.123       |
|    n_updates            | 6830        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 531      |
|    time_elapsed    | 192452   |
|    total_timesteps | 32624640 |
---------------------------------
Eval num_timesteps=32625171, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 32625171    |
| train/                  |             |
|    approx_kl            | 0.012884809 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.336       |
|    learning_rate        | 6.06e-05    |
|    loss                 | 0.108       |
|    n_updates            | 6835        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 532      |
|    time_elapsed    | 192775   |
|    total_timesteps | 32686080 |
---------------------------------
Eval num_timesteps=32686612, episode_reward=0.12 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 32686612    |
| train/                  |             |
|    approx_kl            | 0.012786456 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.334       |
|    learning_rate        | 6.06e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 6840        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 533      |
|    time_elapsed    | 193097   |
|    total_timesteps | 32747520 |
---------------------------------
Eval num_timesteps=32748053, episode_reward=-0.02 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 32748053    |
| train/                  |             |
|    approx_kl            | 0.012723906 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.318       |
|    learning_rate        | 6.05e-05    |
|    loss                 | 0.0508      |
|    n_updates            | 6845        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 534      |
|    time_elapsed    | 193416   |
|    total_timesteps | 32808960 |
---------------------------------
Eval num_timesteps=32809494, episode_reward=0.02 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 32809494    |
| train/                  |             |
|    approx_kl            | 0.012768208 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.335       |
|    learning_rate        | 6.05e-05    |
|    loss                 | 0.0806      |
|    n_updates            | 6850        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 535      |
|    time_elapsed    | 193735   |
|    total_timesteps | 32870400 |
---------------------------------
Eval num_timesteps=32870935, episode_reward=-0.00 +/- 0.98
Episode length: 29.96 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.004     |
| time/                   |            |
|    total_timesteps      | 32870935   |
| train/                  |            |
|    approx_kl            | 0.01243759 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.607     |
|    explained_variance   | 0.34       |
|    learning_rate        | 6.04e-05   |
|    loss                 | 0.104      |
|    n_updates            | 6855       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 536      |
|    time_elapsed    | 194053   |
|    total_timesteps | 32931840 |
---------------------------------
Eval num_timesteps=32932376, episode_reward=0.07 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 32932376    |
| train/                  |             |
|    approx_kl            | 0.013074171 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.33        |
|    learning_rate        | 6.04e-05    |
|    loss                 | 0.153       |
|    n_updates            | 6860        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 537      |
|    time_elapsed    | 194370   |
|    total_timesteps | 32993280 |
---------------------------------
Eval num_timesteps=32993817, episode_reward=0.02 +/- 0.97
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 32993817    |
| train/                  |             |
|    approx_kl            | 0.012713713 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.34        |
|    learning_rate        | 6.03e-05    |
|    loss                 | 0.136       |
|    n_updates            | 6865        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 538      |
|    time_elapsed    | 194688   |
|    total_timesteps | 33054720 |
---------------------------------
Eval num_timesteps=33055258, episode_reward=0.09 +/- 0.99
Episode length: 30.01 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.094      |
| time/                   |            |
|    total_timesteps      | 33055258   |
| train/                  |            |
|    approx_kl            | 0.01282814 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.599     |
|    explained_variance   | 0.338      |
|    learning_rate        | 6.03e-05   |
|    loss                 | 0.0715     |
|    n_updates            | 6870       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 539      |
|    time_elapsed    | 195006   |
|    total_timesteps | 33116160 |
---------------------------------
Eval num_timesteps=33116699, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.008        |
| time/                   |              |
|    total_timesteps      | 33116699     |
| train/                  |              |
|    approx_kl            | 0.0125599485 |
|    clip_fraction        | 0.147        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.601       |
|    explained_variance   | 0.339        |
|    learning_rate        | 6.02e-05     |
|    loss                 | 0.094        |
|    n_updates            | 6875         |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 0.236        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 540      |
|    time_elapsed    | 195324   |
|    total_timesteps | 33177600 |
---------------------------------
Eval num_timesteps=33178140, episode_reward=-0.03 +/- 0.99
Episode length: 29.96 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 33178140    |
| train/                  |             |
|    approx_kl            | 0.012852096 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.324       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 0.102       |
|    n_updates            | 6880        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 541      |
|    time_elapsed    | 195645   |
|    total_timesteps | 33239040 |
---------------------------------
Eval num_timesteps=33239581, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 33239581    |
| train/                  |             |
|    approx_kl            | 0.012913047 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.327       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 0.0814      |
|    n_updates            | 6885        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 542      |
|    time_elapsed    | 195967   |
|    total_timesteps | 33300480 |
---------------------------------
Eval num_timesteps=33301022, episode_reward=0.09 +/- 0.97
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 33301022    |
| train/                  |             |
|    approx_kl            | 0.012448805 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.334       |
|    learning_rate        | 6e-05       |
|    loss                 | 0.091       |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 543      |
|    time_elapsed    | 196289   |
|    total_timesteps | 33361920 |
---------------------------------
Eval num_timesteps=33362463, episode_reward=0.04 +/- 0.97
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 33362463    |
| train/                  |             |
|    approx_kl            | 0.012490001 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.319       |
|    learning_rate        | 6e-05       |
|    loss                 | 0.0678      |
|    n_updates            | 6895        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 544      |
|    time_elapsed    | 196612   |
|    total_timesteps | 33423360 |
---------------------------------
Eval num_timesteps=33423904, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.02         |
| time/                   |              |
|    total_timesteps      | 33423904     |
| train/                  |              |
|    approx_kl            | 0.0126522165 |
|    clip_fraction        | 0.147        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.601       |
|    explained_variance   | 0.335        |
|    learning_rate        | 5.99e-05     |
|    loss                 | 0.0737       |
|    n_updates            | 6900         |
|    policy_gradient_loss | -0.0168      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 545      |
|    time_elapsed    | 196932   |
|    total_timesteps | 33484800 |
---------------------------------
Eval num_timesteps=33485345, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 33485345   |
| train/                  |            |
|    approx_kl            | 0.01269677 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.597     |
|    explained_variance   | 0.35       |
|    learning_rate        | 5.99e-05   |
|    loss                 | 0.0891     |
|    n_updates            | 6905       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 546      |
|    time_elapsed    | 197251   |
|    total_timesteps | 33546240 |
---------------------------------
Eval num_timesteps=33546786, episode_reward=0.06 +/- 0.99
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 33546786    |
| train/                  |             |
|    approx_kl            | 0.012706699 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.332       |
|    learning_rate        | 5.98e-05    |
|    loss                 | 0.102       |
|    n_updates            | 6910        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 547      |
|    time_elapsed    | 197569   |
|    total_timesteps | 33607680 |
---------------------------------
Eval num_timesteps=33608227, episode_reward=0.03 +/- 0.99
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 33608227    |
| train/                  |             |
|    approx_kl            | 0.012324383 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.335       |
|    learning_rate        | 5.98e-05    |
|    loss                 | 0.0551      |
|    n_updates            | 6915        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 548      |
|    time_elapsed    | 197887   |
|    total_timesteps | 33669120 |
---------------------------------
Eval num_timesteps=33669668, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.05         |
| time/                   |              |
|    total_timesteps      | 33669668     |
| train/                  |              |
|    approx_kl            | 0.0126527585 |
|    clip_fraction        | 0.148        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.599       |
|    explained_variance   | 0.329        |
|    learning_rate        | 5.97e-05     |
|    loss                 | 0.0873       |
|    n_updates            | 6920         |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 549      |
|    time_elapsed    | 198204   |
|    total_timesteps | 33730560 |
---------------------------------
Eval num_timesteps=33731109, episode_reward=0.10 +/- 0.98
Episode length: 29.99 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 33731109    |
| train/                  |             |
|    approx_kl            | 0.012134016 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.335       |
|    learning_rate        | 5.96e-05    |
|    loss                 | 0.0553      |
|    n_updates            | 6925        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 550      |
|    time_elapsed    | 198522   |
|    total_timesteps | 33792000 |
---------------------------------
Eval num_timesteps=33792550, episode_reward=0.06 +/- 0.98
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 33792550    |
| train/                  |             |
|    approx_kl            | 0.012776161 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.602      |
|    explained_variance   | 0.341       |
|    learning_rate        | 5.96e-05    |
|    loss                 | 0.0774      |
|    n_updates            | 6930        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 551      |
|    time_elapsed    | 198839   |
|    total_timesteps | 33853440 |
---------------------------------
Eval num_timesteps=33853991, episode_reward=0.04 +/- 0.99
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 33853991    |
| train/                  |             |
|    approx_kl            | 0.012750584 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.313       |
|    learning_rate        | 5.95e-05    |
|    loss                 | 0.0762      |
|    n_updates            | 6935        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 552      |
|    time_elapsed    | 199159   |
|    total_timesteps | 33914880 |
---------------------------------
Eval num_timesteps=33915432, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 33915432    |
| train/                  |             |
|    approx_kl            | 0.012806768 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.328       |
|    learning_rate        | 5.95e-05    |
|    loss                 | 0.102       |
|    n_updates            | 6940        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 553      |
|    time_elapsed    | 199480   |
|    total_timesteps | 33976320 |
---------------------------------
Eval num_timesteps=33976873, episode_reward=0.02 +/- 0.98
Episode length: 30.02 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 33976873    |
| train/                  |             |
|    approx_kl            | 0.012664702 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.331       |
|    learning_rate        | 5.94e-05    |
|    loss                 | 0.08        |
|    n_updates            | 6945        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 554      |
|    time_elapsed    | 199802   |
|    total_timesteps | 34037760 |
---------------------------------
Eval num_timesteps=34038314, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 34038314    |
| train/                  |             |
|    approx_kl            | 0.012643204 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.94e-05    |
|    loss                 | 0.089       |
|    n_updates            | 6950        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 555      |
|    time_elapsed    | 200125   |
|    total_timesteps | 34099200 |
---------------------------------
Eval num_timesteps=34099755, episode_reward=0.03 +/- 0.99
Episode length: 30.03 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 34099755    |
| train/                  |             |
|    approx_kl            | 0.012409828 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.338       |
|    learning_rate        | 5.93e-05    |
|    loss                 | 0.0613      |
|    n_updates            | 6955        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 556      |
|    time_elapsed    | 200446   |
|    total_timesteps | 34160640 |
---------------------------------
Eval num_timesteps=34161196, episode_reward=0.11 +/- 0.97
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 34161196    |
| train/                  |             |
|    approx_kl            | 0.012294676 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.342       |
|    learning_rate        | 5.93e-05    |
|    loss                 | 0.086       |
|    n_updates            | 6960        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 557      |
|    time_elapsed    | 200766   |
|    total_timesteps | 34222080 |
---------------------------------
Eval num_timesteps=34222637, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 34222637    |
| train/                  |             |
|    approx_kl            | 0.012283961 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.315       |
|    learning_rate        | 5.92e-05    |
|    loss                 | 0.0945      |
|    n_updates            | 6965        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 558      |
|    time_elapsed    | 201083   |
|    total_timesteps | 34283520 |
---------------------------------
Eval num_timesteps=34284078, episode_reward=0.08 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 34284078    |
| train/                  |             |
|    approx_kl            | 0.012065844 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.341       |
|    learning_rate        | 5.91e-05    |
|    loss                 | 0.0675      |
|    n_updates            | 6970        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 559      |
|    time_elapsed    | 201401   |
|    total_timesteps | 34344960 |
---------------------------------
Eval num_timesteps=34345519, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 34345519    |
| train/                  |             |
|    approx_kl            | 0.012086378 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.335       |
|    learning_rate        | 5.91e-05    |
|    loss                 | 0.0847      |
|    n_updates            | 6975        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 560      |
|    time_elapsed    | 201719   |
|    total_timesteps | 34406400 |
---------------------------------
Eval num_timesteps=34406960, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.068      |
| time/                   |            |
|    total_timesteps      | 34406960   |
| train/                  |            |
|    approx_kl            | 0.01203956 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.59      |
|    explained_variance   | 0.336      |
|    learning_rate        | 5.9e-05    |
|    loss                 | 0.088      |
|    n_updates            | 6980       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 561      |
|    time_elapsed    | 202037   |
|    total_timesteps | 34467840 |
---------------------------------
Eval num_timesteps=34468401, episode_reward=0.04 +/- 0.97
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 34468401    |
| train/                  |             |
|    approx_kl            | 0.012521055 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.318       |
|    learning_rate        | 5.9e-05     |
|    loss                 | 0.0484      |
|    n_updates            | 6985        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 562      |
|    time_elapsed    | 202355   |
|    total_timesteps | 34529280 |
---------------------------------
Eval num_timesteps=34529842, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 34529842    |
| train/                  |             |
|    approx_kl            | 0.012201395 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.333       |
|    learning_rate        | 5.89e-05    |
|    loss                 | 0.0846      |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 170      |
|    iterations      | 563      |
|    time_elapsed    | 202673   |
|    total_timesteps | 34590720 |
---------------------------------
Eval num_timesteps=34591283, episode_reward=0.11 +/- 0.98
Episode length: 30.09 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 34591283    |
| train/                  |             |
|    approx_kl            | 0.012337323 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.326       |
|    learning_rate        | 5.89e-05    |
|    loss                 | 0.117       |
|    n_updates            | 6995        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 564      |
|    time_elapsed    | 202993   |
|    total_timesteps | 34652160 |
---------------------------------
Eval num_timesteps=34652724, episode_reward=-0.02 +/- 0.99
Episode length: 29.98 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 34652724    |
| train/                  |             |
|    approx_kl            | 0.012078706 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.346       |
|    learning_rate        | 5.88e-05    |
|    loss                 | 0.0862      |
|    n_updates            | 7000        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 170      |
|    iterations      | 565      |
|    time_elapsed    | 203315   |
|    total_timesteps | 34713600 |
---------------------------------
Eval num_timesteps=34714165, episode_reward=0.10 +/- 0.98
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 34714165    |
| train/                  |             |
|    approx_kl            | 0.012317846 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.328       |
|    learning_rate        | 5.88e-05    |
|    loss                 | 0.0724      |
|    n_updates            | 7005        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 170      |
|    iterations      | 566      |
|    time_elapsed    | 203637   |
|    total_timesteps | 34775040 |
---------------------------------
Eval num_timesteps=34775606, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 34775606    |
| train/                  |             |
|    approx_kl            | 0.012262746 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.338       |
|    learning_rate        | 5.87e-05    |
|    loss                 | 0.0722      |
|    n_updates            | 7010        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 567      |
|    time_elapsed    | 203960   |
|    total_timesteps | 34836480 |
---------------------------------
Eval num_timesteps=34837047, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 34837047    |
| train/                  |             |
|    approx_kl            | 0.012165502 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.325       |
|    learning_rate        | 5.86e-05    |
|    loss                 | 0.083       |
|    n_updates            | 7015        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 568      |
|    time_elapsed    | 204281   |
|    total_timesteps | 34897920 |
---------------------------------
Eval num_timesteps=34898488, episode_reward=0.09 +/- 0.98
Episode length: 29.97 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 34898488    |
| train/                  |             |
|    approx_kl            | 0.012092495 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.579      |
|    explained_variance   | 0.325       |
|    learning_rate        | 5.86e-05    |
|    loss                 | 0.127       |
|    n_updates            | 7020        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 569      |
|    time_elapsed    | 204599   |
|    total_timesteps | 34959360 |
---------------------------------
Eval num_timesteps=34959929, episode_reward=0.09 +/- 0.97
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 34959929    |
| train/                  |             |
|    approx_kl            | 0.012279049 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.339       |
|    learning_rate        | 5.85e-05    |
|    loss                 | 0.0867      |
|    n_updates            | 7025        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 570      |
|    time_elapsed    | 204917   |
|    total_timesteps | 35020800 |
---------------------------------
Eval num_timesteps=35021370, episode_reward=0.14 +/- 0.98
Episode length: 30.03 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 35021370    |
| train/                  |             |
|    approx_kl            | 0.011950511 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.329       |
|    learning_rate        | 5.85e-05    |
|    loss                 | 0.105       |
|    n_updates            | 7030        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 571      |
|    time_elapsed    | 205235   |
|    total_timesteps | 35082240 |
---------------------------------
Eval num_timesteps=35082811, episode_reward=0.16 +/- 0.97
Episode length: 30.04 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.16       |
| time/                   |            |
|    total_timesteps      | 35082811   |
| train/                  |            |
|    approx_kl            | 0.01209094 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.578     |
|    explained_variance   | 0.348      |
|    learning_rate        | 5.84e-05   |
|    loss                 | 0.0829     |
|    n_updates            | 7035       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.233      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.16
SELFPLAY: new best model, bumping up generation to 16
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 572      |
|    time_elapsed    | 205552   |
|    total_timesteps | 35143680 |
---------------------------------
Eval num_timesteps=35144252, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 35144252    |
| train/                  |             |
|    approx_kl            | 0.012230144 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.332       |
|    learning_rate        | 5.84e-05    |
|    loss                 | 0.121       |
|    n_updates            | 7040        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 573      |
|    time_elapsed    | 205871   |
|    total_timesteps | 35205120 |
---------------------------------
Eval num_timesteps=35205693, episode_reward=-0.02 +/- 0.98
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 35205693    |
| train/                  |             |
|    approx_kl            | 0.011959688 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.83e-05    |
|    loss                 | 0.0674      |
|    n_updates            | 7045        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 574      |
|    time_elapsed    | 206189   |
|    total_timesteps | 35266560 |
---------------------------------
Eval num_timesteps=35267134, episode_reward=-0.00 +/- 0.99
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 35267134    |
| train/                  |             |
|    approx_kl            | 0.012578993 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.34        |
|    learning_rate        | 5.83e-05    |
|    loss                 | 0.0902      |
|    n_updates            | 7050        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 575      |
|    time_elapsed    | 206508   |
|    total_timesteps | 35328000 |
---------------------------------
Eval num_timesteps=35328575, episode_reward=-0.04 +/- 0.98
Episode length: 29.94 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.042      |
| time/                   |             |
|    total_timesteps      | 35328575    |
| train/                  |             |
|    approx_kl            | 0.012173897 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.331       |
|    learning_rate        | 5.82e-05    |
|    loss                 | 0.0506      |
|    n_updates            | 7055        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 576      |
|    time_elapsed    | 206830   |
|    total_timesteps | 35389440 |
---------------------------------
Eval num_timesteps=35390016, episode_reward=-0.03 +/- 0.99
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 35390016    |
| train/                  |             |
|    approx_kl            | 0.012206543 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.329       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0813      |
|    n_updates            | 7060        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 577      |
|    time_elapsed    | 207152   |
|    total_timesteps | 35450880 |
---------------------------------
Eval num_timesteps=35451457, episode_reward=-0.06 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.062      |
| time/                   |             |
|    total_timesteps      | 35451457    |
| train/                  |             |
|    approx_kl            | 0.012162494 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.329       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0787      |
|    n_updates            | 7065        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 578      |
|    time_elapsed    | 207475   |
|    total_timesteps | 35512320 |
---------------------------------
Eval num_timesteps=35512898, episode_reward=-0.02 +/- 0.99
Episode length: 29.96 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.024      |
| time/                   |             |
|    total_timesteps      | 35512898    |
| train/                  |             |
|    approx_kl            | 0.011885474 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.327       |
|    learning_rate        | 5.8e-05     |
|    loss                 | 0.0528      |
|    n_updates            | 7070        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 579      |
|    time_elapsed    | 207797   |
|    total_timesteps | 35573760 |
---------------------------------
Eval num_timesteps=35574339, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 35574339    |
| train/                  |             |
|    approx_kl            | 0.012160248 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.32        |
|    learning_rate        | 5.8e-05     |
|    loss                 | 0.0668      |
|    n_updates            | 7075        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 580      |
|    time_elapsed    | 208117   |
|    total_timesteps | 35635200 |
---------------------------------
Eval num_timesteps=35635780, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 35635780    |
| train/                  |             |
|    approx_kl            | 0.012050764 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.33        |
|    learning_rate        | 5.79e-05    |
|    loss                 | 0.135       |
|    n_updates            | 7080        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 581      |
|    time_elapsed    | 208436   |
|    total_timesteps | 35696640 |
---------------------------------
Eval num_timesteps=35697221, episode_reward=0.09 +/- 0.97
Episode length: 30.01 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.09       |
| time/                   |            |
|    total_timesteps      | 35697221   |
| train/                  |            |
|    approx_kl            | 0.01230891 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.611     |
|    explained_variance   | 0.337      |
|    learning_rate        | 5.79e-05   |
|    loss                 | 0.0639     |
|    n_updates            | 7085       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 171      |
|    iterations      | 582      |
|    time_elapsed    | 208754   |
|    total_timesteps | 35758080 |
---------------------------------
Eval num_timesteps=35758662, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 35758662    |
| train/                  |             |
|    approx_kl            | 0.012418895 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.322       |
|    learning_rate        | 5.78e-05    |
|    loss                 | 0.135       |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 583      |
|    time_elapsed    | 209072   |
|    total_timesteps | 35819520 |
---------------------------------
Eval num_timesteps=35820103, episode_reward=0.04 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 35820103    |
| train/                  |             |
|    approx_kl            | 0.012261678 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.78e-05    |
|    loss                 | 0.0983      |
|    n_updates            | 7095        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 584      |
|    time_elapsed    | 209390   |
|    total_timesteps | 35880960 |
---------------------------------
Eval num_timesteps=35881544, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 35881544    |
| train/                  |             |
|    approx_kl            | 0.011835108 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.33        |
|    learning_rate        | 5.77e-05    |
|    loss                 | 0.0902      |
|    n_updates            | 7100        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 585      |
|    time_elapsed    | 209708   |
|    total_timesteps | 35942400 |
---------------------------------
Eval num_timesteps=35942985, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 35942985    |
| train/                  |             |
|    approx_kl            | 0.011947541 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.328       |
|    learning_rate        | 5.77e-05    |
|    loss                 | 0.0817      |
|    n_updates            | 7105        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 586      |
|    time_elapsed    | 210026   |
|    total_timesteps | 36003840 |
---------------------------------
Eval num_timesteps=36004426, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 36004426    |
| train/                  |             |
|    approx_kl            | 0.012293174 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.344       |
|    learning_rate        | 5.76e-05    |
|    loss                 | 0.0825      |
|    n_updates            | 7110        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 587      |
|    time_elapsed    | 210346   |
|    total_timesteps | 36065280 |
---------------------------------
Eval num_timesteps=36065867, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 36065867   |
| train/                  |            |
|    approx_kl            | 0.01192299 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.606     |
|    explained_variance   | 0.331      |
|    learning_rate        | 5.75e-05   |
|    loss                 | 0.123      |
|    n_updates            | 7115       |
|    policy_gradient_loss | -0.0167    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 588      |
|    time_elapsed    | 210667   |
|    total_timesteps | 36126720 |
---------------------------------
Eval num_timesteps=36127308, episode_reward=-0.05 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.05       |
| time/                   |             |
|    total_timesteps      | 36127308    |
| train/                  |             |
|    approx_kl            | 0.011532619 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.75e-05    |
|    loss                 | 0.114       |
|    n_updates            | 7120        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 589      |
|    time_elapsed    | 210990   |
|    total_timesteps | 36188160 |
---------------------------------
Eval num_timesteps=36188749, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 36188749    |
| train/                  |             |
|    approx_kl            | 0.012024445 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.74e-05    |
|    loss                 | 0.0662      |
|    n_updates            | 7125        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 590      |
|    time_elapsed    | 211313   |
|    total_timesteps | 36249600 |
---------------------------------
Eval num_timesteps=36250190, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.14        |
| time/                   |             |
|    total_timesteps      | 36250190    |
| train/                  |             |
|    approx_kl            | 0.011863792 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.339       |
|    learning_rate        | 5.74e-05    |
|    loss                 | 0.123       |
|    n_updates            | 7130        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 591      |
|    time_elapsed    | 211635   |
|    total_timesteps | 36311040 |
---------------------------------
Eval num_timesteps=36311631, episode_reward=0.07 +/- 0.98
Episode length: 30.05 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 36311631    |
| train/                  |             |
|    approx_kl            | 0.012031809 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.324       |
|    learning_rate        | 5.73e-05    |
|    loss                 | 0.0711      |
|    n_updates            | 7135        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 592      |
|    time_elapsed    | 211953   |
|    total_timesteps | 36372480 |
---------------------------------
Eval num_timesteps=36373072, episode_reward=0.01 +/- 0.99
Episode length: 30.00 +/- 0.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 36373072    |
| train/                  |             |
|    approx_kl            | 0.012242266 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.339       |
|    learning_rate        | 5.73e-05    |
|    loss                 | 0.111       |
|    n_updates            | 7140        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 593      |
|    time_elapsed    | 212272   |
|    total_timesteps | 36433920 |
---------------------------------
Eval num_timesteps=36434513, episode_reward=0.02 +/- 0.99
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 36434513    |
| train/                  |             |
|    approx_kl            | 0.012192498 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.336       |
|    learning_rate        | 5.72e-05    |
|    loss                 | 0.0701      |
|    n_updates            | 7145        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 594      |
|    time_elapsed    | 212589   |
|    total_timesteps | 36495360 |
---------------------------------
Eval num_timesteps=36495954, episode_reward=0.06 +/- 0.99
Episode length: 29.94 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 36495954    |
| train/                  |             |
|    approx_kl            | 0.011701314 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.333       |
|    learning_rate        | 5.72e-05    |
|    loss                 | 0.0869      |
|    n_updates            | 7150        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 595      |
|    time_elapsed    | 212907   |
|    total_timesteps | 36556800 |
---------------------------------
Eval num_timesteps=36557395, episode_reward=0.01 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 36557395    |
| train/                  |             |
|    approx_kl            | 0.012320416 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.314       |
|    learning_rate        | 5.71e-05    |
|    loss                 | 0.0721      |
|    n_updates            | 7155        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 171      |
|    iterations      | 596      |
|    time_elapsed    | 213225   |
|    total_timesteps | 36618240 |
---------------------------------
Eval num_timesteps=36618836, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 36618836    |
| train/                  |             |
|    approx_kl            | 0.012265101 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.34        |
|    learning_rate        | 5.7e-05     |
|    loss                 | 0.0798      |
|    n_updates            | 7160        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 597      |
|    time_elapsed    | 213543   |
|    total_timesteps | 36679680 |
---------------------------------
Eval num_timesteps=36680277, episode_reward=-0.04 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.04       |
| time/                   |             |
|    total_timesteps      | 36680277    |
| train/                  |             |
|    approx_kl            | 0.011851658 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.325       |
|    learning_rate        | 5.7e-05     |
|    loss                 | 0.0705      |
|    n_updates            | 7165        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 598      |
|    time_elapsed    | 213862   |
|    total_timesteps | 36741120 |
---------------------------------
Eval num_timesteps=36741718, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 36741718    |
| train/                  |             |
|    approx_kl            | 0.011863226 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.613      |
|    explained_variance   | 0.336       |
|    learning_rate        | 5.69e-05    |
|    loss                 | 0.072       |
|    n_updates            | 7170        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 599      |
|    time_elapsed    | 214182   |
|    total_timesteps | 36802560 |
---------------------------------
Eval num_timesteps=36803159, episode_reward=0.11 +/- 0.98
Episode length: 30.06 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 36803159    |
| train/                  |             |
|    approx_kl            | 0.011676782 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.325       |
|    learning_rate        | 5.69e-05    |
|    loss                 | 0.0638      |
|    n_updates            | 7175        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 600      |
|    time_elapsed    | 214504   |
|    total_timesteps | 36864000 |
---------------------------------
Eval num_timesteps=36864600, episode_reward=0.02 +/- 0.99
Episode length: 30.00 +/- 0.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.024        |
| time/                   |              |
|    total_timesteps      | 36864600     |
| train/                  |              |
|    approx_kl            | 0.0118836025 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.612       |
|    explained_variance   | 0.33         |
|    learning_rate        | 5.68e-05     |
|    loss                 | 0.0865       |
|    n_updates            | 7180         |
|    policy_gradient_loss | -0.0168      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 601      |
|    time_elapsed    | 214826   |
|    total_timesteps | 36925440 |
---------------------------------
Eval num_timesteps=36926041, episode_reward=0.00 +/- 0.99
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 36926041    |
| train/                  |             |
|    approx_kl            | 0.011764709 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.339       |
|    learning_rate        | 5.68e-05    |
|    loss                 | 0.114       |
|    n_updates            | 7185        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 602      |
|    time_elapsed    | 215150   |
|    total_timesteps | 36986880 |
---------------------------------
Eval num_timesteps=36987482, episode_reward=0.09 +/- 0.99
Episode length: 30.05 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 36987482    |
| train/                  |             |
|    approx_kl            | 0.011730411 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.613      |
|    explained_variance   | 0.319       |
|    learning_rate        | 5.67e-05    |
|    loss                 | 0.0938      |
|    n_updates            | 7190        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 603      |
|    time_elapsed    | 215470   |
|    total_timesteps | 37048320 |
---------------------------------
Eval num_timesteps=37048923, episode_reward=0.03 +/- 0.99
Episode length: 29.99 +/- 0.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 37048923    |
| train/                  |             |
|    approx_kl            | 0.011754844 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.331       |
|    learning_rate        | 5.67e-05    |
|    loss                 | 0.032       |
|    n_updates            | 7195        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 604      |
|    time_elapsed    | 215788   |
|    total_timesteps | 37109760 |
---------------------------------
Eval num_timesteps=37110364, episode_reward=0.06 +/- 0.99
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 37110364    |
| train/                  |             |
|    approx_kl            | 0.011762419 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.322       |
|    learning_rate        | 5.66e-05    |
|    loss                 | 0.0698      |
|    n_updates            | 7200        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 605      |
|    time_elapsed    | 216106   |
|    total_timesteps | 37171200 |
---------------------------------
Eval num_timesteps=37171805, episode_reward=0.04 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 37171805    |
| train/                  |             |
|    approx_kl            | 0.012232439 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.332       |
|    learning_rate        | 5.65e-05    |
|    loss                 | 0.044       |
|    n_updates            | 7205        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 606      |
|    time_elapsed    | 216424   |
|    total_timesteps | 37232640 |
---------------------------------
Eval num_timesteps=37233246, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 37233246    |
| train/                  |             |
|    approx_kl            | 0.011654731 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.332       |
|    learning_rate        | 5.65e-05    |
|    loss                 | 0.0956      |
|    n_updates            | 7210        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 607      |
|    time_elapsed    | 216741   |
|    total_timesteps | 37294080 |
---------------------------------
Eval num_timesteps=37294687, episode_reward=0.00 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 37294687    |
| train/                  |             |
|    approx_kl            | 0.011834041 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.328       |
|    learning_rate        | 5.64e-05    |
|    loss                 | 0.0833      |
|    n_updates            | 7215        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 608      |
|    time_elapsed    | 217059   |
|    total_timesteps | 37355520 |
---------------------------------
Eval num_timesteps=37356128, episode_reward=-0.01 +/- 0.98
Episode length: 30.01 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 37356128    |
| train/                  |             |
|    approx_kl            | 0.011560635 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.342       |
|    learning_rate        | 5.64e-05    |
|    loss                 | 0.0523      |
|    n_updates            | 7220        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 609      |
|    time_elapsed    | 217377   |
|    total_timesteps | 37416960 |
---------------------------------
Eval num_timesteps=37417569, episode_reward=-0.01 +/- 0.99
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 37417569    |
| train/                  |             |
|    approx_kl            | 0.011560676 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.338       |
|    learning_rate        | 5.63e-05    |
|    loss                 | 0.0785      |
|    n_updates            | 7225        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 610      |
|    time_elapsed    | 217696   |
|    total_timesteps | 37478400 |
---------------------------------
Eval num_timesteps=37479010, episode_reward=0.13 +/- 0.98
Episode length: 30.04 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.13        |
| time/                   |             |
|    total_timesteps      | 37479010    |
| train/                  |             |
|    approx_kl            | 0.011534544 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.324       |
|    learning_rate        | 5.63e-05    |
|    loss                 | 0.0986      |
|    n_updates            | 7230        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 611      |
|    time_elapsed    | 218017   |
|    total_timesteps | 37539840 |
---------------------------------
Eval num_timesteps=37540451, episode_reward=-0.06 +/- 0.98
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.058      |
| time/                   |             |
|    total_timesteps      | 37540451    |
| train/                  |             |
|    approx_kl            | 0.011781147 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.329       |
|    learning_rate        | 5.62e-05    |
|    loss                 | 0.0679      |
|    n_updates            | 7235        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 612      |
|    time_elapsed    | 218339   |
|    total_timesteps | 37601280 |
---------------------------------
Eval num_timesteps=37601892, episode_reward=0.02 +/- 0.99
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 37601892    |
| train/                  |             |
|    approx_kl            | 0.011835176 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.333       |
|    learning_rate        | 5.62e-05    |
|    loss                 | 0.0599      |
|    n_updates            | 7240        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 613      |
|    time_elapsed    | 218663   |
|    total_timesteps | 37662720 |
---------------------------------
Eval num_timesteps=37663333, episode_reward=0.05 +/- 0.98
Episode length: 30.02 +/- 0.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.052        |
| time/                   |              |
|    total_timesteps      | 37663333     |
| train/                  |              |
|    approx_kl            | 0.0118771875 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.604       |
|    explained_variance   | 0.331        |
|    learning_rate        | 5.61e-05     |
|    loss                 | 0.066        |
|    n_updates            | 7245         |
|    policy_gradient_loss | -0.017       |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 614      |
|    time_elapsed    | 218985   |
|    total_timesteps | 37724160 |
---------------------------------
Eval num_timesteps=37724774, episode_reward=0.03 +/- 0.98
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 37724774    |
| train/                  |             |
|    approx_kl            | 0.011404693 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.33        |
|    learning_rate        | 5.6e-05     |
|    loss                 | 0.085       |
|    n_updates            | 7250        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 615      |
|    time_elapsed    | 219304   |
|    total_timesteps | 37785600 |
---------------------------------
Eval num_timesteps=37786215, episode_reward=0.03 +/- 0.98
Episode length: 29.98 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 37786215    |
| train/                  |             |
|    approx_kl            | 0.011493006 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.325       |
|    learning_rate        | 5.6e-05     |
|    loss                 | 0.116       |
|    n_updates            | 7255        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 616      |
|    time_elapsed    | 219622   |
|    total_timesteps | 37847040 |
---------------------------------
Eval num_timesteps=37847656, episode_reward=0.05 +/- 0.98
Episode length: 30.06 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 37847656   |
| train/                  |            |
|    approx_kl            | 0.01123058 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.597     |
|    explained_variance   | 0.321      |
|    learning_rate        | 5.59e-05   |
|    loss                 | 0.0617     |
|    n_updates            | 7260       |
|    policy_gradient_loss | -0.0165    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 617      |
|    time_elapsed    | 219940   |
|    total_timesteps | 37908480 |
---------------------------------
Eval num_timesteps=37909097, episode_reward=-0.00 +/- 0.99
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 37909097    |
| train/                  |             |
|    approx_kl            | 0.011505271 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.335       |
|    learning_rate        | 5.59e-05    |
|    loss                 | 0.0763      |
|    n_updates            | 7265        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 618      |
|    time_elapsed    | 220258   |
|    total_timesteps | 37969920 |
---------------------------------
Eval num_timesteps=37970538, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 37970538    |
| train/                  |             |
|    approx_kl            | 0.011395881 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.335       |
|    learning_rate        | 5.58e-05    |
|    loss                 | 0.0643      |
|    n_updates            | 7270        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 619      |
|    time_elapsed    | 220576   |
|    total_timesteps | 38031360 |
---------------------------------
Eval num_timesteps=38031979, episode_reward=-0.03 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 38031979    |
| train/                  |             |
|    approx_kl            | 0.011880829 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.327       |
|    learning_rate        | 5.58e-05    |
|    loss                 | 0.0451      |
|    n_updates            | 7275        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 620      |
|    time_elapsed    | 220893   |
|    total_timesteps | 38092800 |
---------------------------------
Eval num_timesteps=38093420, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 38093420    |
| train/                  |             |
|    approx_kl            | 0.011507374 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.327       |
|    learning_rate        | 5.57e-05    |
|    loss                 | 0.1         |
|    n_updates            | 7280        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 621      |
|    time_elapsed    | 221212   |
|    total_timesteps | 38154240 |
---------------------------------
Eval num_timesteps=38154861, episode_reward=-0.03 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.032      |
| time/                   |             |
|    total_timesteps      | 38154861    |
| train/                  |             |
|    approx_kl            | 0.011573944 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.335       |
|    learning_rate        | 5.57e-05    |
|    loss                 | 0.0608      |
|    n_updates            | 7285        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 622      |
|    time_elapsed    | 221532   |
|    total_timesteps | 38215680 |
---------------------------------
Eval num_timesteps=38216302, episode_reward=0.01 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 38216302    |
| train/                  |             |
|    approx_kl            | 0.011617304 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.328       |
|    learning_rate        | 5.56e-05    |
|    loss                 | 0.0586      |
|    n_updates            | 7290        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 623      |
|    time_elapsed    | 221853   |
|    total_timesteps | 38277120 |
---------------------------------
Eval num_timesteps=38277743, episode_reward=0.08 +/- 0.98
Episode length: 30.06 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 38277743    |
| train/                  |             |
|    approx_kl            | 0.011835033 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.32        |
|    learning_rate        | 5.56e-05    |
|    loss                 | 0.0693      |
|    n_updates            | 7295        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 624      |
|    time_elapsed    | 222176   |
|    total_timesteps | 38338560 |
---------------------------------
Eval num_timesteps=38339184, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 38339184    |
| train/                  |             |
|    approx_kl            | 0.011797077 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.339       |
|    learning_rate        | 5.55e-05    |
|    loss                 | 0.087       |
|    n_updates            | 7300        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 625      |
|    time_elapsed    | 222499   |
|    total_timesteps | 38400000 |
---------------------------------
Eval num_timesteps=38400625, episode_reward=-0.03 +/- 0.99
Episode length: 30.03 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.032      |
| time/                   |             |
|    total_timesteps      | 38400625    |
| train/                  |             |
|    approx_kl            | 0.011878072 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.346       |
|    learning_rate        | 5.54e-05    |
|    loss                 | 0.0876      |
|    n_updates            | 7305        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 626      |
|    time_elapsed    | 222820   |
|    total_timesteps | 38461440 |
---------------------------------
Eval num_timesteps=38462066, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 38462066    |
| train/                  |             |
|    approx_kl            | 0.011505562 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.335       |
|    learning_rate        | 5.54e-05    |
|    loss                 | 0.0814      |
|    n_updates            | 7310        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 627      |
|    time_elapsed    | 223139   |
|    total_timesteps | 38522880 |
---------------------------------
Eval num_timesteps=38523507, episode_reward=0.02 +/- 0.99
Episode length: 29.99 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.022        |
| time/                   |              |
|    total_timesteps      | 38523507     |
| train/                  |              |
|    approx_kl            | 0.0115712965 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.317        |
|    learning_rate        | 5.53e-05     |
|    loss                 | 0.0637       |
|    n_updates            | 7315         |
|    policy_gradient_loss | -0.0169      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 628      |
|    time_elapsed    | 223457   |
|    total_timesteps | 38584320 |
---------------------------------
Eval num_timesteps=38584948, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 38584948    |
| train/                  |             |
|    approx_kl            | 0.011773157 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.324       |
|    learning_rate        | 5.53e-05    |
|    loss                 | 0.0913      |
|    n_updates            | 7320        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 629      |
|    time_elapsed    | 223775   |
|    total_timesteps | 38645760 |
---------------------------------
Eval num_timesteps=38646389, episode_reward=0.06 +/- 0.98
Episode length: 29.97 +/- 1.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 38646389    |
| train/                  |             |
|    approx_kl            | 0.011143414 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.33        |
|    learning_rate        | 5.52e-05    |
|    loss                 | 0.104       |
|    n_updates            | 7325        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 630      |
|    time_elapsed    | 224093   |
|    total_timesteps | 38707200 |
---------------------------------
Eval num_timesteps=38707830, episode_reward=-0.01 +/- 0.99
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 38707830    |
| train/                  |             |
|    approx_kl            | 0.011710493 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.315       |
|    learning_rate        | 5.52e-05    |
|    loss                 | 0.0873      |
|    n_updates            | 7330        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 631      |
|    time_elapsed    | 224410   |
|    total_timesteps | 38768640 |
---------------------------------
Eval num_timesteps=38769271, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 38769271    |
| train/                  |             |
|    approx_kl            | 0.011788967 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.323       |
|    learning_rate        | 5.51e-05    |
|    loss                 | 0.137       |
|    n_updates            | 7335        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 632      |
|    time_elapsed    | 224728   |
|    total_timesteps | 38830080 |
---------------------------------
Eval num_timesteps=38830712, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 38830712    |
| train/                  |             |
|    approx_kl            | 0.011601997 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.338       |
|    learning_rate        | 5.51e-05    |
|    loss                 | 0.0795      |
|    n_updates            | 7340        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 633      |
|    time_elapsed    | 225046   |
|    total_timesteps | 38891520 |
---------------------------------
Eval num_timesteps=38892153, episode_reward=0.15 +/- 0.97
Episode length: 30.07 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.148       |
| time/                   |             |
|    total_timesteps      | 38892153    |
| train/                  |             |
|    approx_kl            | 0.011154522 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.323       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.101       |
|    n_updates            | 7345        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 17
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 634      |
|    time_elapsed    | 225367   |
|    total_timesteps | 38952960 |
---------------------------------
Eval num_timesteps=38953594, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 38953594    |
| train/                  |             |
|    approx_kl            | 0.011478698 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.339       |
|    learning_rate        | 5.49e-05    |
|    loss                 | 0.106       |
|    n_updates            | 7350        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 635      |
|    time_elapsed    | 225690   |
|    total_timesteps | 39014400 |
---------------------------------
Eval num_timesteps=39015035, episode_reward=-0.03 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 39015035    |
| train/                  |             |
|    approx_kl            | 0.011298993 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.339       |
|    learning_rate        | 5.49e-05    |
|    loss                 | 0.0676      |
|    n_updates            | 7355        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 636      |
|    time_elapsed    | 226012   |
|    total_timesteps | 39075840 |
---------------------------------
Eval num_timesteps=39076476, episode_reward=-0.03 +/- 0.99
Episode length: 29.98 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.034     |
| time/                   |            |
|    total_timesteps      | 39076476   |
| train/                  |            |
|    approx_kl            | 0.01134005 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.589     |
|    explained_variance   | 0.327      |
|    learning_rate        | 5.48e-05   |
|    loss                 | 0.0798     |
|    n_updates            | 7360       |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 637      |
|    time_elapsed    | 226335   |
|    total_timesteps | 39137280 |
---------------------------------
Eval num_timesteps=39137917, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 39137917    |
| train/                  |             |
|    approx_kl            | 0.010987556 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.339       |
|    learning_rate        | 5.48e-05    |
|    loss                 | 0.0673      |
|    n_updates            | 7365        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 638      |
|    time_elapsed    | 226655   |
|    total_timesteps | 39198720 |
---------------------------------
Eval num_timesteps=39199358, episode_reward=-0.06 +/- 0.98
Episode length: 29.99 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.064      |
| time/                   |             |
|    total_timesteps      | 39199358    |
| train/                  |             |
|    approx_kl            | 0.011622258 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.332       |
|    learning_rate        | 5.47e-05    |
|    loss                 | 0.068       |
|    n_updates            | 7370        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 639      |
|    time_elapsed    | 226973   |
|    total_timesteps | 39260160 |
---------------------------------
Eval num_timesteps=39260799, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 39260799    |
| train/                  |             |
|    approx_kl            | 0.011430684 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.315       |
|    learning_rate        | 5.47e-05    |
|    loss                 | 0.059       |
|    n_updates            | 7375        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 640      |
|    time_elapsed    | 227291   |
|    total_timesteps | 39321600 |
---------------------------------
Eval num_timesteps=39322240, episode_reward=-0.04 +/- 0.99
Episode length: 29.93 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 39322240    |
| train/                  |             |
|    approx_kl            | 0.011094924 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.331       |
|    learning_rate        | 5.46e-05    |
|    loss                 | 0.0689      |
|    n_updates            | 7380        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 641      |
|    time_elapsed    | 227610   |
|    total_timesteps | 39383040 |
---------------------------------
Eval num_timesteps=39383681, episode_reward=-0.03 +/- 0.99
Episode length: 30.03 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.028       |
| time/                   |              |
|    total_timesteps      | 39383681     |
| train/                  |              |
|    approx_kl            | 0.0115055675 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.595       |
|    explained_variance   | 0.314        |
|    learning_rate        | 5.46e-05     |
|    loss                 | 0.0723       |
|    n_updates            | 7385         |
|    policy_gradient_loss | -0.0165      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 642      |
|    time_elapsed    | 227927   |
|    total_timesteps | 39444480 |
---------------------------------
Eval num_timesteps=39445122, episode_reward=0.08 +/- 0.99
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 39445122    |
| train/                  |             |
|    approx_kl            | 0.011270076 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.323       |
|    learning_rate        | 5.45e-05    |
|    loss                 | 0.0783      |
|    n_updates            | 7390        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 643      |
|    time_elapsed    | 228245   |
|    total_timesteps | 39505920 |
---------------------------------
Eval num_timesteps=39506563, episode_reward=-0.02 +/- 0.98
Episode length: 29.99 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 39506563    |
| train/                  |             |
|    approx_kl            | 0.010924392 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.329       |
|    learning_rate        | 5.44e-05    |
|    loss                 | 0.0894      |
|    n_updates            | 7395        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 644      |
|    time_elapsed    | 228564   |
|    total_timesteps | 39567360 |
---------------------------------
Eval num_timesteps=39568004, episode_reward=-0.02 +/- 0.99
Episode length: 29.99 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 39568004   |
| train/                  |            |
|    approx_kl            | 0.01135279 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.598     |
|    explained_variance   | 0.331      |
|    learning_rate        | 5.44e-05   |
|    loss                 | 0.0751     |
|    n_updates            | 7400       |
|    policy_gradient_loss | -0.0164    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 645      |
|    time_elapsed    | 228884   |
|    total_timesteps | 39628800 |
---------------------------------
Eval num_timesteps=39629445, episode_reward=0.01 +/- 0.99
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 39629445    |
| train/                  |             |
|    approx_kl            | 0.010904414 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.338       |
|    learning_rate        | 5.43e-05    |
|    loss                 | 0.0499      |
|    n_updates            | 7405        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 646      |
|    time_elapsed    | 229205   |
|    total_timesteps | 39690240 |
---------------------------------
Eval num_timesteps=39690886, episode_reward=0.10 +/- 0.97
Episode length: 29.97 +/- 0.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 39690886    |
| train/                  |             |
|    approx_kl            | 0.011145624 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.346       |
|    learning_rate        | 5.43e-05    |
|    loss                 | 0.0827      |
|    n_updates            | 7410        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 173      |
|    iterations      | 647      |
|    time_elapsed    | 229527   |
|    total_timesteps | 39751680 |
---------------------------------
Eval num_timesteps=39752327, episode_reward=0.12 +/- 0.98
Episode length: 30.06 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 39752327    |
| train/                  |             |
|    approx_kl            | 0.011194739 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.602      |
|    explained_variance   | 0.316       |
|    learning_rate        | 5.42e-05    |
|    loss                 | 0.0902      |
|    n_updates            | 7415        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 648      |
|    time_elapsed    | 229851   |
|    total_timesteps | 39813120 |
---------------------------------
Eval num_timesteps=39813768, episode_reward=-0.07 +/- 0.97
Episode length: 29.99 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.074     |
| time/                   |            |
|    total_timesteps      | 39813768   |
| train/                  |            |
|    approx_kl            | 0.01111238 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.602     |
|    explained_variance   | 0.324      |
|    learning_rate        | 5.42e-05   |
|    loss                 | 0.0442     |
|    n_updates            | 7420       |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 649      |
|    time_elapsed    | 230172   |
|    total_timesteps | 39874560 |
---------------------------------
Eval num_timesteps=39875209, episode_reward=0.06 +/- 0.98
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 39875209    |
| train/                  |             |
|    approx_kl            | 0.011315336 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.324       |
|    learning_rate        | 5.41e-05    |
|    loss                 | 0.085       |
|    n_updates            | 7425        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 650      |
|    time_elapsed    | 230491   |
|    total_timesteps | 39936000 |
---------------------------------
Eval num_timesteps=39936650, episode_reward=0.07 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 39936650    |
| train/                  |             |
|    approx_kl            | 0.010836864 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.342       |
|    learning_rate        | 5.41e-05    |
|    loss                 | 0.107       |
|    n_updates            | 7430        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 651      |
|    time_elapsed    | 230810   |
|    total_timesteps | 39997440 |
---------------------------------
Eval num_timesteps=39998091, episode_reward=0.08 +/- 0.98
Episode length: 29.96 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 39998091    |
| train/                  |             |
|    approx_kl            | 0.011279109 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.342       |
|    learning_rate        | 5.4e-05     |
|    loss                 | 0.0982      |
|    n_updates            | 7435        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 652      |
|    time_elapsed    | 231128   |
|    total_timesteps | 40058880 |
---------------------------------
Eval num_timesteps=40059532, episode_reward=-0.01 +/- 0.99
Episode length: 29.99 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 40059532    |
| train/                  |             |
|    approx_kl            | 0.011375711 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.331       |
|    learning_rate        | 5.39e-05    |
|    loss                 | 0.118       |
|    n_updates            | 7440        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 653      |
|    time_elapsed    | 231446   |
|    total_timesteps | 40120320 |
---------------------------------
Eval num_timesteps=40120973, episode_reward=-0.04 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.042      |
| time/                   |             |
|    total_timesteps      | 40120973    |
| train/                  |             |
|    approx_kl            | 0.010829275 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.345       |
|    learning_rate        | 5.39e-05    |
|    loss                 | 0.0913      |
|    n_updates            | 7445        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 654      |
|    time_elapsed    | 231764   |
|    total_timesteps | 40181760 |
---------------------------------
Eval num_timesteps=40182414, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 40182414    |
| train/                  |             |
|    approx_kl            | 0.011035935 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.326       |
|    learning_rate        | 5.38e-05    |
|    loss                 | 0.0869      |
|    n_updates            | 7450        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 655      |
|    time_elapsed    | 232082   |
|    total_timesteps | 40243200 |
---------------------------------
Eval num_timesteps=40243855, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 40243855    |
| train/                  |             |
|    approx_kl            | 0.010914168 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.317       |
|    learning_rate        | 5.38e-05    |
|    loss                 | 0.0985      |
|    n_updates            | 7455        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 656      |
|    time_elapsed    | 232400   |
|    total_timesteps | 40304640 |
---------------------------------
Eval num_timesteps=40305296, episode_reward=0.01 +/- 0.98
Episode length: 29.99 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 40305296    |
| train/                  |             |
|    approx_kl            | 0.011246839 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.602      |
|    explained_variance   | 0.319       |
|    learning_rate        | 5.37e-05    |
|    loss                 | 0.0538      |
|    n_updates            | 7460        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 657      |
|    time_elapsed    | 232720   |
|    total_timesteps | 40366080 |
---------------------------------
Eval num_timesteps=40366737, episode_reward=0.10 +/- 0.98
Episode length: 30.01 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.096      |
| time/                   |            |
|    total_timesteps      | 40366737   |
| train/                  |            |
|    approx_kl            | 0.01114711 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.601     |
|    explained_variance   | 0.335      |
|    learning_rate        | 5.37e-05   |
|    loss                 | 0.0851     |
|    n_updates            | 7465       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 658      |
|    time_elapsed    | 233042   |
|    total_timesteps | 40427520 |
---------------------------------
Eval num_timesteps=40428178, episode_reward=0.05 +/- 0.98
Episode length: 30.02 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 40428178    |
| train/                  |             |
|    approx_kl            | 0.011149352 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.602      |
|    explained_variance   | 0.328       |
|    learning_rate        | 5.36e-05    |
|    loss                 | 0.0691      |
|    n_updates            | 7470        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 659      |
|    time_elapsed    | 233364   |
|    total_timesteps | 40488960 |
---------------------------------
Eval num_timesteps=40489619, episode_reward=0.07 +/- 0.97
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 40489619    |
| train/                  |             |
|    approx_kl            | 0.011195902 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.341       |
|    learning_rate        | 5.36e-05    |
|    loss                 | 0.0563      |
|    n_updates            | 7475        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 660      |
|    time_elapsed    | 233688   |
|    total_timesteps | 40550400 |
---------------------------------
Eval num_timesteps=40551060, episode_reward=-0.03 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 40551060    |
| train/                  |             |
|    approx_kl            | 0.011051954 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.329       |
|    learning_rate        | 5.35e-05    |
|    loss                 | 0.0685      |
|    n_updates            | 7480        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 661      |
|    time_elapsed    | 234009   |
|    total_timesteps | 40611840 |
---------------------------------
Eval num_timesteps=40612501, episode_reward=0.04 +/- 0.99
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 40612501    |
| train/                  |             |
|    approx_kl            | 0.010862007 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.324       |
|    learning_rate        | 5.34e-05    |
|    loss                 | 0.0664      |
|    n_updates            | 7485        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 173      |
|    iterations      | 662      |
|    time_elapsed    | 234327   |
|    total_timesteps | 40673280 |
---------------------------------
Eval num_timesteps=40673942, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 40673942    |
| train/                  |             |
|    approx_kl            | 0.010752844 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.336       |
|    learning_rate        | 5.34e-05    |
|    loss                 | 0.0605      |
|    n_updates            | 7490        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 663      |
|    time_elapsed    | 234645   |
|    total_timesteps | 40734720 |
---------------------------------
Eval num_timesteps=40735383, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.052        |
| time/                   |              |
|    total_timesteps      | 40735383     |
| train/                  |              |
|    approx_kl            | 0.0112103475 |
|    clip_fraction        | 0.136        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.6         |
|    explained_variance   | 0.319        |
|    learning_rate        | 5.33e-05     |
|    loss                 | 0.102        |
|    n_updates            | 7495         |
|    policy_gradient_loss | -0.0165      |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 664      |
|    time_elapsed    | 234964   |
|    total_timesteps | 40796160 |
---------------------------------
Eval num_timesteps=40796824, episode_reward=0.04 +/- 0.98
Episode length: 30.05 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 40796824    |
| train/                  |             |
|    approx_kl            | 0.011271026 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.322       |
|    learning_rate        | 5.33e-05    |
|    loss                 | 0.137       |
|    n_updates            | 7500        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 665      |
|    time_elapsed    | 235282   |
|    total_timesteps | 40857600 |
---------------------------------
Eval num_timesteps=40858265, episode_reward=0.10 +/- 0.98
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 40858265    |
| train/                  |             |
|    approx_kl            | 0.011048331 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.333       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 0.0567      |
|    n_updates            | 7505        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 666      |
|    time_elapsed    | 235600   |
|    total_timesteps | 40919040 |
---------------------------------
Eval num_timesteps=40919706, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 40919706    |
| train/                  |             |
|    approx_kl            | 0.011108636 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.336       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 0.0825      |
|    n_updates            | 7510        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 667      |
|    time_elapsed    | 235917   |
|    total_timesteps | 40980480 |
---------------------------------
Eval num_timesteps=40981147, episode_reward=0.09 +/- 0.98
Episode length: 30.03 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 40981147    |
| train/                  |             |
|    approx_kl            | 0.011313244 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.309       |
|    learning_rate        | 5.31e-05    |
|    loss                 | 0.124       |
|    n_updates            | 7515        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 668      |
|    time_elapsed    | 236236   |
|    total_timesteps | 41041920 |
---------------------------------
Eval num_timesteps=41042588, episode_reward=0.05 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 41042588    |
| train/                  |             |
|    approx_kl            | 0.011247724 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.316       |
|    learning_rate        | 5.31e-05    |
|    loss                 | 0.107       |
|    n_updates            | 7520        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 669      |
|    time_elapsed    | 236557   |
|    total_timesteps | 41103360 |
---------------------------------
Eval num_timesteps=41104029, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 41104029    |
| train/                  |             |
|    approx_kl            | 0.011155924 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.341       |
|    learning_rate        | 5.3e-05     |
|    loss                 | 0.0545      |
|    n_updates            | 7525        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 173      |
|    iterations      | 670      |
|    time_elapsed    | 236879   |
|    total_timesteps | 41164800 |
---------------------------------
Eval num_timesteps=41165470, episode_reward=0.06 +/- 0.98
Episode length: 30.01 +/- 0.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 41165470    |
| train/                  |             |
|    approx_kl            | 0.011242316 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.35        |
|    learning_rate        | 5.3e-05     |
|    loss                 | 0.105       |
|    n_updates            | 7530        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 671      |
|    time_elapsed    | 237202   |
|    total_timesteps | 41226240 |
---------------------------------
Eval num_timesteps=41226911, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 1.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 41226911    |
| train/                  |             |
|    approx_kl            | 0.011081058 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.327       |
|    learning_rate        | 5.29e-05    |
|    loss                 | 0.0686      |
|    n_updates            | 7535        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 672      |
|    time_elapsed    | 237525   |
|    total_timesteps | 41287680 |
---------------------------------
Eval num_timesteps=41288352, episode_reward=-0.02 +/- 0.99
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 41288352    |
| train/                  |             |
|    approx_kl            | 0.011457758 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.34        |
|    learning_rate        | 5.28e-05    |
|    loss                 | 0.0689      |
|    n_updates            | 7540        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 673      |
|    time_elapsed    | 237844   |
|    total_timesteps | 41349120 |
---------------------------------
Eval num_timesteps=41349793, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 41349793    |
| train/                  |             |
|    approx_kl            | 0.011049471 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.328       |
|    learning_rate        | 5.28e-05    |
|    loss                 | 0.127       |
|    n_updates            | 7545        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 674      |
|    time_elapsed    | 238163   |
|    total_timesteps | 41410560 |
---------------------------------
Eval num_timesteps=41411234, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 41411234    |
| train/                  |             |
|    approx_kl            | 0.010975664 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.338       |
|    learning_rate        | 5.27e-05    |
|    loss                 | 0.0667      |
|    n_updates            | 7550        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 675      |
|    time_elapsed    | 238480   |
|    total_timesteps | 41472000 |
---------------------------------
Eval num_timesteps=41472675, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 41472675    |
| train/                  |             |
|    approx_kl            | 0.010959283 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.32        |
|    learning_rate        | 5.27e-05    |
|    loss                 | 0.129       |
|    n_updates            | 7555        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 676      |
|    time_elapsed    | 238797   |
|    total_timesteps | 41533440 |
---------------------------------
Eval num_timesteps=41534116, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 41534116    |
| train/                  |             |
|    approx_kl            | 0.010721208 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.329       |
|    learning_rate        | 5.26e-05    |
|    loss                 | 0.13        |
|    n_updates            | 7560        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 677      |
|    time_elapsed    | 239115   |
|    total_timesteps | 41594880 |
---------------------------------
Eval num_timesteps=41595557, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 41595557    |
| train/                  |             |
|    approx_kl            | 0.011166573 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.334       |
|    learning_rate        | 5.26e-05    |
|    loss                 | 0.0737      |
|    n_updates            | 7565        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 678      |
|    time_elapsed    | 239432   |
|    total_timesteps | 41656320 |
---------------------------------
Eval num_timesteps=41656998, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 41656998    |
| train/                  |             |
|    approx_kl            | 0.011108572 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.326       |
|    learning_rate        | 5.25e-05    |
|    loss                 | 0.0631      |
|    n_updates            | 7570        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 679      |
|    time_elapsed    | 239751   |
|    total_timesteps | 41717760 |
---------------------------------
Eval num_timesteps=41718439, episode_reward=0.03 +/- 0.98
Episode length: 30.04 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.028      |
| time/                   |            |
|    total_timesteps      | 41718439   |
| train/                  |            |
|    approx_kl            | 0.01146349 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.589     |
|    explained_variance   | 0.331      |
|    learning_rate        | 5.25e-05   |
|    loss                 | 0.0843     |
|    n_updates            | 7575       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 680      |
|    time_elapsed    | 240071   |
|    total_timesteps | 41779200 |
---------------------------------
Eval num_timesteps=41779880, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 41779880    |
| train/                  |             |
|    approx_kl            | 0.010960186 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.315       |
|    learning_rate        | 5.24e-05    |
|    loss                 | 0.0978      |
|    n_updates            | 7580        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 681      |
|    time_elapsed    | 240392   |
|    total_timesteps | 41840640 |
---------------------------------
Eval num_timesteps=41841321, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 41841321    |
| train/                  |             |
|    approx_kl            | 0.010856947 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.327       |
|    learning_rate        | 5.23e-05    |
|    loss                 | 0.0704      |
|    n_updates            | 7585        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 682      |
|    time_elapsed    | 240714   |
|    total_timesteps | 41902080 |
---------------------------------
Eval num_timesteps=41902762, episode_reward=0.03 +/- 0.98
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 41902762    |
| train/                  |             |
|    approx_kl            | 0.011620894 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.345       |
|    learning_rate        | 5.23e-05    |
|    loss                 | 0.0659      |
|    n_updates            | 7590        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 683      |
|    time_elapsed    | 241038   |
|    total_timesteps | 41963520 |
---------------------------------
Eval num_timesteps=41964203, episode_reward=0.06 +/- 0.98
Episode length: 29.88 +/- 1.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 41964203    |
| train/                  |             |
|    approx_kl            | 0.010719177 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.334       |
|    learning_rate        | 5.22e-05    |
|    loss                 | 0.0578      |
|    n_updates            | 7595        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 684      |
|    time_elapsed    | 241357   |
|    total_timesteps | 42024960 |
---------------------------------
Eval num_timesteps=42025644, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 42025644    |
| train/                  |             |
|    approx_kl            | 0.010500352 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.332       |
|    learning_rate        | 5.22e-05    |
|    loss                 | 0.0674      |
|    n_updates            | 7600        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 685      |
|    time_elapsed    | 241676   |
|    total_timesteps | 42086400 |
---------------------------------
Eval num_timesteps=42087085, episode_reward=0.06 +/- 0.97
Episode length: 29.95 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 42087085    |
| train/                  |             |
|    approx_kl            | 0.010672534 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.321       |
|    learning_rate        | 5.21e-05    |
|    loss                 | 0.0588      |
|    n_updates            | 7605        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 686      |
|    time_elapsed    | 241995   |
|    total_timesteps | 42147840 |
---------------------------------
Eval num_timesteps=42148526, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 42148526    |
| train/                  |             |
|    approx_kl            | 0.011409486 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.328       |
|    learning_rate        | 5.21e-05    |
|    loss                 | 0.112       |
|    n_updates            | 7610        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 687      |
|    time_elapsed    | 242311   |
|    total_timesteps | 42209280 |
---------------------------------
Eval num_timesteps=42209967, episode_reward=0.03 +/- 0.98
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 42209967    |
| train/                  |             |
|    approx_kl            | 0.010756154 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.583      |
|    explained_variance   | 0.334       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0806      |
|    n_updates            | 7615        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 688      |
|    time_elapsed    | 242628   |
|    total_timesteps | 42270720 |
---------------------------------
Eval num_timesteps=42271408, episode_reward=0.09 +/- 0.97
Episode length: 29.97 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 42271408    |
| train/                  |             |
|    approx_kl            | 0.011040636 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.31        |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0917      |
|    n_updates            | 7620        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 689      |
|    time_elapsed    | 242945   |
|    total_timesteps | 42332160 |
---------------------------------
Eval num_timesteps=42332849, episode_reward=0.03 +/- 0.98
Episode length: 29.78 +/- 2.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 42332849    |
| train/                  |             |
|    approx_kl            | 0.010957043 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.19e-05    |
|    loss                 | 0.0676      |
|    n_updates            | 7625        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 174      |
|    iterations      | 690      |
|    time_elapsed    | 243263   |
|    total_timesteps | 42393600 |
---------------------------------
Eval num_timesteps=42394290, episode_reward=0.11 +/- 0.98
Episode length: 29.95 +/- 1.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.106        |
| time/                   |              |
|    total_timesteps      | 42394290     |
| train/                  |              |
|    approx_kl            | 0.0108702285 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.584       |
|    explained_variance   | 0.347        |
|    learning_rate        | 5.18e-05     |
|    loss                 | 0.116        |
|    n_updates            | 7630         |
|    policy_gradient_loss | -0.0167      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 691      |
|    time_elapsed    | 243582   |
|    total_timesteps | 42455040 |
---------------------------------
Eval num_timesteps=42455731, episode_reward=0.04 +/- 0.99
Episode length: 29.86 +/- 2.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 42455731    |
| train/                  |             |
|    approx_kl            | 0.010771649 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.327       |
|    learning_rate        | 5.18e-05    |
|    loss                 | 0.0724      |
|    n_updates            | 7635        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 692      |
|    time_elapsed    | 243902   |
|    total_timesteps | 42516480 |
---------------------------------
Eval num_timesteps=42517172, episode_reward=0.04 +/- 0.99
Episode length: 29.93 +/- 1.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.044        |
| time/                   |              |
|    total_timesteps      | 42517172     |
| train/                  |              |
|    approx_kl            | 0.0109532485 |
|    clip_fraction        | 0.132        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.579       |
|    explained_variance   | 0.33         |
|    learning_rate        | 5.17e-05     |
|    loss                 | 0.0714       |
|    n_updates            | 7640         |
|    policy_gradient_loss | -0.0164      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 693      |
|    time_elapsed    | 244224   |
|    total_timesteps | 42577920 |
---------------------------------
Eval num_timesteps=42578613, episode_reward=0.04 +/- 0.98
Episode length: 29.57 +/- 3.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.6        |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 42578613    |
| train/                  |             |
|    approx_kl            | 0.010586385 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.17e-05    |
|    loss                 | 0.0614      |
|    n_updates            | 7645        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 694      |
|    time_elapsed    | 244546   |
|    total_timesteps | 42639360 |
---------------------------------
Eval num_timesteps=42640054, episode_reward=0.01 +/- 0.98
Episode length: 29.69 +/- 2.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 42640054    |
| train/                  |             |
|    approx_kl            | 0.010687357 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.335       |
|    learning_rate        | 5.16e-05    |
|    loss                 | 0.114       |
|    n_updates            | 7650        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 695      |
|    time_elapsed    | 244867   |
|    total_timesteps | 42700800 |
---------------------------------
Eval num_timesteps=42701495, episode_reward=0.04 +/- 0.98
Episode length: 29.17 +/- 4.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 42701495    |
| train/                  |             |
|    approx_kl            | 0.010945445 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.577      |
|    explained_variance   | 0.322       |
|    learning_rate        | 5.16e-05    |
|    loss                 | 0.0927      |
|    n_updates            | 7655        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 696      |
|    time_elapsed    | 245184   |
|    total_timesteps | 42762240 |
---------------------------------
Eval num_timesteps=42762936, episode_reward=0.15 +/- 0.98
Episode length: 29.55 +/- 3.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.148       |
| time/                   |             |
|    total_timesteps      | 42762936    |
| train/                  |             |
|    approx_kl            | 0.010785639 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.343       |
|    learning_rate        | 5.15e-05    |
|    loss                 | 0.0937      |
|    n_updates            | 7660        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.234       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 18
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.2     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 697      |
|    time_elapsed    | 245502   |
|    total_timesteps | 42823680 |
---------------------------------
Eval num_timesteps=42824377, episode_reward=0.00 +/- 0.99
Episode length: 28.87 +/- 4.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.9        |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 42824377    |
| train/                  |             |
|    approx_kl            | 0.010988965 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.334       |
|    learning_rate        | 5.15e-05    |
|    loss                 | 0.0598      |
|    n_updates            | 7665        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.8     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 698      |
|    time_elapsed    | 245819   |
|    total_timesteps | 42885120 |
---------------------------------
Eval num_timesteps=42885818, episode_reward=0.05 +/- 0.98
Episode length: 29.40 +/- 3.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.4        |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 42885818    |
| train/                  |             |
|    approx_kl            | 0.010720903 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.14e-05    |
|    loss                 | 0.0906      |
|    n_updates            | 7670        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 699      |
|    time_elapsed    | 246135   |
|    total_timesteps | 42946560 |
---------------------------------
Eval num_timesteps=42947259, episode_reward=0.09 +/- 0.98
Episode length: 29.48 +/- 3.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 42947259    |
| train/                  |             |
|    approx_kl            | 0.010821528 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.33        |
|    learning_rate        | 5.13e-05    |
|    loss                 | 0.108       |
|    n_updates            | 7675        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 700      |
|    time_elapsed    | 246453   |
|    total_timesteps | 43008000 |
---------------------------------
Eval num_timesteps=43008700, episode_reward=0.01 +/- 0.99
Episode length: 29.36 +/- 3.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.4        |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 43008700    |
| train/                  |             |
|    approx_kl            | 0.010698264 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.327       |
|    learning_rate        | 5.13e-05    |
|    loss                 | 0.0908      |
|    n_updates            | 7680        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 701      |
|    time_elapsed    | 246770   |
|    total_timesteps | 43069440 |
---------------------------------
Eval num_timesteps=43070141, episode_reward=0.05 +/- 0.98
Episode length: 29.03 +/- 4.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 43070141    |
| train/                  |             |
|    approx_kl            | 0.010749894 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.34        |
|    learning_rate        | 5.12e-05    |
|    loss                 | 0.0752      |
|    n_updates            | 7685        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 702      |
|    time_elapsed    | 247087   |
|    total_timesteps | 43130880 |
---------------------------------
Eval num_timesteps=43131582, episode_reward=0.01 +/- 0.99
Episode length: 29.22 +/- 4.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 43131582    |
| train/                  |             |
|    approx_kl            | 0.010745098 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.326       |
|    learning_rate        | 5.12e-05    |
|    loss                 | 0.0987      |
|    n_updates            | 7690        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.2     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 703      |
|    time_elapsed    | 247406   |
|    total_timesteps | 43192320 |
---------------------------------
Eval num_timesteps=43193023, episode_reward=0.07 +/- 0.98
Episode length: 29.49 +/- 3.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 43193023    |
| train/                  |             |
|    approx_kl            | 0.010902109 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.338       |
|    learning_rate        | 5.11e-05    |
|    loss                 | 0.0376      |
|    n_updates            | 7695        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 704      |
|    time_elapsed    | 247727   |
|    total_timesteps | 43253760 |
---------------------------------
Eval num_timesteps=43254464, episode_reward=0.02 +/- 0.98
Episode length: 29.40 +/- 3.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.4        |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 43254464    |
| train/                  |             |
|    approx_kl            | 0.010901907 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.347       |
|    learning_rate        | 5.11e-05    |
|    loss                 | 0.0746      |
|    n_updates            | 7700        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 705      |
|    time_elapsed    | 248049   |
|    total_timesteps | 43315200 |
---------------------------------
Eval num_timesteps=43315905, episode_reward=0.05 +/- 0.99
Episode length: 29.06 +/- 4.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.1        |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 43315905    |
| train/                  |             |
|    approx_kl            | 0.010710297 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.345       |
|    learning_rate        | 5.1e-05     |
|    loss                 | 0.114       |
|    n_updates            | 7705        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.3     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 706      |
|    time_elapsed    | 248370   |
|    total_timesteps | 43376640 |
---------------------------------
Eval num_timesteps=43377346, episode_reward=0.07 +/- 0.98
Episode length: 29.28 +/- 4.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 43377346    |
| train/                  |             |
|    approx_kl            | 0.010611888 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.343       |
|    learning_rate        | 5.1e-05     |
|    loss                 | 0.0943      |
|    n_updates            | 7710        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.3     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 707      |
|    time_elapsed    | 248688   |
|    total_timesteps | 43438080 |
---------------------------------
Eval num_timesteps=43438787, episode_reward=0.10 +/- 0.98
Episode length: 29.51 +/- 3.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 43438787    |
| train/                  |             |
|    approx_kl            | 0.010425979 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.334       |
|    learning_rate        | 5.09e-05    |
|    loss                 | 0.0643      |
|    n_updates            | 7715        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 708      |
|    time_elapsed    | 249005   |
|    total_timesteps | 43499520 |
---------------------------------
Eval num_timesteps=43500228, episode_reward=0.03 +/- 0.98
Episode length: 29.33 +/- 3.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 43500228    |
| train/                  |             |
|    approx_kl            | 0.010702248 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.331       |
|    learning_rate        | 5.09e-05    |
|    loss                 | 0.0912      |
|    n_updates            | 7720        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.3     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 709      |
|    time_elapsed    | 249322   |
|    total_timesteps | 43560960 |
---------------------------------
Eval num_timesteps=43561669, episode_reward=0.07 +/- 0.99
Episode length: 29.02 +/- 4.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 43561669    |
| train/                  |             |
|    approx_kl            | 0.010548684 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.329       |
|    learning_rate        | 5.08e-05    |
|    loss                 | 0.0708      |
|    n_updates            | 7725        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 710      |
|    time_elapsed    | 249639   |
|    total_timesteps | 43622400 |
---------------------------------
Eval num_timesteps=43623110, episode_reward=0.10 +/- 0.98
Episode length: 29.31 +/- 3.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 43623110    |
| train/                  |             |
|    approx_kl            | 0.010533739 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.33        |
|    learning_rate        | 5.07e-05    |
|    loss                 | 0.064       |
|    n_updates            | 7730        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 711      |
|    time_elapsed    | 249955   |
|    total_timesteps | 43683840 |
---------------------------------
Eval num_timesteps=43684551, episode_reward=-0.00 +/- 0.98
Episode length: 29.35 +/- 3.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 43684551    |
| train/                  |             |
|    approx_kl            | 0.010946596 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.348       |
|    learning_rate        | 5.07e-05    |
|    loss                 | 0.0757      |
|    n_updates            | 7735        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.4     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 712      |
|    time_elapsed    | 250271   |
|    total_timesteps | 43745280 |
---------------------------------
Eval num_timesteps=43745992, episode_reward=0.08 +/- 0.98
Episode length: 29.20 +/- 4.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.2       |
|    mean_reward          | 0.084      |
| time/                   |            |
|    total_timesteps      | 43745992   |
| train/                  |            |
|    approx_kl            | 0.01053404 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.574     |
|    explained_variance   | 0.318      |
|    learning_rate        | 5.06e-05   |
|    loss                 | 0.0664     |
|    n_updates            | 7740       |
|    policy_gradient_loss | -0.0164    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.1     |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 713      |
|    time_elapsed    | 250588   |
|    total_timesteps | 43806720 |
---------------------------------
Eval num_timesteps=43807433, episode_reward=0.02 +/- 0.98
Episode length: 29.02 +/- 4.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29         |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 43807433   |
| train/                  |            |
|    approx_kl            | 0.01056549 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.572     |
|    explained_variance   | 0.339      |
|    learning_rate        | 5.06e-05   |
|    loss                 | 0.046      |
|    n_updates            | 7745       |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 714      |
|    time_elapsed    | 250907   |
|    total_timesteps | 43868160 |
---------------------------------
Eval num_timesteps=43868874, episode_reward=0.08 +/- 0.97
Episode length: 29.24 +/- 4.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 43868874    |
| train/                  |             |
|    approx_kl            | 0.010689155 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.331       |
|    learning_rate        | 5.05e-05    |
|    loss                 | 0.123       |
|    n_updates            | 7750        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 715      |
|    time_elapsed    | 251227   |
|    total_timesteps | 43929600 |
---------------------------------
Eval num_timesteps=43930315, episode_reward=0.14 +/- 0.98
Episode length: 29.13 +/- 4.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.1        |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 43930315    |
| train/                  |             |
|    approx_kl            | 0.010762344 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.05e-05    |
|    loss                 | 0.0713      |
|    n_updates            | 7755        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 716      |
|    time_elapsed    | 251548   |
|    total_timesteps | 43991040 |
---------------------------------
Eval num_timesteps=43991756, episode_reward=0.08 +/- 0.98
Episode length: 29.18 +/- 4.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 43991756    |
| train/                  |             |
|    approx_kl            | 0.010092707 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.564      |
|    explained_variance   | 0.338       |
|    learning_rate        | 5.04e-05    |
|    loss                 | 0.0647      |
|    n_updates            | 7760        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 717      |
|    time_elapsed    | 251869   |
|    total_timesteps | 44052480 |
---------------------------------
Eval num_timesteps=44053197, episode_reward=0.16 +/- 0.96
Episode length: 29.23 +/- 4.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.2         |
|    mean_reward          | 0.162        |
| time/                   |              |
|    total_timesteps      | 44053197     |
| train/                  |              |
|    approx_kl            | 0.0101991445 |
|    clip_fraction        | 0.127        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.564       |
|    explained_variance   | 0.326        |
|    learning_rate        | 5.04e-05     |
|    loss                 | 0.0948       |
|    n_updates            | 7765         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.242        |
------------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.162
SELFPLAY: new best model, bumping up generation to 19
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 718      |
|    time_elapsed    | 252187   |
|    total_timesteps | 44113920 |
---------------------------------
Eval num_timesteps=44114638, episode_reward=0.02 +/- 0.98
Episode length: 30.03 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 44114638    |
| train/                  |             |
|    approx_kl            | 0.010966124 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.559      |
|    explained_variance   | 0.333       |
|    learning_rate        | 5.03e-05    |
|    loss                 | 0.106       |
|    n_updates            | 7770        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 719      |
|    time_elapsed    | 252506   |
|    total_timesteps | 44175360 |
---------------------------------
Eval num_timesteps=44176079, episode_reward=0.01 +/- 0.99
Episode length: 30.01 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 44176079    |
| train/                  |             |
|    approx_kl            | 0.010410142 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0.346       |
|    learning_rate        | 5.02e-05    |
|    loss                 | 0.0677      |
|    n_updates            | 7775        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 720      |
|    time_elapsed    | 252825   |
|    total_timesteps | 44236800 |
---------------------------------
Eval num_timesteps=44237520, episode_reward=-0.03 +/- 0.99
Episode length: 29.99 +/- 0.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.028       |
| time/                   |              |
|    total_timesteps      | 44237520     |
| train/                  |              |
|    approx_kl            | 0.0103791775 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.558       |
|    explained_variance   | 0.329        |
|    learning_rate        | 5.02e-05     |
|    loss                 | 0.0727       |
|    n_updates            | 7780         |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 721      |
|    time_elapsed    | 253143   |
|    total_timesteps | 44298240 |
---------------------------------
Eval num_timesteps=44298961, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 44298961    |
| train/                  |             |
|    approx_kl            | 0.010393907 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0.328       |
|    learning_rate        | 5.01e-05    |
|    loss                 | 0.0708      |
|    n_updates            | 7785        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 722      |
|    time_elapsed    | 253462   |
|    total_timesteps | 44359680 |
---------------------------------
Eval num_timesteps=44360402, episode_reward=-0.01 +/- 0.98
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 44360402    |
| train/                  |             |
|    approx_kl            | 0.010309397 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.337       |
|    learning_rate        | 5.01e-05    |
|    loss                 | 0.0553      |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 723      |
|    time_elapsed    | 253780   |
|    total_timesteps | 44421120 |
---------------------------------
Eval num_timesteps=44421843, episode_reward=0.10 +/- 0.97
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 44421843    |
| train/                  |             |
|    approx_kl            | 0.010149014 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.55       |
|    explained_variance   | 0.341       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.101       |
|    n_updates            | 7795        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 724      |
|    time_elapsed    | 254097   |
|    total_timesteps | 44482560 |
---------------------------------
Eval num_timesteps=44483284, episode_reward=0.00 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 44483284    |
| train/                  |             |
|    approx_kl            | 0.010077374 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.351       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.115       |
|    n_updates            | 7800        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 725      |
|    time_elapsed    | 254417   |
|    total_timesteps | 44544000 |
---------------------------------
Eval num_timesteps=44544725, episode_reward=-0.12 +/- 0.98
Episode length: 29.80 +/- 1.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.8         |
|    mean_reward          | -0.118       |
| time/                   |              |
|    total_timesteps      | 44544725     |
| train/                  |              |
|    approx_kl            | 0.0102518285 |
|    clip_fraction        | 0.127        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.547       |
|    explained_variance   | 0.341        |
|    learning_rate        | 4.99e-05     |
|    loss                 | 0.0736       |
|    n_updates            | 7805         |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 175      |
|    iterations      | 726      |
|    time_elapsed    | 254738   |
|    total_timesteps | 44605440 |
---------------------------------
Eval num_timesteps=44606166, episode_reward=0.04 +/- 0.99
Episode length: 29.93 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 44606166    |
| train/                  |             |
|    approx_kl            | 0.010397797 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.54       |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.99e-05    |
|    loss                 | 0.0888      |
|    n_updates            | 7810        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 727      |
|    time_elapsed    | 255061   |
|    total_timesteps | 44666880 |
---------------------------------
Eval num_timesteps=44667607, episode_reward=-0.01 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 44667607    |
| train/                  |             |
|    approx_kl            | 0.010252857 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.536      |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.98e-05    |
|    loss                 | 0.127       |
|    n_updates            | 7815        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 728      |
|    time_elapsed    | 255384   |
|    total_timesteps | 44728320 |
---------------------------------
Eval num_timesteps=44729048, episode_reward=0.05 +/- 0.99
Episode length: 29.98 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 44729048    |
| train/                  |             |
|    approx_kl            | 0.010042176 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.357       |
|    learning_rate        | 4.97e-05    |
|    loss                 | 0.087       |
|    n_updates            | 7820        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 729      |
|    time_elapsed    | 255706   |
|    total_timesteps | 44789760 |
---------------------------------
Eval num_timesteps=44790489, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 44790489    |
| train/                  |             |
|    approx_kl            | 0.010196519 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.336       |
|    learning_rate        | 4.97e-05    |
|    loss                 | 0.0833      |
|    n_updates            | 7825        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 730      |
|    time_elapsed    | 256026   |
|    total_timesteps | 44851200 |
---------------------------------
Eval num_timesteps=44851930, episode_reward=0.11 +/- 0.99
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 44851930    |
| train/                  |             |
|    approx_kl            | 0.010089322 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.349       |
|    learning_rate        | 4.96e-05    |
|    loss                 | 0.0607      |
|    n_updates            | 7830        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 731      |
|    time_elapsed    | 256344   |
|    total_timesteps | 44912640 |
---------------------------------
Eval num_timesteps=44913371, episode_reward=0.02 +/- 0.99
Episode length: 29.91 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 44913371    |
| train/                  |             |
|    approx_kl            | 0.010225596 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.532      |
|    explained_variance   | 0.343       |
|    learning_rate        | 4.96e-05    |
|    loss                 | 0.0613      |
|    n_updates            | 7835        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 732      |
|    time_elapsed    | 256662   |
|    total_timesteps | 44974080 |
---------------------------------
Eval num_timesteps=44974812, episode_reward=0.08 +/- 0.98
Episode length: 29.94 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 44974812    |
| train/                  |             |
|    approx_kl            | 0.010012933 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.529      |
|    explained_variance   | 0.362       |
|    learning_rate        | 4.95e-05    |
|    loss                 | 0.055       |
|    n_updates            | 7840        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 733      |
|    time_elapsed    | 256980   |
|    total_timesteps | 45035520 |
---------------------------------
Eval num_timesteps=45036253, episode_reward=0.03 +/- 0.98
Episode length: 30.02 +/- 0.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.026        |
| time/                   |              |
|    total_timesteps      | 45036253     |
| train/                  |              |
|    approx_kl            | 0.0104383575 |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.528       |
|    explained_variance   | 0.347        |
|    learning_rate        | 4.95e-05     |
|    loss                 | 0.106        |
|    n_updates            | 7845         |
|    policy_gradient_loss | -0.0162      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 734      |
|    time_elapsed    | 257298   |
|    total_timesteps | 45096960 |
---------------------------------
Eval num_timesteps=45097694, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 45097694    |
| train/                  |             |
|    approx_kl            | 0.010099768 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.349       |
|    learning_rate        | 4.94e-05    |
|    loss                 | 0.089       |
|    n_updates            | 7850        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 735      |
|    time_elapsed    | 257615   |
|    total_timesteps | 45158400 |
---------------------------------
Eval num_timesteps=45159135, episode_reward=0.11 +/- 0.98
Episode length: 30.01 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 45159135    |
| train/                  |             |
|    approx_kl            | 0.010197266 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.362       |
|    learning_rate        | 4.94e-05    |
|    loss                 | 0.116       |
|    n_updates            | 7855        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 736      |
|    time_elapsed    | 257933   |
|    total_timesteps | 45219840 |
---------------------------------
Eval num_timesteps=45220576, episode_reward=-0.01 +/- 0.99
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 45220576    |
| train/                  |             |
|    approx_kl            | 0.009963989 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.36        |
|    learning_rate        | 4.93e-05    |
|    loss                 | 0.134       |
|    n_updates            | 7860        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 737      |
|    time_elapsed    | 258253   |
|    total_timesteps | 45281280 |
---------------------------------
Eval num_timesteps=45282017, episode_reward=0.13 +/- 0.98
Episode length: 30.05 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.132       |
| time/                   |             |
|    total_timesteps      | 45282017    |
| train/                  |             |
|    approx_kl            | 0.010244024 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.347       |
|    learning_rate        | 4.92e-05    |
|    loss                 | 0.145       |
|    n_updates            | 7865        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 738      |
|    time_elapsed    | 258575   |
|    total_timesteps | 45342720 |
---------------------------------
Eval num_timesteps=45343458, episode_reward=0.10 +/- 0.98
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 45343458    |
| train/                  |             |
|    approx_kl            | 0.010111809 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.344       |
|    learning_rate        | 4.92e-05    |
|    loss                 | 0.109       |
|    n_updates            | 7870        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 739      |
|    time_elapsed    | 258898   |
|    total_timesteps | 45404160 |
---------------------------------
Eval num_timesteps=45404899, episode_reward=0.08 +/- 0.98
Episode length: 29.95 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 45404899    |
| train/                  |             |
|    approx_kl            | 0.010031504 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.521      |
|    explained_variance   | 0.357       |
|    learning_rate        | 4.91e-05    |
|    loss                 | 0.0778      |
|    n_updates            | 7875        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 740      |
|    time_elapsed    | 259221   |
|    total_timesteps | 45465600 |
---------------------------------
Eval num_timesteps=45466340, episode_reward=0.03 +/- 0.97
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 45466340    |
| train/                  |             |
|    approx_kl            | 0.009945632 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0.364       |
|    learning_rate        | 4.91e-05    |
|    loss                 | 0.0685      |
|    n_updates            | 7880        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 741      |
|    time_elapsed    | 259541   |
|    total_timesteps | 45527040 |
---------------------------------
Eval num_timesteps=45527781, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 45527781    |
| train/                  |             |
|    approx_kl            | 0.010296729 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.358       |
|    learning_rate        | 4.9e-05     |
|    loss                 | 0.115       |
|    n_updates            | 7885        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 742      |
|    time_elapsed    | 259860   |
|    total_timesteps | 45588480 |
---------------------------------
Eval num_timesteps=45589222, episode_reward=0.08 +/- 0.97
Episode length: 30.05 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 45589222    |
| train/                  |             |
|    approx_kl            | 0.009903691 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.516      |
|    explained_variance   | 0.353       |
|    learning_rate        | 4.9e-05     |
|    loss                 | 0.0887      |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 743      |
|    time_elapsed    | 260178   |
|    total_timesteps | 45649920 |
---------------------------------
Eval num_timesteps=45650663, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 1.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 45650663    |
| train/                  |             |
|    approx_kl            | 0.009983574 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.358       |
|    learning_rate        | 4.89e-05    |
|    loss                 | 0.101       |
|    n_updates            | 7895        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 744      |
|    time_elapsed    | 260495   |
|    total_timesteps | 45711360 |
---------------------------------
Eval num_timesteps=45712104, episode_reward=-0.02 +/- 0.97
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 45712104    |
| train/                  |             |
|    approx_kl            | 0.009712468 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.37        |
|    learning_rate        | 4.89e-05    |
|    loss                 | 0.0945      |
|    n_updates            | 7900        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 745      |
|    time_elapsed    | 260812   |
|    total_timesteps | 45772800 |
---------------------------------
Eval num_timesteps=45773545, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 45773545    |
| train/                  |             |
|    approx_kl            | 0.009857436 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.513      |
|    explained_variance   | 0.357       |
|    learning_rate        | 4.88e-05    |
|    loss                 | 0.0461      |
|    n_updates            | 7905        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 746      |
|    time_elapsed    | 261130   |
|    total_timesteps | 45834240 |
---------------------------------
Eval num_timesteps=45834986, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.106      |
| time/                   |            |
|    total_timesteps      | 45834986   |
| train/                  |            |
|    approx_kl            | 0.01035421 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.515     |
|    explained_variance   | 0.361      |
|    learning_rate        | 4.87e-05   |
|    loss                 | 0.0479     |
|    n_updates            | 7910       |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 747      |
|    time_elapsed    | 261447   |
|    total_timesteps | 45895680 |
---------------------------------
Eval num_timesteps=45896427, episode_reward=0.14 +/- 0.97
Episode length: 30.06 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.14        |
| time/                   |             |
|    total_timesteps      | 45896427    |
| train/                  |             |
|    approx_kl            | 0.010198265 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.365       |
|    learning_rate        | 4.87e-05    |
|    loss                 | 0.0912      |
|    n_updates            | 7915        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 748      |
|    time_elapsed    | 261767   |
|    total_timesteps | 45957120 |
---------------------------------
Eval num_timesteps=45957868, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 45957868    |
| train/                  |             |
|    approx_kl            | 0.010005842 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.358       |
|    learning_rate        | 4.86e-05    |
|    loss                 | 0.079       |
|    n_updates            | 7920        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 749      |
|    time_elapsed    | 262088   |
|    total_timesteps | 46018560 |
---------------------------------
Eval num_timesteps=46019309, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 46019309    |
| train/                  |             |
|    approx_kl            | 0.009811072 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.357       |
|    learning_rate        | 4.86e-05    |
|    loss                 | 0.0662      |
|    n_updates            | 7925        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 750      |
|    time_elapsed    | 262410   |
|    total_timesteps | 46080000 |
---------------------------------
Eval num_timesteps=46080750, episode_reward=0.10 +/- 0.97
Episode length: 30.01 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 46080750    |
| train/                  |             |
|    approx_kl            | 0.009955378 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.358       |
|    learning_rate        | 4.85e-05    |
|    loss                 | 0.0809      |
|    n_updates            | 7930        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 751      |
|    time_elapsed    | 262732   |
|    total_timesteps | 46141440 |
---------------------------------
Eval num_timesteps=46142191, episode_reward=0.10 +/- 0.98
Episode length: 30.06 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 46142191    |
| train/                  |             |
|    approx_kl            | 0.009850276 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.365       |
|    learning_rate        | 4.85e-05    |
|    loss                 | 0.0791      |
|    n_updates            | 7935        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 752      |
|    time_elapsed    | 263054   |
|    total_timesteps | 46202880 |
---------------------------------
Eval num_timesteps=46203632, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 46203632    |
| train/                  |             |
|    approx_kl            | 0.009979287 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.352       |
|    learning_rate        | 4.84e-05    |
|    loss                 | 0.0482      |
|    n_updates            | 7940        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 753      |
|    time_elapsed    | 263373   |
|    total_timesteps | 46264320 |
---------------------------------
Eval num_timesteps=46265073, episode_reward=0.07 +/- 0.98
Episode length: 29.94 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 46265073    |
| train/                  |             |
|    approx_kl            | 0.009653407 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.372       |
|    learning_rate        | 4.84e-05    |
|    loss                 | 0.104       |
|    n_updates            | 7945        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 754      |
|    time_elapsed    | 263691   |
|    total_timesteps | 46325760 |
---------------------------------
Eval num_timesteps=46326514, episode_reward=0.06 +/- 0.98
Episode length: 29.91 +/- 1.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 46326514    |
| train/                  |             |
|    approx_kl            | 0.009994485 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.355       |
|    learning_rate        | 4.83e-05    |
|    loss                 | 0.135       |
|    n_updates            | 7950        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 755      |
|    time_elapsed    | 264008   |
|    total_timesteps | 46387200 |
---------------------------------
Eval num_timesteps=46387955, episode_reward=0.07 +/- 0.99
Episode length: 29.99 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 46387955    |
| train/                  |             |
|    approx_kl            | 0.010166087 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.371       |
|    learning_rate        | 4.83e-05    |
|    loss                 | 0.0416      |
|    n_updates            | 7955        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 175      |
|    iterations      | 756      |
|    time_elapsed    | 264325   |
|    total_timesteps | 46448640 |
---------------------------------
Eval num_timesteps=46449396, episode_reward=0.16 +/- 0.97
Episode length: 29.96 +/- 1.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.16        |
| time/                   |             |
|    total_timesteps      | 46449396    |
| train/                  |             |
|    approx_kl            | 0.009715642 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.361       |
|    learning_rate        | 4.82e-05    |
|    loss                 | 0.0815      |
|    n_updates            | 7960        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.229       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.16
SELFPLAY: new best model, bumping up generation to 20
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.22    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 757      |
|    time_elapsed    | 264643   |
|    total_timesteps | 46510080 |
---------------------------------
Eval num_timesteps=46510837, episode_reward=0.01 +/- 0.99
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 46510837    |
| train/                  |             |
|    approx_kl            | 0.010027866 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.358       |
|    learning_rate        | 4.81e-05    |
|    loss                 | 0.0671      |
|    n_updates            | 7965        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 758      |
|    time_elapsed    | 264961   |
|    total_timesteps | 46571520 |
---------------------------------
Eval num_timesteps=46572278, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 46572278    |
| train/                  |             |
|    approx_kl            | 0.009644941 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.364       |
|    learning_rate        | 4.81e-05    |
|    loss                 | 0.0746      |
|    n_updates            | 7970        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 175      |
|    iterations      | 759      |
|    time_elapsed    | 265279   |
|    total_timesteps | 46632960 |
---------------------------------
Eval num_timesteps=46633719, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 46633719    |
| train/                  |             |
|    approx_kl            | 0.009663839 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.339       |
|    learning_rate        | 4.8e-05     |
|    loss                 | 0.065       |
|    n_updates            | 7975        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 760      |
|    time_elapsed    | 265600   |
|    total_timesteps | 46694400 |
---------------------------------
Eval num_timesteps=46695160, episode_reward=-0.00 +/- 0.99
Episode length: 29.97 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.004     |
| time/                   |            |
|    total_timesteps      | 46695160   |
| train/                  |            |
|    approx_kl            | 0.00998557 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.354      |
|    learning_rate        | 4.8e-05    |
|    loss                 | 0.106      |
|    n_updates            | 7980       |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 761      |
|    time_elapsed    | 265922   |
|    total_timesteps | 46755840 |
---------------------------------
Eval num_timesteps=46756601, episode_reward=0.08 +/- 0.99
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 46756601    |
| train/                  |             |
|    approx_kl            | 0.009960012 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0.35        |
|    learning_rate        | 4.79e-05    |
|    loss                 | 0.0853      |
|    n_updates            | 7985        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 175      |
|    iterations      | 762      |
|    time_elapsed    | 266244   |
|    total_timesteps | 46817280 |
---------------------------------
Eval num_timesteps=46818042, episode_reward=0.01 +/- 0.98
Episode length: 29.93 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 46818042    |
| train/                  |             |
|    approx_kl            | 0.009689242 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.525      |
|    explained_variance   | 0.367       |
|    learning_rate        | 4.79e-05    |
|    loss                 | 0.041       |
|    n_updates            | 7990        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 763      |
|    time_elapsed    | 266568   |
|    total_timesteps | 46878720 |
---------------------------------
Eval num_timesteps=46879483, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 46879483    |
| train/                  |             |
|    approx_kl            | 0.009864911 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.36        |
|    learning_rate        | 4.78e-05    |
|    loss                 | 0.0516      |
|    n_updates            | 7995        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 764      |
|    time_elapsed    | 266889   |
|    total_timesteps | 46940160 |
---------------------------------
Eval num_timesteps=46940924, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 46940924    |
| train/                  |             |
|    approx_kl            | 0.010005055 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.343       |
|    learning_rate        | 4.78e-05    |
|    loss                 | 0.0784      |
|    n_updates            | 8000        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 765      |
|    time_elapsed    | 267208   |
|    total_timesteps | 47001600 |
---------------------------------
Eval num_timesteps=47002365, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 1.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 47002365   |
| train/                  |            |
|    approx_kl            | 0.01009004 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.542     |
|    explained_variance   | 0.354      |
|    learning_rate        | 4.77e-05   |
|    loss                 | 0.0721     |
|    n_updates            | 8005       |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 766      |
|    time_elapsed    | 267526   |
|    total_timesteps | 47063040 |
---------------------------------
Eval num_timesteps=47063806, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 1.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 47063806    |
| train/                  |             |
|    approx_kl            | 0.009851497 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.547      |
|    explained_variance   | 0.335       |
|    learning_rate        | 4.76e-05    |
|    loss                 | 0.0938      |
|    n_updates            | 8010        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 767      |
|    time_elapsed    | 267844   |
|    total_timesteps | 47124480 |
---------------------------------
Eval num_timesteps=47125247, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 47125247    |
| train/                  |             |
|    approx_kl            | 0.009843542 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.348       |
|    learning_rate        | 4.76e-05    |
|    loss                 | 0.0787      |
|    n_updates            | 8015        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 768      |
|    time_elapsed    | 268162   |
|    total_timesteps | 47185920 |
---------------------------------
Eval num_timesteps=47186688, episode_reward=0.04 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 47186688    |
| train/                  |             |
|    approx_kl            | 0.009582088 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.356       |
|    learning_rate        | 4.75e-05    |
|    loss                 | 0.102       |
|    n_updates            | 8020        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 769      |
|    time_elapsed    | 268480   |
|    total_timesteps | 47247360 |
---------------------------------
Eval num_timesteps=47248129, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.028      |
| time/                   |            |
|    total_timesteps      | 47248129   |
| train/                  |            |
|    approx_kl            | 0.00951156 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.551     |
|    explained_variance   | 0.344      |
|    learning_rate        | 4.75e-05   |
|    loss                 | 0.102      |
|    n_updates            | 8025       |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 770      |
|    time_elapsed    | 268798   |
|    total_timesteps | 47308800 |
---------------------------------
Eval num_timesteps=47309570, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 47309570    |
| train/                  |             |
|    approx_kl            | 0.009798744 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0.348       |
|    learning_rate        | 4.74e-05    |
|    loss                 | 0.104       |
|    n_updates            | 8030        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 771      |
|    time_elapsed    | 269117   |
|    total_timesteps | 47370240 |
---------------------------------
Eval num_timesteps=47371011, episode_reward=0.00 +/- 0.98
Episode length: 30.02 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.002      |
| time/                   |            |
|    total_timesteps      | 47371011   |
| train/                  |            |
|    approx_kl            | 0.00985391 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.553     |
|    explained_variance   | 0.353      |
|    learning_rate        | 4.74e-05   |
|    loss                 | 0.0849     |
|    n_updates            | 8035       |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 772      |
|    time_elapsed    | 269437   |
|    total_timesteps | 47431680 |
---------------------------------
Eval num_timesteps=47432452, episode_reward=0.15 +/- 0.97
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.146       |
| time/                   |             |
|    total_timesteps      | 47432452    |
| train/                  |             |
|    approx_kl            | 0.009872244 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.353       |
|    learning_rate        | 4.73e-05    |
|    loss                 | 0.104       |
|    n_updates            | 8040        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.234       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.146
SELFPLAY: new best model, bumping up generation to 21
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 773      |
|    time_elapsed    | 269759   |
|    total_timesteps | 47493120 |
---------------------------------
Eval num_timesteps=47493893, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 47493893    |
| train/                  |             |
|    approx_kl            | 0.009958225 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.34        |
|    learning_rate        | 4.73e-05    |
|    loss                 | 0.0553      |
|    n_updates            | 8045        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 774      |
|    time_elapsed    | 270082   |
|    total_timesteps | 47554560 |
---------------------------------
Eval num_timesteps=47555334, episode_reward=-0.00 +/- 0.99
Episode length: 29.99 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 47555334    |
| train/                  |             |
|    approx_kl            | 0.009617188 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.331       |
|    learning_rate        | 4.72e-05    |
|    loss                 | 0.0476      |
|    n_updates            | 8050        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 775      |
|    time_elapsed    | 270405   |
|    total_timesteps | 47616000 |
---------------------------------
Eval num_timesteps=47616775, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.016        |
| time/                   |              |
|    total_timesteps      | 47616775     |
| train/                  |              |
|    approx_kl            | 0.0098646125 |
|    clip_fraction        | 0.127        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.594       |
|    explained_variance   | 0.333        |
|    learning_rate        | 4.71e-05     |
|    loss                 | 0.151        |
|    n_updates            | 8055         |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 776      |
|    time_elapsed    | 270725   |
|    total_timesteps | 47677440 |
---------------------------------
Eval num_timesteps=47678216, episode_reward=0.02 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 47678216    |
| train/                  |             |
|    approx_kl            | 0.009971843 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.343       |
|    learning_rate        | 4.71e-05    |
|    loss                 | 0.0686      |
|    n_updates            | 8060        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 777      |
|    time_elapsed    | 271044   |
|    total_timesteps | 47738880 |
---------------------------------
Eval num_timesteps=47739657, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 47739657    |
| train/                  |             |
|    approx_kl            | 0.009939746 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.341       |
|    learning_rate        | 4.7e-05     |
|    loss                 | 0.0766      |
|    n_updates            | 8065        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 778      |
|    time_elapsed    | 271362   |
|    total_timesteps | 47800320 |
---------------------------------
Eval num_timesteps=47801098, episode_reward=-0.01 +/- 0.99
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 47801098    |
| train/                  |             |
|    approx_kl            | 0.010711402 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.318       |
|    learning_rate        | 4.7e-05     |
|    loss                 | 0.0865      |
|    n_updates            | 8070        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 779      |
|    time_elapsed    | 271679   |
|    total_timesteps | 47861760 |
---------------------------------
Eval num_timesteps=47862539, episode_reward=0.03 +/- 0.98
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.03       |
| time/                   |            |
|    total_timesteps      | 47862539   |
| train/                  |            |
|    approx_kl            | 0.00964746 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.588     |
|    explained_variance   | 0.32       |
|    learning_rate        | 4.69e-05   |
|    loss                 | 0.0751     |
|    n_updates            | 8075       |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 780      |
|    time_elapsed    | 271996   |
|    total_timesteps | 47923200 |
---------------------------------
Eval num_timesteps=47923980, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 47923980    |
| train/                  |             |
|    approx_kl            | 0.009817639 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.346       |
|    learning_rate        | 4.69e-05    |
|    loss                 | 0.068       |
|    n_updates            | 8080        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 781      |
|    time_elapsed    | 272314   |
|    total_timesteps | 47984640 |
---------------------------------
Eval num_timesteps=47985421, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 47985421    |
| train/                  |             |
|    approx_kl            | 0.010255408 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.338       |
|    learning_rate        | 4.68e-05    |
|    loss                 | 0.0435      |
|    n_updates            | 8085        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 782      |
|    time_elapsed    | 272632   |
|    total_timesteps | 48046080 |
---------------------------------
Eval num_timesteps=48046862, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 48046862    |
| train/                  |             |
|    approx_kl            | 0.009840467 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.33        |
|    learning_rate        | 4.68e-05    |
|    loss                 | 0.123       |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 783      |
|    time_elapsed    | 272952   |
|    total_timesteps | 48107520 |
---------------------------------
Eval num_timesteps=48108303, episode_reward=0.01 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 48108303    |
| train/                  |             |
|    approx_kl            | 0.009744926 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.334       |
|    learning_rate        | 4.67e-05    |
|    loss                 | 0.0564      |
|    n_updates            | 8095        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 784      |
|    time_elapsed    | 273273   |
|    total_timesteps | 48168960 |
---------------------------------
Eval num_timesteps=48169744, episode_reward=0.06 +/- 0.97
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 48169744    |
| train/                  |             |
|    approx_kl            | 0.010031281 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.326       |
|    learning_rate        | 4.66e-05    |
|    loss                 | 0.0904      |
|    n_updates            | 8100        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 785      |
|    time_elapsed    | 273595   |
|    total_timesteps | 48230400 |
---------------------------------
Eval num_timesteps=48231185, episode_reward=0.00 +/- 0.98
Episode length: 29.96 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 48231185    |
| train/                  |             |
|    approx_kl            | 0.009702493 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.333       |
|    learning_rate        | 4.66e-05    |
|    loss                 | 0.0665      |
|    n_updates            | 8105        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 786      |
|    time_elapsed    | 273919   |
|    total_timesteps | 48291840 |
---------------------------------
Eval num_timesteps=48292626, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 48292626   |
| train/                  |            |
|    approx_kl            | 0.00986028 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.588     |
|    explained_variance   | 0.35       |
|    learning_rate        | 4.65e-05   |
|    loss                 | 0.103      |
|    n_updates            | 8110       |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 787      |
|    time_elapsed    | 274241   |
|    total_timesteps | 48353280 |
---------------------------------
Eval num_timesteps=48354067, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 48354067    |
| train/                  |             |
|    approx_kl            | 0.009759917 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.348       |
|    learning_rate        | 4.65e-05    |
|    loss                 | 0.0385      |
|    n_updates            | 8115        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 788      |
|    time_elapsed    | 274560   |
|    total_timesteps | 48414720 |
---------------------------------
Eval num_timesteps=48415508, episode_reward=-0.01 +/- 0.98
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 48415508    |
| train/                  |             |
|    approx_kl            | 0.009926651 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.349       |
|    learning_rate        | 4.64e-05    |
|    loss                 | 0.0945      |
|    n_updates            | 8120        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 789      |
|    time_elapsed    | 274878   |
|    total_timesteps | 48476160 |
---------------------------------
Eval num_timesteps=48476949, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 48476949    |
| train/                  |             |
|    approx_kl            | 0.009923001 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.354       |
|    learning_rate        | 4.64e-05    |
|    loss                 | 0.101       |
|    n_updates            | 8125        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 790      |
|    time_elapsed    | 275196   |
|    total_timesteps | 48537600 |
---------------------------------
Eval num_timesteps=48538390, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 48538390    |
| train/                  |             |
|    approx_kl            | 0.010148396 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.331       |
|    learning_rate        | 4.63e-05    |
|    loss                 | 0.119       |
|    n_updates            | 8130        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 791      |
|    time_elapsed    | 275514   |
|    total_timesteps | 48599040 |
---------------------------------
Eval num_timesteps=48599831, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 48599831    |
| train/                  |             |
|    approx_kl            | 0.010036199 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.344       |
|    learning_rate        | 4.63e-05    |
|    loss                 | 0.0971      |
|    n_updates            | 8135        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 792      |
|    time_elapsed    | 275831   |
|    total_timesteps | 48660480 |
---------------------------------
Eval num_timesteps=48661272, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 48661272    |
| train/                  |             |
|    approx_kl            | 0.009714963 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.335       |
|    learning_rate        | 4.62e-05    |
|    loss                 | 0.0705      |
|    n_updates            | 8140        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 793      |
|    time_elapsed    | 276148   |
|    total_timesteps | 48721920 |
---------------------------------
Eval num_timesteps=48722713, episode_reward=-0.06 +/- 0.98
Episode length: 29.91 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.06       |
| time/                   |             |
|    total_timesteps      | 48722713    |
| train/                  |             |
|    approx_kl            | 0.009792434 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.322       |
|    learning_rate        | 4.62e-05    |
|    loss                 | 0.0961      |
|    n_updates            | 8145        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 794      |
|    time_elapsed    | 276467   |
|    total_timesteps | 48783360 |
---------------------------------
Eval num_timesteps=48784154, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 48784154    |
| train/                  |             |
|    approx_kl            | 0.009488923 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.336       |
|    learning_rate        | 4.61e-05    |
|    loss                 | 0.116       |
|    n_updates            | 8150        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 795      |
|    time_elapsed    | 276787   |
|    total_timesteps | 48844800 |
---------------------------------
Eval num_timesteps=48845595, episode_reward=0.05 +/- 0.98
Episode length: 29.96 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 48845595    |
| train/                  |             |
|    approx_kl            | 0.009569309 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.33        |
|    learning_rate        | 4.6e-05     |
|    loss                 | 0.0931      |
|    n_updates            | 8155        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 796      |
|    time_elapsed    | 277108   |
|    total_timesteps | 48906240 |
---------------------------------
Eval num_timesteps=48907036, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 48907036    |
| train/                  |             |
|    approx_kl            | 0.009579559 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.331       |
|    learning_rate        | 4.6e-05     |
|    loss                 | 0.0832      |
|    n_updates            | 8160        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 797      |
|    time_elapsed    | 277431   |
|    total_timesteps | 48967680 |
---------------------------------
Eval num_timesteps=48968477, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 48968477    |
| train/                  |             |
|    approx_kl            | 0.009590741 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.583      |
|    explained_variance   | 0.32        |
|    learning_rate        | 4.59e-05    |
|    loss                 | 0.0498      |
|    n_updates            | 8165        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 798      |
|    time_elapsed    | 277754   |
|    total_timesteps | 49029120 |
---------------------------------
Eval num_timesteps=49029918, episode_reward=0.15 +/- 0.97
Episode length: 30.06 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.15        |
| time/                   |             |
|    total_timesteps      | 49029918    |
| train/                  |             |
|    approx_kl            | 0.009753632 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.344       |
|    learning_rate        | 4.59e-05    |
|    loss                 | 0.0728      |
|    n_updates            | 8170        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.15
SELFPLAY: new best model, bumping up generation to 22
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 799      |
|    time_elapsed    | 278074   |
|    total_timesteps | 49090560 |
---------------------------------
Eval num_timesteps=49091359, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 49091359    |
| train/                  |             |
|    approx_kl            | 0.009408279 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.337       |
|    learning_rate        | 4.58e-05    |
|    loss                 | 0.074       |
|    n_updates            | 8175        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 800      |
|    time_elapsed    | 278393   |
|    total_timesteps | 49152000 |
---------------------------------
Eval num_timesteps=49152800, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.054        |
| time/                   |              |
|    total_timesteps      | 49152800     |
| train/                  |              |
|    approx_kl            | 0.0096226875 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.325        |
|    learning_rate        | 4.58e-05     |
|    loss                 | 0.0709       |
|    n_updates            | 8180         |
|    policy_gradient_loss | -0.0156      |
|    value_loss           | 0.242        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.29    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 801      |
|    time_elapsed    | 278711   |
|    total_timesteps | 49213440 |
---------------------------------
Eval num_timesteps=49214241, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 49214241    |
| train/                  |             |
|    approx_kl            | 0.009634819 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.579      |
|    explained_variance   | 0.331       |
|    learning_rate        | 4.57e-05    |
|    loss                 | 0.0515      |
|    n_updates            | 8185        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 802      |
|    time_elapsed    | 279029   |
|    total_timesteps | 49274880 |
---------------------------------
Eval num_timesteps=49275682, episode_reward=0.00 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 49275682    |
| train/                  |             |
|    approx_kl            | 0.009338065 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.579      |
|    explained_variance   | 0.341       |
|    learning_rate        | 4.57e-05    |
|    loss                 | 0.0899      |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 803      |
|    time_elapsed    | 279347   |
|    total_timesteps | 49336320 |
---------------------------------
Eval num_timesteps=49337123, episode_reward=-0.03 +/- 0.99
Episode length: 29.99 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.026       |
| time/                   |              |
|    total_timesteps      | 49337123     |
| train/                  |              |
|    approx_kl            | 0.0096522905 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.334        |
|    learning_rate        | 4.56e-05     |
|    loss                 | 0.0713       |
|    n_updates            | 8195         |
|    policy_gradient_loss | -0.0159      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 804      |
|    time_elapsed    | 279665   |
|    total_timesteps | 49397760 |
---------------------------------
Eval num_timesteps=49398564, episode_reward=-0.00 +/- 0.98
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 49398564    |
| train/                  |             |
|    approx_kl            | 0.009204807 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.34        |
|    learning_rate        | 4.55e-05    |
|    loss                 | 0.06        |
|    n_updates            | 8200        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 805      |
|    time_elapsed    | 279983   |
|    total_timesteps | 49459200 |
---------------------------------
Eval num_timesteps=49460005, episode_reward=0.03 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 49460005    |
| train/                  |             |
|    approx_kl            | 0.009866579 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.579      |
|    explained_variance   | 0.339       |
|    learning_rate        | 4.55e-05    |
|    loss                 | 0.0717      |
|    n_updates            | 8205        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 176      |
|    iterations      | 806      |
|    time_elapsed    | 280303   |
|    total_timesteps | 49520640 |
---------------------------------
Eval num_timesteps=49521446, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 49521446   |
| train/                  |            |
|    approx_kl            | 0.00965183 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.576     |
|    explained_variance   | 0.332      |
|    learning_rate        | 4.54e-05   |
|    loss                 | 0.0544     |
|    n_updates            | 8210       |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 807      |
|    time_elapsed    | 280624   |
|    total_timesteps | 49582080 |
---------------------------------
Eval num_timesteps=49582887, episode_reward=-0.04 +/- 0.99
Episode length: 29.95 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.042      |
| time/                   |             |
|    total_timesteps      | 49582887    |
| train/                  |             |
|    approx_kl            | 0.009547828 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.54e-05    |
|    loss                 | 0.0692      |
|    n_updates            | 8215        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 808      |
|    time_elapsed    | 280946   |
|    total_timesteps | 49643520 |
---------------------------------
Eval num_timesteps=49644328, episode_reward=0.00 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 49644328    |
| train/                  |             |
|    approx_kl            | 0.009376244 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.53e-05    |
|    loss                 | 0.0835      |
|    n_updates            | 8220        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 809      |
|    time_elapsed    | 281269   |
|    total_timesteps | 49704960 |
---------------------------------
Eval num_timesteps=49705769, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 49705769    |
| train/                  |             |
|    approx_kl            | 0.009432843 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.574      |
|    explained_variance   | 0.333       |
|    learning_rate        | 4.53e-05    |
|    loss                 | 0.117       |
|    n_updates            | 8225        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 176      |
|    iterations      | 810      |
|    time_elapsed    | 281592   |
|    total_timesteps | 49766400 |
---------------------------------
Eval num_timesteps=49767210, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.044      |
| time/                   |            |
|    total_timesteps      | 49767210   |
| train/                  |            |
|    approx_kl            | 0.00957103 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.576     |
|    explained_variance   | 0.332      |
|    learning_rate        | 4.52e-05   |
|    loss                 | 0.0755     |
|    n_updates            | 8230       |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 811      |
|    time_elapsed    | 281911   |
|    total_timesteps | 49827840 |
---------------------------------
Eval num_timesteps=49828651, episode_reward=-0.02 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 49828651    |
| train/                  |             |
|    approx_kl            | 0.009496861 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.571      |
|    explained_variance   | 0.35        |
|    learning_rate        | 4.52e-05    |
|    loss                 | 0.0751      |
|    n_updates            | 8235        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 812      |
|    time_elapsed    | 282230   |
|    total_timesteps | 49889280 |
---------------------------------
Eval num_timesteps=49890092, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 49890092    |
| train/                  |             |
|    approx_kl            | 0.009097697 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.316       |
|    learning_rate        | 4.51e-05    |
|    loss                 | 0.054       |
|    n_updates            | 8240        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 813      |
|    time_elapsed    | 282549   |
|    total_timesteps | 49950720 |
---------------------------------
Eval num_timesteps=49951533, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 49951533    |
| train/                  |             |
|    approx_kl            | 0.009282948 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.321       |
|    learning_rate        | 4.5e-05     |
|    loss                 | 0.0805      |
|    n_updates            | 8245        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 814      |
|    time_elapsed    | 282866   |
|    total_timesteps | 50012160 |
---------------------------------
Eval num_timesteps=50012974, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 50012974    |
| train/                  |             |
|    approx_kl            | 0.009075937 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.5e-05     |
|    loss                 | 0.112       |
|    n_updates            | 8250        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 815      |
|    time_elapsed    | 283184   |
|    total_timesteps | 50073600 |
---------------------------------
Eval num_timesteps=50074415, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 50074415    |
| train/                  |             |
|    approx_kl            | 0.009382624 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.566      |
|    explained_variance   | 0.336       |
|    learning_rate        | 4.49e-05    |
|    loss                 | 0.0792      |
|    n_updates            | 8255        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 816      |
|    time_elapsed    | 283502   |
|    total_timesteps | 50135040 |
---------------------------------
Eval num_timesteps=50135856, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 50135856    |
| train/                  |             |
|    approx_kl            | 0.009463068 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.34        |
|    learning_rate        | 4.49e-05    |
|    loss                 | 0.0463      |
|    n_updates            | 8260        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 817      |
|    time_elapsed    | 283820   |
|    total_timesteps | 50196480 |
---------------------------------
Eval num_timesteps=50197297, episode_reward=0.01 +/- 0.98
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 50197297    |
| train/                  |             |
|    approx_kl            | 0.009095195 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.33        |
|    learning_rate        | 4.48e-05    |
|    loss                 | 0.108       |
|    n_updates            | 8265        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 818      |
|    time_elapsed    | 284140   |
|    total_timesteps | 50257920 |
---------------------------------
Eval num_timesteps=50258738, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 50258738    |
| train/                  |             |
|    approx_kl            | 0.009061664 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.571      |
|    explained_variance   | 0.333       |
|    learning_rate        | 4.48e-05    |
|    loss                 | 0.119       |
|    n_updates            | 8270        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 819      |
|    time_elapsed    | 284461   |
|    total_timesteps | 50319360 |
---------------------------------
Eval num_timesteps=50320179, episode_reward=0.03 +/- 0.98
Episode length: 30.02 +/- 0.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.026     |
| time/                   |           |
|    total_timesteps      | 50320179  |
| train/                  |           |
|    approx_kl            | 0.0091651 |
|    clip_fraction        | 0.118     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.565    |
|    explained_variance   | 0.338     |
|    learning_rate        | 4.47e-05  |
|    loss                 | 0.104     |
|    n_updates            | 8275      |
|    policy_gradient_loss | -0.0157   |
|    value_loss           | 0.239     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 820      |
|    time_elapsed    | 284783   |
|    total_timesteps | 50380800 |
---------------------------------
Eval num_timesteps=50381620, episode_reward=0.02 +/- 0.99
Episode length: 29.99 +/- 0.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.016     |
| time/                   |           |
|    total_timesteps      | 50381620  |
| train/                  |           |
|    approx_kl            | 0.0094058 |
|    clip_fraction        | 0.119     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.565    |
|    explained_variance   | 0.323     |
|    learning_rate        | 4.47e-05  |
|    loss                 | 0.0948    |
|    n_updates            | 8280      |
|    policy_gradient_loss | -0.0159   |
|    value_loss           | 0.242     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 821      |
|    time_elapsed    | 285105   |
|    total_timesteps | 50442240 |
---------------------------------
Eval num_timesteps=50443061, episode_reward=0.06 +/- 0.99
Episode length: 30.04 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 50443061    |
| train/                  |             |
|    approx_kl            | 0.008948235 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.34        |
|    learning_rate        | 4.46e-05    |
|    loss                 | 0.064       |
|    n_updates            | 8285        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 822      |
|    time_elapsed    | 285428   |
|    total_timesteps | 50503680 |
---------------------------------
Eval num_timesteps=50504502, episode_reward=-0.01 +/- 0.99
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 50504502    |
| train/                  |             |
|    approx_kl            | 0.009224448 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.45e-05    |
|    loss                 | 0.0889      |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 823      |
|    time_elapsed    | 285747   |
|    total_timesteps | 50565120 |
---------------------------------
Eval num_timesteps=50565943, episode_reward=0.05 +/- 0.99
Episode length: 30.00 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.048        |
| time/                   |              |
|    total_timesteps      | 50565943     |
| train/                  |              |
|    approx_kl            | 0.0092475945 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.564       |
|    explained_variance   | 0.334        |
|    learning_rate        | 4.45e-05     |
|    loss                 | 0.0913       |
|    n_updates            | 8295         |
|    policy_gradient_loss | -0.0159      |
|    value_loss           | 0.242        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 824      |
|    time_elapsed    | 286065   |
|    total_timesteps | 50626560 |
---------------------------------
Eval num_timesteps=50627384, episode_reward=0.06 +/- 0.97
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 50627384    |
| train/                  |             |
|    approx_kl            | 0.009365255 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.337       |
|    learning_rate        | 4.44e-05    |
|    loss                 | 0.0896      |
|    n_updates            | 8300        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 825      |
|    time_elapsed    | 286384   |
|    total_timesteps | 50688000 |
---------------------------------
Eval num_timesteps=50688825, episode_reward=-0.02 +/- 0.98
Episode length: 29.96 +/- 0.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.016       |
| time/                   |              |
|    total_timesteps      | 50688825     |
| train/                  |              |
|    approx_kl            | 0.0089016175 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.558       |
|    explained_variance   | 0.334        |
|    learning_rate        | 4.44e-05     |
|    loss                 | 0.13         |
|    n_updates            | 8305         |
|    policy_gradient_loss | -0.0153      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 826      |
|    time_elapsed    | 286701   |
|    total_timesteps | 50749440 |
---------------------------------
Eval num_timesteps=50750266, episode_reward=-0.01 +/- 0.99
Episode length: 29.95 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 50750266    |
| train/                  |             |
|    approx_kl            | 0.008998302 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.559      |
|    explained_variance   | 0.331       |
|    learning_rate        | 4.43e-05    |
|    loss                 | 0.0642      |
|    n_updates            | 8310        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 177      |
|    iterations      | 827      |
|    time_elapsed    | 287019   |
|    total_timesteps | 50810880 |
---------------------------------
Eval num_timesteps=50811707, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.058      |
| time/                   |            |
|    total_timesteps      | 50811707   |
| train/                  |            |
|    approx_kl            | 0.00898327 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.561     |
|    explained_variance   | 0.351      |
|    learning_rate        | 4.43e-05   |
|    loss                 | 0.0898     |
|    n_updates            | 8315       |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 828      |
|    time_elapsed    | 287337   |
|    total_timesteps | 50872320 |
---------------------------------
Eval num_timesteps=50873148, episode_reward=-0.00 +/- 0.99
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 50873148    |
| train/                  |             |
|    approx_kl            | 0.009480945 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.334       |
|    learning_rate        | 4.42e-05    |
|    loss                 | 0.1         |
|    n_updates            | 8320        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 829      |
|    time_elapsed    | 287655   |
|    total_timesteps | 50933760 |
---------------------------------
Eval num_timesteps=50934589, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 50934589    |
| train/                  |             |
|    approx_kl            | 0.009255469 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.318       |
|    learning_rate        | 4.42e-05    |
|    loss                 | 0.0613      |
|    n_updates            | 8325        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 830      |
|    time_elapsed    | 287975   |
|    total_timesteps | 50995200 |
---------------------------------
Eval num_timesteps=50996030, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 50996030    |
| train/                  |             |
|    approx_kl            | 0.009307842 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.34        |
|    learning_rate        | 4.41e-05    |
|    loss                 | 0.0749      |
|    n_updates            | 8330        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 831      |
|    time_elapsed    | 288296   |
|    total_timesteps | 51056640 |
---------------------------------
Eval num_timesteps=51057471, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 51057471    |
| train/                  |             |
|    approx_kl            | 0.009029602 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.556      |
|    explained_variance   | 0.328       |
|    learning_rate        | 4.4e-05     |
|    loss                 | 0.0484      |
|    n_updates            | 8335        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 832      |
|    time_elapsed    | 288619   |
|    total_timesteps | 51118080 |
---------------------------------
Eval num_timesteps=51118912, episode_reward=0.07 +/- 0.98
Episode length: 30.03 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 51118912    |
| train/                  |             |
|    approx_kl            | 0.008988272 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.555      |
|    explained_variance   | 0.344       |
|    learning_rate        | 4.4e-05     |
|    loss                 | 0.078       |
|    n_updates            | 8340        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 833      |
|    time_elapsed    | 288942   |
|    total_timesteps | 51179520 |
---------------------------------
Eval num_timesteps=51180353, episode_reward=0.00 +/- 0.99
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 51180353    |
| train/                  |             |
|    approx_kl            | 0.008822057 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.322       |
|    learning_rate        | 4.39e-05    |
|    loss                 | 0.134       |
|    n_updates            | 8345        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 834      |
|    time_elapsed    | 289264   |
|    total_timesteps | 51240960 |
---------------------------------
Eval num_timesteps=51241794, episode_reward=0.04 +/- 0.97
Episode length: 29.99 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 51241794    |
| train/                  |             |
|    approx_kl            | 0.009148612 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.322       |
|    learning_rate        | 4.39e-05    |
|    loss                 | 0.0972      |
|    n_updates            | 8350        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 835      |
|    time_elapsed    | 289582   |
|    total_timesteps | 51302400 |
---------------------------------
Eval num_timesteps=51303235, episode_reward=0.00 +/- 0.98
Episode length: 29.99 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 51303235    |
| train/                  |             |
|    approx_kl            | 0.008974685 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.338       |
|    learning_rate        | 4.38e-05    |
|    loss                 | 0.0711      |
|    n_updates            | 8355        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 836      |
|    time_elapsed    | 289901   |
|    total_timesteps | 51363840 |
---------------------------------
Eval num_timesteps=51364676, episode_reward=-0.02 +/- 0.99
Episode length: 29.95 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.024      |
| time/                   |             |
|    total_timesteps      | 51364676    |
| train/                  |             |
|    approx_kl            | 0.008916672 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.319       |
|    learning_rate        | 4.38e-05    |
|    loss                 | 0.0838      |
|    n_updates            | 8360        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 837      |
|    time_elapsed    | 290218   |
|    total_timesteps | 51425280 |
---------------------------------
Eval num_timesteps=51426117, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 51426117    |
| train/                  |             |
|    approx_kl            | 0.009220511 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.547      |
|    explained_variance   | 0.328       |
|    learning_rate        | 4.37e-05    |
|    loss                 | 0.0787      |
|    n_updates            | 8365        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 838      |
|    time_elapsed    | 290536   |
|    total_timesteps | 51486720 |
---------------------------------
Eval num_timesteps=51487558, episode_reward=-0.00 +/- 0.99
Episode length: 29.95 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 51487558    |
| train/                  |             |
|    approx_kl            | 0.008932599 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.55       |
|    explained_variance   | 0.332       |
|    learning_rate        | 4.37e-05    |
|    loss                 | 0.087       |
|    n_updates            | 8370        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 839      |
|    time_elapsed    | 290854   |
|    total_timesteps | 51548160 |
---------------------------------
Eval num_timesteps=51548999, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 51548999    |
| train/                  |             |
|    approx_kl            | 0.008973091 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.553      |
|    explained_variance   | 0.32        |
|    learning_rate        | 4.36e-05    |
|    loss                 | 0.0862      |
|    n_updates            | 8375        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 840      |
|    time_elapsed    | 291172   |
|    total_timesteps | 51609600 |
---------------------------------
Eval num_timesteps=51610440, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 51610440   |
| train/                  |            |
|    approx_kl            | 0.00887489 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.549     |
|    explained_variance   | 0.327      |
|    learning_rate        | 4.36e-05   |
|    loss                 | 0.0961     |
|    n_updates            | 8380       |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 841      |
|    time_elapsed    | 291491   |
|    total_timesteps | 51671040 |
---------------------------------
Eval num_timesteps=51671881, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.042        |
| time/                   |              |
|    total_timesteps      | 51671881     |
| train/                  |              |
|    approx_kl            | 0.0090347845 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.33         |
|    learning_rate        | 4.35e-05     |
|    loss                 | 0.0855       |
|    n_updates            | 8385         |
|    policy_gradient_loss | -0.016       |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 842      |
|    time_elapsed    | 291812   |
|    total_timesteps | 51732480 |
---------------------------------
Eval num_timesteps=51733322, episode_reward=0.03 +/- 0.97
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.028      |
| time/                   |            |
|    total_timesteps      | 51733322   |
| train/                  |            |
|    approx_kl            | 0.00910181 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.552     |
|    explained_variance   | 0.336      |
|    learning_rate        | 4.34e-05   |
|    loss                 | 0.102      |
|    n_updates            | 8390       |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 843      |
|    time_elapsed    | 292134   |
|    total_timesteps | 51793920 |
---------------------------------
Eval num_timesteps=51794763, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 51794763    |
| train/                  |             |
|    approx_kl            | 0.008871325 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.553      |
|    explained_variance   | 0.326       |
|    learning_rate        | 4.34e-05    |
|    loss                 | 0.0394      |
|    n_updates            | 8395        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 177      |
|    iterations      | 844      |
|    time_elapsed    | 292457   |
|    total_timesteps | 51855360 |
---------------------------------
Eval num_timesteps=51856204, episode_reward=0.03 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 51856204    |
| train/                  |             |
|    approx_kl            | 0.008849949 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.341       |
|    learning_rate        | 4.33e-05    |
|    loss                 | 0.0612      |
|    n_updates            | 8400        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 845      |
|    time_elapsed    | 292779   |
|    total_timesteps | 51916800 |
---------------------------------
Eval num_timesteps=51917645, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 51917645    |
| train/                  |             |
|    approx_kl            | 0.008948173 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.332       |
|    learning_rate        | 4.33e-05    |
|    loss                 | 0.0686      |
|    n_updates            | 8405        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 177      |
|    iterations      | 846      |
|    time_elapsed    | 293100   |
|    total_timesteps | 51978240 |
---------------------------------
Eval num_timesteps=51979086, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 51979086    |
| train/                  |             |
|    approx_kl            | 0.009165287 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.34        |
|    learning_rate        | 4.32e-05    |
|    loss                 | 0.027       |
|    n_updates            | 8410        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 847      |
|    time_elapsed    | 293419   |
|    total_timesteps | 52039680 |
---------------------------------
Eval num_timesteps=52040527, episode_reward=0.12 +/- 0.97
Episode length: 30.06 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 52040527    |
| train/                  |             |
|    approx_kl            | 0.008667185 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.338       |
|    learning_rate        | 4.32e-05    |
|    loss                 | 0.113       |
|    n_updates            | 8415        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 848      |
|    time_elapsed    | 293737   |
|    total_timesteps | 52101120 |
---------------------------------
Eval num_timesteps=52101968, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 52101968    |
| train/                  |             |
|    approx_kl            | 0.009051169 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.547      |
|    explained_variance   | 0.334       |
|    learning_rate        | 4.31e-05    |
|    loss                 | 0.0629      |
|    n_updates            | 8420        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 849      |
|    time_elapsed    | 294056   |
|    total_timesteps | 52162560 |
---------------------------------
Eval num_timesteps=52163409, episode_reward=-0.02 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 52163409    |
| train/                  |             |
|    approx_kl            | 0.008941512 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.543      |
|    explained_variance   | 0.332       |
|    learning_rate        | 4.31e-05    |
|    loss                 | 0.079       |
|    n_updates            | 8425        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 850      |
|    time_elapsed    | 294373   |
|    total_timesteps | 52224000 |
---------------------------------
Eval num_timesteps=52224850, episode_reward=0.03 +/- 0.98
Episode length: 29.98 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 52224850    |
| train/                  |             |
|    approx_kl            | 0.008837861 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.33        |
|    learning_rate        | 4.3e-05     |
|    loss                 | 0.106       |
|    n_updates            | 8430        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 851      |
|    time_elapsed    | 294690   |
|    total_timesteps | 52285440 |
---------------------------------
Eval num_timesteps=52286291, episode_reward=0.01 +/- 0.99
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 52286291    |
| train/                  |             |
|    approx_kl            | 0.008600278 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.325       |
|    learning_rate        | 4.29e-05    |
|    loss                 | 0.066       |
|    n_updates            | 8435        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 852      |
|    time_elapsed    | 295008   |
|    total_timesteps | 52346880 |
---------------------------------
Eval num_timesteps=52347732, episode_reward=0.10 +/- 0.98
Episode length: 30.07 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 52347732    |
| train/                  |             |
|    approx_kl            | 0.008840412 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.329       |
|    learning_rate        | 4.29e-05    |
|    loss                 | 0.0915      |
|    n_updates            | 8440        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 853      |
|    time_elapsed    | 295327   |
|    total_timesteps | 52408320 |
---------------------------------
Eval num_timesteps=52409173, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 52409173    |
| train/                  |             |
|    approx_kl            | 0.008742946 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.329       |
|    learning_rate        | 4.28e-05    |
|    loss                 | 0.0549      |
|    n_updates            | 8445        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 854      |
|    time_elapsed    | 295648   |
|    total_timesteps | 52469760 |
---------------------------------
Eval num_timesteps=52470614, episode_reward=0.04 +/- 0.99
Episode length: 30.02 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 52470614    |
| train/                  |             |
|    approx_kl            | 0.008868058 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.321       |
|    learning_rate        | 4.28e-05    |
|    loss                 | 0.0564      |
|    n_updates            | 8450        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 855      |
|    time_elapsed    | 295970   |
|    total_timesteps | 52531200 |
---------------------------------
Eval num_timesteps=52532055, episode_reward=0.10 +/- 0.98
Episode length: 30.00 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 52532055     |
| train/                  |              |
|    approx_kl            | 0.0088492185 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.538       |
|    explained_variance   | 0.345        |
|    learning_rate        | 4.27e-05     |
|    loss                 | 0.0558       |
|    n_updates            | 8455         |
|    policy_gradient_loss | -0.0158      |
|    value_loss           | 0.235        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 856      |
|    time_elapsed    | 296293   |
|    total_timesteps | 52592640 |
---------------------------------
Eval num_timesteps=52593496, episode_reward=0.03 +/- 0.99
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 52593496    |
| train/                  |             |
|    approx_kl            | 0.009054265 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.32        |
|    learning_rate        | 4.27e-05    |
|    loss                 | 0.114       |
|    n_updates            | 8460        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 857      |
|    time_elapsed    | 296615   |
|    total_timesteps | 52654080 |
---------------------------------
Eval num_timesteps=52654937, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 52654937    |
| train/                  |             |
|    approx_kl            | 0.008744752 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.337       |
|    learning_rate        | 4.26e-05    |
|    loss                 | 0.0267      |
|    n_updates            | 8465        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 858      |
|    time_elapsed    | 296934   |
|    total_timesteps | 52715520 |
---------------------------------
Eval num_timesteps=52716378, episode_reward=0.15 +/- 0.98
Episode length: 30.05 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.152      |
| time/                   |            |
|    total_timesteps      | 52716378   |
| train/                  |            |
|    approx_kl            | 0.00843073 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.535     |
|    explained_variance   | 0.34       |
|    learning_rate        | 4.26e-05   |
|    loss                 | 0.0706     |
|    n_updates            | 8470       |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 0.238      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.152
SELFPLAY: new best model, bumping up generation to 23
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 859      |
|    time_elapsed    | 297253   |
|    total_timesteps | 52776960 |
---------------------------------
Eval num_timesteps=52777819, episode_reward=-0.00 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 52777819    |
| train/                  |             |
|    approx_kl            | 0.008527536 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.325       |
|    learning_rate        | 4.25e-05    |
|    loss                 | 0.134       |
|    n_updates            | 8475        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 860      |
|    time_elapsed    | 297571   |
|    total_timesteps | 52838400 |
---------------------------------
Eval num_timesteps=52839260, episode_reward=-0.02 +/- 0.99
Episode length: 30.00 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.024       |
| time/                   |              |
|    total_timesteps      | 52839260     |
| train/                  |              |
|    approx_kl            | 0.0087977545 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.533       |
|    explained_variance   | 0.32         |
|    learning_rate        | 4.24e-05     |
|    loss                 | 0.103        |
|    n_updates            | 8480         |
|    policy_gradient_loss | -0.0153      |
|    value_loss           | 0.242        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 861      |
|    time_elapsed    | 297889   |
|    total_timesteps | 52899840 |
---------------------------------
Eval num_timesteps=52900701, episode_reward=-0.02 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 52900701    |
| train/                  |             |
|    approx_kl            | 0.008622714 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.338       |
|    learning_rate        | 4.24e-05    |
|    loss                 | 0.0418      |
|    n_updates            | 8485        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 862      |
|    time_elapsed    | 298207   |
|    total_timesteps | 52961280 |
---------------------------------
Eval num_timesteps=52962142, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 52962142    |
| train/                  |             |
|    approx_kl            | 0.008935331 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.23e-05    |
|    loss                 | 0.068       |
|    n_updates            | 8490        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 863      |
|    time_elapsed    | 298525   |
|    total_timesteps | 53022720 |
---------------------------------
Eval num_timesteps=53023583, episode_reward=0.04 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 53023583    |
| train/                  |             |
|    approx_kl            | 0.009008547 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.333       |
|    learning_rate        | 4.23e-05    |
|    loss                 | 0.0771      |
|    n_updates            | 8495        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 864      |
|    time_elapsed    | 298843   |
|    total_timesteps | 53084160 |
---------------------------------
Eval num_timesteps=53085024, episode_reward=-0.00 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 53085024    |
| train/                  |             |
|    approx_kl            | 0.008472177 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.527      |
|    explained_variance   | 0.325       |
|    learning_rate        | 4.22e-05    |
|    loss                 | 0.0534      |
|    n_updates            | 8500        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 865      |
|    time_elapsed    | 299164   |
|    total_timesteps | 53145600 |
---------------------------------
Eval num_timesteps=53146465, episode_reward=0.16 +/- 0.97
Episode length: 30.05 +/- 1.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.16        |
| time/                   |             |
|    total_timesteps      | 53146465    |
| train/                  |             |
|    approx_kl            | 0.008390134 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.53       |
|    explained_variance   | 0.329       |
|    learning_rate        | 4.22e-05    |
|    loss                 | 0.0646      |
|    n_updates            | 8505        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.241       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.16
SELFPLAY: new best model, bumping up generation to 24
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 866      |
|    time_elapsed    | 299485   |
|    total_timesteps | 53207040 |
---------------------------------
Eval num_timesteps=53207906, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 53207906    |
| train/                  |             |
|    approx_kl            | 0.008525117 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.327       |
|    learning_rate        | 4.21e-05    |
|    loss                 | 0.0951      |
|    n_updates            | 8510        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 867      |
|    time_elapsed    | 299808   |
|    total_timesteps | 53268480 |
---------------------------------
Eval num_timesteps=53269347, episode_reward=-0.07 +/- 0.97
Episode length: 29.94 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.066      |
| time/                   |             |
|    total_timesteps      | 53269347    |
| train/                  |             |
|    approx_kl            | 0.008827017 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.329       |
|    learning_rate        | 4.21e-05    |
|    loss                 | 0.0876      |
|    n_updates            | 8515        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 868      |
|    time_elapsed    | 300131   |
|    total_timesteps | 53329920 |
---------------------------------
Eval num_timesteps=53330788, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 1.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 53330788    |
| train/                  |             |
|    approx_kl            | 0.008655834 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.332       |
|    learning_rate        | 4.2e-05     |
|    loss                 | 0.078       |
|    n_updates            | 8520        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 869      |
|    time_elapsed    | 300452   |
|    total_timesteps | 53391360 |
---------------------------------
Eval num_timesteps=53392229, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 1.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.03         |
| time/                   |              |
|    total_timesteps      | 53392229     |
| train/                  |              |
|    approx_kl            | 0.0087780515 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.53        |
|    explained_variance   | 0.345        |
|    learning_rate        | 4.19e-05     |
|    loss                 | 0.103        |
|    n_updates            | 8525         |
|    policy_gradient_loss | -0.0153      |
|    value_loss           | 0.235        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 870      |
|    time_elapsed    | 300772   |
|    total_timesteps | 53452800 |
---------------------------------
Eval num_timesteps=53453670, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 53453670    |
| train/                  |             |
|    approx_kl            | 0.008836665 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.331       |
|    learning_rate        | 4.19e-05    |
|    loss                 | 0.107       |
|    n_updates            | 8530        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 871      |
|    time_elapsed    | 301090   |
|    total_timesteps | 53514240 |
---------------------------------
Eval num_timesteps=53515111, episode_reward=0.02 +/- 0.99
Episode length: 30.02 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 53515111    |
| train/                  |             |
|    approx_kl            | 0.008491609 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.328       |
|    learning_rate        | 4.18e-05    |
|    loss                 | 0.109       |
|    n_updates            | 8535        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.24    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 872      |
|    time_elapsed    | 301408   |
|    total_timesteps | 53575680 |
---------------------------------
Eval num_timesteps=53576552, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.052        |
| time/                   |              |
|    total_timesteps      | 53576552     |
| train/                  |              |
|    approx_kl            | 0.0085457405 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.534       |
|    explained_variance   | 0.336        |
|    learning_rate        | 4.18e-05     |
|    loss                 | 0.0867       |
|    n_updates            | 8540         |
|    policy_gradient_loss | -0.0152      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 873      |
|    time_elapsed    | 301726   |
|    total_timesteps | 53637120 |
---------------------------------
Eval num_timesteps=53637993, episode_reward=-0.05 +/- 0.98
Episode length: 29.96 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.048       |
| time/                   |              |
|    total_timesteps      | 53637993     |
| train/                  |              |
|    approx_kl            | 0.0083905915 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.531       |
|    explained_variance   | 0.342        |
|    learning_rate        | 4.17e-05     |
|    loss                 | 0.0567       |
|    n_updates            | 8545         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 874      |
|    time_elapsed    | 302044   |
|    total_timesteps | 53698560 |
---------------------------------
Eval num_timesteps=53699434, episode_reward=0.04 +/- 0.99
Episode length: 29.96 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 53699434    |
| train/                  |             |
|    approx_kl            | 0.008577557 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.536      |
|    explained_variance   | 0.339       |
|    learning_rate        | 4.17e-05    |
|    loss                 | 0.0622      |
|    n_updates            | 8550        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 875      |
|    time_elapsed    | 302362   |
|    total_timesteps | 53760000 |
---------------------------------
Eval num_timesteps=53760875, episode_reward=-0.01 +/- 0.98
Episode length: 30.03 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 53760875    |
| train/                  |             |
|    approx_kl            | 0.008599092 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.345       |
|    learning_rate        | 4.16e-05    |
|    loss                 | 0.0886      |
|    n_updates            | 8555        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 876      |
|    time_elapsed    | 302681   |
|    total_timesteps | 53821440 |
---------------------------------
Eval num_timesteps=53822316, episode_reward=0.03 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 53822316    |
| train/                  |             |
|    approx_kl            | 0.008179086 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.332       |
|    learning_rate        | 4.16e-05    |
|    loss                 | 0.108       |
|    n_updates            | 8560        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 877      |
|    time_elapsed    | 303001   |
|    total_timesteps | 53882880 |
---------------------------------
Eval num_timesteps=53883757, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 53883757    |
| train/                  |             |
|    approx_kl            | 0.008576214 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.527      |
|    explained_variance   | 0.33        |
|    learning_rate        | 4.15e-05    |
|    loss                 | 0.0843      |
|    n_updates            | 8565        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 177      |
|    iterations      | 878      |
|    time_elapsed    | 303323   |
|    total_timesteps | 53944320 |
---------------------------------
Eval num_timesteps=53945198, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 53945198    |
| train/                  |             |
|    approx_kl            | 0.008292658 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.327       |
|    learning_rate        | 4.15e-05    |
|    loss                 | 0.0431      |
|    n_updates            | 8570        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 879      |
|    time_elapsed    | 303646   |
|    total_timesteps | 54005760 |
---------------------------------
Eval num_timesteps=54006639, episode_reward=0.05 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 54006639    |
| train/                  |             |
|    approx_kl            | 0.008356794 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0.328       |
|    learning_rate        | 4.14e-05    |
|    loss                 | 0.0787      |
|    n_updates            | 8575        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 880      |
|    time_elapsed    | 303969   |
|    total_timesteps | 54067200 |
---------------------------------
Eval num_timesteps=54068080, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.088        |
| time/                   |              |
|    total_timesteps      | 54068080     |
| train/                  |              |
|    approx_kl            | 0.0083720945 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.523       |
|    explained_variance   | 0.324        |
|    learning_rate        | 4.13e-05     |
|    loss                 | 0.0897       |
|    n_updates            | 8580         |
|    policy_gradient_loss | -0.0154      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 177      |
|    iterations      | 881      |
|    time_elapsed    | 304289   |
|    total_timesteps | 54128640 |
---------------------------------
Eval num_timesteps=54129521, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 54129521    |
| train/                  |             |
|    approx_kl            | 0.008366338 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.341       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0785      |
|    n_updates            | 8585        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 882      |
|    time_elapsed    | 304608   |
|    total_timesteps | 54190080 |
---------------------------------
Eval num_timesteps=54190962, episode_reward=0.05 +/- 0.98
Episode length: 29.84 +/- 1.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 54190962    |
| train/                  |             |
|    approx_kl            | 0.008447709 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.323       |
|    learning_rate        | 4.12e-05    |
|    loss                 | 0.0643      |
|    n_updates            | 8590        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 883      |
|    time_elapsed    | 304926   |
|    total_timesteps | 54251520 |
---------------------------------
Eval num_timesteps=54252403, episode_reward=0.09 +/- 0.98
Episode length: 29.89 +/- 1.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 54252403    |
| train/                  |             |
|    approx_kl            | 0.008391479 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.325       |
|    learning_rate        | 4.12e-05    |
|    loss                 | 0.131       |
|    n_updates            | 8595        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 884      |
|    time_elapsed    | 305243   |
|    total_timesteps | 54312960 |
---------------------------------
Eval num_timesteps=54313844, episode_reward=0.12 +/- 0.97
Episode length: 29.77 +/- 2.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 54313844    |
| train/                  |             |
|    approx_kl            | 0.007996783 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.11e-05    |
|    loss                 | 0.0892      |
|    n_updates            | 8600        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 885      |
|    time_elapsed    | 305560   |
|    total_timesteps | 54374400 |
---------------------------------
Eval num_timesteps=54375285, episode_reward=0.06 +/- 0.98
Episode length: 29.85 +/- 1.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 54375285    |
| train/                  |             |
|    approx_kl            | 0.008220985 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.11e-05    |
|    loss                 | 0.108       |
|    n_updates            | 8605        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 886      |
|    time_elapsed    | 305877   |
|    total_timesteps | 54435840 |
---------------------------------
Eval num_timesteps=54436726, episode_reward=-0.02 +/- 0.99
Episode length: 29.86 +/- 1.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 54436726    |
| train/                  |             |
|    approx_kl            | 0.008552644 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.328       |
|    learning_rate        | 4.1e-05     |
|    loss                 | 0.107       |
|    n_updates            | 8610        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 887      |
|    time_elapsed    | 306195   |
|    total_timesteps | 54497280 |
---------------------------------
Eval num_timesteps=54498167, episode_reward=0.13 +/- 0.98
Episode length: 29.86 +/- 1.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.126       |
| time/                   |             |
|    total_timesteps      | 54498167    |
| train/                  |             |
|    approx_kl            | 0.008322677 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.323       |
|    learning_rate        | 4.1e-05     |
|    loss                 | 0.0604      |
|    n_updates            | 8615        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 888      |
|    time_elapsed    | 306514   |
|    total_timesteps | 54558720 |
---------------------------------
Eval num_timesteps=54559608, episode_reward=0.06 +/- 0.98
Episode length: 29.90 +/- 1.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 54559608   |
| train/                  |            |
|    approx_kl            | 0.00830856 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.515     |
|    explained_variance   | 0.332      |
|    learning_rate        | 4.09e-05   |
|    loss                 | 0.0823     |
|    n_updates            | 8620       |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 889      |
|    time_elapsed    | 306835   |
|    total_timesteps | 54620160 |
---------------------------------
Eval num_timesteps=54621049, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 54621049    |
| train/                  |             |
|    approx_kl            | 0.008057261 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.324       |
|    learning_rate        | 4.08e-05    |
|    loss                 | 0.0835      |
|    n_updates            | 8625        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 890      |
|    time_elapsed    | 307156   |
|    total_timesteps | 54681600 |
---------------------------------
Eval num_timesteps=54682490, episode_reward=0.10 +/- 0.97
Episode length: 29.71 +/- 2.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 54682490    |
| train/                  |             |
|    approx_kl            | 0.008524585 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.324       |
|    learning_rate        | 4.08e-05    |
|    loss                 | 0.0668      |
|    n_updates            | 8630        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 891      |
|    time_elapsed    | 307479   |
|    total_timesteps | 54743040 |
---------------------------------
Eval num_timesteps=54743931, episode_reward=0.11 +/- 0.97
Episode length: 29.92 +/- 1.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 54743931    |
| train/                  |             |
|    approx_kl            | 0.008393473 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.346       |
|    learning_rate        | 4.07e-05    |
|    loss                 | 0.075       |
|    n_updates            | 8635        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 892      |
|    time_elapsed    | 307801   |
|    total_timesteps | 54804480 |
---------------------------------
Eval num_timesteps=54805372, episode_reward=0.05 +/- 0.98
Episode length: 29.84 +/- 1.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.8         |
|    mean_reward          | 0.046        |
| time/                   |              |
|    total_timesteps      | 54805372     |
| train/                  |              |
|    approx_kl            | 0.0081104105 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.504       |
|    explained_variance   | 0.338        |
|    learning_rate        | 4.07e-05     |
|    loss                 | 0.123        |
|    n_updates            | 8640         |
|    policy_gradient_loss | -0.0151      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 893      |
|    time_elapsed    | 308119   |
|    total_timesteps | 54865920 |
---------------------------------
Eval num_timesteps=54866813, episode_reward=0.06 +/- 0.99
Episode length: 29.88 +/- 1.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 54866813    |
| train/                  |             |
|    approx_kl            | 0.008551755 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.33        |
|    learning_rate        | 4.06e-05    |
|    loss                 | 0.0791      |
|    n_updates            | 8645        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 894      |
|    time_elapsed    | 308438   |
|    total_timesteps | 54927360 |
---------------------------------
Eval num_timesteps=54928254, episode_reward=0.13 +/- 0.97
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.132       |
| time/                   |             |
|    total_timesteps      | 54928254    |
| train/                  |             |
|    approx_kl            | 0.008080709 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.342       |
|    learning_rate        | 4.06e-05    |
|    loss                 | 0.0881      |
|    n_updates            | 8650        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 895      |
|    time_elapsed    | 308756   |
|    total_timesteps | 54988800 |
---------------------------------
Eval num_timesteps=54989695, episode_reward=0.04 +/- 0.98
Episode length: 29.88 +/- 1.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 54989695    |
| train/                  |             |
|    approx_kl            | 0.008377049 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.498      |
|    explained_variance   | 0.349       |
|    learning_rate        | 4.05e-05    |
|    loss                 | 0.0594      |
|    n_updates            | 8655        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 896      |
|    time_elapsed    | 309074   |
|    total_timesteps | 55050240 |
---------------------------------
Eval num_timesteps=55051136, episode_reward=0.08 +/- 0.98
Episode length: 29.89 +/- 1.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 55051136    |
| train/                  |             |
|    approx_kl            | 0.008404022 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.495      |
|    explained_variance   | 0.333       |
|    learning_rate        | 4.05e-05    |
|    loss                 | 0.101       |
|    n_updates            | 8660        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 897      |
|    time_elapsed    | 309391   |
|    total_timesteps | 55111680 |
---------------------------------
Eval num_timesteps=55112577, episode_reward=0.02 +/- 0.98
Episode length: 29.78 +/- 2.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 55112577    |
| train/                  |             |
|    approx_kl            | 0.008240821 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.495      |
|    explained_variance   | 0.328       |
|    learning_rate        | 4.04e-05    |
|    loss                 | 0.0766      |
|    n_updates            | 8665        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 898      |
|    time_elapsed    | 309709   |
|    total_timesteps | 55173120 |
---------------------------------
Eval num_timesteps=55174018, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 55174018    |
| train/                  |             |
|    approx_kl            | 0.007879634 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.493      |
|    explained_variance   | 0.33        |
|    learning_rate        | 4.03e-05    |
|    loss                 | 0.0524      |
|    n_updates            | 8670        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 899      |
|    time_elapsed    | 310027   |
|    total_timesteps | 55234560 |
---------------------------------
Eval num_timesteps=55235459, episode_reward=0.05 +/- 0.98
Episode length: 29.85 +/- 1.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 55235459    |
| train/                  |             |
|    approx_kl            | 0.007996344 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.493      |
|    explained_variance   | 0.329       |
|    learning_rate        | 4.03e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 8675        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 900      |
|    time_elapsed    | 310347   |
|    total_timesteps | 55296000 |
---------------------------------
Eval num_timesteps=55296900, episode_reward=0.02 +/- 0.97
Episode length: 29.91 +/- 1.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 55296900    |
| train/                  |             |
|    approx_kl            | 0.008288183 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.493      |
|    explained_variance   | 0.33        |
|    learning_rate        | 4.02e-05    |
|    loss                 | 0.0773      |
|    n_updates            | 8680        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 901      |
|    time_elapsed    | 310669   |
|    total_timesteps | 55357440 |
---------------------------------
Eval num_timesteps=55358341, episode_reward=0.09 +/- 0.98
Episode length: 29.80 +/- 2.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 55358341    |
| train/                  |             |
|    approx_kl            | 0.007975107 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.483      |
|    explained_variance   | 0.337       |
|    learning_rate        | 4.02e-05    |
|    loss                 | 0.0904      |
|    n_updates            | 8685        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.3     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 902      |
|    time_elapsed    | 310991   |
|    total_timesteps | 55418880 |
---------------------------------
Eval num_timesteps=55419782, episode_reward=0.05 +/- 0.98
Episode length: 29.71 +/- 2.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 55419782    |
| train/                  |             |
|    approx_kl            | 0.008138121 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.486      |
|    explained_variance   | 0.329       |
|    learning_rate        | 4.01e-05    |
|    loss                 | 0.0818      |
|    n_updates            | 8690        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 903      |
|    time_elapsed    | 311314   |
|    total_timesteps | 55480320 |
---------------------------------
Eval num_timesteps=55481223, episode_reward=0.11 +/- 0.98
Episode length: 29.89 +/- 1.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 55481223    |
| train/                  |             |
|    approx_kl            | 0.008378264 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.348       |
|    learning_rate        | 4.01e-05    |
|    loss                 | 0.1         |
|    n_updates            | 8695        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 904      |
|    time_elapsed    | 311634   |
|    total_timesteps | 55541760 |
---------------------------------
Eval num_timesteps=55542664, episode_reward=0.15 +/- 0.97
Episode length: 29.67 +/- 2.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.7         |
|    mean_reward          | 0.15         |
| time/                   |              |
|    total_timesteps      | 55542664     |
| train/                  |              |
|    approx_kl            | 0.0081646675 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.485       |
|    explained_variance   | 0.333        |
|    learning_rate        | 4e-05        |
|    loss                 | 0.0922       |
|    n_updates            | 8700         |
|    policy_gradient_loss | -0.015       |
|    value_loss           | 0.242        |
------------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.15
SELFPLAY: new best model, bumping up generation to 25
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 905      |
|    time_elapsed    | 311953   |
|    total_timesteps | 55603200 |
---------------------------------
Eval num_timesteps=55604105, episode_reward=0.09 +/- 0.99
Episode length: 29.49 +/- 3.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.5       |
|    mean_reward          | 0.088      |
| time/                   |            |
|    total_timesteps      | 55604105   |
| train/                  |            |
|    approx_kl            | 0.00887773 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.536     |
|    explained_variance   | 0.333      |
|    learning_rate        | 4e-05      |
|    loss                 | 0.0921     |
|    n_updates            | 8705       |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 906      |
|    time_elapsed    | 312270   |
|    total_timesteps | 55664640 |
---------------------------------
Eval num_timesteps=55665546, episode_reward=0.03 +/- 0.99
Episode length: 29.41 +/- 3.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.4        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 55665546    |
| train/                  |             |
|    approx_kl            | 0.008279032 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.339       |
|    learning_rate        | 3.99e-05    |
|    loss                 | 0.122       |
|    n_updates            | 8710        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.3     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 907      |
|    time_elapsed    | 312587   |
|    total_timesteps | 55726080 |
---------------------------------
Eval num_timesteps=55726987, episode_reward=0.08 +/- 0.98
Episode length: 29.40 +/- 3.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.4        |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 55726987    |
| train/                  |             |
|    approx_kl            | 0.007944899 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.331       |
|    learning_rate        | 3.98e-05    |
|    loss                 | 0.0884      |
|    n_updates            | 8715        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.5     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 908      |
|    time_elapsed    | 312904   |
|    total_timesteps | 55787520 |
---------------------------------
Eval num_timesteps=55788428, episode_reward=0.00 +/- 0.98
Episode length: 29.35 +/- 3.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 55788428    |
| train/                  |             |
|    approx_kl            | 0.008232285 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.536      |
|    explained_variance   | 0.349       |
|    learning_rate        | 3.98e-05    |
|    loss                 | 0.108       |
|    n_updates            | 8720        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 909      |
|    time_elapsed    | 313221   |
|    total_timesteps | 55848960 |
---------------------------------
Eval num_timesteps=55849869, episode_reward=-0.02 +/- 0.98
Episode length: 29.28 +/- 3.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 55849869    |
| train/                  |             |
|    approx_kl            | 0.008249014 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.328       |
|    learning_rate        | 3.97e-05    |
|    loss                 | 0.0695      |
|    n_updates            | 8725        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 910      |
|    time_elapsed    | 313538   |
|    total_timesteps | 55910400 |
---------------------------------
Eval num_timesteps=55911310, episode_reward=-0.03 +/- 0.99
Episode length: 29.28 +/- 3.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 55911310    |
| train/                  |             |
|    approx_kl            | 0.008199889 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.343       |
|    learning_rate        | 3.97e-05    |
|    loss                 | 0.0679      |
|    n_updates            | 8730        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 911      |
|    time_elapsed    | 313857   |
|    total_timesteps | 55971840 |
---------------------------------
Eval num_timesteps=55972751, episode_reward=0.05 +/- 0.98
Episode length: 29.30 +/- 3.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.3       |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 55972751   |
| train/                  |            |
|    approx_kl            | 0.00790156 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.54      |
|    explained_variance   | 0.346      |
|    learning_rate        | 3.96e-05   |
|    loss                 | 0.0713     |
|    n_updates            | 8735       |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 912      |
|    time_elapsed    | 314177   |
|    total_timesteps | 56033280 |
---------------------------------
Eval num_timesteps=56034192, episode_reward=0.11 +/- 0.98
Episode length: 29.35 +/- 3.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 56034192    |
| train/                  |             |
|    approx_kl            | 0.007994885 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.348       |
|    learning_rate        | 3.96e-05    |
|    loss                 | 0.062       |
|    n_updates            | 8740        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 913      |
|    time_elapsed    | 314498   |
|    total_timesteps | 56094720 |
---------------------------------
Eval num_timesteps=56095633, episode_reward=-0.04 +/- 0.98
Episode length: 28.96 +/- 4.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | -0.042      |
| time/                   |             |
|    total_timesteps      | 56095633    |
| train/                  |             |
|    approx_kl            | 0.008101315 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.356       |
|    learning_rate        | 3.95e-05    |
|    loss                 | 0.106       |
|    n_updates            | 8745        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 914      |
|    time_elapsed    | 314820   |
|    total_timesteps | 56156160 |
---------------------------------
Eval num_timesteps=56157074, episode_reward=0.07 +/- 0.98
Episode length: 29.09 +/- 4.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.1        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 56157074    |
| train/                  |             |
|    approx_kl            | 0.007963366 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.326       |
|    learning_rate        | 3.95e-05    |
|    loss                 | 0.0925      |
|    n_updates            | 8750        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 178      |
|    iterations      | 915      |
|    time_elapsed    | 315140   |
|    total_timesteps | 56217600 |
---------------------------------
Eval num_timesteps=56218515, episode_reward=0.12 +/- 0.98
Episode length: 29.20 +/- 4.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 56218515    |
| train/                  |             |
|    approx_kl            | 0.007974355 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.532      |
|    explained_variance   | 0.335       |
|    learning_rate        | 3.94e-05    |
|    loss                 | 0.0957      |
|    n_updates            | 8755        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 916      |
|    time_elapsed    | 315458   |
|    total_timesteps | 56279040 |
---------------------------------
Eval num_timesteps=56279956, episode_reward=0.01 +/- 0.98
Episode length: 29.47 +/- 3.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 56279956    |
| train/                  |             |
|    approx_kl            | 0.008073895 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.341       |
|    learning_rate        | 3.93e-05    |
|    loss                 | 0.0751      |
|    n_updates            | 8760        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 917      |
|    time_elapsed    | 315775   |
|    total_timesteps | 56340480 |
---------------------------------
Eval num_timesteps=56341397, episode_reward=0.04 +/- 0.99
Episode length: 29.35 +/- 3.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 56341397    |
| train/                  |             |
|    approx_kl            | 0.008294501 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0.334       |
|    learning_rate        | 3.93e-05    |
|    loss                 | 0.129       |
|    n_updates            | 8765        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 918      |
|    time_elapsed    | 316092   |
|    total_timesteps | 56401920 |
---------------------------------
Eval num_timesteps=56402838, episode_reward=0.13 +/- 0.98
Episode length: 29.13 +/- 4.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.1        |
|    mean_reward          | 0.126       |
| time/                   |             |
|    total_timesteps      | 56402838    |
| train/                  |             |
|    approx_kl            | 0.008010677 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.346       |
|    learning_rate        | 3.92e-05    |
|    loss                 | 0.0806      |
|    n_updates            | 8770        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 919      |
|    time_elapsed    | 316409   |
|    total_timesteps | 56463360 |
---------------------------------
Eval num_timesteps=56464279, episode_reward=0.13 +/- 0.97
Episode length: 29.03 +/- 4.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 0.134       |
| time/                   |             |
|    total_timesteps      | 56464279    |
| train/                  |             |
|    approx_kl            | 0.007884863 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.352       |
|    learning_rate        | 3.92e-05    |
|    loss                 | 0.0934      |
|    n_updates            | 8775        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 920      |
|    time_elapsed    | 316724   |
|    total_timesteps | 56524800 |
---------------------------------
Eval num_timesteps=56525720, episode_reward=0.10 +/- 0.98
Episode length: 29.24 +/- 4.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.2        |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 56525720    |
| train/                  |             |
|    approx_kl            | 0.008003181 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.339       |
|    learning_rate        | 3.91e-05    |
|    loss                 | 0.07        |
|    n_updates            | 8780        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 921      |
|    time_elapsed    | 317041   |
|    total_timesteps | 56586240 |
---------------------------------
Eval num_timesteps=56587161, episode_reward=0.16 +/- 0.97
Episode length: 29.35 +/- 3.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.3       |
|    mean_reward          | 0.156      |
| time/                   |            |
|    total_timesteps      | 56587161   |
| train/                  |            |
|    approx_kl            | 0.00815505 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.521     |
|    explained_variance   | 0.341      |
|    learning_rate        | 3.91e-05   |
|    loss                 | 0.0926     |
|    n_updates            | 8785       |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.242      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.156
SELFPLAY: new best model, bumping up generation to 26
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 922      |
|    time_elapsed    | 317359   |
|    total_timesteps | 56647680 |
---------------------------------
Eval num_timesteps=56648602, episode_reward=0.10 +/- 0.98
Episode length: 29.89 +/- 1.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.096       |
| time/                   |             |
|    total_timesteps      | 56648602    |
| train/                  |             |
|    approx_kl            | 0.008005833 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.319       |
|    learning_rate        | 3.9e-05     |
|    loss                 | 0.112       |
|    n_updates            | 8790        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 923      |
|    time_elapsed    | 317681   |
|    total_timesteps | 56709120 |
---------------------------------
Eval num_timesteps=56710043, episode_reward=-0.02 +/- 0.98
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 56710043    |
| train/                  |             |
|    approx_kl            | 0.008083945 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.54       |
|    explained_variance   | 0.328       |
|    learning_rate        | 3.9e-05     |
|    loss                 | 0.0794      |
|    n_updates            | 8795        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 924      |
|    time_elapsed    | 318003   |
|    total_timesteps | 56770560 |
---------------------------------
Eval num_timesteps=56771484, episode_reward=0.03 +/- 0.99
Episode length: 29.94 +/- 1.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 56771484    |
| train/                  |             |
|    approx_kl            | 0.007997883 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.338       |
|    learning_rate        | 3.89e-05    |
|    loss                 | 0.1         |
|    n_updates            | 8800        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 925      |
|    time_elapsed    | 318326   |
|    total_timesteps | 56832000 |
---------------------------------
Eval num_timesteps=56832925, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 56832925    |
| train/                  |             |
|    approx_kl            | 0.008062687 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.35        |
|    learning_rate        | 3.89e-05    |
|    loss                 | 0.0555      |
|    n_updates            | 8805        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 926      |
|    time_elapsed    | 318648   |
|    total_timesteps | 56893440 |
---------------------------------
Eval num_timesteps=56894366, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 56894366    |
| train/                  |             |
|    approx_kl            | 0.007858287 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.346       |
|    learning_rate        | 3.88e-05    |
|    loss                 | 0.0565      |
|    n_updates            | 8810        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 927      |
|    time_elapsed    | 318967   |
|    total_timesteps | 56954880 |
---------------------------------
Eval num_timesteps=56955807, episode_reward=-0.01 +/- 0.98
Episode length: 29.83 +/- 1.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 56955807    |
| train/                  |             |
|    approx_kl            | 0.007920874 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.532      |
|    explained_variance   | 0.333       |
|    learning_rate        | 3.87e-05    |
|    loss                 | 0.119       |
|    n_updates            | 8815        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 928      |
|    time_elapsed    | 319285   |
|    total_timesteps | 57016320 |
---------------------------------
Eval num_timesteps=57017248, episode_reward=0.01 +/- 0.98
Episode length: 29.86 +/- 1.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 57017248    |
| train/                  |             |
|    approx_kl            | 0.007868658 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.349       |
|    learning_rate        | 3.87e-05    |
|    loss                 | 0.0816      |
|    n_updates            | 8820        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 929      |
|    time_elapsed    | 319603   |
|    total_timesteps | 57077760 |
---------------------------------
Eval num_timesteps=57078689, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 1.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.032      |
| time/                   |            |
|    total_timesteps      | 57078689   |
| train/                  |            |
|    approx_kl            | 0.00827472 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.341      |
|    learning_rate        | 3.86e-05   |
|    loss                 | 0.105      |
|    n_updates            | 8825       |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 930      |
|    time_elapsed    | 319921   |
|    total_timesteps | 57139200 |
---------------------------------
Eval num_timesteps=57140130, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 57140130    |
| train/                  |             |
|    approx_kl            | 0.007944284 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.513      |
|    explained_variance   | 0.34        |
|    learning_rate        | 3.86e-05    |
|    loss                 | 0.096       |
|    n_updates            | 8830        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 931      |
|    time_elapsed    | 320238   |
|    total_timesteps | 57200640 |
---------------------------------
Eval num_timesteps=57201571, episode_reward=0.13 +/- 0.97
Episode length: 30.06 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.126       |
| time/                   |             |
|    total_timesteps      | 57201571    |
| train/                  |             |
|    approx_kl            | 0.008000548 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.503      |
|    explained_variance   | 0.357       |
|    learning_rate        | 3.85e-05    |
|    loss                 | 0.0954      |
|    n_updates            | 8835        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 932      |
|    time_elapsed    | 320555   |
|    total_timesteps | 57262080 |
---------------------------------
Eval num_timesteps=57263012, episode_reward=0.15 +/- 0.97
Episode length: 29.94 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.154       |
| time/                   |             |
|    total_timesteps      | 57263012    |
| train/                  |             |
|    approx_kl            | 0.008168702 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.502      |
|    explained_variance   | 0.35        |
|    learning_rate        | 3.85e-05    |
|    loss                 | 0.0576      |
|    n_updates            | 8840        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.233       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.154
SELFPLAY: new best model, bumping up generation to 27
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 933      |
|    time_elapsed    | 320873   |
|    total_timesteps | 57323520 |
---------------------------------
Eval num_timesteps=57324453, episode_reward=-0.11 +/- 0.98
Episode length: 29.82 +/- 1.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.8         |
|    mean_reward          | -0.106       |
| time/                   |              |
|    total_timesteps      | 57324453     |
| train/                  |              |
|    approx_kl            | 0.0077331956 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.466       |
|    explained_variance   | 0.327        |
|    learning_rate        | 3.84e-05     |
|    loss                 | 0.116        |
|    n_updates            | 8845         |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 934      |
|    time_elapsed    | 321193   |
|    total_timesteps | 57384960 |
---------------------------------
Eval num_timesteps=57385894, episode_reward=0.06 +/- 0.98
Episode length: 29.91 +/- 1.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.06         |
| time/                   |              |
|    total_timesteps      | 57385894     |
| train/                  |              |
|    approx_kl            | 0.0076560536 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.469       |
|    explained_variance   | 0.351        |
|    learning_rate        | 3.84e-05     |
|    loss                 | 0.104        |
|    n_updates            | 8850         |
|    policy_gradient_loss | -0.0141      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 935      |
|    time_elapsed    | 321515   |
|    total_timesteps | 57446400 |
---------------------------------
Eval num_timesteps=57447335, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 57447335    |
| train/                  |             |
|    approx_kl            | 0.007839083 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.472      |
|    explained_variance   | 0.364       |
|    learning_rate        | 3.83e-05    |
|    loss                 | 0.0574      |
|    n_updates            | 8855        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 936      |
|    time_elapsed    | 321837   |
|    total_timesteps | 57507840 |
---------------------------------
Eval num_timesteps=57508776, episode_reward=0.05 +/- 0.98
Episode length: 29.90 +/- 1.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.05       |
| time/                   |            |
|    total_timesteps      | 57508776   |
| train/                  |            |
|    approx_kl            | 0.00761024 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.475     |
|    explained_variance   | 0.351      |
|    learning_rate        | 3.82e-05   |
|    loss                 | 0.0568     |
|    n_updates            | 8860       |
|    policy_gradient_loss | -0.0142    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 937      |
|    time_elapsed    | 322161   |
|    total_timesteps | 57569280 |
---------------------------------
Eval num_timesteps=57570217, episode_reward=0.16 +/- 0.97
Episode length: 29.92 +/- 1.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.162      |
| time/                   |            |
|    total_timesteps      | 57570217   |
| train/                  |            |
|    approx_kl            | 0.00761045 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.478     |
|    explained_variance   | 0.351      |
|    learning_rate        | 3.82e-05   |
|    loss                 | 0.0812     |
|    n_updates            | 8865       |
|    policy_gradient_loss | -0.0144    |
|    value_loss           | 0.234      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.162
SELFPLAY: new best model, bumping up generation to 28
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 938      |
|    time_elapsed    | 322481   |
|    total_timesteps | 57630720 |
---------------------------------
Eval num_timesteps=57631658, episode_reward=0.05 +/- 0.99
Episode length: 29.75 +/- 2.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 57631658    |
| train/                  |             |
|    approx_kl            | 0.007750121 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.47       |
|    explained_variance   | 0.359       |
|    learning_rate        | 3.81e-05    |
|    loss                 | 0.0626      |
|    n_updates            | 8870        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 939      |
|    time_elapsed    | 322799   |
|    total_timesteps | 57692160 |
---------------------------------
Eval num_timesteps=57693099, episode_reward=0.08 +/- 0.99
Episode length: 29.88 +/- 1.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.078        |
| time/                   |              |
|    total_timesteps      | 57693099     |
| train/                  |              |
|    approx_kl            | 0.0077450355 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.472       |
|    explained_variance   | 0.361        |
|    learning_rate        | 3.81e-05     |
|    loss                 | 0.0747       |
|    n_updates            | 8875         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 940      |
|    time_elapsed    | 323117   |
|    total_timesteps | 57753600 |
---------------------------------
Eval num_timesteps=57754540, episode_reward=0.09 +/- 0.98
Episode length: 29.86 +/- 2.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0.092      |
| time/                   |            |
|    total_timesteps      | 57754540   |
| train/                  |            |
|    approx_kl            | 0.00765409 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.477     |
|    explained_variance   | 0.365      |
|    learning_rate        | 3.8e-05    |
|    loss                 | 0.0805     |
|    n_updates            | 8880       |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 941      |
|    time_elapsed    | 323434   |
|    total_timesteps | 57815040 |
---------------------------------
Eval num_timesteps=57815981, episode_reward=0.10 +/- 0.98
Episode length: 29.84 +/- 1.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.8         |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 57815981     |
| train/                  |              |
|    approx_kl            | 0.0076160505 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.361        |
|    learning_rate        | 3.8e-05      |
|    loss                 | 0.0725       |
|    n_updates            | 8885         |
|    policy_gradient_loss | -0.0145      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 942      |
|    time_elapsed    | 323751   |
|    total_timesteps | 57876480 |
---------------------------------
Eval num_timesteps=57877422, episode_reward=0.15 +/- 0.97
Episode length: 29.80 +/- 2.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.8        |
|    mean_reward          | 0.148       |
| time/                   |             |
|    total_timesteps      | 57877422    |
| train/                  |             |
|    approx_kl            | 0.007689947 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.463      |
|    explained_variance   | 0.353       |
|    learning_rate        | 3.79e-05    |
|    loss                 | 0.0919      |
|    n_updates            | 8890        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 29
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 943      |
|    time_elapsed    | 324069   |
|    total_timesteps | 57937920 |
---------------------------------
Eval num_timesteps=57938863, episode_reward=0.07 +/- 0.99
Episode length: 29.53 +/- 2.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.5        |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 57938863    |
| train/                  |             |
|    approx_kl            | 0.007266149 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.441      |
|    explained_variance   | 0.393       |
|    learning_rate        | 3.79e-05    |
|    loss                 | 0.0685      |
|    n_updates            | 8895        |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 944      |
|    time_elapsed    | 324386   |
|    total_timesteps | 57999360 |
---------------------------------
Eval num_timesteps=58000304, episode_reward=0.05 +/- 0.99
Episode length: 29.82 +/- 2.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.8         |
|    mean_reward          | 0.054        |
| time/                   |              |
|    total_timesteps      | 58000304     |
| train/                  |              |
|    approx_kl            | 0.0076611657 |
|    clip_fraction        | 0.1          |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.438       |
|    explained_variance   | 0.376        |
|    learning_rate        | 3.78e-05     |
|    loss                 | 0.0577       |
|    n_updates            | 8900         |
|    policy_gradient_loss | -0.0136      |
|    value_loss           | 0.236        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 945      |
|    time_elapsed    | 324705   |
|    total_timesteps | 58060800 |
---------------------------------
Eval num_timesteps=58061745, episode_reward=0.03 +/- 0.98
Episode length: 29.79 +/- 2.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.8         |
|    mean_reward          | 0.034        |
| time/                   |              |
|    total_timesteps      | 58061745     |
| train/                  |              |
|    approx_kl            | 0.0075386036 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.446       |
|    explained_variance   | 0.383        |
|    learning_rate        | 3.77e-05     |
|    loss                 | 0.0642       |
|    n_updates            | 8905         |
|    policy_gradient_loss | -0.0139      |
|    value_loss           | 0.233        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 946      |
|    time_elapsed    | 325025   |
|    total_timesteps | 58122240 |
---------------------------------
Eval num_timesteps=58123186, episode_reward=0.08 +/- 0.98
Episode length: 29.74 +/- 2.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.7         |
|    mean_reward          | 0.078        |
| time/                   |              |
|    total_timesteps      | 58123186     |
| train/                  |              |
|    approx_kl            | 0.0075347787 |
|    clip_fraction        | 0.0991       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.441       |
|    explained_variance   | 0.403        |
|    learning_rate        | 3.77e-05     |
|    loss                 | 0.0912       |
|    n_updates            | 8910         |
|    policy_gradient_loss | -0.0139      |
|    value_loss           | 0.224        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 947      |
|    time_elapsed    | 325346   |
|    total_timesteps | 58183680 |
---------------------------------
Eval num_timesteps=58184627, episode_reward=0.14 +/- 0.97
Episode length: 29.92 +/- 1.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.136        |
| time/                   |              |
|    total_timesteps      | 58184627     |
| train/                  |              |
|    approx_kl            | 0.0072341044 |
|    clip_fraction        | 0.0982       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.444       |
|    explained_variance   | 0.397        |
|    learning_rate        | 3.76e-05     |
|    loss                 | 0.133        |
|    n_updates            | 8915         |
|    policy_gradient_loss | -0.0137      |
|    value_loss           | 0.228        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 948      |
|    time_elapsed    | 325668   |
|    total_timesteps | 58245120 |
---------------------------------
Eval num_timesteps=58246068, episode_reward=0.11 +/- 0.97
Episode length: 29.97 +/- 1.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.112        |
| time/                   |              |
|    total_timesteps      | 58246068     |
| train/                  |              |
|    approx_kl            | 0.0074752797 |
|    clip_fraction        | 0.0995       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.4          |
|    learning_rate        | 3.76e-05     |
|    loss                 | 0.065        |
|    n_updates            | 8920         |
|    policy_gradient_loss | -0.0139      |
|    value_loss           | 0.229        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.6     |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 949      |
|    time_elapsed    | 325989   |
|    total_timesteps | 58306560 |
---------------------------------
Eval num_timesteps=58307509, episode_reward=0.12 +/- 0.98
Episode length: 29.96 +/- 1.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.116        |
| time/                   |              |
|    total_timesteps      | 58307509     |
| train/                  |              |
|    approx_kl            | 0.0074304463 |
|    clip_fraction        | 0.0989       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.449       |
|    explained_variance   | 0.39         |
|    learning_rate        | 3.75e-05     |
|    loss                 | 0.0658       |
|    n_updates            | 8925         |
|    policy_gradient_loss | -0.0137      |
|    value_loss           | 0.225        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 950      |
|    time_elapsed    | 326307   |
|    total_timesteps | 58368000 |
---------------------------------
Eval num_timesteps=58368950, episode_reward=0.13 +/- 0.97
Episode length: 30.04 +/- 1.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.13         |
| time/                   |              |
|    total_timesteps      | 58368950     |
| train/                  |              |
|    approx_kl            | 0.0076474072 |
|    clip_fraction        | 0.099        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.451       |
|    explained_variance   | 0.422        |
|    learning_rate        | 3.75e-05     |
|    loss                 | 0.0647       |
|    n_updates            | 8930         |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 0.217        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 951      |
|    time_elapsed    | 326624   |
|    total_timesteps | 58429440 |
---------------------------------
Eval num_timesteps=58430391, episode_reward=0.22 +/- 0.96
Episode length: 30.03 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.218       |
| time/                   |             |
|    total_timesteps      | 58430391    |
| train/                  |             |
|    approx_kl            | 0.007833765 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.46       |
|    explained_variance   | 0.396       |
|    learning_rate        | 3.74e-05    |
|    loss                 | 0.085       |
|    n_updates            | 8935        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.221       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.218
SELFPLAY: new best model, bumping up generation to 30
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 952      |
|    time_elapsed    | 326942   |
|    total_timesteps | 58490880 |
---------------------------------
Eval num_timesteps=58491832, episode_reward=0.05 +/- 0.99
Episode length: 29.93 +/- 1.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.052        |
| time/                   |              |
|    total_timesteps      | 58491832     |
| train/                  |              |
|    approx_kl            | 0.0074691204 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.496       |
|    explained_variance   | 0.359        |
|    learning_rate        | 3.74e-05     |
|    loss                 | 0.1          |
|    n_updates            | 8940         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 953      |
|    time_elapsed    | 327259   |
|    total_timesteps | 58552320 |
---------------------------------
Eval num_timesteps=58553273, episode_reward=-0.01 +/- 0.99
Episode length: 29.94 +/- 0.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | -0.01        |
| time/                   |              |
|    total_timesteps      | 58553273     |
| train/                  |              |
|    approx_kl            | 0.0076538813 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.504       |
|    explained_variance   | 0.348        |
|    learning_rate        | 3.73e-05     |
|    loss                 | 0.124        |
|    n_updates            | 8945         |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 954      |
|    time_elapsed    | 327578   |
|    total_timesteps | 58613760 |
---------------------------------
Eval num_timesteps=58614714, episode_reward=0.01 +/- 0.99
Episode length: 29.94 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 58614714    |
| train/                  |             |
|    approx_kl            | 0.007832316 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.358       |
|    learning_rate        | 3.72e-05    |
|    loss                 | 0.0746      |
|    n_updates            | 8950        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 955      |
|    time_elapsed    | 327895   |
|    total_timesteps | 58675200 |
---------------------------------
Eval num_timesteps=58676155, episode_reward=0.14 +/- 0.98
Episode length: 30.04 +/- 0.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 58676155    |
| train/                  |             |
|    approx_kl            | 0.007642714 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.363       |
|    learning_rate        | 3.72e-05    |
|    loss                 | 0.0903      |
|    n_updates            | 8955        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 956      |
|    time_elapsed    | 328214   |
|    total_timesteps | 58736640 |
---------------------------------
Eval num_timesteps=58737596, episode_reward=0.03 +/- 0.98
Episode length: 29.95 +/- 1.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.034        |
| time/                   |              |
|    total_timesteps      | 58737596     |
| train/                  |              |
|    approx_kl            | 0.0075482563 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.515       |
|    explained_variance   | 0.334        |
|    learning_rate        | 3.71e-05     |
|    loss                 | 0.0736       |
|    n_updates            | 8960         |
|    policy_gradient_loss | -0.0147      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 957      |
|    time_elapsed    | 328534   |
|    total_timesteps | 58798080 |
---------------------------------
Eval num_timesteps=58799037, episode_reward=0.09 +/- 0.98
Episode length: 29.94 +/- 1.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.092        |
| time/                   |              |
|    total_timesteps      | 58799037     |
| train/                  |              |
|    approx_kl            | 0.0075128763 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.522       |
|    explained_variance   | 0.34         |
|    learning_rate        | 3.71e-05     |
|    loss                 | 0.0788       |
|    n_updates            | 8965         |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.242        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 958      |
|    time_elapsed    | 328855   |
|    total_timesteps | 58859520 |
---------------------------------
Eval num_timesteps=58860478, episode_reward=0.13 +/- 0.98
Episode length: 29.94 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.128       |
| time/                   |             |
|    total_timesteps      | 58860478    |
| train/                  |             |
|    approx_kl            | 0.007581174 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.34        |
|    learning_rate        | 3.7e-05     |
|    loss                 | 0.0908      |
|    n_updates            | 8970        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 959      |
|    time_elapsed    | 329177   |
|    total_timesteps | 58920960 |
---------------------------------
Eval num_timesteps=58921919, episode_reward=0.11 +/- 0.98
Episode length: 30.01 +/- 1.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 58921919    |
| train/                  |             |
|    approx_kl            | 0.007303633 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0.339       |
|    learning_rate        | 3.7e-05     |
|    loss                 | 0.0976      |
|    n_updates            | 8975        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 960      |
|    time_elapsed    | 329501   |
|    total_timesteps | 58982400 |
---------------------------------
Eval num_timesteps=58983360, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.112        |
| time/                   |              |
|    total_timesteps      | 58983360     |
| train/                  |              |
|    approx_kl            | 0.0075062644 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.522       |
|    explained_variance   | 0.346        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0703       |
|    n_updates            | 8980         |
|    policy_gradient_loss | -0.0147      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 961      |
|    time_elapsed    | 329821   |
|    total_timesteps | 59043840 |
---------------------------------
Eval num_timesteps=59044801, episode_reward=0.04 +/- 0.98
Episode length: 30.05 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 59044801    |
| train/                  |             |
|    approx_kl            | 0.007818738 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.322       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.107       |
|    n_updates            | 8985        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 962      |
|    time_elapsed    | 330139   |
|    total_timesteps | 59105280 |
---------------------------------
Eval num_timesteps=59106242, episode_reward=0.07 +/- 0.99
Episode length: 30.03 +/- 0.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 59106242    |
| train/                  |             |
|    approx_kl            | 0.007596204 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0.352       |
|    learning_rate        | 3.68e-05    |
|    loss                 | 0.0596      |
|    n_updates            | 8990        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 963      |
|    time_elapsed    | 330457   |
|    total_timesteps | 59166720 |
---------------------------------
Eval num_timesteps=59167683, episode_reward=0.15 +/- 0.97
Episode length: 30.01 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.146       |
| time/                   |             |
|    total_timesteps      | 59167683    |
| train/                  |             |
|    approx_kl            | 0.007500808 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.344       |
|    learning_rate        | 3.67e-05    |
|    loss                 | 0.0864      |
|    n_updates            | 8995        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.238       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.146
SELFPLAY: new best model, bumping up generation to 31
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 964      |
|    time_elapsed    | 330775   |
|    total_timesteps | 59228160 |
---------------------------------
Eval num_timesteps=59229124, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 1.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.016       |
| time/                   |              |
|    total_timesteps      | 59229124     |
| train/                  |              |
|    approx_kl            | 0.0077665937 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.557       |
|    explained_variance   | 0.343        |
|    learning_rate        | 3.67e-05     |
|    loss                 | 0.0583       |
|    n_updates            | 9000         |
|    policy_gradient_loss | -0.0152      |
|    value_loss           | 0.236        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 965      |
|    time_elapsed    | 331093   |
|    total_timesteps | 59289600 |
---------------------------------
Eval num_timesteps=59290565, episode_reward=0.01 +/- 0.99
Episode length: 30.00 +/- 0.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 59290565    |
| train/                  |             |
|    approx_kl            | 0.007890311 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.559      |
|    explained_variance   | 0.321       |
|    learning_rate        | 3.66e-05    |
|    loss                 | 0.0828      |
|    n_updates            | 9005        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 966      |
|    time_elapsed    | 331411   |
|    total_timesteps | 59351040 |
---------------------------------
Eval num_timesteps=59352006, episode_reward=0.08 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 59352006    |
| train/                  |             |
|    approx_kl            | 0.007822455 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.336       |
|    learning_rate        | 3.66e-05    |
|    loss                 | 0.0853      |
|    n_updates            | 9010        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 967      |
|    time_elapsed    | 331729   |
|    total_timesteps | 59412480 |
---------------------------------
Eval num_timesteps=59413447, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.024      |
| time/                   |             |
|    total_timesteps      | 59413447    |
| train/                  |             |
|    approx_kl            | 0.007602744 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.553      |
|    explained_variance   | 0.327       |
|    learning_rate        | 3.65e-05    |
|    loss                 | 0.0826      |
|    n_updates            | 9015        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 968      |
|    time_elapsed    | 332049   |
|    total_timesteps | 59473920 |
---------------------------------
Eval num_timesteps=59474888, episode_reward=-0.01 +/- 0.98
Episode length: 30.01 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.012       |
| time/                   |              |
|    total_timesteps      | 59474888     |
| train/                  |              |
|    approx_kl            | 0.0076256203 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.346        |
|    learning_rate        | 3.65e-05     |
|    loss                 | 0.102        |
|    n_updates            | 9020         |
|    policy_gradient_loss | -0.015       |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 969      |
|    time_elapsed    | 332370   |
|    total_timesteps | 59535360 |
---------------------------------
Eval num_timesteps=59536329, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.008       |
| time/                   |              |
|    total_timesteps      | 59536329     |
| train/                  |              |
|    approx_kl            | 0.0075652706 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.328        |
|    learning_rate        | 3.64e-05     |
|    loss                 | 0.0611       |
|    n_updates            | 9025         |
|    policy_gradient_loss | -0.0154      |
|    value_loss           | 0.244        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 970      |
|    time_elapsed    | 332692   |
|    total_timesteps | 59596800 |
---------------------------------
Eval num_timesteps=59597770, episode_reward=0.06 +/- 0.98
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 59597770    |
| train/                  |             |
|    approx_kl            | 0.007593753 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.335       |
|    learning_rate        | 3.64e-05    |
|    loss                 | 0.0733      |
|    n_updates            | 9030        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 971      |
|    time_elapsed    | 333015   |
|    total_timesteps | 59658240 |
---------------------------------
Eval num_timesteps=59659211, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.022       |
| time/                   |              |
|    total_timesteps      | 59659211     |
| train/                  |              |
|    approx_kl            | 0.0076930197 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.326        |
|    learning_rate        | 3.63e-05     |
|    loss                 | 0.0804       |
|    n_updates            | 9035         |
|    policy_gradient_loss | -0.0152      |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 972      |
|    time_elapsed    | 333339   |
|    total_timesteps | 59719680 |
---------------------------------
Eval num_timesteps=59720652, episode_reward=-0.02 +/- 0.98
Episode length: 29.96 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.02        |
| time/                   |              |
|    total_timesteps      | 59720652     |
| train/                  |              |
|    approx_kl            | 0.0075000855 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.337        |
|    learning_rate        | 3.63e-05     |
|    loss                 | 0.0999       |
|    n_updates            | 9040         |
|    policy_gradient_loss | -0.0151      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 973      |
|    time_elapsed    | 333659   |
|    total_timesteps | 59781120 |
---------------------------------
Eval num_timesteps=59782093, episode_reward=-0.02 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 59782093    |
| train/                  |             |
|    approx_kl            | 0.007562557 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.556      |
|    explained_variance   | 0.333       |
|    learning_rate        | 3.62e-05    |
|    loss                 | 0.0703      |
|    n_updates            | 9045        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 974      |
|    time_elapsed    | 333978   |
|    total_timesteps | 59842560 |
---------------------------------
Eval num_timesteps=59843534, episode_reward=-0.00 +/- 0.98
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 59843534    |
| train/                  |             |
|    approx_kl            | 0.007741929 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.335       |
|    learning_rate        | 3.61e-05    |
|    loss                 | 0.0869      |
|    n_updates            | 9050        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 975      |
|    time_elapsed    | 334296   |
|    total_timesteps | 59904000 |
---------------------------------
Eval num_timesteps=59904975, episode_reward=0.07 +/- 0.97
Episode length: 29.97 +/- 0.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.072        |
| time/                   |              |
|    total_timesteps      | 59904975     |
| train/                  |              |
|    approx_kl            | 0.0076276003 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.553       |
|    explained_variance   | 0.33         |
|    learning_rate        | 3.61e-05     |
|    loss                 | 0.0775       |
|    n_updates            | 9055         |
|    policy_gradient_loss | -0.0154      |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 976      |
|    time_elapsed    | 334614   |
|    total_timesteps | 59965440 |
---------------------------------
Eval num_timesteps=59966416, episode_reward=0.01 +/- 0.98
Episode length: 29.99 +/- 0.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.014        |
| time/                   |              |
|    total_timesteps      | 59966416     |
| train/                  |              |
|    approx_kl            | 0.0077781808 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.551       |
|    explained_variance   | 0.342        |
|    learning_rate        | 3.6e-05      |
|    loss                 | 0.105        |
|    n_updates            | 9060         |
|    policy_gradient_loss | -0.0152      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 977      |
|    time_elapsed    | 334932   |
|    total_timesteps | 60026880 |
---------------------------------
Eval num_timesteps=60027857, episode_reward=0.03 +/- 0.98
Episode length: 30.05 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 60027857    |
| train/                  |             |
|    approx_kl            | 0.007409537 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.333       |
|    learning_rate        | 3.6e-05     |
|    loss                 | 0.106       |
|    n_updates            | 9065        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 978      |
|    time_elapsed    | 335250   |
|    total_timesteps | 60088320 |
---------------------------------
Eval num_timesteps=60089298, episode_reward=-0.05 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.052      |
| time/                   |             |
|    total_timesteps      | 60089298    |
| train/                  |             |
|    approx_kl            | 0.007320014 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.324       |
|    learning_rate        | 3.59e-05    |
|    loss                 | 0.0991      |
|    n_updates            | 9070        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 179      |
|    iterations      | 979      |
|    time_elapsed    | 335568   |
|    total_timesteps | 60149760 |
---------------------------------
Eval num_timesteps=60150739, episode_reward=0.05 +/- 0.99
Episode length: 30.03 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.052        |
| time/                   |              |
|    total_timesteps      | 60150739     |
| train/                  |              |
|    approx_kl            | 0.0074140322 |
|    clip_fraction        | 0.0996       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.548       |
|    explained_variance   | 0.345        |
|    learning_rate        | 3.59e-05     |
|    loss                 | 0.112        |
|    n_updates            | 9075         |
|    policy_gradient_loss | -0.0152      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 980      |
|    time_elapsed    | 335888   |
|    total_timesteps | 60211200 |
---------------------------------
Eval num_timesteps=60212180, episode_reward=-0.05 +/- 0.98
Episode length: 30.00 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.054       |
| time/                   |              |
|    total_timesteps      | 60212180     |
| train/                  |              |
|    approx_kl            | 0.0074890703 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.337        |
|    learning_rate        | 3.58e-05     |
|    loss                 | 0.071        |
|    n_updates            | 9080         |
|    policy_gradient_loss | -0.0152      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 981      |
|    time_elapsed    | 336209   |
|    total_timesteps | 60272640 |
---------------------------------
Eval num_timesteps=60273621, episode_reward=0.06 +/- 0.98
Episode length: 29.94 +/- 0.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.058        |
| time/                   |              |
|    total_timesteps      | 60273621     |
| train/                  |              |
|    approx_kl            | 0.0070967874 |
|    clip_fraction        | 0.0979       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.547       |
|    explained_variance   | 0.334        |
|    learning_rate        | 3.58e-05     |
|    loss                 | 0.0583       |
|    n_updates            | 9085         |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 982      |
|    time_elapsed    | 336532   |
|    total_timesteps | 60334080 |
---------------------------------
Eval num_timesteps=60335062, episode_reward=-0.02 +/- 0.98
Episode length: 29.96 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.018       |
| time/                   |              |
|    total_timesteps      | 60335062     |
| train/                  |              |
|    approx_kl            | 0.0072933203 |
|    clip_fraction        | 0.1          |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.545       |
|    explained_variance   | 0.336        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0611       |
|    n_updates            | 9090         |
|    policy_gradient_loss | -0.015       |
|    value_loss           | 0.242        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 983      |
|    time_elapsed    | 336855   |
|    total_timesteps | 60395520 |
---------------------------------
Eval num_timesteps=60396503, episode_reward=0.04 +/- 0.98
Episode length: 29.94 +/- 1.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 29.9         |
|    mean_reward          | 0.044        |
| time/                   |              |
|    total_timesteps      | 60396503     |
| train/                  |              |
|    approx_kl            | 0.0074013895 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.544       |
|    explained_variance   | 0.333        |
|    learning_rate        | 3.56e-05     |
|    loss                 | 0.0993       |
|    n_updates            | 9095         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 984      |
|    time_elapsed    | 337176   |
|    total_timesteps | 60456960 |
---------------------------------
Eval num_timesteps=60457944, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.008       |
| time/                   |              |
|    total_timesteps      | 60457944     |
| train/                  |              |
|    approx_kl            | 0.0072723837 |
|    clip_fraction        | 0.0997       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.543       |
|    explained_variance   | 0.346        |
|    learning_rate        | 3.56e-05     |
|    loss                 | 0.0687       |
|    n_updates            | 9100         |
|    policy_gradient_loss | -0.0151      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 985      |
|    time_elapsed    | 337496   |
|    total_timesteps | 60518400 |
---------------------------------
Eval num_timesteps=60519385, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 60519385    |
| train/                  |             |
|    approx_kl            | 0.007370689 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.329       |
|    learning_rate        | 3.55e-05    |
|    loss                 | 0.0663      |
|    n_updates            | 9105        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 986      |
|    time_elapsed    | 337814   |
|    total_timesteps | 60579840 |
---------------------------------
Eval num_timesteps=60580826, episode_reward=0.01 +/- 0.98
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.006       |
| time/                   |             |
|    total_timesteps      | 60580826    |
| train/                  |             |
|    approx_kl            | 0.007322486 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.338       |
|    learning_rate        | 3.55e-05    |
|    loss                 | 0.12        |
|    n_updates            | 9110        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 987      |
|    time_elapsed    | 338133   |
|    total_timesteps | 60641280 |
---------------------------------
Eval num_timesteps=60642267, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 60642267    |
| train/                  |             |
|    approx_kl            | 0.007239325 |
|    clip_fraction        | 0.0988      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.325       |
|    learning_rate        | 3.54e-05    |
|    loss                 | 0.0615      |
|    n_updates            | 9115        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 988      |
|    time_elapsed    | 338450   |
|    total_timesteps | 60702720 |
---------------------------------
Eval num_timesteps=60703708, episode_reward=0.00 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 60703708    |
| train/                  |             |
|    approx_kl            | 0.007290553 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.345       |
|    learning_rate        | 3.54e-05    |
|    loss                 | 0.164       |
|    n_updates            | 9120        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 179      |
|    iterations      | 989      |
|    time_elapsed    | 338767   |
|    total_timesteps | 60764160 |
---------------------------------
Eval num_timesteps=60765149, episode_reward=0.06 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 60765149    |
| train/                  |             |
|    approx_kl            | 0.007236863 |
|    clip_fraction        | 0.0991      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.342       |
|    learning_rate        | 3.53e-05    |
|    loss                 | 0.1         |
|    n_updates            | 9125        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 990      |
|    time_elapsed    | 339085   |
|    total_timesteps | 60825600 |
---------------------------------
Eval num_timesteps=60826590, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.044        |
| time/                   |              |
|    total_timesteps      | 60826590     |
| train/                  |              |
|    approx_kl            | 0.0072572753 |
|    clip_fraction        | 0.0979       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.319        |
|    learning_rate        | 3.53e-05     |
|    loss                 | 0.0814       |
|    n_updates            | 9130         |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 991      |
|    time_elapsed    | 339404   |
|    total_timesteps | 60887040 |
---------------------------------
Eval num_timesteps=60888031, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.106        |
| time/                   |              |
|    total_timesteps      | 60888031     |
| train/                  |              |
|    approx_kl            | 0.0070641795 |
|    clip_fraction        | 0.0962       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.331        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.11         |
|    n_updates            | 9135         |
|    policy_gradient_loss | -0.0148      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 992      |
|    time_elapsed    | 339724   |
|    total_timesteps | 60948480 |
---------------------------------
Eval num_timesteps=60949472, episode_reward=0.04 +/- 0.98
Episode length: 30.03 +/- 0.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 60949472    |
| train/                  |             |
|    approx_kl            | 0.007359773 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.35        |
|    learning_rate        | 3.51e-05    |
|    loss                 | 0.0627      |
|    n_updates            | 9140        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 993      |
|    time_elapsed    | 340046   |
|    total_timesteps | 61009920 |
---------------------------------
Eval num_timesteps=61010913, episode_reward=0.07 +/- 0.97
Episode length: 30.01 +/- 0.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.074        |
| time/                   |              |
|    total_timesteps      | 61010913     |
| train/                  |              |
|    approx_kl            | 0.0072481027 |
|    clip_fraction        | 0.097        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.535       |
|    explained_variance   | 0.348        |
|    learning_rate        | 3.51e-05     |
|    loss                 | 0.0497       |
|    n_updates            | 9145         |
|    policy_gradient_loss | -0.0146      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 994      |
|    time_elapsed    | 340368   |
|    total_timesteps | 61071360 |
---------------------------------
Eval num_timesteps=61072354, episode_reward=0.02 +/- 0.99
Episode length: 30.03 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.024        |
| time/                   |              |
|    total_timesteps      | 61072354     |
| train/                  |              |
|    approx_kl            | 0.0071378173 |
|    clip_fraction        | 0.0989       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.326        |
|    learning_rate        | 3.5e-05      |
|    loss                 | 0.0798       |
|    n_updates            | 9150         |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 995      |
|    time_elapsed    | 340692   |
|    total_timesteps | 61132800 |
---------------------------------
Eval num_timesteps=61133795, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.068        |
| time/                   |              |
|    total_timesteps      | 61133795     |
| train/                  |              |
|    approx_kl            | 0.0072926106 |
|    clip_fraction        | 0.0964       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.531       |
|    explained_variance   | 0.324        |
|    learning_rate        | 3.5e-05      |
|    loss                 | 0.0741       |
|    n_updates            | 9155         |
|    policy_gradient_loss | -0.0147      |
|    value_loss           | 0.243        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 996      |
|    time_elapsed    | 341013   |
|    total_timesteps | 61194240 |
---------------------------------
Eval num_timesteps=61195236, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 61195236    |
| train/                  |             |
|    approx_kl            | 0.007292877 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.346       |
|    learning_rate        | 3.49e-05    |
|    loss                 | 0.0848      |
|    n_updates            | 9160        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 997      |
|    time_elapsed    | 341331   |
|    total_timesteps | 61255680 |
---------------------------------
Eval num_timesteps=61256677, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 61256677    |
| train/                  |             |
|    approx_kl            | 0.007004433 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.327       |
|    learning_rate        | 3.49e-05    |
|    loss                 | 0.106       |
|    n_updates            | 9165        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 998      |
|    time_elapsed    | 341649   |
|    total_timesteps | 61317120 |
---------------------------------
Eval num_timesteps=61318118, episode_reward=-0.01 +/- 0.98
Episode length: 29.96 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 61318118    |
| train/                  |             |
|    approx_kl            | 0.007175797 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.333       |
|    learning_rate        | 3.48e-05    |
|    loss                 | 0.0716      |
|    n_updates            | 9170        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 999      |
|    time_elapsed    | 341967   |
|    total_timesteps | 61378560 |
---------------------------------
Eval num_timesteps=61379559, episode_reward=0.08 +/- 0.98
Episode length: 30.08 +/- 0.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.084        |
| time/                   |              |
|    total_timesteps      | 61379559     |
| train/                  |              |
|    approx_kl            | 0.0071222503 |
|    clip_fraction        | 0.0977       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.534       |
|    explained_variance   | 0.337        |
|    learning_rate        | 3.48e-05     |
|    loss                 | 0.135        |
|    n_updates            | 9175         |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1000     |
|    time_elapsed    | 342285   |
|    total_timesteps | 61440000 |
---------------------------------
Eval num_timesteps=61441000, episode_reward=0.04 +/- 0.98
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 61441000    |
| train/                  |             |
|    approx_kl            | 0.007448285 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.34        |
|    learning_rate        | 3.47e-05    |
|    loss                 | 0.0839      |
|    n_updates            | 9180        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1001     |
|    time_elapsed    | 342603   |
|    total_timesteps | 61501440 |
---------------------------------
Eval num_timesteps=61502441, episode_reward=0.10 +/- 0.97
Episode length: 29.99 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.102        |
| time/                   |              |
|    total_timesteps      | 61502441     |
| train/                  |              |
|    approx_kl            | 0.0069399457 |
|    clip_fraction        | 0.0967       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.531       |
|    explained_variance   | 0.332        |
|    learning_rate        | 3.46e-05     |
|    loss                 | 0.075        |
|    n_updates            | 9185         |
|    policy_gradient_loss | -0.0148      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1002     |
|    time_elapsed    | 342920   |
|    total_timesteps | 61562880 |
---------------------------------
Eval num_timesteps=61563882, episode_reward=-0.00 +/- 0.99
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.004     |
| time/                   |            |
|    total_timesteps      | 61563882   |
| train/                  |            |
|    approx_kl            | 0.00694996 |
|    clip_fraction        | 0.0951     |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.531     |
|    explained_variance   | 0.344      |
|    learning_rate        | 3.46e-05   |
|    loss                 | 0.074      |
|    n_updates            | 9190       |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1003     |
|    time_elapsed    | 343239   |
|    total_timesteps | 61624320 |
---------------------------------
Eval num_timesteps=61625323, episode_reward=-0.02 +/- 0.98
Episode length: 29.98 +/- 0.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.022       |
| time/                   |              |
|    total_timesteps      | 61625323     |
| train/                  |              |
|    approx_kl            | 0.0072859954 |
|    clip_fraction        | 0.0977       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.329        |
|    learning_rate        | 3.45e-05     |
|    loss                 | 0.0733       |
|    n_updates            | 9195         |
|    policy_gradient_loss | -0.0148      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1004     |
|    time_elapsed    | 343560   |
|    total_timesteps | 61685760 |
---------------------------------
Eval num_timesteps=61686764, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 61686764    |
| train/                  |             |
|    approx_kl            | 0.007065676 |
|    clip_fraction        | 0.0954      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.527      |
|    explained_variance   | 0.325       |
|    learning_rate        | 3.45e-05    |
|    loss                 | 0.106       |
|    n_updates            | 9200        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1005     |
|    time_elapsed    | 343882   |
|    total_timesteps | 61747200 |
---------------------------------
Eval num_timesteps=61748205, episode_reward=0.13 +/- 0.97
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.134       |
| time/                   |             |
|    total_timesteps      | 61748205    |
| train/                  |             |
|    approx_kl            | 0.006965498 |
|    clip_fraction        | 0.0958      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.527      |
|    explained_variance   | 0.344       |
|    learning_rate        | 3.44e-05    |
|    loss                 | 0.068       |
|    n_updates            | 9205        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1006     |
|    time_elapsed    | 344204   |
|    total_timesteps | 61808640 |
---------------------------------
Eval num_timesteps=61809646, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.036        |
| time/                   |              |
|    total_timesteps      | 61809646     |
| train/                  |              |
|    approx_kl            | 0.0068214615 |
|    clip_fraction        | 0.0957       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.525       |
|    explained_variance   | 0.334        |
|    learning_rate        | 3.44e-05     |
|    loss                 | 0.0802       |
|    n_updates            | 9210         |
|    policy_gradient_loss | -0.0147      |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1007     |
|    time_elapsed    | 344528   |
|    total_timesteps | 61870080 |
---------------------------------
Eval num_timesteps=61871087, episode_reward=0.08 +/- 0.98
Episode length: 29.98 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 61871087   |
| train/                  |            |
|    approx_kl            | 0.00683492 |
|    clip_fraction        | 0.0955     |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.524     |
|    explained_variance   | 0.339      |
|    learning_rate        | 3.43e-05   |
|    loss                 | 0.0955     |
|    n_updates            | 9215       |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1008     |
|    time_elapsed    | 344849   |
|    total_timesteps | 61931520 |
---------------------------------
Eval num_timesteps=61932528, episode_reward=0.14 +/- 0.97
Episode length: 30.08 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.136       |
| time/                   |             |
|    total_timesteps      | 61932528    |
| train/                  |             |
|    approx_kl            | 0.006826521 |
|    clip_fraction        | 0.0948      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.34        |
|    learning_rate        | 3.43e-05    |
|    loss                 | 0.0797      |
|    n_updates            | 9220        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1009     |
|    time_elapsed    | 345168   |
|    total_timesteps | 61992960 |
---------------------------------
Eval num_timesteps=61993969, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 61993969    |
| train/                  |             |
|    approx_kl            | 0.007015758 |
|    clip_fraction        | 0.096       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.339       |
|    learning_rate        | 3.42e-05    |
|    loss                 | 0.13        |
|    n_updates            | 9225        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1010     |
|    time_elapsed    | 345486   |
|    total_timesteps | 62054400 |
---------------------------------
Eval num_timesteps=62055410, episode_reward=0.13 +/- 0.98
Episode length: 30.04 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.132        |
| time/                   |              |
|    total_timesteps      | 62055410     |
| train/                  |              |
|    approx_kl            | 0.0068631354 |
|    clip_fraction        | 0.0946       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.525       |
|    explained_variance   | 0.324        |
|    learning_rate        | 3.42e-05     |
|    loss                 | 0.112        |
|    n_updates            | 9230         |
|    policy_gradient_loss | -0.0146      |
|    value_loss           | 0.245        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1011     |
|    time_elapsed    | 345803   |
|    total_timesteps | 62115840 |
---------------------------------
Eval num_timesteps=62116851, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.074        |
| time/                   |              |
|    total_timesteps      | 62116851     |
| train/                  |              |
|    approx_kl            | 0.0068704886 |
|    clip_fraction        | 0.0952       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.519       |
|    explained_variance   | 0.328        |
|    learning_rate        | 3.41e-05     |
|    loss                 | 0.0736       |
|    n_updates            | 9235         |
|    policy_gradient_loss | -0.015       |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1012     |
|    time_elapsed    | 346121   |
|    total_timesteps | 62177280 |
---------------------------------
Eval num_timesteps=62178292, episode_reward=0.12 +/- 0.98
Episode length: 30.08 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.118      |
| time/                   |            |
|    total_timesteps      | 62178292   |
| train/                  |            |
|    approx_kl            | 0.00703208 |
|    clip_fraction        | 0.0942     |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.525     |
|    explained_variance   | 0.337      |
|    learning_rate        | 3.4e-05    |
|    loss                 | 0.0992     |
|    n_updates            | 9240       |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1013     |
|    time_elapsed    | 346438   |
|    total_timesteps | 62238720 |
---------------------------------
Eval num_timesteps=62239733, episode_reward=0.12 +/- 0.97
Episode length: 30.06 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.124        |
| time/                   |              |
|    total_timesteps      | 62239733     |
| train/                  |              |
|    approx_kl            | 0.0071011516 |
|    clip_fraction        | 0.095        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.52        |
|    explained_variance   | 0.328        |
|    learning_rate        | 3.4e-05      |
|    loss                 | 0.124        |
|    n_updates            | 9245         |
|    policy_gradient_loss | -0.0147      |
|    value_loss           | 0.243        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1014     |
|    time_elapsed    | 346756   |
|    total_timesteps | 62300160 |
---------------------------------
Eval num_timesteps=62301174, episode_reward=0.09 +/- 0.98
Episode length: 30.05 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 62301174    |
| train/                  |             |
|    approx_kl            | 0.007119769 |
|    clip_fraction        | 0.0952      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.521      |
|    explained_variance   | 0.333       |
|    learning_rate        | 3.39e-05    |
|    loss                 | 0.0857      |
|    n_updates            | 9250        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1015     |
|    time_elapsed    | 347075   |
|    total_timesteps | 62361600 |
---------------------------------
Eval num_timesteps=62362615, episode_reward=0.19 +/- 0.96
Episode length: 30.07 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.188       |
| time/                   |             |
|    total_timesteps      | 62362615    |
| train/                  |             |
|    approx_kl            | 0.006829261 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.521      |
|    explained_variance   | 0.34        |
|    learning_rate        | 3.39e-05    |
|    loss                 | 0.058       |
|    n_updates            | 9255        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.188
SELFPLAY: new best model, bumping up generation to 32
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1016     |
|    time_elapsed    | 347396   |
|    total_timesteps | 62423040 |
---------------------------------
Eval num_timesteps=62424056, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.014        |
| time/                   |              |
|    total_timesteps      | 62424056     |
| train/                  |              |
|    approx_kl            | 0.0067115626 |
|    clip_fraction        | 0.0933       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.52        |
|    explained_variance   | 0.343        |
|    learning_rate        | 3.38e-05     |
|    loss                 | 0.0841       |
|    n_updates            | 9260         |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.19    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1017     |
|    time_elapsed    | 347718   |
|    total_timesteps | 62484480 |
---------------------------------
Eval num_timesteps=62485497, episode_reward=-0.01 +/- 0.98
Episode length: 29.96 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.008     |
| time/                   |            |
|    total_timesteps      | 62485497   |
| train/                  |            |
|    approx_kl            | 0.00707043 |
|    clip_fraction        | 0.0972     |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.517     |
|    explained_variance   | 0.344      |
|    learning_rate        | 3.38e-05   |
|    loss                 | 0.0768     |
|    n_updates            | 9265       |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1018     |
|    time_elapsed    | 348041   |
|    total_timesteps | 62545920 |
---------------------------------
Eval num_timesteps=62546938, episode_reward=-0.03 +/- 0.98
Episode length: 29.99 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | -0.028    |
| time/                   |           |
|    total_timesteps      | 62546938  |
| train/                  |           |
|    approx_kl            | 0.0067212 |
|    clip_fraction        | 0.0927    |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.519    |
|    explained_variance   | 0.328     |
|    learning_rate        | 3.37e-05  |
|    loss                 | 0.0689    |
|    n_updates            | 9270      |
|    policy_gradient_loss | -0.0141   |
|    value_loss           | 0.242     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1019     |
|    time_elapsed    | 348364   |
|    total_timesteps | 62607360 |
---------------------------------
Eval num_timesteps=62608379, episode_reward=0.03 +/- 0.98
Episode length: 29.98 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 62608379    |
| train/                  |             |
|    approx_kl            | 0.006876877 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.338       |
|    learning_rate        | 3.37e-05    |
|    loss                 | 0.106       |
|    n_updates            | 9275        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1020     |
|    time_elapsed    | 348683   |
|    total_timesteps | 62668800 |
---------------------------------
Eval num_timesteps=62669820, episode_reward=0.01 +/- 0.97
Episode length: 29.97 +/- 0.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.006        |
| time/                   |              |
|    total_timesteps      | 62669820     |
| train/                  |              |
|    approx_kl            | 0.0068291556 |
|    clip_fraction        | 0.0935       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.517       |
|    explained_variance   | 0.325        |
|    learning_rate        | 3.36e-05     |
|    loss                 | 0.0687       |
|    n_updates            | 9280         |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1021     |
|    time_elapsed    | 349002   |
|    total_timesteps | 62730240 |
---------------------------------
Eval num_timesteps=62731261, episode_reward=0.06 +/- 0.98
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 62731261    |
| train/                  |             |
|    approx_kl            | 0.006555536 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.35        |
|    learning_rate        | 3.35e-05    |
|    loss                 | 0.0759      |
|    n_updates            | 9285        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1022     |
|    time_elapsed    | 349320   |
|    total_timesteps | 62791680 |
---------------------------------
Eval num_timesteps=62792702, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.024        |
| time/                   |              |
|    total_timesteps      | 62792702     |
| train/                  |              |
|    approx_kl            | 0.0067958594 |
|    clip_fraction        | 0.0935       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.515       |
|    explained_variance   | 0.324        |
|    learning_rate        | 3.35e-05     |
|    loss                 | 0.1          |
|    n_updates            | 9290         |
|    policy_gradient_loss | -0.0141      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1023     |
|    time_elapsed    | 349638   |
|    total_timesteps | 62853120 |
---------------------------------
Eval num_timesteps=62854143, episode_reward=0.10 +/- 0.98
Episode length: 30.01 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.098        |
| time/                   |              |
|    total_timesteps      | 62854143     |
| train/                  |              |
|    approx_kl            | 0.0065030525 |
|    clip_fraction        | 0.0919       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.518       |
|    explained_variance   | 0.345        |
|    learning_rate        | 3.34e-05     |
|    loss                 | 0.0936       |
|    n_updates            | 9295         |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1024     |
|    time_elapsed    | 349956   |
|    total_timesteps | 62914560 |
---------------------------------
Eval num_timesteps=62915584, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.026        |
| time/                   |              |
|    total_timesteps      | 62915584     |
| train/                  |              |
|    approx_kl            | 0.0067971754 |
|    clip_fraction        | 0.0934       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.517       |
|    explained_variance   | 0.343        |
|    learning_rate        | 3.34e-05     |
|    loss                 | 0.112        |
|    n_updates            | 9300         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1025     |
|    time_elapsed    | 350273   |
|    total_timesteps | 62976000 |
---------------------------------
Eval num_timesteps=62977025, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.054        |
| time/                   |              |
|    total_timesteps      | 62977025     |
| train/                  |              |
|    approx_kl            | 0.0066458085 |
|    clip_fraction        | 0.0924       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.515       |
|    explained_variance   | 0.342        |
|    learning_rate        | 3.33e-05     |
|    loss                 | 0.0753       |
|    n_updates            | 9305         |
|    policy_gradient_loss | -0.0141      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1026     |
|    time_elapsed    | 350591   |
|    total_timesteps | 63037440 |
---------------------------------
Eval num_timesteps=63038466, episode_reward=-0.00 +/- 0.99
Episode length: 29.97 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.002       |
| time/                   |              |
|    total_timesteps      | 63038466     |
| train/                  |              |
|    approx_kl            | 0.0067191063 |
|    clip_fraction        | 0.0933       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.518       |
|    explained_variance   | 0.335        |
|    learning_rate        | 3.33e-05     |
|    loss                 | 0.0969       |
|    n_updates            | 9310         |
|    policy_gradient_loss | -0.0147      |
|    value_loss           | 0.242        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1027     |
|    time_elapsed    | 350911   |
|    total_timesteps | 63098880 |
---------------------------------
Eval num_timesteps=63099907, episode_reward=0.10 +/- 0.98
Episode length: 30.01 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.098        |
| time/                   |              |
|    total_timesteps      | 63099907     |
| train/                  |              |
|    approx_kl            | 0.0067836866 |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.514       |
|    explained_variance   | 0.333        |
|    learning_rate        | 3.32e-05     |
|    loss                 | 0.0939       |
|    n_updates            | 9315         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1028     |
|    time_elapsed    | 351232   |
|    total_timesteps | 63160320 |
---------------------------------
Eval num_timesteps=63161348, episode_reward=-0.03 +/- 0.99
Episode length: 30.01 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.032      |
| time/                   |             |
|    total_timesteps      | 63161348    |
| train/                  |             |
|    approx_kl            | 0.006778723 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.339       |
|    learning_rate        | 3.32e-05    |
|    loss                 | 0.098       |
|    n_updates            | 9320        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1029     |
|    time_elapsed    | 351554   |
|    total_timesteps | 63221760 |
---------------------------------
Eval num_timesteps=63222789, episode_reward=0.12 +/- 0.98
Episode length: 30.01 +/- 0.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.116        |
| time/                   |              |
|    total_timesteps      | 63222789     |
| train/                  |              |
|    approx_kl            | 0.0066772387 |
|    clip_fraction        | 0.0914       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.507       |
|    explained_variance   | 0.348        |
|    learning_rate        | 3.31e-05     |
|    loss                 | 0.0816       |
|    n_updates            | 9325         |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1030     |
|    time_elapsed    | 351878   |
|    total_timesteps | 63283200 |
---------------------------------
Eval num_timesteps=63284230, episode_reward=0.01 +/- 0.98
Episode length: 30.01 +/- 0.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.01         |
| time/                   |              |
|    total_timesteps      | 63284230     |
| train/                  |              |
|    approx_kl            | 0.0065861624 |
|    clip_fraction        | 0.0932       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.507       |
|    explained_variance   | 0.352        |
|    learning_rate        | 3.3e-05      |
|    loss                 | 0.137        |
|    n_updates            | 9330         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.236        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1031     |
|    time_elapsed    | 352200   |
|    total_timesteps | 63344640 |
---------------------------------
Eval num_timesteps=63345671, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 63345671    |
| train/                  |             |
|    approx_kl            | 0.006563525 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.344       |
|    learning_rate        | 3.3e-05     |
|    loss                 | 0.103       |
|    n_updates            | 9335        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1032     |
|    time_elapsed    | 352519   |
|    total_timesteps | 63406080 |
---------------------------------
Eval num_timesteps=63407112, episode_reward=0.12 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.124       |
| time/                   |             |
|    total_timesteps      | 63407112    |
| train/                  |             |
|    approx_kl            | 0.006688525 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.34        |
|    learning_rate        | 3.29e-05    |
|    loss                 | 0.0829      |
|    n_updates            | 9340        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1033     |
|    time_elapsed    | 352838   |
|    total_timesteps | 63467520 |
---------------------------------
Eval num_timesteps=63468553, episode_reward=0.00 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 63468553    |
| train/                  |             |
|    approx_kl            | 0.006621065 |
|    clip_fraction        | 0.0918      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.348       |
|    learning_rate        | 3.29e-05    |
|    loss                 | 0.0571      |
|    n_updates            | 9345        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1034     |
|    time_elapsed    | 353155   |
|    total_timesteps | 63528960 |
---------------------------------
Eval num_timesteps=63529994, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.062        |
| time/                   |              |
|    total_timesteps      | 63529994     |
| train/                  |              |
|    approx_kl            | 0.0067389403 |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.503       |
|    explained_variance   | 0.34         |
|    learning_rate        | 3.28e-05     |
|    loss                 | 0.0783       |
|    n_updates            | 9350         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1035     |
|    time_elapsed    | 353472   |
|    total_timesteps | 63590400 |
---------------------------------
Eval num_timesteps=63591435, episode_reward=0.06 +/- 0.97
Episode length: 29.97 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.06         |
| time/                   |              |
|    total_timesteps      | 63591435     |
| train/                  |              |
|    approx_kl            | 0.0066273706 |
|    clip_fraction        | 0.09         |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.5         |
|    explained_variance   | 0.34         |
|    learning_rate        | 3.28e-05     |
|    loss                 | 0.1          |
|    n_updates            | 9355         |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1036     |
|    time_elapsed    | 353791   |
|    total_timesteps | 63651840 |
---------------------------------
Eval num_timesteps=63652876, episode_reward=0.02 +/- 0.98
Episode length: 30.06 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.024        |
| time/                   |              |
|    total_timesteps      | 63652876     |
| train/                  |              |
|    approx_kl            | 0.0066726794 |
|    clip_fraction        | 0.0914       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.5         |
|    explained_variance   | 0.339        |
|    learning_rate        | 3.27e-05     |
|    loss                 | 0.11         |
|    n_updates            | 9360         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1037     |
|    time_elapsed    | 354109   |
|    total_timesteps | 63713280 |
---------------------------------
Eval num_timesteps=63714317, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 63714317    |
| train/                  |             |
|    approx_kl            | 0.006346735 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.497      |
|    explained_variance   | 0.348       |
|    learning_rate        | 3.27e-05    |
|    loss                 | 0.0827      |
|    n_updates            | 9365        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1038     |
|    time_elapsed    | 354427   |
|    total_timesteps | 63774720 |
---------------------------------
Eval num_timesteps=63775758, episode_reward=0.10 +/- 0.98
Episode length: 30.08 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.1         |
|    mean_reward          | 0.096        |
| time/                   |              |
|    total_timesteps      | 63775758     |
| train/                  |              |
|    approx_kl            | 0.0065902714 |
|    clip_fraction        | 0.0892       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.498       |
|    explained_variance   | 0.347        |
|    learning_rate        | 3.26e-05     |
|    loss                 | 0.108        |
|    n_updates            | 9370         |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1039     |
|    time_elapsed    | 354748   |
|    total_timesteps | 63836160 |
---------------------------------
Eval num_timesteps=63837199, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 63837199   |
| train/                  |            |
|    approx_kl            | 0.00657961 |
|    clip_fraction        | 0.0888     |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.495     |
|    explained_variance   | 0.358      |
|    learning_rate        | 3.25e-05   |
|    loss                 | 0.0738     |
|    n_updates            | 9375       |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1040     |
|    time_elapsed    | 355069   |
|    total_timesteps | 63897600 |
---------------------------------
Eval num_timesteps=63898640, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.076        |
| time/                   |              |
|    total_timesteps      | 63898640     |
| train/                  |              |
|    approx_kl            | 0.0066008633 |
|    clip_fraction        | 0.09         |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.493       |
|    explained_variance   | 0.332        |
|    learning_rate        | 3.25e-05     |
|    loss                 | 0.0825       |
|    n_updates            | 9380         |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.242        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1041     |
|    time_elapsed    | 355392   |
|    total_timesteps | 63959040 |
---------------------------------
Eval num_timesteps=63960081, episode_reward=0.11 +/- 0.98
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 63960081    |
| train/                  |             |
|    approx_kl            | 0.006349395 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.488      |
|    explained_variance   | 0.364       |
|    learning_rate        | 3.24e-05    |
|    loss                 | 0.0722      |
|    n_updates            | 9385        |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1042     |
|    time_elapsed    | 355715   |
|    total_timesteps | 64020480 |
---------------------------------
Eval num_timesteps=64021522, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 64021522    |
| train/                  |             |
|    approx_kl            | 0.006424229 |
|    clip_fraction        | 0.0879      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.359       |
|    learning_rate        | 3.24e-05    |
|    loss                 | 0.0781      |
|    n_updates            | 9390        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1043     |
|    time_elapsed    | 356036   |
|    total_timesteps | 64081920 |
---------------------------------
Eval num_timesteps=64082963, episode_reward=0.03 +/- 0.99
Episode length: 30.04 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.032        |
| time/                   |              |
|    total_timesteps      | 64082963     |
| train/                  |              |
|    approx_kl            | 0.0065675457 |
|    clip_fraction        | 0.0876       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.49        |
|    explained_variance   | 0.339        |
|    learning_rate        | 3.23e-05     |
|    loss                 | 0.109        |
|    n_updates            | 9395         |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 0.243        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 1044     |
|    time_elapsed    | 356354   |
|    total_timesteps | 64143360 |
---------------------------------
Eval num_timesteps=64144404, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.114        |
| time/                   |              |
|    total_timesteps      | 64144404     |
| train/                  |              |
|    approx_kl            | 0.0063876105 |
|    clip_fraction        | 0.0885       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.352        |
|    learning_rate        | 3.23e-05     |
|    loss                 | 0.0767       |
|    n_updates            | 9400         |
|    policy_gradient_loss | -0.0138      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1045     |
|    time_elapsed    | 356672   |
|    total_timesteps | 64204800 |
---------------------------------
Eval num_timesteps=64205845, episode_reward=0.04 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 64205845    |
| train/                  |             |
|    approx_kl            | 0.006437691 |
|    clip_fraction        | 0.0881      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.484      |
|    explained_variance   | 0.345       |
|    learning_rate        | 3.22e-05    |
|    loss                 | 0.0832      |
|    n_updates            | 9405        |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1046     |
|    time_elapsed    | 356990   |
|    total_timesteps | 64266240 |
---------------------------------
Eval num_timesteps=64267286, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.058        |
| time/                   |              |
|    total_timesteps      | 64267286     |
| train/                  |              |
|    approx_kl            | 0.0064063976 |
|    clip_fraction        | 0.0879       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.486       |
|    explained_variance   | 0.345        |
|    learning_rate        | 3.22e-05     |
|    loss                 | 0.128        |
|    n_updates            | 9410         |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1047     |
|    time_elapsed    | 357308   |
|    total_timesteps | 64327680 |
---------------------------------
Eval num_timesteps=64328727, episode_reward=0.01 +/- 0.98
Episode length: 30.04 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 64328727    |
| train/                  |             |
|    approx_kl            | 0.006458101 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.488      |
|    explained_variance   | 0.357       |
|    learning_rate        | 3.21e-05    |
|    loss                 | 0.127       |
|    n_updates            | 9415        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1048     |
|    time_elapsed    | 357626   |
|    total_timesteps | 64389120 |
---------------------------------
Eval num_timesteps=64390168, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.05         |
| time/                   |              |
|    total_timesteps      | 64390168     |
| train/                  |              |
|    approx_kl            | 0.0065288604 |
|    clip_fraction        | 0.0893       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.351        |
|    learning_rate        | 3.2e-05      |
|    loss                 | 0.124        |
|    n_updates            | 9420         |
|    policy_gradient_loss | -0.0139      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1049     |
|    time_elapsed    | 357943   |
|    total_timesteps | 64450560 |
---------------------------------
Eval num_timesteps=64451609, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 64451609    |
| train/                  |             |
|    approx_kl            | 0.006225435 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.484      |
|    explained_variance   | 0.341       |
|    learning_rate        | 3.2e-05     |
|    loss                 | 0.062       |
|    n_updates            | 9425        |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1050     |
|    time_elapsed    | 358262   |
|    total_timesteps | 64512000 |
---------------------------------
Eval num_timesteps=64513050, episode_reward=0.13 +/- 0.97
Episode length: 30.04 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.126        |
| time/                   |              |
|    total_timesteps      | 64513050     |
| train/                  |              |
|    approx_kl            | 0.0062725893 |
|    clip_fraction        | 0.0879       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.483       |
|    explained_variance   | 0.348        |
|    learning_rate        | 3.19e-05     |
|    loss                 | 0.129        |
|    n_updates            | 9430         |
|    policy_gradient_loss | -0.0141      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1051     |
|    time_elapsed    | 358583   |
|    total_timesteps | 64573440 |
---------------------------------
Eval num_timesteps=64574491, episode_reward=0.07 +/- 0.97
Episode length: 30.03 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.07         |
| time/                   |              |
|    total_timesteps      | 64574491     |
| train/                  |              |
|    approx_kl            | 0.0064695342 |
|    clip_fraction        | 0.0865       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.481       |
|    explained_variance   | 0.346        |
|    learning_rate        | 3.19e-05     |
|    loss                 | 0.0623       |
|    n_updates            | 9435         |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1052     |
|    time_elapsed    | 358905   |
|    total_timesteps | 64634880 |
---------------------------------
Eval num_timesteps=64635932, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.136        |
| time/                   |              |
|    total_timesteps      | 64635932     |
| train/                  |              |
|    approx_kl            | 0.0062549696 |
|    clip_fraction        | 0.0878       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.486       |
|    explained_variance   | 0.348        |
|    learning_rate        | 3.18e-05     |
|    loss                 | 0.0682       |
|    n_updates            | 9440         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1053     |
|    time_elapsed    | 359224   |
|    total_timesteps | 64696320 |
---------------------------------
Eval num_timesteps=64697373, episode_reward=-0.02 +/- 0.98
Episode length: 30.03 +/- 0.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.018       |
| time/                   |              |
|    total_timesteps      | 64697373     |
| train/                  |              |
|    approx_kl            | 0.0066330438 |
|    clip_fraction        | 0.0876       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.484       |
|    explained_variance   | 0.347        |
|    learning_rate        | 3.18e-05     |
|    loss                 | 0.0764       |
|    n_updates            | 9445         |
|    policy_gradient_loss | -0.0141      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1054     |
|    time_elapsed    | 359417   |
|    total_timesteps | 64757760 |
---------------------------------
Eval num_timesteps=64758814, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.066        |
| time/                   |              |
|    total_timesteps      | 64758814     |
| train/                  |              |
|    approx_kl            | 0.0064207176 |
|    clip_fraction        | 0.0891       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.486       |
|    explained_variance   | 0.355        |
|    learning_rate        | 3.17e-05     |
|    loss                 | 0.0721       |
|    n_updates            | 9450         |
|    policy_gradient_loss | -0.0146      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1055     |
|    time_elapsed    | 359611   |
|    total_timesteps | 64819200 |
---------------------------------
Eval num_timesteps=64820255, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.054        |
| time/                   |              |
|    total_timesteps      | 64820255     |
| train/                  |              |
|    approx_kl            | 0.0063785925 |
|    clip_fraction        | 0.087        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.478       |
|    explained_variance   | 0.355        |
|    learning_rate        | 3.17e-05     |
|    loss                 | 0.111        |
|    n_updates            | 9455         |
|    policy_gradient_loss | -0.0141      |
|    value_loss           | 0.235        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 1056     |
|    time_elapsed    | 359804   |
|    total_timesteps | 64880640 |
---------------------------------
Eval num_timesteps=64881696, episode_reward=0.06 +/- 0.98
Episode length: 30.01 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.058        |
| time/                   |              |
|    total_timesteps      | 64881696     |
| train/                  |              |
|    approx_kl            | 0.0060376427 |
|    clip_fraction        | 0.0865       |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.479       |
|    explained_variance   | 0.349        |
|    learning_rate        | 3.16e-05     |
|    loss                 | 0.076        |
|    n_updates            | 9460         |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 0.24         |
------------------------------------------
slurmstepd-n16: error: *** JOB 583 ON n16 CANCELLED AT 2024-06-14T03:33:57 DUE TO TIME LIMIT ***
