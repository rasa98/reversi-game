CUDA Available: True
CPU Model: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
GPU Model: NVIDIA GeForce RTX 2070 SUPER
2024-06-09 23:33:56.016716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-09 23:33:56.164374: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-06-09 23:33:57.032355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-09 23:33:57.032466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/student/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/cuda-11.8/lib64:/opt/cuda-11.8/extras/CUPTI/lib64:/opt/cuda-11.8/lib64/stubs:/opt/cuda-11.8/targets/x86_64-linux/lib
2024-06-09 23:33:57.032478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
CUDA available: True
net architecture - {'net_arch': {'pi': [64, 64, 64, 64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64]}}
params: 
NUM_TIMESTEPS -100000000
EVAL_FREQ=61441
EVAL_EPISODES=500
BEST_THRESHOLD=0.14
LOGDIR=scripts/rl/output/v3v3/
model params: 
 {'learning_rate': <function linear_schedule.<locals>.func at 0x7f09edb3ff70>, 'n_steps': 61440, 'n_epochs': 5, 'batch_size': 64}
Wrapping the env in a DummyVecEnv.
starting model: scripts/rl/output/v3/history_0020
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 1        |
|    time_elapsed    | 288      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61441, episode_reward=0.01 +/- 0.97
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 61441       |
| train/                  |             |
|    approx_kl            | 0.022792924 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.769      |
|    explained_variance   | 0.271       |
|    learning_rate        | 8.99e-05    |
|    loss                 | 0.0824      |
|    n_updates            | 4185        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 183      |
|    iterations      | 2        |
|    time_elapsed    | 668      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=122882, episode_reward=-0.03 +/- 0.98
Episode length: 29.95 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 122882      |
| train/                  |             |
|    approx_kl            | 0.021966577 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.311       |
|    learning_rate        | 8.99e-05    |
|    loss                 | 0.0683      |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 3        |
|    time_elapsed    | 1056     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184323, episode_reward=-0.02 +/- 0.98
Episode length: 29.95 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 184323     |
| train/                  |            |
|    approx_kl            | 0.02211949 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.3        |
|    learning_rate        | 8.98e-05   |
|    loss                 | 0.0885     |
|    n_updates            | 4195       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 4        |
|    time_elapsed    | 1440     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=245764, episode_reward=0.07 +/- 0.98
Episode length: 29.93 +/- 1.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 245764      |
| train/                  |             |
|    approx_kl            | 0.022393577 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.98e-05    |
|    loss                 | 0.0595      |
|    n_updates            | 4200        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 5        |
|    time_elapsed    | 1823     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307205, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 307205      |
| train/                  |             |
|    approx_kl            | 0.021617362 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.769      |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.97e-05    |
|    loss                 | 0.111       |
|    n_updates            | 4205        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 6        |
|    time_elapsed    | 2206     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=368646, episode_reward=-0.00 +/- 0.98
Episode length: 29.92 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 368646     |
| train/                  |            |
|    approx_kl            | 0.02150069 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.321      |
|    learning_rate        | 8.97e-05   |
|    loss                 | 0.0901     |
|    n_updates            | 4210       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 7        |
|    time_elapsed    | 2587     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430087, episode_reward=-0.02 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 430087      |
| train/                  |             |
|    approx_kl            | 0.022412479 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.306       |
|    learning_rate        | 8.96e-05    |
|    loss                 | 0.0683      |
|    n_updates            | 4215        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 8        |
|    time_elapsed    | 2959     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=491528, episode_reward=-0.03 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.032      |
| time/                   |             |
|    total_timesteps      | 491528      |
| train/                  |             |
|    approx_kl            | 0.021174219 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.298       |
|    learning_rate        | 8.96e-05    |
|    loss                 | 0.0714      |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 165      |
|    iterations      | 9        |
|    time_elapsed    | 3341     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=552969, episode_reward=-0.02 +/- 0.99
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 552969      |
| train/                  |             |
|    approx_kl            | 0.021263449 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.95e-05    |
|    loss                 | 0.121       |
|    n_updates            | 4225        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 10       |
|    time_elapsed    | 3724     |
|    total_timesteps | 614400   |
---------------------------------
Eval num_timesteps=614410, episode_reward=-0.06 +/- 0.98
Episode length: 29.88 +/- 1.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.056      |
| time/                   |             |
|    total_timesteps      | 614410      |
| train/                  |             |
|    approx_kl            | 0.021307152 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.94e-05    |
|    loss                 | 0.0676      |
|    n_updates            | 4230        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 11       |
|    time_elapsed    | 4105     |
|    total_timesteps | 675840   |
---------------------------------
Eval num_timesteps=675851, episode_reward=0.00 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 675851      |
| train/                  |             |
|    approx_kl            | 0.020992203 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.338       |
|    learning_rate        | 8.94e-05    |
|    loss                 | 0.065       |
|    n_updates            | 4235        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 12       |
|    time_elapsed    | 4488     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=737292, episode_reward=0.04 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 737292      |
| train/                  |             |
|    approx_kl            | 0.021580892 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.769      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.93e-05    |
|    loss                 | 0.0777      |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 13       |
|    time_elapsed    | 4871     |
|    total_timesteps | 798720   |
---------------------------------
Eval num_timesteps=798733, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 798733      |
| train/                  |             |
|    approx_kl            | 0.021657629 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.93e-05    |
|    loss                 | 0.0655      |
|    n_updates            | 4245        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 14       |
|    time_elapsed    | 5247     |
|    total_timesteps | 860160   |
---------------------------------
Eval num_timesteps=860174, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 860174      |
| train/                  |             |
|    approx_kl            | 0.021254355 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.326       |
|    learning_rate        | 8.92e-05    |
|    loss                 | 0.0648      |
|    n_updates            | 4250        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 15       |
|    time_elapsed    | 5627     |
|    total_timesteps | 921600   |
---------------------------------
Eval num_timesteps=921615, episode_reward=0.07 +/- 0.99
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 921615      |
| train/                  |             |
|    approx_kl            | 0.021293871 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.92e-05    |
|    loss                 | 0.0521      |
|    n_updates            | 4255        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 16       |
|    time_elapsed    | 6006     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=983056, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 983056      |
| train/                  |             |
|    approx_kl            | 0.021458734 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.329       |
|    learning_rate        | 8.91e-05    |
|    loss                 | 0.0692      |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 17       |
|    time_elapsed    | 6380     |
|    total_timesteps | 1044480  |
---------------------------------
Eval num_timesteps=1044497, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.088      |
| time/                   |            |
|    total_timesteps      | 1044497    |
| train/                  |            |
|    approx_kl            | 0.02162784 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.307      |
|    learning_rate        | 8.91e-05   |
|    loss                 | 0.109      |
|    n_updates            | 4265       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 18       |
|    time_elapsed    | 6755     |
|    total_timesteps | 1105920  |
---------------------------------
Eval num_timesteps=1105938, episode_reward=0.12 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 1105938     |
| train/                  |             |
|    approx_kl            | 0.021085646 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.9e-05     |
|    loss                 | 0.113       |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 19       |
|    time_elapsed    | 7136     |
|    total_timesteps | 1167360  |
---------------------------------
Eval num_timesteps=1167379, episode_reward=0.06 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 1167379     |
| train/                  |             |
|    approx_kl            | 0.021266619 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.89e-05    |
|    loss                 | 0.0816      |
|    n_updates            | 4275        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 20       |
|    time_elapsed    | 7516     |
|    total_timesteps | 1228800  |
---------------------------------
Eval num_timesteps=1228820, episode_reward=0.12 +/- 0.98
Episode length: 30.06 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 1228820     |
| train/                  |             |
|    approx_kl            | 0.021449776 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.315       |
|    learning_rate        | 8.89e-05    |
|    loss                 | 0.0788      |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 21       |
|    time_elapsed    | 7900     |
|    total_timesteps | 1290240  |
---------------------------------
Eval num_timesteps=1290261, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 1290261     |
| train/                  |             |
|    approx_kl            | 0.020866103 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.88e-05    |
|    loss                 | 0.0553      |
|    n_updates            | 4285        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 22       |
|    time_elapsed    | 8273     |
|    total_timesteps | 1351680  |
---------------------------------
Eval num_timesteps=1351702, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 1351702     |
| train/                  |             |
|    approx_kl            | 0.021269482 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.314       |
|    learning_rate        | 8.88e-05    |
|    loss                 | 0.0943      |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 23       |
|    time_elapsed    | 8657     |
|    total_timesteps | 1413120  |
---------------------------------
Eval num_timesteps=1413143, episode_reward=0.03 +/- 0.99
Episode length: 29.89 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 1413143     |
| train/                  |             |
|    approx_kl            | 0.021115707 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.337       |
|    learning_rate        | 8.87e-05    |
|    loss                 | 0.0535      |
|    n_updates            | 4295        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 24       |
|    time_elapsed    | 9041     |
|    total_timesteps | 1474560  |
---------------------------------
Eval num_timesteps=1474584, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 1474584     |
| train/                  |             |
|    approx_kl            | 0.022162955 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.87e-05    |
|    loss                 | 0.11        |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 25       |
|    time_elapsed    | 9420     |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1536025, episode_reward=0.16 +/- 0.98
Episode length: 30.06 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.156       |
| time/                   |             |
|    total_timesteps      | 1536025     |
| train/                  |             |
|    approx_kl            | 0.021092596 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.331       |
|    learning_rate        | 8.86e-05    |
|    loss                 | 0.065       |
|    n_updates            | 4305        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.156
SELFPLAY: new best model, bumping up generation to 1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 26       |
|    time_elapsed    | 9801     |
|    total_timesteps | 1597440  |
---------------------------------
Eval num_timesteps=1597466, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.062      |
| time/                   |            |
|    total_timesteps      | 1597466    |
| train/                  |            |
|    approx_kl            | 0.02127773 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.761     |
|    explained_variance   | 0.327      |
|    learning_rate        | 8.86e-05   |
|    loss                 | 0.0826     |
|    n_updates            | 4310       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 27       |
|    time_elapsed    | 10183    |
|    total_timesteps | 1658880  |
---------------------------------
Eval num_timesteps=1658907, episode_reward=-0.06 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.064      |
| time/                   |             |
|    total_timesteps      | 1658907     |
| train/                  |             |
|    approx_kl            | 0.020882858 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.761      |
|    explained_variance   | 0.294       |
|    learning_rate        | 8.85e-05    |
|    loss                 | 0.0784      |
|    n_updates            | 4315        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.246       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 28       |
|    time_elapsed    | 10556    |
|    total_timesteps | 1720320  |
---------------------------------
Eval num_timesteps=1720348, episode_reward=-0.03 +/- 0.98
Episode length: 29.94 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 1720348    |
| train/                  |            |
|    approx_kl            | 0.02107683 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.311      |
|    learning_rate        | 8.85e-05   |
|    loss                 | 0.0546     |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 29       |
|    time_elapsed    | 10938    |
|    total_timesteps | 1781760  |
---------------------------------
Eval num_timesteps=1781789, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 1781789     |
| train/                  |             |
|    approx_kl            | 0.021095494 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.337       |
|    learning_rate        | 8.84e-05    |
|    loss                 | 0.0866      |
|    n_updates            | 4325        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 30       |
|    time_elapsed    | 11320    |
|    total_timesteps | 1843200  |
---------------------------------
Eval num_timesteps=1843230, episode_reward=-0.03 +/- 0.99
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.034      |
| time/                   |             |
|    total_timesteps      | 1843230     |
| train/                  |             |
|    approx_kl            | 0.020307107 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.83e-05    |
|    loss                 | 0.0563      |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 31       |
|    time_elapsed    | 11701    |
|    total_timesteps | 1904640  |
---------------------------------
Eval num_timesteps=1904671, episode_reward=-0.03 +/- 0.99
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 1904671     |
| train/                  |             |
|    approx_kl            | 0.020866137 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.83e-05    |
|    loss                 | 0.0785      |
|    n_updates            | 4335        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 32       |
|    time_elapsed    | 12075    |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1966112, episode_reward=0.06 +/- 0.99
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 1966112     |
| train/                  |             |
|    approx_kl            | 0.020979516 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.82e-05    |
|    loss                 | 0.124       |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 33       |
|    time_elapsed    | 12456    |
|    total_timesteps | 2027520  |
---------------------------------
Eval num_timesteps=2027553, episode_reward=0.03 +/- 0.98
Episode length: 30.04 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 2027553     |
| train/                  |             |
|    approx_kl            | 0.020871881 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.306       |
|    learning_rate        | 8.82e-05    |
|    loss                 | 0.073       |
|    n_updates            | 4345        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 34       |
|    time_elapsed    | 12830    |
|    total_timesteps | 2088960  |
---------------------------------
Eval num_timesteps=2088994, episode_reward=0.12 +/- 0.97
Episode length: 30.03 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.116       |
| time/                   |             |
|    total_timesteps      | 2088994     |
| train/                  |             |
|    approx_kl            | 0.020371385 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.334       |
|    learning_rate        | 8.81e-05    |
|    loss                 | 0.096       |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 35       |
|    time_elapsed    | 13212    |
|    total_timesteps | 2150400  |
---------------------------------
Eval num_timesteps=2150435, episode_reward=0.01 +/- 0.98
Episode length: 30.01 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.008      |
| time/                   |            |
|    total_timesteps      | 2150435    |
| train/                  |            |
|    approx_kl            | 0.02079815 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.762     |
|    explained_variance   | 0.299      |
|    learning_rate        | 8.81e-05   |
|    loss                 | 0.0368     |
|    n_updates            | 4355       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 36       |
|    time_elapsed    | 13593    |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2211876, episode_reward=0.04 +/- 0.98
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.04       |
| time/                   |            |
|    total_timesteps      | 2211876    |
| train/                  |            |
|    approx_kl            | 0.02035747 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.334      |
|    learning_rate        | 8.8e-05    |
|    loss                 | 0.0661     |
|    n_updates            | 4360       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 37       |
|    time_elapsed    | 13964    |
|    total_timesteps | 2273280  |
---------------------------------
Eval num_timesteps=2273317, episode_reward=-0.02 +/- 0.99
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 2273317     |
| train/                  |             |
|    approx_kl            | 0.020630274 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.8e-05     |
|    loss                 | 0.0718      |
|    n_updates            | 4365        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 38       |
|    time_elapsed    | 14344    |
|    total_timesteps | 2334720  |
---------------------------------
Eval num_timesteps=2334758, episode_reward=0.04 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 2334758     |
| train/                  |             |
|    approx_kl            | 0.020539582 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.298       |
|    learning_rate        | 8.79e-05    |
|    loss                 | 0.0426      |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 39       |
|    time_elapsed    | 14725    |
|    total_timesteps | 2396160  |
---------------------------------
Eval num_timesteps=2396199, episode_reward=0.05 +/- 0.98
Episode length: 30.05 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 2396199    |
| train/                  |            |
|    approx_kl            | 0.02034525 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.33       |
|    learning_rate        | 8.78e-05   |
|    loss                 | 0.0624     |
|    n_updates            | 4375       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 40       |
|    time_elapsed    | 15107    |
|    total_timesteps | 2457600  |
---------------------------------
Eval num_timesteps=2457640, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 0.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.01       |
| time/                   |            |
|    total_timesteps      | 2457640    |
| train/                  |            |
|    approx_kl            | 0.02060372 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.308      |
|    learning_rate        | 8.78e-05   |
|    loss                 | 0.0482     |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 162      |
|    iterations      | 41       |
|    time_elapsed    | 15465    |
|    total_timesteps | 2519040  |
---------------------------------
Eval num_timesteps=2519081, episode_reward=0.10 +/- 0.98
Episode length: 30.06 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.1       |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 2519081    |
| train/                  |            |
|    approx_kl            | 0.02071855 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.754     |
|    explained_variance   | 0.338      |
|    learning_rate        | 8.77e-05   |
|    loss                 | 0.0777     |
|    n_updates            | 4385       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.234      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 163      |
|    iterations      | 42       |
|    time_elapsed    | 15807    |
|    total_timesteps | 2580480  |
---------------------------------
Eval num_timesteps=2580522, episode_reward=0.10 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 2580522     |
| train/                  |             |
|    approx_kl            | 0.020295104 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.751      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.77e-05    |
|    loss                 | 0.0871      |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 163      |
|    iterations      | 43       |
|    time_elapsed    | 16151    |
|    total_timesteps | 2641920  |
---------------------------------
Eval num_timesteps=2641963, episode_reward=0.09 +/- 0.98
Episode length: 29.98 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 2641963     |
| train/                  |             |
|    approx_kl            | 0.020365536 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.321       |
|    learning_rate        | 8.76e-05    |
|    loss                 | 0.0835      |
|    n_updates            | 4395        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 44       |
|    time_elapsed    | 16501    |
|    total_timesteps | 2703360  |
---------------------------------
Eval num_timesteps=2703404, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 2703404     |
| train/                  |             |
|    approx_kl            | 0.020729624 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.293       |
|    learning_rate        | 8.76e-05    |
|    loss                 | 0.101       |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 45       |
|    time_elapsed    | 16850    |
|    total_timesteps | 2764800  |
---------------------------------
Eval num_timesteps=2764845, episode_reward=0.09 +/- 0.97
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 2764845     |
| train/                  |             |
|    approx_kl            | 0.020412052 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.317       |
|    learning_rate        | 8.75e-05    |
|    loss                 | 0.0999      |
|    n_updates            | 4405        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 46       |
|    time_elapsed    | 17199    |
|    total_timesteps | 2826240  |
---------------------------------
Eval num_timesteps=2826286, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 2826286     |
| train/                  |             |
|    approx_kl            | 0.020367507 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.326       |
|    learning_rate        | 8.75e-05    |
|    loss                 | 0.0944      |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 47       |
|    time_elapsed    | 17547    |
|    total_timesteps | 2887680  |
---------------------------------
Eval num_timesteps=2887727, episode_reward=0.07 +/- 0.98
Episode length: 30.04 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 2887727     |
| train/                  |             |
|    approx_kl            | 0.020090152 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.74e-05    |
|    loss                 | 0.0665      |
|    n_updates            | 4415        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 48       |
|    time_elapsed    | 17894    |
|    total_timesteps | 2949120  |
---------------------------------
Eval num_timesteps=2949168, episode_reward=0.23 +/- 0.96
Episode length: 30.01 +/- 0.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.226      |
| time/                   |            |
|    total_timesteps      | 2949168    |
| train/                  |            |
|    approx_kl            | 0.01984762 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.334      |
|    learning_rate        | 8.73e-05   |
|    loss                 | 0.0663     |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.236      |
----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.226
SELFPLAY: new best model, bumping up generation to 2
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 49       |
|    time_elapsed    | 18232    |
|    total_timesteps | 3010560  |
---------------------------------
Eval num_timesteps=3010609, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 3010609     |
| train/                  |             |
|    approx_kl            | 0.020330714 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.746      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.73e-05    |
|    loss                 | 0.0569      |
|    n_updates            | 4425        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 50       |
|    time_elapsed    | 18580    |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3072050, episode_reward=0.02 +/- 0.99
Episode length: 29.99 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 3072050    |
| train/                  |            |
|    approx_kl            | 0.02024635 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.749     |
|    explained_variance   | 0.315      |
|    learning_rate        | 8.72e-05   |
|    loss                 | 0.0739     |
|    n_updates            | 4430       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 51       |
|    time_elapsed    | 18917    |
|    total_timesteps | 3133440  |
---------------------------------
Eval num_timesteps=3133491, episode_reward=-0.05 +/- 0.99
Episode length: 29.96 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.052      |
| time/                   |             |
|    total_timesteps      | 3133491     |
| train/                  |             |
|    approx_kl            | 0.020162333 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.306       |
|    learning_rate        | 8.72e-05    |
|    loss                 | 0.142       |
|    n_updates            | 4435        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 52       |
|    time_elapsed    | 19265    |
|    total_timesteps | 3194880  |
---------------------------------
Eval num_timesteps=3194932, episode_reward=0.04 +/- 0.98
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 3194932     |
| train/                  |             |
|    approx_kl            | 0.020505702 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.326       |
|    learning_rate        | 8.71e-05    |
|    loss                 | 0.091       |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 53       |
|    time_elapsed    | 19613    |
|    total_timesteps | 3256320  |
---------------------------------
Eval num_timesteps=3256373, episode_reward=0.01 +/- 0.98
Episode length: 30.04 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 3256373     |
| train/                  |             |
|    approx_kl            | 0.019883566 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.315       |
|    learning_rate        | 8.71e-05    |
|    loss                 | 0.0925      |
|    n_updates            | 4445        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 54       |
|    time_elapsed    | 19953    |
|    total_timesteps | 3317760  |
---------------------------------
Eval num_timesteps=3317814, episode_reward=-0.00 +/- 0.98
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 3317814     |
| train/                  |             |
|    approx_kl            | 0.019796094 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.7e-05     |
|    loss                 | 0.0479      |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 55       |
|    time_elapsed    | 20303    |
|    total_timesteps | 3379200  |
---------------------------------
Eval num_timesteps=3379255, episode_reward=0.01 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 3379255     |
| train/                  |             |
|    approx_kl            | 0.019702436 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.308       |
|    learning_rate        | 8.7e-05     |
|    loss                 | 0.118       |
|    n_updates            | 4455        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 166      |
|    iterations      | 56       |
|    time_elapsed    | 20653    |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3440696, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 3440696     |
| train/                  |             |
|    approx_kl            | 0.019617235 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.69e-05    |
|    loss                 | 0.0787      |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 57       |
|    time_elapsed    | 20997    |
|    total_timesteps | 3502080  |
---------------------------------
Eval num_timesteps=3502137, episode_reward=-0.04 +/- 0.98
Episode length: 29.97 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.036     |
| time/                   |            |
|    total_timesteps      | 3502137    |
| train/                  |            |
|    approx_kl            | 0.01998671 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.747     |
|    explained_variance   | 0.321      |
|    learning_rate        | 8.68e-05   |
|    loss                 | 0.106      |
|    n_updates            | 4465       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 58       |
|    time_elapsed    | 21341    |
|    total_timesteps | 3563520  |
---------------------------------
Eval num_timesteps=3563578, episode_reward=-0.04 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.038      |
| time/                   |             |
|    total_timesteps      | 3563578     |
| train/                  |             |
|    approx_kl            | 0.020668339 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.68e-05    |
|    loss                 | 0.0555      |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 59       |
|    time_elapsed    | 21682    |
|    total_timesteps | 3624960  |
---------------------------------
Eval num_timesteps=3625019, episode_reward=0.16 +/- 0.97
Episode length: 30.00 +/- 0.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.164     |
| time/                   |           |
|    total_timesteps      | 3625019   |
| train/                  |           |
|    approx_kl            | 0.0198542 |
|    clip_fraction        | 0.214     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.747    |
|    explained_variance   | 0.292     |
|    learning_rate        | 8.67e-05  |
|    loss                 | 0.104     |
|    n_updates            | 4475      |
|    policy_gradient_loss | -0.0172   |
|    value_loss           | 0.245     |
---------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.164
SELFPLAY: new best model, bumping up generation to 3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 60       |
|    time_elapsed    | 22026    |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3686460, episode_reward=-0.04 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 3686460     |
| train/                  |             |
|    approx_kl            | 0.020452352 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.305       |
|    learning_rate        | 8.67e-05    |
|    loss                 | 0.105       |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 61       |
|    time_elapsed    | 22367    |
|    total_timesteps | 3747840  |
---------------------------------
Eval num_timesteps=3747901, episode_reward=-0.04 +/- 0.99
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 3747901     |
| train/                  |             |
|    approx_kl            | 0.020168344 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.66e-05    |
|    loss                 | 0.0503      |
|    n_updates            | 4485        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 167      |
|    iterations      | 62       |
|    time_elapsed    | 22711    |
|    total_timesteps | 3809280  |
---------------------------------
Eval num_timesteps=3809342, episode_reward=-0.03 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 3809342     |
| train/                  |             |
|    approx_kl            | 0.020055562 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.753      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.66e-05    |
|    loss                 | 0.0832      |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 63       |
|    time_elapsed    | 23052    |
|    total_timesteps | 3870720  |
---------------------------------
Eval num_timesteps=3870783, episode_reward=0.07 +/- 0.97
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 3870783     |
| train/                  |             |
|    approx_kl            | 0.020008603 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.336       |
|    learning_rate        | 8.65e-05    |
|    loss                 | 0.0915      |
|    n_updates            | 4495        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 64       |
|    time_elapsed    | 23396    |
|    total_timesteps | 3932160  |
---------------------------------
Eval num_timesteps=3932224, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 3932224     |
| train/                  |             |
|    approx_kl            | 0.019599298 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.65e-05    |
|    loss                 | 0.0532      |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 65       |
|    time_elapsed    | 23741    |
|    total_timesteps | 3993600  |
---------------------------------
Eval num_timesteps=3993665, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 3993665     |
| train/                  |             |
|    approx_kl            | 0.019597221 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.64e-05    |
|    loss                 | 0.0528      |
|    n_updates            | 4505        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.17    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 66       |
|    time_elapsed    | 24093    |
|    total_timesteps | 4055040  |
---------------------------------
Eval num_timesteps=4055106, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 4055106     |
| train/                  |             |
|    approx_kl            | 0.019592626 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.64e-05    |
|    loss                 | 0.0585      |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 67       |
|    time_elapsed    | 24438    |
|    total_timesteps | 4116480  |
---------------------------------
Eval num_timesteps=4116547, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 4116547     |
| train/                  |             |
|    approx_kl            | 0.019324092 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.29        |
|    learning_rate        | 8.63e-05    |
|    loss                 | 0.0908      |
|    n_updates            | 4515        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 68       |
|    time_elapsed    | 24784    |
|    total_timesteps | 4177920  |
---------------------------------
Eval num_timesteps=4177988, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 4177988     |
| train/                  |             |
|    approx_kl            | 0.019241739 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.62e-05    |
|    loss                 | 0.102       |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 168      |
|    iterations      | 69       |
|    time_elapsed    | 25126    |
|    total_timesteps | 4239360  |
---------------------------------
Eval num_timesteps=4239429, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 4239429    |
| train/                  |            |
|    approx_kl            | 0.01922259 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.748     |
|    explained_variance   | 0.312      |
|    learning_rate        | 8.62e-05   |
|    loss                 | 0.0544     |
|    n_updates            | 4525       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 70       |
|    time_elapsed    | 25470    |
|    total_timesteps | 4300800  |
---------------------------------
Eval num_timesteps=4300870, episode_reward=-0.03 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 4300870     |
| train/                  |             |
|    approx_kl            | 0.018953362 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.746      |
|    explained_variance   | 0.317       |
|    learning_rate        | 8.61e-05    |
|    loss                 | 0.104       |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 71       |
|    time_elapsed    | 25811    |
|    total_timesteps | 4362240  |
---------------------------------
Eval num_timesteps=4362311, episode_reward=-0.07 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.074      |
| time/                   |             |
|    total_timesteps      | 4362311     |
| train/                  |             |
|    approx_kl            | 0.019844143 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.321       |
|    learning_rate        | 8.61e-05    |
|    loss                 | 0.0637      |
|    n_updates            | 4535        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 72       |
|    time_elapsed    | 26155    |
|    total_timesteps | 4423680  |
---------------------------------
Eval num_timesteps=4423752, episode_reward=0.06 +/- 0.97
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 4423752     |
| train/                  |             |
|    approx_kl            | 0.019322308 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.314       |
|    learning_rate        | 8.6e-05     |
|    loss                 | 0.059       |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 73       |
|    time_elapsed    | 26497    |
|    total_timesteps | 4485120  |
---------------------------------
Eval num_timesteps=4485193, episode_reward=0.01 +/- 0.98
Episode length: 29.94 +/- 0.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 4485193     |
| train/                  |             |
|    approx_kl            | 0.019290026 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.6e-05     |
|    loss                 | 0.0708      |
|    n_updates            | 4545        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 74       |
|    time_elapsed    | 26828    |
|    total_timesteps | 4546560  |
---------------------------------
Eval num_timesteps=4546634, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 4546634     |
| train/                  |             |
|    approx_kl            | 0.019619914 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.59e-05    |
|    loss                 | 0.118       |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 75       |
|    time_elapsed    | 27146    |
|    total_timesteps | 4608000  |
---------------------------------
Eval num_timesteps=4608075, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 4608075     |
| train/                  |             |
|    approx_kl            | 0.019235427 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.59e-05    |
|    loss                 | 0.0683      |
|    n_updates            | 4555        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 76       |
|    time_elapsed    | 27466    |
|    total_timesteps | 4669440  |
---------------------------------
Eval num_timesteps=4669516, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 4669516     |
| train/                  |             |
|    approx_kl            | 0.019372748 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.58e-05    |
|    loss                 | 0.064       |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 77       |
|    time_elapsed    | 27787    |
|    total_timesteps | 4730880  |
---------------------------------
Eval num_timesteps=4730957, episode_reward=0.19 +/- 0.97
Episode length: 30.06 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.194       |
| time/                   |             |
|    total_timesteps      | 4730957     |
| train/                  |             |
|    approx_kl            | 0.019504452 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.57e-05    |
|    loss                 | 0.0755      |
|    n_updates            | 4565        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.194
SELFPLAY: new best model, bumping up generation to 4
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 78       |
|    time_elapsed    | 28109    |
|    total_timesteps | 4792320  |
---------------------------------
Eval num_timesteps=4792398, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.086       |
| time/                   |             |
|    total_timesteps      | 4792398     |
| train/                  |             |
|    approx_kl            | 0.019629804 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.57e-05    |
|    loss                 | 0.0375      |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 79       |
|    time_elapsed    | 28431    |
|    total_timesteps | 4853760  |
---------------------------------
Eval num_timesteps=4853839, episode_reward=-0.02 +/- 0.99
Episode length: 29.99 +/- 0.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.024     |
| time/                   |            |
|    total_timesteps      | 4853839    |
| train/                  |            |
|    approx_kl            | 0.01936958 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.745     |
|    explained_variance   | 0.315      |
|    learning_rate        | 8.56e-05   |
|    loss                 | 0.0783     |
|    n_updates            | 4575       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 80       |
|    time_elapsed    | 28751    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4915280, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 4915280     |
| train/                  |             |
|    approx_kl            | 0.019639881 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.33        |
|    learning_rate        | 8.56e-05    |
|    loss                 | 0.0633      |
|    n_updates            | 4580        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 81       |
|    time_elapsed    | 29068    |
|    total_timesteps | 4976640  |
---------------------------------
Eval num_timesteps=4976721, episode_reward=0.04 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 4976721     |
| train/                  |             |
|    approx_kl            | 0.019401932 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.746      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.55e-05    |
|    loss                 | 0.0704      |
|    n_updates            | 4585        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 82       |
|    time_elapsed    | 29386    |
|    total_timesteps | 5038080  |
---------------------------------
Eval num_timesteps=5038162, episode_reward=0.01 +/- 0.99
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 5038162     |
| train/                  |             |
|    approx_kl            | 0.019606046 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.745      |
|    explained_variance   | 0.314       |
|    learning_rate        | 8.55e-05    |
|    loss                 | 0.103       |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 83       |
|    time_elapsed    | 29703    |
|    total_timesteps | 5099520  |
---------------------------------
Eval num_timesteps=5099603, episode_reward=0.01 +/- 0.98
Episode length: 30.04 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 5099603     |
| train/                  |             |
|    approx_kl            | 0.019176692 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.342       |
|    learning_rate        | 8.54e-05    |
|    loss                 | 0.0835      |
|    n_updates            | 4595        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 84       |
|    time_elapsed    | 30021    |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5161044, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 5161044    |
| train/                  |            |
|    approx_kl            | 0.01919082 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.306      |
|    learning_rate        | 8.54e-05   |
|    loss                 | 0.0723     |
|    n_updates            | 4600       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 85       |
|    time_elapsed    | 30338    |
|    total_timesteps | 5222400  |
---------------------------------
Eval num_timesteps=5222485, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 5222485     |
| train/                  |             |
|    approx_kl            | 0.018697377 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.296       |
|    learning_rate        | 8.53e-05    |
|    loss                 | 0.077       |
|    n_updates            | 4605        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 86       |
|    time_elapsed    | 30655    |
|    total_timesteps | 5283840  |
---------------------------------
Eval num_timesteps=5283926, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.022      |
| time/                   |            |
|    total_timesteps      | 5283926    |
| train/                  |            |
|    approx_kl            | 0.01973129 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.325      |
|    learning_rate        | 8.52e-05   |
|    loss                 | 0.0454     |
|    n_updates            | 4610       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.233      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 87       |
|    time_elapsed    | 30974    |
|    total_timesteps | 5345280  |
---------------------------------
Eval num_timesteps=5345367, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 5345367     |
| train/                  |             |
|    approx_kl            | 0.019317053 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.52e-05    |
|    loss                 | 0.105       |
|    n_updates            | 4615        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 88       |
|    time_elapsed    | 31294    |
|    total_timesteps | 5406720  |
---------------------------------
Eval num_timesteps=5406808, episode_reward=0.02 +/- 0.98
Episode length: 29.88 +/- 1.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 5406808     |
| train/                  |             |
|    approx_kl            | 0.019018346 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.51e-05    |
|    loss                 | 0.0603      |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 89       |
|    time_elapsed    | 31615    |
|    total_timesteps | 5468160  |
---------------------------------
Eval num_timesteps=5468249, episode_reward=0.01 +/- 0.99
Episode length: 29.98 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.01      |
| time/                   |           |
|    total_timesteps      | 5468249   |
| train/                  |           |
|    approx_kl            | 0.0195026 |
|    clip_fraction        | 0.207     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.74     |
|    explained_variance   | 0.31      |
|    learning_rate        | 8.51e-05  |
|    loss                 | 0.111     |
|    n_updates            | 4625      |
|    policy_gradient_loss | -0.0175   |
|    value_loss           | 0.237     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 90       |
|    time_elapsed    | 31937    |
|    total_timesteps | 5529600  |
---------------------------------
Eval num_timesteps=5529690, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 5529690     |
| train/                  |             |
|    approx_kl            | 0.018892242 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.742      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.112       |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 91       |
|    time_elapsed    | 32258    |
|    total_timesteps | 5591040  |
---------------------------------
Eval num_timesteps=5591131, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 5591131    |
| train/                  |            |
|    approx_kl            | 0.01923114 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.326      |
|    learning_rate        | 8.5e-05    |
|    loss                 | 0.0673     |
|    n_updates            | 4635       |
|    policy_gradient_loss | -0.018     |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 92       |
|    time_elapsed    | 32576    |
|    total_timesteps | 5652480  |
---------------------------------
Eval num_timesteps=5652572, episode_reward=0.07 +/- 0.98
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 5652572     |
| train/                  |             |
|    approx_kl            | 0.018542886 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.49e-05    |
|    loss                 | 0.0706      |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 93       |
|    time_elapsed    | 32893    |
|    total_timesteps | 5713920  |
---------------------------------
Eval num_timesteps=5714013, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.008     |
| time/                   |            |
|    total_timesteps      | 5714013    |
| train/                  |            |
|    approx_kl            | 0.01917092 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.314      |
|    learning_rate        | 8.49e-05   |
|    loss                 | 0.0835     |
|    n_updates            | 4645       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 94       |
|    time_elapsed    | 33210    |
|    total_timesteps | 5775360  |
---------------------------------
Eval num_timesteps=5775454, episode_reward=-0.01 +/- 0.98
Episode length: 29.93 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 5775454     |
| train/                  |             |
|    approx_kl            | 0.018662462 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.48e-05    |
|    loss                 | 0.067       |
|    n_updates            | 4650        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 95       |
|    time_elapsed    | 33527    |
|    total_timesteps | 5836800  |
---------------------------------
Eval num_timesteps=5836895, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 5836895     |
| train/                  |             |
|    approx_kl            | 0.018683437 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.732      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.47e-05    |
|    loss                 | 0.0824      |
|    n_updates            | 4655        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 96       |
|    time_elapsed    | 33844    |
|    total_timesteps | 5898240  |
---------------------------------
Eval num_timesteps=5898336, episode_reward=-0.09 +/- 0.97
Episode length: 29.89 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.088      |
| time/                   |             |
|    total_timesteps      | 5898336     |
| train/                  |             |
|    approx_kl            | 0.018733233 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.317       |
|    learning_rate        | 8.47e-05    |
|    loss                 | 0.0799      |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 97       |
|    time_elapsed    | 34161    |
|    total_timesteps | 5959680  |
---------------------------------
Eval num_timesteps=5959777, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 5959777     |
| train/                  |             |
|    approx_kl            | 0.018951198 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.736      |
|    explained_variance   | 0.308       |
|    learning_rate        | 8.46e-05    |
|    loss                 | 0.0496      |
|    n_updates            | 4665        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 98       |
|    time_elapsed    | 34478    |
|    total_timesteps | 6021120  |
---------------------------------
Eval num_timesteps=6021218, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 6021218     |
| train/                  |             |
|    approx_kl            | 0.018309517 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.298       |
|    learning_rate        | 8.46e-05    |
|    loss                 | 0.105       |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 99       |
|    time_elapsed    | 34797    |
|    total_timesteps | 6082560  |
---------------------------------
Eval num_timesteps=6082659, episode_reward=0.06 +/- 0.98
Episode length: 30.05 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 6082659     |
| train/                  |             |
|    approx_kl            | 0.018623741 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.731      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.45e-05    |
|    loss                 | 0.0756      |
|    n_updates            | 4675        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 100      |
|    time_elapsed    | 35117    |
|    total_timesteps | 6144000  |
---------------------------------
Eval num_timesteps=6144100, episode_reward=0.04 +/- 0.98
Episode length: 29.95 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 6144100     |
| train/                  |             |
|    approx_kl            | 0.019260736 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.731      |
|    explained_variance   | 0.324       |
|    learning_rate        | 8.45e-05    |
|    loss                 | 0.0838      |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 101      |
|    time_elapsed    | 35439    |
|    total_timesteps | 6205440  |
---------------------------------
Eval num_timesteps=6205541, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 6205541     |
| train/                  |             |
|    approx_kl            | 0.019017534 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.0628      |
|    n_updates            | 4685        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 102      |
|    time_elapsed    | 35760    |
|    total_timesteps | 6266880  |
---------------------------------
Eval num_timesteps=6266982, episode_reward=-0.05 +/- 0.98
Episode length: 29.97 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.046     |
| time/                   |            |
|    total_timesteps      | 6266982    |
| train/                  |            |
|    approx_kl            | 0.01847287 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.31       |
|    learning_rate        | 8.44e-05   |
|    loss                 | 0.0755     |
|    n_updates            | 4690       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 103      |
|    time_elapsed    | 36080    |
|    total_timesteps | 6328320  |
---------------------------------
Eval num_timesteps=6328423, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 6328423     |
| train/                  |             |
|    approx_kl            | 0.018578133 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.43e-05    |
|    loss                 | 0.0838      |
|    n_updates            | 4695        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 104      |
|    time_elapsed    | 36397    |
|    total_timesteps | 6389760  |
---------------------------------
Eval num_timesteps=6389864, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 6389864     |
| train/                  |             |
|    approx_kl            | 0.018597163 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.42e-05    |
|    loss                 | 0.0717      |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 105      |
|    time_elapsed    | 36714    |
|    total_timesteps | 6451200  |
---------------------------------
Eval num_timesteps=6451305, episode_reward=-0.01 +/- 0.98
Episode length: 29.95 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 6451305     |
| train/                  |             |
|    approx_kl            | 0.018056631 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.324       |
|    learning_rate        | 8.42e-05    |
|    loss                 | 0.0661      |
|    n_updates            | 4705        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 106      |
|    time_elapsed    | 37031    |
|    total_timesteps | 6512640  |
---------------------------------
Eval num_timesteps=6512746, episode_reward=0.11 +/- 0.98
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 6512746     |
| train/                  |             |
|    approx_kl            | 0.018626135 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.41e-05    |
|    loss                 | 0.0606      |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 107      |
|    time_elapsed    | 37349    |
|    total_timesteps | 6574080  |
---------------------------------
Eval num_timesteps=6574187, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 6574187     |
| train/                  |             |
|    approx_kl            | 0.017904732 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.41e-05    |
|    loss                 | 0.0659      |
|    n_updates            | 4715        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 108      |
|    time_elapsed    | 37665    |
|    total_timesteps | 6635520  |
---------------------------------
Eval num_timesteps=6635628, episode_reward=0.01 +/- 0.98
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.012       |
| time/                   |             |
|    total_timesteps      | 6635628     |
| train/                  |             |
|    approx_kl            | 0.018181821 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.4e-05     |
|    loss                 | 0.0396      |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 109      |
|    time_elapsed    | 37983    |
|    total_timesteps | 6696960  |
---------------------------------
Eval num_timesteps=6697069, episode_reward=-0.02 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 6697069     |
| train/                  |             |
|    approx_kl            | 0.018397411 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.4e-05     |
|    loss                 | 0.081       |
|    n_updates            | 4725        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 110      |
|    time_elapsed    | 38301    |
|    total_timesteps | 6758400  |
---------------------------------
Eval num_timesteps=6758510, episode_reward=0.06 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 6758510     |
| train/                  |             |
|    approx_kl            | 0.018284826 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.39e-05    |
|    loss                 | 0.122       |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 111      |
|    time_elapsed    | 38621    |
|    total_timesteps | 6819840  |
---------------------------------
Eval num_timesteps=6819951, episode_reward=0.02 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 6819951     |
| train/                  |             |
|    approx_kl            | 0.018171556 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.331       |
|    learning_rate        | 8.39e-05    |
|    loss                 | 0.0627      |
|    n_updates            | 4735        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 112      |
|    time_elapsed    | 38941    |
|    total_timesteps | 6881280  |
---------------------------------
Eval num_timesteps=6881392, episode_reward=0.03 +/- 0.98
Episode length: 30.05 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 6881392     |
| train/                  |             |
|    approx_kl            | 0.018549003 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.38e-05    |
|    loss                 | 0.0947      |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 176      |
|    iterations      | 113      |
|    time_elapsed    | 39263    |
|    total_timesteps | 6942720  |
---------------------------------
Eval num_timesteps=6942833, episode_reward=0.09 +/- 0.97
Episode length: 30.05 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 6942833     |
| train/                  |             |
|    approx_kl            | 0.018627478 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.303       |
|    learning_rate        | 8.38e-05    |
|    loss                 | 0.0876      |
|    n_updates            | 4745        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 114      |
|    time_elapsed    | 39585    |
|    total_timesteps | 7004160  |
---------------------------------
Eval num_timesteps=7004274, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.06       |
| time/                   |            |
|    total_timesteps      | 7004274    |
| train/                  |            |
|    approx_kl            | 0.01830752 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.705     |
|    explained_variance   | 0.314      |
|    learning_rate        | 8.37e-05   |
|    loss                 | 0.0564     |
|    n_updates            | 4750       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 115      |
|    time_elapsed    | 39904    |
|    total_timesteps | 7065600  |
---------------------------------
Eval num_timesteps=7065715, episode_reward=0.08 +/- 0.98
Episode length: 29.94 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 7065715     |
| train/                  |             |
|    approx_kl            | 0.018104954 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.329       |
|    learning_rate        | 8.36e-05    |
|    loss                 | 0.0593      |
|    n_updates            | 4755        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.231       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 116      |
|    time_elapsed    | 40222    |
|    total_timesteps | 7127040  |
---------------------------------
Eval num_timesteps=7127156, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.078      |
| time/                   |            |
|    total_timesteps      | 7127156    |
| train/                  |            |
|    approx_kl            | 0.01782045 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.324      |
|    learning_rate        | 8.36e-05   |
|    loss                 | 0.0681     |
|    n_updates            | 4760       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 117      |
|    time_elapsed    | 40539    |
|    total_timesteps | 7188480  |
---------------------------------
Eval num_timesteps=7188597, episode_reward=0.21 +/- 0.96
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.21        |
| time/                   |             |
|    total_timesteps      | 7188597     |
| train/                  |             |
|    approx_kl            | 0.018219313 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.327       |
|    learning_rate        | 8.35e-05    |
|    loss                 | 0.0804      |
|    n_updates            | 4765        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.21
SELFPLAY: new best model, bumping up generation to 5
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 118      |
|    time_elapsed    | 40857    |
|    total_timesteps | 7249920  |
---------------------------------
Eval num_timesteps=7250038, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 7250038     |
| train/                  |             |
|    approx_kl            | 0.018564073 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.35e-05    |
|    loss                 | 0.0924      |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 119      |
|    time_elapsed    | 41175    |
|    total_timesteps | 7311360  |
---------------------------------
Eval num_timesteps=7311479, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 7311479     |
| train/                  |             |
|    approx_kl            | 0.017995143 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.335       |
|    learning_rate        | 8.34e-05    |
|    loss                 | 0.0473      |
|    n_updates            | 4775        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 120      |
|    time_elapsed    | 41492    |
|    total_timesteps | 7372800  |
---------------------------------
Eval num_timesteps=7372920, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 7372920     |
| train/                  |             |
|    approx_kl            | 0.017546065 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.329       |
|    learning_rate        | 8.34e-05    |
|    loss                 | 0.0762      |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 121      |
|    time_elapsed    | 41809    |
|    total_timesteps | 7434240  |
---------------------------------
Eval num_timesteps=7434361, episode_reward=-0.02 +/- 0.98
Episode length: 29.96 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 7434361    |
| train/                  |            |
|    approx_kl            | 0.01779716 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.709     |
|    explained_variance   | 0.326      |
|    learning_rate        | 8.33e-05   |
|    loss                 | 0.0923     |
|    n_updates            | 4785       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 122      |
|    time_elapsed    | 42128    |
|    total_timesteps | 7495680  |
---------------------------------
Eval num_timesteps=7495802, episode_reward=0.00 +/- 0.99
Episode length: 29.95 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.002      |
| time/                   |            |
|    total_timesteps      | 7495802    |
| train/                  |            |
|    approx_kl            | 0.01793491 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.32       |
|    learning_rate        | 8.33e-05   |
|    loss                 | 0.0442     |
|    n_updates            | 4790       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 123      |
|    time_elapsed    | 42449    |
|    total_timesteps | 7557120  |
---------------------------------
Eval num_timesteps=7557243, episode_reward=-0.02 +/- 0.99
Episode length: 29.95 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 7557243     |
| train/                  |             |
|    approx_kl            | 0.017894156 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.303       |
|    learning_rate        | 8.32e-05    |
|    loss                 | 0.0715      |
|    n_updates            | 4795        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 124      |
|    time_elapsed    | 42770    |
|    total_timesteps | 7618560  |
---------------------------------
Eval num_timesteps=7618684, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 7618684     |
| train/                  |             |
|    approx_kl            | 0.017480906 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.319       |
|    learning_rate        | 8.31e-05    |
|    loss                 | 0.0695      |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 125      |
|    time_elapsed    | 43094    |
|    total_timesteps | 7680000  |
---------------------------------
Eval num_timesteps=7680125, episode_reward=0.05 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 7680125     |
| train/                  |             |
|    approx_kl            | 0.017738339 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.709      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.31e-05    |
|    loss                 | 0.0983      |
|    n_updates            | 4805        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 126      |
|    time_elapsed    | 43414    |
|    total_timesteps | 7741440  |
---------------------------------
Eval num_timesteps=7741566, episode_reward=0.01 +/- 0.98
Episode length: 30.03 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 7741566     |
| train/                  |             |
|    approx_kl            | 0.018013712 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.318       |
|    learning_rate        | 8.3e-05     |
|    loss                 | 0.061       |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 127      |
|    time_elapsed    | 43732    |
|    total_timesteps | 7802880  |
---------------------------------
Eval num_timesteps=7803007, episode_reward=0.03 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 7803007     |
| train/                  |             |
|    approx_kl            | 0.018030994 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.303       |
|    learning_rate        | 8.3e-05     |
|    loss                 | 0.0809      |
|    n_updates            | 4815        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 128      |
|    time_elapsed    | 44050    |
|    total_timesteps | 7864320  |
---------------------------------
Eval num_timesteps=7864448, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 7864448     |
| train/                  |             |
|    approx_kl            | 0.018215578 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.29e-05    |
|    loss                 | 0.0769      |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 129      |
|    time_elapsed    | 44366    |
|    total_timesteps | 7925760  |
---------------------------------
Eval num_timesteps=7925889, episode_reward=-0.01 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 7925889     |
| train/                  |             |
|    approx_kl            | 0.017565824 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.29e-05    |
|    loss                 | 0.0817      |
|    n_updates            | 4825        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 130      |
|    time_elapsed    | 44684    |
|    total_timesteps | 7987200  |
---------------------------------
Eval num_timesteps=7987330, episode_reward=0.08 +/- 0.98
Episode length: 29.99 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.076      |
| time/                   |            |
|    total_timesteps      | 7987330    |
| train/                  |            |
|    approx_kl            | 0.01748898 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.308      |
|    learning_rate        | 8.28e-05   |
|    loss                 | 0.0822     |
|    n_updates            | 4830       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 131      |
|    time_elapsed    | 45001    |
|    total_timesteps | 8048640  |
---------------------------------
Eval num_timesteps=8048771, episode_reward=0.08 +/- 0.98
Episode length: 30.05 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.082      |
| time/                   |            |
|    total_timesteps      | 8048771    |
| train/                  |            |
|    approx_kl            | 0.01795539 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.321      |
|    learning_rate        | 8.28e-05   |
|    loss                 | 0.104      |
|    n_updates            | 4835       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 132      |
|    time_elapsed    | 45319    |
|    total_timesteps | 8110080  |
---------------------------------
Eval num_timesteps=8110212, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 8110212     |
| train/                  |             |
|    approx_kl            | 0.017706493 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.27e-05    |
|    loss                 | 0.0967      |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 133      |
|    time_elapsed    | 45636    |
|    total_timesteps | 8171520  |
---------------------------------
Eval num_timesteps=8171653, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 8171653     |
| train/                  |             |
|    approx_kl            | 0.017217766 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.31        |
|    learning_rate        | 8.26e-05    |
|    loss                 | 0.0883      |
|    n_updates            | 4845        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 134      |
|    time_elapsed    | 45956    |
|    total_timesteps | 8232960  |
---------------------------------
Eval num_timesteps=8233094, episode_reward=0.10 +/- 0.98
Episode length: 30.07 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 8233094     |
| train/                  |             |
|    approx_kl            | 0.017139263 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.26e-05    |
|    loss                 | 0.0949      |
|    n_updates            | 4850        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 135      |
|    time_elapsed    | 46276    |
|    total_timesteps | 8294400  |
---------------------------------
Eval num_timesteps=8294535, episode_reward=0.04 +/- 0.98
Episode length: 29.92 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 8294535     |
| train/                  |             |
|    approx_kl            | 0.018485116 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8.25e-05    |
|    loss                 | 0.048       |
|    n_updates            | 4855        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 136      |
|    time_elapsed    | 46597    |
|    total_timesteps | 8355840  |
---------------------------------
Eval num_timesteps=8355976, episode_reward=0.02 +/- 0.99
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 8355976     |
| train/                  |             |
|    approx_kl            | 0.017295027 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.25e-05    |
|    loss                 | 0.0831      |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 137      |
|    time_elapsed    | 46919    |
|    total_timesteps | 8417280  |
---------------------------------
Eval num_timesteps=8417417, episode_reward=0.07 +/- 0.99
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 8417417     |
| train/                  |             |
|    approx_kl            | 0.017685981 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.315       |
|    learning_rate        | 8.24e-05    |
|    loss                 | 0.128       |
|    n_updates            | 4865        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 138      |
|    time_elapsed    | 47239    |
|    total_timesteps | 8478720  |
---------------------------------
Eval num_timesteps=8478858, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 8478858     |
| train/                  |             |
|    approx_kl            | 0.017661806 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.24e-05    |
|    loss                 | 0.0549      |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 139      |
|    time_elapsed    | 47557    |
|    total_timesteps | 8540160  |
---------------------------------
Eval num_timesteps=8540299, episode_reward=0.02 +/- 0.99
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 8540299     |
| train/                  |             |
|    approx_kl            | 0.017523713 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.324       |
|    learning_rate        | 8.23e-05    |
|    loss                 | 0.0531      |
|    n_updates            | 4875        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 140      |
|    time_elapsed    | 47874    |
|    total_timesteps | 8601600  |
---------------------------------
Eval num_timesteps=8601740, episode_reward=-0.01 +/- 0.99
Episode length: 29.99 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 8601740     |
| train/                  |             |
|    approx_kl            | 0.017866341 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.301       |
|    learning_rate        | 8.23e-05    |
|    loss                 | 0.0523      |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 141      |
|    time_elapsed    | 48191    |
|    total_timesteps | 8663040  |
---------------------------------
Eval num_timesteps=8663181, episode_reward=0.12 +/- 0.97
Episode length: 30.02 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 8663181     |
| train/                  |             |
|    approx_kl            | 0.017023286 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.703      |
|    explained_variance   | 0.329       |
|    learning_rate        | 8.22e-05    |
|    loss                 | 0.0953      |
|    n_updates            | 4885        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 142      |
|    time_elapsed    | 48508    |
|    total_timesteps | 8724480  |
---------------------------------
Eval num_timesteps=8724622, episode_reward=0.12 +/- 0.97
Episode length: 30.04 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 8724622     |
| train/                  |             |
|    approx_kl            | 0.017582063 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.21e-05    |
|    loss                 | 0.0812      |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 143      |
|    time_elapsed    | 48825    |
|    total_timesteps | 8785920  |
---------------------------------
Eval num_timesteps=8786063, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.142       |
| time/                   |             |
|    total_timesteps      | 8786063     |
| train/                  |             |
|    approx_kl            | 0.016488828 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.339       |
|    learning_rate        | 8.21e-05    |
|    loss                 | 0.072       |
|    n_updates            | 4895        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.234       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.142
SELFPLAY: new best model, bumping up generation to 6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 144      |
|    time_elapsed    | 49142    |
|    total_timesteps | 8847360  |
---------------------------------
Eval num_timesteps=8847504, episode_reward=-0.03 +/- 0.98
Episode length: 30.01 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 8847504     |
| train/                  |             |
|    approx_kl            | 0.017101271 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.334       |
|    learning_rate        | 8.2e-05     |
|    loss                 | 0.104       |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 145      |
|    time_elapsed    | 49461    |
|    total_timesteps | 8908800  |
---------------------------------
Eval num_timesteps=8908945, episode_reward=-0.06 +/- 0.98
Episode length: 29.93 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.064      |
| time/                   |             |
|    total_timesteps      | 8908945     |
| train/                  |             |
|    approx_kl            | 0.017491838 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.332       |
|    learning_rate        | 8.2e-05     |
|    loss                 | 0.0631      |
|    n_updates            | 4905        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.19    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 146      |
|    time_elapsed    | 49781    |
|    total_timesteps | 8970240  |
---------------------------------
Eval num_timesteps=8970386, episode_reward=-0.01 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 8970386     |
| train/                  |             |
|    approx_kl            | 0.018288044 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.19e-05    |
|    loss                 | 0.124       |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 147      |
|    time_elapsed    | 50102    |
|    total_timesteps | 9031680  |
---------------------------------
Eval num_timesteps=9031827, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 9031827     |
| train/                  |             |
|    approx_kl            | 0.017775487 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.19e-05    |
|    loss                 | 0.0654      |
|    n_updates            | 4915        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 148      |
|    time_elapsed    | 50424    |
|    total_timesteps | 9093120  |
---------------------------------
Eval num_timesteps=9093268, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.016      |
| time/                   |            |
|    total_timesteps      | 9093268    |
| train/                  |            |
|    approx_kl            | 0.01771807 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.314      |
|    learning_rate        | 8.18e-05   |
|    loss                 | 0.0892     |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 180      |
|    iterations      | 149      |
|    time_elapsed    | 50745    |
|    total_timesteps | 9154560  |
---------------------------------
Eval num_timesteps=9154709, episode_reward=-0.03 +/- 0.99
Episode length: 29.95 +/- 1.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.028     |
| time/                   |            |
|    total_timesteps      | 9154709    |
| train/                  |            |
|    approx_kl            | 0.01750758 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.328      |
|    learning_rate        | 8.18e-05   |
|    loss                 | 0.0985     |
|    n_updates            | 4925       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 150      |
|    time_elapsed    | 51063    |
|    total_timesteps | 9216000  |
---------------------------------
Eval num_timesteps=9216150, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 9216150     |
| train/                  |             |
|    approx_kl            | 0.017444933 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.301       |
|    learning_rate        | 8.17e-05    |
|    loss                 | 0.108       |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 180      |
|    iterations      | 151      |
|    time_elapsed    | 51380    |
|    total_timesteps | 9277440  |
---------------------------------
Eval num_timesteps=9277591, episode_reward=-0.04 +/- 0.98
Episode length: 29.98 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.044     |
| time/                   |            |
|    total_timesteps      | 9277591    |
| train/                  |            |
|    approx_kl            | 0.01708657 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.316      |
|    learning_rate        | 8.17e-05   |
|    loss                 | 0.0699     |
|    n_updates            | 4935       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 152      |
|    time_elapsed    | 51698    |
|    total_timesteps | 9338880  |
---------------------------------
Eval num_timesteps=9339032, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 9339032     |
| train/                  |             |
|    approx_kl            | 0.017359884 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.16e-05    |
|    loss                 | 0.0779      |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 153      |
|    time_elapsed    | 52015    |
|    total_timesteps | 9400320  |
---------------------------------
Eval num_timesteps=9400473, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.038       |
| time/                   |             |
|    total_timesteps      | 9400473     |
| train/                  |             |
|    approx_kl            | 0.017730536 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.322       |
|    learning_rate        | 8.15e-05    |
|    loss                 | 0.105       |
|    n_updates            | 4945        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 154      |
|    time_elapsed    | 52332    |
|    total_timesteps | 9461760  |
---------------------------------
Eval num_timesteps=9461914, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 1.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 9461914     |
| train/                  |             |
|    approx_kl            | 0.017184565 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.325       |
|    learning_rate        | 8.15e-05    |
|    loss                 | 0.0964      |
|    n_updates            | 4950        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 155      |
|    time_elapsed    | 52649    |
|    total_timesteps | 9523200  |
---------------------------------
Eval num_timesteps=9523355, episode_reward=-0.01 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 9523355     |
| train/                  |             |
|    approx_kl            | 0.017334336 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.14e-05    |
|    loss                 | 0.0453      |
|    n_updates            | 4955        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 156      |
|    time_elapsed    | 52967    |
|    total_timesteps | 9584640  |
---------------------------------
Eval num_timesteps=9584796, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 9584796     |
| train/                  |             |
|    approx_kl            | 0.017365614 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.304       |
|    learning_rate        | 8.14e-05    |
|    loss                 | 0.0905      |
|    n_updates            | 4960        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 157      |
|    time_elapsed    | 53286    |
|    total_timesteps | 9646080  |
---------------------------------
Eval num_timesteps=9646237, episode_reward=-0.00 +/- 0.98
Episode length: 29.96 +/- 1.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.002     |
| time/                   |            |
|    total_timesteps      | 9646237    |
| train/                  |            |
|    approx_kl            | 0.01742645 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.312      |
|    learning_rate        | 8.13e-05   |
|    loss                 | 0.0893     |
|    n_updates            | 4965       |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 158      |
|    time_elapsed    | 53606    |
|    total_timesteps | 9707520  |
---------------------------------
Eval num_timesteps=9707678, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.08       |
| time/                   |            |
|    total_timesteps      | 9707678    |
| train/                  |            |
|    approx_kl            | 0.01698091 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.319      |
|    learning_rate        | 8.13e-05   |
|    loss                 | 0.106      |
|    n_updates            | 4970       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 159      |
|    time_elapsed    | 53927    |
|    total_timesteps | 9768960  |
---------------------------------
Eval num_timesteps=9769119, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 9769119     |
| train/                  |             |
|    approx_kl            | 0.017332833 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.12e-05    |
|    loss                 | 0.0574      |
|    n_updates            | 4975        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 160      |
|    time_elapsed    | 54249    |
|    total_timesteps | 9830400  |
---------------------------------
Eval num_timesteps=9830560, episode_reward=0.03 +/- 0.98
Episode length: 29.95 +/- 1.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 9830560     |
| train/                  |             |
|    approx_kl            | 0.016994582 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.317       |
|    learning_rate        | 8.12e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 161      |
|    time_elapsed    | 54569    |
|    total_timesteps | 9891840  |
---------------------------------
Eval num_timesteps=9892001, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.026     |
| time/                   |           |
|    total_timesteps      | 9892001   |
| train/                  |           |
|    approx_kl            | 0.0173416 |
|    clip_fraction        | 0.191     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.722    |
|    explained_variance   | 0.305     |
|    learning_rate        | 8.11e-05  |
|    loss                 | 0.0746    |
|    n_updates            | 4985      |
|    policy_gradient_loss | -0.0174   |
|    value_loss           | 0.239     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 181      |
|    iterations      | 162      |
|    time_elapsed    | 54887    |
|    total_timesteps | 9953280  |
---------------------------------
Eval num_timesteps=9953442, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 9953442     |
| train/                  |             |
|    approx_kl            | 0.017290099 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.311       |
|    learning_rate        | 8.1e-05     |
|    loss                 | 0.0397      |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 163      |
|    time_elapsed    | 55204    |
|    total_timesteps | 10014720 |
---------------------------------
Eval num_timesteps=10014883, episode_reward=-0.07 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.068      |
| time/                   |             |
|    total_timesteps      | 10014883    |
| train/                  |             |
|    approx_kl            | 0.017015181 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.307       |
|    learning_rate        | 8.1e-05     |
|    loss                 | 0.0361      |
|    n_updates            | 4995        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 164      |
|    time_elapsed    | 55520    |
|    total_timesteps | 10076160 |
---------------------------------
Eval num_timesteps=10076324, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 0.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 10076324    |
| train/                  |             |
|    approx_kl            | 0.017514149 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.09e-05    |
|    loss                 | 0.0618      |
|    n_updates            | 5000        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 165      |
|    time_elapsed    | 55837    |
|    total_timesteps | 10137600 |
---------------------------------
Eval num_timesteps=10137765, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 10137765    |
| train/                  |             |
|    approx_kl            | 0.016861923 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.315       |
|    learning_rate        | 8.09e-05    |
|    loss                 | 0.104       |
|    n_updates            | 5005        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 166      |
|    time_elapsed    | 56154    |
|    total_timesteps | 10199040 |
---------------------------------
Eval num_timesteps=10199206, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 10199206    |
| train/                  |             |
|    approx_kl            | 0.017236773 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.323       |
|    learning_rate        | 8.08e-05    |
|    loss                 | 0.104       |
|    n_updates            | 5010        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 167      |
|    time_elapsed    | 56471    |
|    total_timesteps | 10260480 |
---------------------------------
Eval num_timesteps=10260647, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 10260647    |
| train/                  |             |
|    approx_kl            | 0.018084887 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.31        |
|    learning_rate        | 8.08e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 5015        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 168      |
|    time_elapsed    | 56789    |
|    total_timesteps | 10321920 |
---------------------------------
Eval num_timesteps=10322088, episode_reward=0.00 +/- 0.99
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 10322088    |
| train/                  |             |
|    approx_kl            | 0.017508456 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.304       |
|    learning_rate        | 8.07e-05    |
|    loss                 | 0.0777      |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 169      |
|    time_elapsed    | 57109    |
|    total_timesteps | 10383360 |
---------------------------------
Eval num_timesteps=10383529, episode_reward=-0.04 +/- 0.98
Episode length: 29.98 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 10383529    |
| train/                  |             |
|    approx_kl            | 0.017127255 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.308       |
|    learning_rate        | 8.07e-05    |
|    loss                 | 0.0691      |
|    n_updates            | 5025        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 170      |
|    time_elapsed    | 57429    |
|    total_timesteps | 10444800 |
---------------------------------
Eval num_timesteps=10444970, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 10444970    |
| train/                  |             |
|    approx_kl            | 0.017494457 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.309       |
|    learning_rate        | 8.06e-05    |
|    loss                 | 0.0549      |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 171      |
|    time_elapsed    | 57751    |
|    total_timesteps | 10506240 |
---------------------------------
Eval num_timesteps=10506411, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 10506411    |
| train/                  |             |
|    approx_kl            | 0.017111413 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.313       |
|    learning_rate        | 8.05e-05    |
|    loss                 | 0.106       |
|    n_updates            | 5035        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 181      |
|    iterations      | 172      |
|    time_elapsed    | 58072    |
|    total_timesteps | 10567680 |
---------------------------------
Eval num_timesteps=10567852, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 10567852    |
| train/                  |             |
|    approx_kl            | 0.017441908 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.305       |
|    learning_rate        | 8.05e-05    |
|    loss                 | 0.122       |
|    n_updates            | 5040        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 173      |
|    time_elapsed    | 58392    |
|    total_timesteps | 10629120 |
---------------------------------
Eval num_timesteps=10629293, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 10629293    |
| train/                  |             |
|    approx_kl            | 0.017163651 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.31        |
|    learning_rate        | 8.04e-05    |
|    loss                 | 0.0907      |
|    n_updates            | 5045        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 174      |
|    time_elapsed    | 58708    |
|    total_timesteps | 10690560 |
---------------------------------
Eval num_timesteps=10690734, episode_reward=-0.00 +/- 0.98
Episode length: 29.94 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 10690734    |
| train/                  |             |
|    approx_kl            | 0.016990645 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.302       |
|    learning_rate        | 8.04e-05    |
|    loss                 | 0.08        |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 182      |
|    iterations      | 175      |
|    time_elapsed    | 59025    |
|    total_timesteps | 10752000 |
---------------------------------
Eval num_timesteps=10752175, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 10752175    |
| train/                  |             |
|    approx_kl            | 0.017308788 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.314       |
|    learning_rate        | 8.03e-05    |
|    loss                 | 0.081       |
|    n_updates            | 5055        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 176      |
|    time_elapsed    | 59342    |
|    total_timesteps | 10813440 |
---------------------------------
Eval num_timesteps=10813616, episode_reward=0.02 +/- 0.98
Episode length: 30.01 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 10813616    |
| train/                  |             |
|    approx_kl            | 0.017618103 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.299       |
|    learning_rate        | 8.03e-05    |
|    loss                 | 0.0646      |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 177      |
|    time_elapsed    | 59660    |
|    total_timesteps | 10874880 |
---------------------------------
Eval num_timesteps=10875057, episode_reward=0.02 +/- 0.98
Episode length: 29.97 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 10875057    |
| train/                  |             |
|    approx_kl            | 0.017357111 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.316       |
|    learning_rate        | 8.02e-05    |
|    loss                 | 0.0749      |
|    n_updates            | 5065        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 178      |
|    time_elapsed    | 59976    |
|    total_timesteps | 10936320 |
---------------------------------
Eval num_timesteps=10936498, episode_reward=0.05 +/- 0.99
Episode length: 29.95 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 10936498    |
| train/                  |             |
|    approx_kl            | 0.017567521 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8.02e-05    |
|    loss                 | 0.0639      |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 179      |
|    time_elapsed    | 60294    |
|    total_timesteps | 10997760 |
---------------------------------
Eval num_timesteps=10997939, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 10997939    |
| train/                  |             |
|    approx_kl            | 0.016770227 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.311       |
|    learning_rate        | 8.01e-05    |
|    loss                 | 0.0992      |
|    n_updates            | 5075        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 180      |
|    time_elapsed    | 60612    |
|    total_timesteps | 11059200 |
---------------------------------
Eval num_timesteps=11059380, episode_reward=0.04 +/- 0.99
Episode length: 30.04 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 11059380    |
| train/                  |             |
|    approx_kl            | 0.017649146 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.312       |
|    learning_rate        | 8e-05       |
|    loss                 | 0.088       |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 181      |
|    time_elapsed    | 60932    |
|    total_timesteps | 11120640 |
---------------------------------
Eval num_timesteps=11120821, episode_reward=0.10 +/- 0.98
Episode length: 29.98 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 11120821    |
| train/                  |             |
|    approx_kl            | 0.017215911 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.32        |
|    learning_rate        | 8e-05       |
|    loss                 | 0.0979      |
|    n_updates            | 5085        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 182      |
|    time_elapsed    | 61253    |
|    total_timesteps | 11182080 |
---------------------------------
Eval num_timesteps=11182262, episode_reward=-0.03 +/- 0.98
Episode length: 29.94 +/- 1.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 11182262    |
| train/                  |             |
|    approx_kl            | 0.017068116 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.99e-05    |
|    loss                 | 0.0658      |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 183      |
|    time_elapsed    | 61575    |
|    total_timesteps | 11243520 |
---------------------------------
Eval num_timesteps=11243703, episode_reward=0.02 +/- 0.99
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 11243703    |
| train/                  |             |
|    approx_kl            | 0.017016836 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.31        |
|    learning_rate        | 7.99e-05    |
|    loss                 | 0.0926      |
|    n_updates            | 5095        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 184      |
|    time_elapsed    | 61895    |
|    total_timesteps | 11304960 |
---------------------------------
Eval num_timesteps=11305144, episode_reward=0.03 +/- 0.99
Episode length: 30.01 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 11305144    |
| train/                  |             |
|    approx_kl            | 0.017353557 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.321       |
|    learning_rate        | 7.98e-05    |
|    loss                 | 0.0833      |
|    n_updates            | 5100        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 185      |
|    time_elapsed    | 62213    |
|    total_timesteps | 11366400 |
---------------------------------
Eval num_timesteps=11366585, episode_reward=0.02 +/- 0.98
Episode length: 29.95 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 11366585    |
| train/                  |             |
|    approx_kl            | 0.016641563 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.326       |
|    learning_rate        | 7.98e-05    |
|    loss                 | 0.0417      |
|    n_updates            | 5105        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 186      |
|    time_elapsed    | 62530    |
|    total_timesteps | 11427840 |
---------------------------------
Eval num_timesteps=11428026, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 11428026    |
| train/                  |             |
|    approx_kl            | 0.016939878 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.321       |
|    learning_rate        | 7.97e-05    |
|    loss                 | 0.0879      |
|    n_updates            | 5110        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 187      |
|    time_elapsed    | 62847    |
|    total_timesteps | 11489280 |
---------------------------------
Eval num_timesteps=11489467, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 11489467    |
| train/                  |             |
|    approx_kl            | 0.016703213 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.306       |
|    learning_rate        | 7.97e-05    |
|    loss                 | 0.0759      |
|    n_updates            | 5115        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 188      |
|    time_elapsed    | 63165    |
|    total_timesteps | 11550720 |
---------------------------------
Eval num_timesteps=11550908, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 11550908    |
| train/                  |             |
|    approx_kl            | 0.016877966 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.96e-05    |
|    loss                 | 0.0908      |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 189      |
|    time_elapsed    | 63481    |
|    total_timesteps | 11612160 |
---------------------------------
Eval num_timesteps=11612349, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.038      |
| time/                   |            |
|    total_timesteps      | 11612349   |
| train/                  |            |
|    approx_kl            | 0.01702377 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.315      |
|    learning_rate        | 7.95e-05   |
|    loss                 | 0.0305     |
|    n_updates            | 5125       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 190      |
|    time_elapsed    | 63803    |
|    total_timesteps | 11673600 |
---------------------------------
Eval num_timesteps=11673790, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 11673790    |
| train/                  |             |
|    approx_kl            | 0.016949024 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.95e-05    |
|    loss                 | 0.0703      |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 191      |
|    time_elapsed    | 64128    |
|    total_timesteps | 11735040 |
---------------------------------
Eval num_timesteps=11735231, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 11735231    |
| train/                  |             |
|    approx_kl            | 0.017374424 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.317       |
|    learning_rate        | 7.94e-05    |
|    loss                 | 0.114       |
|    n_updates            | 5135        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 192      |
|    time_elapsed    | 64473    |
|    total_timesteps | 11796480 |
---------------------------------
Eval num_timesteps=11796672, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 11796672    |
| train/                  |             |
|    approx_kl            | 0.016969329 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.94e-05    |
|    loss                 | 0.0505      |
|    n_updates            | 5140        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 193      |
|    time_elapsed    | 64811    |
|    total_timesteps | 11857920 |
---------------------------------
Eval num_timesteps=11858113, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 11858113   |
| train/                  |            |
|    approx_kl            | 0.01660774 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.717     |
|    explained_variance   | 0.299      |
|    learning_rate        | 7.93e-05   |
|    loss                 | 0.0654     |
|    n_updates            | 5145       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 194      |
|    time_elapsed    | 65148    |
|    total_timesteps | 11919360 |
---------------------------------
Eval num_timesteps=11919554, episode_reward=0.09 +/- 0.99
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 11919554    |
| train/                  |             |
|    approx_kl            | 0.016907278 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.326       |
|    learning_rate        | 7.93e-05    |
|    loss                 | 0.0781      |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 195      |
|    time_elapsed    | 65485    |
|    total_timesteps | 11980800 |
---------------------------------
Eval num_timesteps=11980995, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.11        |
| time/                   |             |
|    total_timesteps      | 11980995    |
| train/                  |             |
|    approx_kl            | 0.017232196 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.92e-05    |
|    loss                 | 0.0556      |
|    n_updates            | 5155        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.32     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 196      |
|    time_elapsed    | 65825    |
|    total_timesteps | 12042240 |
---------------------------------
Eval num_timesteps=12042436, episode_reward=0.01 +/- 0.99
Episode length: 29.93 +/- 1.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 12042436    |
| train/                  |             |
|    approx_kl            | 0.016886117 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.339       |
|    learning_rate        | 7.92e-05    |
|    loss                 | 0.0457      |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 197      |
|    time_elapsed    | 66176    |
|    total_timesteps | 12103680 |
---------------------------------
Eval num_timesteps=12103877, episode_reward=0.03 +/- 0.98
Episode length: 30.07 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 12103877    |
| train/                  |             |
|    approx_kl            | 0.017367505 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.91e-05    |
|    loss                 | 0.0706      |
|    n_updates            | 5165        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 182      |
|    iterations      | 198      |
|    time_elapsed    | 66538    |
|    total_timesteps | 12165120 |
---------------------------------
Eval num_timesteps=12165318, episode_reward=0.00 +/- 0.98
Episode length: 29.93 +/- 1.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 12165318   |
| train/                  |            |
|    approx_kl            | 0.01706206 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.719     |
|    explained_variance   | 0.297      |
|    learning_rate        | 7.91e-05   |
|    loss                 | 0.0716     |
|    n_updates            | 5170       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 199      |
|    time_elapsed    | 66913    |
|    total_timesteps | 12226560 |
---------------------------------
Eval num_timesteps=12226759, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 12226759    |
| train/                  |             |
|    approx_kl            | 0.017088873 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.299       |
|    learning_rate        | 7.9e-05     |
|    loss                 | 0.0935      |
|    n_updates            | 5175        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 200      |
|    time_elapsed    | 67289    |
|    total_timesteps | 12288000 |
---------------------------------
Eval num_timesteps=12288200, episode_reward=0.13 +/- 0.98
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.126      |
| time/                   |            |
|    total_timesteps      | 12288200   |
| train/                  |            |
|    approx_kl            | 0.01670417 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.329      |
|    learning_rate        | 7.89e-05   |
|    loss                 | 0.0723     |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.231      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 201      |
|    time_elapsed    | 67670    |
|    total_timesteps | 12349440 |
---------------------------------
Eval num_timesteps=12349641, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 12349641    |
| train/                  |             |
|    approx_kl            | 0.016799442 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.89e-05    |
|    loss                 | 0.047       |
|    n_updates            | 5185        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 202      |
|    time_elapsed    | 68044    |
|    total_timesteps | 12410880 |
---------------------------------
Eval num_timesteps=12411082, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.112      |
| time/                   |            |
|    total_timesteps      | 12411082   |
| train/                  |            |
|    approx_kl            | 0.01666797 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.318      |
|    learning_rate        | 7.88e-05   |
|    loss                 | 0.089      |
|    n_updates            | 5190       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 203      |
|    time_elapsed    | 68426    |
|    total_timesteps | 12472320 |
---------------------------------
Eval num_timesteps=12472523, episode_reward=0.11 +/- 0.98
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 12472523    |
| train/                  |             |
|    approx_kl            | 0.017033877 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.322       |
|    learning_rate        | 7.88e-05    |
|    loss                 | 0.06        |
|    n_updates            | 5195        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 204      |
|    time_elapsed    | 68805    |
|    total_timesteps | 12533760 |
---------------------------------
Eval num_timesteps=12533964, episode_reward=0.06 +/- 0.98
Episode length: 30.04 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 12533964    |
| train/                  |             |
|    approx_kl            | 0.017323399 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.87e-05    |
|    loss                 | 0.0657      |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 182      |
|    iterations      | 205      |
|    time_elapsed    | 69187    |
|    total_timesteps | 12595200 |
---------------------------------
Eval num_timesteps=12595405, episode_reward=0.08 +/- 0.98
Episode length: 30.04 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 12595405    |
| train/                  |             |
|    approx_kl            | 0.016528646 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.31        |
|    learning_rate        | 7.87e-05    |
|    loss                 | 0.0457      |
|    n_updates            | 5205        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 181      |
|    iterations      | 206      |
|    time_elapsed    | 69569    |
|    total_timesteps | 12656640 |
---------------------------------
Eval num_timesteps=12656846, episode_reward=0.08 +/- 0.98
Episode length: 30.07 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 12656846    |
| train/                  |             |
|    approx_kl            | 0.016590774 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.86e-05    |
|    loss                 | 0.0557      |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 207      |
|    time_elapsed    | 69942    |
|    total_timesteps | 12718080 |
---------------------------------
Eval num_timesteps=12718287, episode_reward=0.05 +/- 0.98
Episode length: 29.98 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.046      |
| time/                   |            |
|    total_timesteps      | 12718287   |
| train/                  |            |
|    approx_kl            | 0.01663589 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.327      |
|    learning_rate        | 7.86e-05   |
|    loss                 | 0.0582     |
|    n_updates            | 5215       |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 208      |
|    time_elapsed    | 70323    |
|    total_timesteps | 12779520 |
---------------------------------
Eval num_timesteps=12779728, episode_reward=0.05 +/- 0.97
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 12779728    |
| train/                  |             |
|    approx_kl            | 0.016939156 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.303       |
|    learning_rate        | 7.85e-05    |
|    loss                 | 0.0681      |
|    n_updates            | 5220        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 209      |
|    time_elapsed    | 70696    |
|    total_timesteps | 12840960 |
---------------------------------
Eval num_timesteps=12841169, episode_reward=0.06 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 12841169    |
| train/                  |             |
|    approx_kl            | 0.016539915 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.84e-05    |
|    loss                 | 0.0849      |
|    n_updates            | 5225        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 210      |
|    time_elapsed    | 71078    |
|    total_timesteps | 12902400 |
---------------------------------
Eval num_timesteps=12902610, episode_reward=0.14 +/- 0.97
Episode length: 30.03 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.138       |
| time/                   |             |
|    total_timesteps      | 12902610    |
| train/                  |             |
|    approx_kl            | 0.016831426 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.84e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 211      |
|    time_elapsed    | 71450    |
|    total_timesteps | 12963840 |
---------------------------------
Eval num_timesteps=12964051, episode_reward=0.01 +/- 0.98
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.012      |
| time/                   |            |
|    total_timesteps      | 12964051   |
| train/                  |            |
|    approx_kl            | 0.01688428 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.317      |
|    learning_rate        | 7.83e-05   |
|    loss                 | 0.0691     |
|    n_updates            | 5235       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 212      |
|    time_elapsed    | 71832    |
|    total_timesteps | 13025280 |
---------------------------------
Eval num_timesteps=13025492, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 13025492   |
| train/                  |            |
|    approx_kl            | 0.01651804 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.304      |
|    learning_rate        | 7.83e-05   |
|    loss                 | 0.0634     |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 213      |
|    time_elapsed    | 72205    |
|    total_timesteps | 13086720 |
---------------------------------
Eval num_timesteps=13086933, episode_reward=0.05 +/- 0.99
Episode length: 30.00 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 13086933    |
| train/                  |             |
|    approx_kl            | 0.017542768 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.317       |
|    learning_rate        | 7.82e-05    |
|    loss                 | 0.112       |
|    n_updates            | 5245        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 214      |
|    time_elapsed    | 72589    |
|    total_timesteps | 13148160 |
---------------------------------
Eval num_timesteps=13148374, episode_reward=0.13 +/- 0.98
Episode length: 30.05 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.134       |
| time/                   |             |
|    total_timesteps      | 13148374    |
| train/                  |             |
|    approx_kl            | 0.016149469 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.33        |
|    learning_rate        | 7.82e-05    |
|    loss                 | 0.0781      |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 215      |
|    time_elapsed    | 73089    |
|    total_timesteps | 13209600 |
---------------------------------
Eval num_timesteps=13209815, episode_reward=0.14 +/- 0.97
Episode length: 30.05 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 13209815    |
| train/                  |             |
|    approx_kl            | 0.016774887 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.81e-05    |
|    loss                 | 0.0639      |
|    n_updates            | 5255        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.237       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 216      |
|    time_elapsed    | 73514    |
|    total_timesteps | 13271040 |
---------------------------------
Eval num_timesteps=13271256, episode_reward=0.04 +/- 0.99
Episode length: 30.01 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 13271256    |
| train/                  |             |
|    approx_kl            | 0.016647771 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.81e-05    |
|    loss                 | 0.0813      |
|    n_updates            | 5260        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 217      |
|    time_elapsed    | 73894    |
|    total_timesteps | 13332480 |
---------------------------------
Eval num_timesteps=13332697, episode_reward=0.02 +/- 0.99
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 13332697    |
| train/                  |             |
|    approx_kl            | 0.016434094 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.8e-05     |
|    loss                 | 0.0892      |
|    n_updates            | 5265        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 218      |
|    time_elapsed    | 74271    |
|    total_timesteps | 13393920 |
---------------------------------
Eval num_timesteps=13394138, episode_reward=0.07 +/- 0.97
Episode length: 29.99 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.07       |
| time/                   |            |
|    total_timesteps      | 13394138   |
| train/                  |            |
|    approx_kl            | 0.01627668 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.313      |
|    learning_rate        | 7.79e-05   |
|    loss                 | 0.0731     |
|    n_updates            | 5270       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 219      |
|    time_elapsed    | 74651    |
|    total_timesteps | 13455360 |
---------------------------------
Eval num_timesteps=13455579, episode_reward=-0.00 +/- 0.98
Episode length: 29.95 +/- 1.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 13455579    |
| train/                  |             |
|    approx_kl            | 0.016582675 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.308       |
|    learning_rate        | 7.79e-05    |
|    loss                 | 0.0801      |
|    n_updates            | 5275        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 220      |
|    time_elapsed    | 75025    |
|    total_timesteps | 13516800 |
---------------------------------
Eval num_timesteps=13517020, episode_reward=-0.04 +/- 0.98
Episode length: 29.97 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.038     |
| time/                   |            |
|    total_timesteps      | 13517020   |
| train/                  |            |
|    approx_kl            | 0.01663043 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.328      |
|    learning_rate        | 7.78e-05   |
|    loss                 | 0.0476     |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 180      |
|    iterations      | 221      |
|    time_elapsed    | 75405    |
|    total_timesteps | 13578240 |
---------------------------------
Eval num_timesteps=13578461, episode_reward=0.02 +/- 0.99
Episode length: 29.99 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.02       |
| time/                   |            |
|    total_timesteps      | 13578461   |
| train/                  |            |
|    approx_kl            | 0.01630614 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.309      |
|    learning_rate        | 7.78e-05   |
|    loss                 | 0.132      |
|    n_updates            | 5285       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 222      |
|    time_elapsed    | 75783    |
|    total_timesteps | 13639680 |
---------------------------------
Eval num_timesteps=13639902, episode_reward=0.01 +/- 0.98
Episode length: 29.96 +/- 0.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 13639902    |
| train/                  |             |
|    approx_kl            | 0.016839776 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.77e-05    |
|    loss                 | 0.0627      |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 223      |
|    time_elapsed    | 76158    |
|    total_timesteps | 13701120 |
---------------------------------
Eval num_timesteps=13701343, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 13701343   |
| train/                  |            |
|    approx_kl            | 0.01659941 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.319      |
|    learning_rate        | 7.77e-05   |
|    loss                 | 0.0818     |
|    n_updates            | 5295       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 224      |
|    time_elapsed    | 76545    |
|    total_timesteps | 13762560 |
---------------------------------
Eval num_timesteps=13762784, episode_reward=0.08 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 13762784    |
| train/                  |             |
|    approx_kl            | 0.016228355 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.76e-05    |
|    loss                 | 0.0653      |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 225      |
|    time_elapsed    | 76918    |
|    total_timesteps | 13824000 |
---------------------------------
Eval num_timesteps=13824225, episode_reward=-0.03 +/- 0.98
Episode length: 30.01 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 13824225    |
| train/                  |             |
|    approx_kl            | 0.016833914 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.287       |
|    learning_rate        | 7.76e-05    |
|    loss                 | 0.03        |
|    n_updates            | 5305        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 226      |
|    time_elapsed    | 77307    |
|    total_timesteps | 13885440 |
---------------------------------
Eval num_timesteps=13885666, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 13885666    |
| train/                  |             |
|    approx_kl            | 0.016867897 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.75e-05    |
|    loss                 | 0.0781      |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 227      |
|    time_elapsed    | 77679    |
|    total_timesteps | 13946880 |
---------------------------------
Eval num_timesteps=13947107, episode_reward=-0.01 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 13947107    |
| train/                  |             |
|    approx_kl            | 0.016610181 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.74e-05    |
|    loss                 | 0.0422      |
|    n_updates            | 5315        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 228      |
|    time_elapsed    | 78063    |
|    total_timesteps | 14008320 |
---------------------------------
Eval num_timesteps=14008548, episode_reward=-0.03 +/- 0.99
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.034      |
| time/                   |             |
|    total_timesteps      | 14008548    |
| train/                  |             |
|    approx_kl            | 0.016118152 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.317       |
|    learning_rate        | 7.74e-05    |
|    loss                 | 0.105       |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 229      |
|    time_elapsed    | 78437    |
|    total_timesteps | 14069760 |
---------------------------------
Eval num_timesteps=14069989, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.032      |
| time/                   |            |
|    total_timesteps      | 14069989   |
| train/                  |            |
|    approx_kl            | 0.01636862 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.724     |
|    explained_variance   | 0.314      |
|    learning_rate        | 7.73e-05   |
|    loss                 | 0.0318     |
|    n_updates            | 5325       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 230      |
|    time_elapsed    | 78816    |
|    total_timesteps | 14131200 |
---------------------------------
Eval num_timesteps=14131430, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 14131430    |
| train/                  |             |
|    approx_kl            | 0.016344046 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.73e-05    |
|    loss                 | 0.0832      |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 231      |
|    time_elapsed    | 79194    |
|    total_timesteps | 14192640 |
---------------------------------
Eval num_timesteps=14192871, episode_reward=0.01 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 14192871    |
| train/                  |             |
|    approx_kl            | 0.016493073 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.72e-05    |
|    loss                 | 0.119       |
|    n_updates            | 5335        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 179      |
|    iterations      | 232      |
|    time_elapsed    | 79571    |
|    total_timesteps | 14254080 |
---------------------------------
Eval num_timesteps=14254312, episode_reward=-0.00 +/- 0.98
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 14254312    |
| train/                  |             |
|    approx_kl            | 0.016694251 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.72e-05    |
|    loss                 | 0.101       |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 233      |
|    time_elapsed    | 80083    |
|    total_timesteps | 14315520 |
---------------------------------
Eval num_timesteps=14315753, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 14315753    |
| train/                  |             |
|    approx_kl            | 0.016296398 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.303       |
|    learning_rate        | 7.71e-05    |
|    loss                 | 0.109       |
|    n_updates            | 5345        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 234      |
|    time_elapsed    | 80496    |
|    total_timesteps | 14376960 |
---------------------------------
Eval num_timesteps=14377194, episode_reward=-0.06 +/- 0.98
Episode length: 29.97 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.058      |
| time/                   |             |
|    total_timesteps      | 14377194    |
| train/                  |             |
|    approx_kl            | 0.016329778 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.317       |
|    learning_rate        | 7.71e-05    |
|    loss                 | 0.112       |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 235      |
|    time_elapsed    | 80874    |
|    total_timesteps | 14438400 |
---------------------------------
Eval num_timesteps=14438635, episode_reward=-0.06 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.062      |
| time/                   |             |
|    total_timesteps      | 14438635    |
| train/                  |             |
|    approx_kl            | 0.016102323 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.7e-05     |
|    loss                 | 0.046       |
|    n_updates            | 5355        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 236      |
|    time_elapsed    | 81258    |
|    total_timesteps | 14499840 |
---------------------------------
Eval num_timesteps=14500076, episode_reward=0.03 +/- 0.98
Episode length: 30.01 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 14500076    |
| train/                  |             |
|    approx_kl            | 0.016117835 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.7e-05     |
|    loss                 | 0.0664      |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 237      |
|    time_elapsed    | 81637    |
|    total_timesteps | 14561280 |
---------------------------------
Eval num_timesteps=14561517, episode_reward=0.05 +/- 0.98
Episode length: 29.99 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 14561517   |
| train/                  |            |
|    approx_kl            | 0.01612157 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.717     |
|    explained_variance   | 0.327      |
|    learning_rate        | 7.69e-05   |
|    loss                 | 0.0457     |
|    n_updates            | 5365       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 238      |
|    time_elapsed    | 82019    |
|    total_timesteps | 14622720 |
---------------------------------
Eval num_timesteps=14622958, episode_reward=-0.04 +/- 0.97
Episode length: 30.01 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.038      |
| time/                   |             |
|    total_timesteps      | 14622958    |
| train/                  |             |
|    approx_kl            | 0.016012326 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.325       |
|    learning_rate        | 7.68e-05    |
|    loss                 | 0.0863      |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 178      |
|    iterations      | 239      |
|    time_elapsed    | 82396    |
|    total_timesteps | 14684160 |
---------------------------------
Eval num_timesteps=14684399, episode_reward=-0.03 +/- 0.98
Episode length: 29.99 +/- 0.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 14684399   |
| train/                  |            |
|    approx_kl            | 0.01648122 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.717     |
|    explained_variance   | 0.315      |
|    learning_rate        | 7.68e-05   |
|    loss                 | 0.106      |
|    n_updates            | 5375       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 240      |
|    time_elapsed    | 82776    |
|    total_timesteps | 14745600 |
---------------------------------
Eval num_timesteps=14745840, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 14745840    |
| train/                  |             |
|    approx_kl            | 0.016728071 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.67e-05    |
|    loss                 | 0.0943      |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 241      |
|    time_elapsed    | 83149    |
|    total_timesteps | 14807040 |
---------------------------------
Eval num_timesteps=14807281, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.076       |
| time/                   |             |
|    total_timesteps      | 14807281    |
| train/                  |             |
|    approx_kl            | 0.016255958 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.304       |
|    learning_rate        | 7.67e-05    |
|    loss                 | 0.0843      |
|    n_updates            | 5385        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 242      |
|    time_elapsed    | 83531    |
|    total_timesteps | 14868480 |
---------------------------------
Eval num_timesteps=14868722, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 14868722    |
| train/                  |             |
|    approx_kl            | 0.015984626 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.66e-05    |
|    loss                 | 0.111       |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 243      |
|    time_elapsed    | 83903    |
|    total_timesteps | 14929920 |
---------------------------------
Eval num_timesteps=14930163, episode_reward=0.03 +/- 0.99
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 14930163    |
| train/                  |             |
|    approx_kl            | 0.016456338 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.66e-05    |
|    loss                 | 0.104       |
|    n_updates            | 5395        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 244      |
|    time_elapsed    | 84285    |
|    total_timesteps | 14991360 |
---------------------------------
Eval num_timesteps=14991604, episode_reward=0.07 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.074       |
| time/                   |             |
|    total_timesteps      | 14991604    |
| train/                  |             |
|    approx_kl            | 0.015956901 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.333       |
|    learning_rate        | 7.65e-05    |
|    loss                 | 0.115       |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 245      |
|    time_elapsed    | 84661    |
|    total_timesteps | 15052800 |
---------------------------------
Eval num_timesteps=15053045, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.018      |
| time/                   |            |
|    total_timesteps      | 15053045   |
| train/                  |            |
|    approx_kl            | 0.01624627 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.307      |
|    learning_rate        | 7.65e-05   |
|    loss                 | 0.0742     |
|    n_updates            | 5405       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 246      |
|    time_elapsed    | 85042    |
|    total_timesteps | 15114240 |
---------------------------------
Eval num_timesteps=15114486, episode_reward=0.02 +/- 0.98
Episode length: 29.96 +/- 1.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 15114486    |
| train/                  |             |
|    approx_kl            | 0.016145399 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.64e-05    |
|    loss                 | 0.0818      |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 247      |
|    time_elapsed    | 85426    |
|    total_timesteps | 15175680 |
---------------------------------
Eval num_timesteps=15175927, episode_reward=0.06 +/- 0.98
Episode length: 30.03 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.056      |
| time/                   |            |
|    total_timesteps      | 15175927   |
| train/                  |            |
|    approx_kl            | 0.01602823 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.328      |
|    learning_rate        | 7.63e-05   |
|    loss                 | 0.0822     |
|    n_updates            | 5415       |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 248      |
|    time_elapsed    | 85755    |
|    total_timesteps | 15237120 |
---------------------------------
Eval num_timesteps=15237368, episode_reward=-0.03 +/- 0.99
Episode length: 29.99 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.026      |
| time/                   |             |
|    total_timesteps      | 15237368    |
| train/                  |             |
|    approx_kl            | 0.016351407 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7.63e-05    |
|    loss                 | 0.0569      |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 249      |
|    time_elapsed    | 86122    |
|    total_timesteps | 15298560 |
---------------------------------
Eval num_timesteps=15298809, episode_reward=0.10 +/- 0.98
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.102       |
| time/                   |             |
|    total_timesteps      | 15298809    |
| train/                  |             |
|    approx_kl            | 0.016153278 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.304       |
|    learning_rate        | 7.62e-05    |
|    loss                 | 0.0746      |
|    n_updates            | 5425        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 250      |
|    time_elapsed    | 86493    |
|    total_timesteps | 15360000 |
---------------------------------
Eval num_timesteps=15360250, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 15360250    |
| train/                  |             |
|    approx_kl            | 0.016100701 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.62e-05    |
|    loss                 | 0.0659      |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 251      |
|    time_elapsed    | 86859    |
|    total_timesteps | 15421440 |
---------------------------------
Eval num_timesteps=15421691, episode_reward=0.03 +/- 0.98
Episode length: 29.99 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 15421691    |
| train/                  |             |
|    approx_kl            | 0.016296167 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.61e-05    |
|    loss                 | 0.0739      |
|    n_updates            | 5435        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 252      |
|    time_elapsed    | 87231    |
|    total_timesteps | 15482880 |
---------------------------------
Eval num_timesteps=15483132, episode_reward=0.01 +/- 0.98
Episode length: 29.98 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 15483132   |
| train/                  |            |
|    approx_kl            | 0.01623659 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.707     |
|    explained_variance   | 0.325      |
|    learning_rate        | 7.61e-05   |
|    loss                 | 0.0741     |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 253      |
|    time_elapsed    | 87596    |
|    total_timesteps | 15544320 |
---------------------------------
Eval num_timesteps=15544573, episode_reward=0.05 +/- 0.98
Episode length: 29.96 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.054       |
| time/                   |             |
|    total_timesteps      | 15544573    |
| train/                  |             |
|    approx_kl            | 0.016572902 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.6e-05     |
|    loss                 | 0.098       |
|    n_updates            | 5445        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 177      |
|    iterations      | 254      |
|    time_elapsed    | 87971    |
|    total_timesteps | 15605760 |
---------------------------------
Eval num_timesteps=15606014, episode_reward=-0.02 +/- 0.98
Episode length: 29.94 +/- 0.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.02      |
| time/                   |            |
|    total_timesteps      | 15606014   |
| train/                  |            |
|    approx_kl            | 0.01636534 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.307      |
|    learning_rate        | 7.6e-05    |
|    loss                 | 0.0438     |
|    n_updates            | 5450       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.18    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 255      |
|    time_elapsed    | 88365    |
|    total_timesteps | 15667200 |
---------------------------------
Eval num_timesteps=15667455, episode_reward=0.07 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 15667455    |
| train/                  |             |
|    approx_kl            | 0.016229996 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.59e-05    |
|    loss                 | 0.072       |
|    n_updates            | 5455        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 177      |
|    iterations      | 256      |
|    time_elapsed    | 88852    |
|    total_timesteps | 15728640 |
---------------------------------
Eval num_timesteps=15728896, episode_reward=0.08 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 15728896    |
| train/                  |             |
|    approx_kl            | 0.016634485 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.0623      |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 257      |
|    time_elapsed    | 89232    |
|    total_timesteps | 15790080 |
---------------------------------
Eval num_timesteps=15790337, episode_reward=0.09 +/- 0.97
Episode length: 30.04 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 15790337    |
| train/                  |             |
|    approx_kl            | 0.016802164 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.0792      |
|    n_updates            | 5465        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 258      |
|    time_elapsed    | 89610    |
|    total_timesteps | 15851520 |
---------------------------------
Eval num_timesteps=15851778, episode_reward=0.06 +/- 0.97
Episode length: 30.01 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.064      |
| time/                   |            |
|    total_timesteps      | 15851778   |
| train/                  |            |
|    approx_kl            | 0.01663903 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.707     |
|    explained_variance   | 0.296      |
|    learning_rate        | 7.57e-05   |
|    loss                 | 0.0738     |
|    n_updates            | 5470       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 259      |
|    time_elapsed    | 89985    |
|    total_timesteps | 15912960 |
---------------------------------
Eval num_timesteps=15913219, episode_reward=0.09 +/- 0.98
Episode length: 29.99 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.092       |
| time/                   |             |
|    total_timesteps      | 15913219    |
| train/                  |             |
|    approx_kl            | 0.016075326 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.57e-05    |
|    loss                 | 0.105       |
|    n_updates            | 5475        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 260      |
|    time_elapsed    | 90360    |
|    total_timesteps | 15974400 |
---------------------------------
Eval num_timesteps=15974660, episode_reward=0.05 +/- 0.99
Episode length: 29.96 +/- 1.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.046     |
| time/                   |           |
|    total_timesteps      | 15974660  |
| train/                  |           |
|    approx_kl            | 0.0161785 |
|    clip_fraction        | 0.181     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.705    |
|    explained_variance   | 0.314     |
|    learning_rate        | 7.56e-05  |
|    loss                 | 0.0916    |
|    n_updates            | 5480      |
|    policy_gradient_loss | -0.0172   |
|    value_loss           | 0.238     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 261      |
|    time_elapsed    | 90732    |
|    total_timesteps | 16035840 |
---------------------------------
Eval num_timesteps=16036101, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.088       |
| time/                   |             |
|    total_timesteps      | 16036101    |
| train/                  |             |
|    approx_kl            | 0.016173184 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.56e-05    |
|    loss                 | 0.0694      |
|    n_updates            | 5485        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 262      |
|    time_elapsed    | 91106    |
|    total_timesteps | 16097280 |
---------------------------------
Eval num_timesteps=16097542, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total_timesteps      | 16097542   |
| train/                  |            |
|    approx_kl            | 0.01567343 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.705     |
|    explained_variance   | 0.312      |
|    learning_rate        | 7.55e-05   |
|    loss                 | 0.0593     |
|    n_updates            | 5490       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 263      |
|    time_elapsed    | 91556    |
|    total_timesteps | 16158720 |
---------------------------------
Eval num_timesteps=16158983, episode_reward=0.03 +/- 0.98
Episode length: 29.96 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 16158983    |
| train/                  |             |
|    approx_kl            | 0.015885886 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.326       |
|    learning_rate        | 7.55e-05    |
|    loss                 | 0.1         |
|    n_updates            | 5495        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 264      |
|    time_elapsed    | 91996    |
|    total_timesteps | 16220160 |
---------------------------------
Eval num_timesteps=16220424, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 16220424    |
| train/                  |             |
|    approx_kl            | 0.016281392 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.295       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0527      |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 265      |
|    time_elapsed    | 92369    |
|    total_timesteps | 16281600 |
---------------------------------
Eval num_timesteps=16281865, episode_reward=0.04 +/- 0.98
Episode length: 30.03 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 16281865    |
| train/                  |             |
|    approx_kl            | 0.016428642 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.298       |
|    learning_rate        | 7.53e-05    |
|    loss                 | 0.0879      |
|    n_updates            | 5505        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 266      |
|    time_elapsed    | 92744    |
|    total_timesteps | 16343040 |
---------------------------------
Eval num_timesteps=16343306, episode_reward=0.05 +/- 0.99
Episode length: 30.04 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 16343306   |
| train/                  |            |
|    approx_kl            | 0.01624669 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.699     |
|    explained_variance   | 0.313      |
|    learning_rate        | 7.53e-05   |
|    loss                 | 0.065      |
|    n_updates            | 5510       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 267      |
|    time_elapsed    | 93116    |
|    total_timesteps | 16404480 |
---------------------------------
Eval num_timesteps=16404747, episode_reward=0.10 +/- 0.98
Episode length: 29.97 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.102      |
| time/                   |            |
|    total_timesteps      | 16404747   |
| train/                  |            |
|    approx_kl            | 0.01579856 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.694     |
|    explained_variance   | 0.325      |
|    learning_rate        | 7.52e-05   |
|    loss                 | 0.0654     |
|    n_updates            | 5515       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 176      |
|    iterations      | 268      |
|    time_elapsed    | 93492    |
|    total_timesteps | 16465920 |
---------------------------------
Eval num_timesteps=16466188, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.054      |
| time/                   |            |
|    total_timesteps      | 16466188   |
| train/                  |            |
|    approx_kl            | 0.01614362 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.302      |
|    learning_rate        | 7.52e-05   |
|    loss                 | 0.0991     |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.243      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 269      |
|    time_elapsed    | 93862    |
|    total_timesteps | 16527360 |
---------------------------------
Eval num_timesteps=16527629, episode_reward=0.04 +/- 0.97
Episode length: 29.96 +/- 1.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 16527629    |
| train/                  |             |
|    approx_kl            | 0.016161168 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.51e-05    |
|    loss                 | 0.0737      |
|    n_updates            | 5525        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 270      |
|    time_elapsed    | 94277    |
|    total_timesteps | 16588800 |
---------------------------------
Eval num_timesteps=16589070, episode_reward=0.09 +/- 0.98
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 16589070    |
| train/                  |             |
|    approx_kl            | 0.016031047 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.51e-05    |
|    loss                 | 0.0784      |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 271      |
|    time_elapsed    | 94703    |
|    total_timesteps | 16650240 |
---------------------------------
Eval num_timesteps=16650511, episode_reward=0.15 +/- 0.97
Episode length: 30.05 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.148       |
| time/                   |             |
|    total_timesteps      | 16650511    |
| train/                  |             |
|    approx_kl            | 0.015817897 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.691      |
|    explained_variance   | 0.308       |
|    learning_rate        | 7.5e-05     |
|    loss                 | 0.0793      |
|    n_updates            | 5535        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.148
SELFPLAY: new best model, bumping up generation to 8
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 272      |
|    time_elapsed    | 95132    |
|    total_timesteps | 16711680 |
---------------------------------
Eval num_timesteps=16711952, episode_reward=-0.04 +/- 0.98
Episode length: 29.93 +/- 0.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.04      |
| time/                   |            |
|    total_timesteps      | 16711952   |
| train/                  |            |
|    approx_kl            | 0.01597316 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.695     |
|    explained_variance   | 0.292      |
|    learning_rate        | 7.5e-05    |
|    loss                 | 0.0551     |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 273      |
|    time_elapsed    | 95497    |
|    total_timesteps | 16773120 |
---------------------------------
Eval num_timesteps=16773393, episode_reward=-0.04 +/- 0.99
Episode length: 29.99 +/- 0.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 16773393    |
| train/                  |             |
|    approx_kl            | 0.016012449 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.49e-05    |
|    loss                 | 0.0615      |
|    n_updates            | 5545        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 274      |
|    time_elapsed    | 95869    |
|    total_timesteps | 16834560 |
---------------------------------
Eval num_timesteps=16834834, episode_reward=0.01 +/- 0.99
Episode length: 29.96 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 16834834    |
| train/                  |             |
|    approx_kl            | 0.016243335 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.31        |
|    learning_rate        | 7.48e-05    |
|    loss                 | 0.0748      |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 275      |
|    time_elapsed    | 96235    |
|    total_timesteps | 16896000 |
---------------------------------
Eval num_timesteps=16896275, episode_reward=0.11 +/- 0.97
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 16896275    |
| train/                  |             |
|    approx_kl            | 0.016060468 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.48e-05    |
|    loss                 | 0.0772      |
|    n_updates            | 5555        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 276      |
|    time_elapsed    | 96610    |
|    total_timesteps | 16957440 |
---------------------------------
Eval num_timesteps=16957716, episode_reward=-0.05 +/- 0.99
Episode length: 29.95 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.048     |
| time/                   |            |
|    total_timesteps      | 16957716   |
| train/                  |            |
|    approx_kl            | 0.01651083 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.699     |
|    explained_variance   | 0.315      |
|    learning_rate        | 7.47e-05   |
|    loss                 | 0.0471     |
|    n_updates            | 5560       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 277      |
|    time_elapsed    | 97003    |
|    total_timesteps | 17018880 |
---------------------------------
Eval num_timesteps=17019157, episode_reward=-0.04 +/- 0.99
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.036      |
| time/                   |             |
|    total_timesteps      | 17019157    |
| train/                  |             |
|    approx_kl            | 0.015482637 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.325       |
|    learning_rate        | 7.47e-05    |
|    loss                 | 0.0691      |
|    n_updates            | 5565        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 278      |
|    time_elapsed    | 97412    |
|    total_timesteps | 17080320 |
---------------------------------
Eval num_timesteps=17080598, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.062       |
| time/                   |             |
|    total_timesteps      | 17080598    |
| train/                  |             |
|    approx_kl            | 0.015627708 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.46e-05    |
|    loss                 | 0.0268      |
|    n_updates            | 5570        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 279      |
|    time_elapsed    | 97832    |
|    total_timesteps | 17141760 |
---------------------------------
Eval num_timesteps=17142039, episode_reward=-0.03 +/- 0.98
Episode length: 30.01 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 17142039   |
| train/                  |            |
|    approx_kl            | 0.01547621 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.696     |
|    explained_variance   | 0.32       |
|    learning_rate        | 7.46e-05   |
|    loss                 | 0.103      |
|    n_updates            | 5575       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 280      |
|    time_elapsed    | 98251    |
|    total_timesteps | 17203200 |
---------------------------------
Eval num_timesteps=17203480, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 17203480    |
| train/                  |             |
|    approx_kl            | 0.015761148 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.45e-05    |
|    loss                 | 0.0521      |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 175      |
|    iterations      | 281      |
|    time_elapsed    | 98626    |
|    total_timesteps | 17264640 |
---------------------------------
Eval num_timesteps=17264921, episode_reward=-0.09 +/- 0.98
Episode length: 29.93 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.086      |
| time/                   |             |
|    total_timesteps      | 17264921    |
| train/                  |             |
|    approx_kl            | 0.016140299 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.31        |
|    learning_rate        | 7.45e-05    |
|    loss                 | 0.0785      |
|    n_updates            | 5585        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 282      |
|    time_elapsed    | 99001    |
|    total_timesteps | 17326080 |
---------------------------------
Eval num_timesteps=17326362, episode_reward=-0.01 +/- 0.99
Episode length: 29.99 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.008      |
| time/                   |             |
|    total_timesteps      | 17326362    |
| train/                  |             |
|    approx_kl            | 0.016043423 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.304       |
|    learning_rate        | 7.44e-05    |
|    loss                 | 0.0548      |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 283      |
|    time_elapsed    | 99376    |
|    total_timesteps | 17387520 |
---------------------------------
Eval num_timesteps=17387803, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 17387803   |
| train/                  |            |
|    approx_kl            | 0.01586615 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.697     |
|    explained_variance   | 0.325      |
|    learning_rate        | 7.44e-05   |
|    loss                 | 0.0703     |
|    n_updates            | 5595       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 174      |
|    iterations      | 284      |
|    time_elapsed    | 99748    |
|    total_timesteps | 17448960 |
---------------------------------
Eval num_timesteps=17449244, episode_reward=0.01 +/- 0.98
Episode length: 30.03 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 17449244    |
| train/                  |             |
|    approx_kl            | 0.016004955 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.297       |
|    learning_rate        | 7.43e-05    |
|    loss                 | 0.055       |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 285      |
|    time_elapsed    | 100178   |
|    total_timesteps | 17510400 |
---------------------------------
Eval num_timesteps=17510685, episode_reward=-0.05 +/- 0.98
Episode length: 29.96 +/- 0.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | -0.046    |
| time/                   |           |
|    total_timesteps      | 17510685  |
| train/                  |           |
|    approx_kl            | 0.0158278 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.696    |
|    explained_variance   | 0.321     |
|    learning_rate        | 7.42e-05  |
|    loss                 | 0.0633    |
|    n_updates            | 5605      |
|    policy_gradient_loss | -0.0177   |
|    value_loss           | 0.235     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 286      |
|    time_elapsed    | 100550   |
|    total_timesteps | 17571840 |
---------------------------------
Eval num_timesteps=17572126, episode_reward=0.00 +/- 0.98
Episode length: 29.98 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 17572126    |
| train/                  |             |
|    approx_kl            | 0.016030189 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.322       |
|    learning_rate        | 7.42e-05    |
|    loss                 | 0.0621      |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 287      |
|    time_elapsed    | 100989   |
|    total_timesteps | 17633280 |
---------------------------------
Eval num_timesteps=17633567, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 17633567    |
| train/                  |             |
|    approx_kl            | 0.015604302 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.41e-05    |
|    loss                 | 0.0707      |
|    n_updates            | 5615        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 288      |
|    time_elapsed    | 101388   |
|    total_timesteps | 17694720 |
---------------------------------
Eval num_timesteps=17695008, episode_reward=0.04 +/- 0.99
Episode length: 30.01 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 17695008    |
| train/                  |             |
|    approx_kl            | 0.015746374 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.41e-05    |
|    loss                 | 0.15        |
|    n_updates            | 5620        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 289      |
|    time_elapsed    | 101761   |
|    total_timesteps | 17756160 |
---------------------------------
Eval num_timesteps=17756449, episode_reward=0.06 +/- 0.98
Episode length: 29.98 +/- 0.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30        |
|    mean_reward          | 0.06      |
| time/                   |           |
|    total_timesteps      | 17756449  |
| train/                  |           |
|    approx_kl            | 0.0158723 |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.15      |
|    entropy_loss         | -0.696    |
|    explained_variance   | 0.316     |
|    learning_rate        | 7.4e-05   |
|    loss                 | 0.075     |
|    n_updates            | 5625      |
|    policy_gradient_loss | -0.0177   |
|    value_loss           | 0.24      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 174      |
|    iterations      | 290      |
|    time_elapsed    | 102138   |
|    total_timesteps | 17817600 |
---------------------------------
Eval num_timesteps=17817890, episode_reward=0.00 +/- 0.99
Episode length: 29.95 +/- 1.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 17817890    |
| train/                  |             |
|    approx_kl            | 0.015607874 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.4e-05     |
|    loss                 | 0.121       |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 291      |
|    time_elapsed    | 102513   |
|    total_timesteps | 17879040 |
---------------------------------
Eval num_timesteps=17879331, episode_reward=-0.01 +/- 0.99
Episode length: 29.97 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.006      |
| time/                   |             |
|    total_timesteps      | 17879331    |
| train/                  |             |
|    approx_kl            | 0.015795987 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.329       |
|    learning_rate        | 7.39e-05    |
|    loss                 | 0.093       |
|    n_updates            | 5635        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 292      |
|    time_elapsed    | 102941   |
|    total_timesteps | 17940480 |
---------------------------------
Eval num_timesteps=17940772, episode_reward=0.06 +/- 0.98
Episode length: 30.06 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 17940772    |
| train/                  |             |
|    approx_kl            | 0.015915819 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.695      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.39e-05    |
|    loss                 | 0.0509      |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 174      |
|    iterations      | 293      |
|    time_elapsed    | 103313   |
|    total_timesteps | 18001920 |
---------------------------------
Eval num_timesteps=18002213, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 18002213    |
| train/                  |             |
|    approx_kl            | 0.015448594 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.38e-05    |
|    loss                 | 0.0682      |
|    n_updates            | 5645        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 294      |
|    time_elapsed    | 103698   |
|    total_timesteps | 18063360 |
---------------------------------
Eval num_timesteps=18063654, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 18063654    |
| train/                  |             |
|    approx_kl            | 0.015862694 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.326       |
|    learning_rate        | 7.37e-05    |
|    loss                 | 0.101       |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 174      |
|    iterations      | 295      |
|    time_elapsed    | 104146   |
|    total_timesteps | 18124800 |
---------------------------------
Eval num_timesteps=18125095, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.048       |
| time/                   |             |
|    total_timesteps      | 18125095    |
| train/                  |             |
|    approx_kl            | 0.015442976 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.695      |
|    explained_variance   | 0.302       |
|    learning_rate        | 7.37e-05    |
|    loss                 | 0.0844      |
|    n_updates            | 5655        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 296      |
|    time_elapsed    | 104515   |
|    total_timesteps | 18186240 |
---------------------------------
Eval num_timesteps=18186536, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.036       |
| time/                   |             |
|    total_timesteps      | 18186536    |
| train/                  |             |
|    approx_kl            | 0.015906697 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.331       |
|    learning_rate        | 7.36e-05    |
|    loss                 | 0.064       |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 297      |
|    time_elapsed    | 104887   |
|    total_timesteps | 18247680 |
---------------------------------
Eval num_timesteps=18247977, episode_reward=0.04 +/- 0.98
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 18247977    |
| train/                  |             |
|    approx_kl            | 0.015980856 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.306       |
|    learning_rate        | 7.36e-05    |
|    loss                 | 0.0963      |
|    n_updates            | 5665        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 298      |
|    time_elapsed    | 105253   |
|    total_timesteps | 18309120 |
---------------------------------
Eval num_timesteps=18309418, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.082       |
| time/                   |             |
|    total_timesteps      | 18309418    |
| train/                  |             |
|    approx_kl            | 0.015660703 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.337       |
|    learning_rate        | 7.35e-05    |
|    loss                 | 0.0396      |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 173      |
|    iterations      | 299      |
|    time_elapsed    | 105672   |
|    total_timesteps | 18370560 |
---------------------------------
Eval num_timesteps=18370859, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.072      |
| time/                   |            |
|    total_timesteps      | 18370859   |
| train/                  |            |
|    approx_kl            | 0.01563114 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.312      |
|    learning_rate        | 7.35e-05   |
|    loss                 | 0.0849     |
|    n_updates            | 5675       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 300      |
|    time_elapsed    | 106058   |
|    total_timesteps | 18432000 |
---------------------------------
Eval num_timesteps=18432300, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 18432300    |
| train/                  |             |
|    approx_kl            | 0.015763378 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.34e-05    |
|    loss                 | 0.102       |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 301      |
|    time_elapsed    | 106432   |
|    total_timesteps | 18493440 |
---------------------------------
Eval num_timesteps=18493741, episode_reward=0.03 +/- 0.98
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 18493741    |
| train/                  |             |
|    approx_kl            | 0.015307063 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.34e-05    |
|    loss                 | 0.0773      |
|    n_updates            | 5685        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 302      |
|    time_elapsed    | 106839   |
|    total_timesteps | 18554880 |
---------------------------------
Eval num_timesteps=18555182, episode_reward=0.06 +/- 0.99
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 18555182    |
| train/                  |             |
|    approx_kl            | 0.015765237 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.322       |
|    learning_rate        | 7.33e-05    |
|    loss                 | 0.0986      |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 303      |
|    time_elapsed    | 107273   |
|    total_timesteps | 18616320 |
---------------------------------
Eval num_timesteps=18616623, episode_reward=0.06 +/- 0.98
Episode length: 30.05 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 18616623    |
| train/                  |             |
|    approx_kl            | 0.015922565 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.32e-05    |
|    loss                 | 0.0722      |
|    n_updates            | 5695        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 304      |
|    time_elapsed    | 107646   |
|    total_timesteps | 18677760 |
---------------------------------
Eval num_timesteps=18678064, episode_reward=-0.03 +/- 0.98
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.032      |
| time/                   |             |
|    total_timesteps      | 18678064    |
| train/                  |             |
|    approx_kl            | 0.015514074 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.32e-05    |
|    loss                 | 0.0722      |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 305      |
|    time_elapsed    | 108021   |
|    total_timesteps | 18739200 |
---------------------------------
Eval num_timesteps=18739505, episode_reward=0.03 +/- 0.99
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.026       |
| time/                   |             |
|    total_timesteps      | 18739505    |
| train/                  |             |
|    approx_kl            | 0.015475394 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.31e-05    |
|    loss                 | 0.0887      |
|    n_updates            | 5705        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 306      |
|    time_elapsed    | 108426   |
|    total_timesteps | 18800640 |
---------------------------------
Eval num_timesteps=18800946, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.062        |
| time/                   |              |
|    total_timesteps      | 18800946     |
| train/                  |              |
|    approx_kl            | 0.0153832855 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.69        |
|    explained_variance   | 0.317        |
|    learning_rate        | 7.31e-05     |
|    loss                 | 0.0739       |
|    n_updates            | 5710         |
|    policy_gradient_loss | -0.0175      |
|    value_loss           | 0.24         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 307      |
|    time_elapsed    | 108822   |
|    total_timesteps | 18862080 |
---------------------------------
Eval num_timesteps=18862387, episode_reward=-0.03 +/- 0.99
Episode length: 29.96 +/- 0.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.028       |
| time/                   |              |
|    total_timesteps      | 18862387     |
| train/                  |              |
|    approx_kl            | 0.0152323805 |
|    clip_fraction        | 0.175        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.689       |
|    explained_variance   | 0.33         |
|    learning_rate        | 7.3e-05      |
|    loss                 | 0.0846       |
|    n_updates            | 5715         |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.236        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 308      |
|    time_elapsed    | 109197   |
|    total_timesteps | 18923520 |
---------------------------------
Eval num_timesteps=18923828, episode_reward=0.07 +/- 0.99
Episode length: 30.04 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.07         |
| time/                   |              |
|    total_timesteps      | 18923828     |
| train/                  |              |
|    approx_kl            | 0.0156245865 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.692       |
|    explained_variance   | 0.32         |
|    learning_rate        | 7.3e-05      |
|    loss                 | 0.0338       |
|    n_updates            | 5720         |
|    policy_gradient_loss | -0.0179      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 173      |
|    iterations      | 309      |
|    time_elapsed    | 109567   |
|    total_timesteps | 18984960 |
---------------------------------
Eval num_timesteps=18985269, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.066       |
| time/                   |             |
|    total_timesteps      | 18985269    |
| train/                  |             |
|    approx_kl            | 0.015163052 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.323       |
|    learning_rate        | 7.29e-05    |
|    loss                 | 0.0857      |
|    n_updates            | 5725        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 310      |
|    time_elapsed    | 109996   |
|    total_timesteps | 19046400 |
---------------------------------
Eval num_timesteps=19046710, episode_reward=0.06 +/- 0.98
Episode length: 30.02 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.064       |
| time/                   |             |
|    total_timesteps      | 19046710    |
| train/                  |             |
|    approx_kl            | 0.015550792 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7.29e-05    |
|    loss                 | 0.0785      |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 311      |
|    time_elapsed    | 110406   |
|    total_timesteps | 19107840 |
---------------------------------
Eval num_timesteps=19108151, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.112       |
| time/                   |             |
|    total_timesteps      | 19108151    |
| train/                  |             |
|    approx_kl            | 0.015385026 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.688      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.28e-05    |
|    loss                 | 0.0861      |
|    n_updates            | 5735        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 312      |
|    time_elapsed    | 110780   |
|    total_timesteps | 19169280 |
---------------------------------
Eval num_timesteps=19169592, episode_reward=-0.04 +/- 0.98
Episode length: 30.00 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.038       |
| time/                   |              |
|    total_timesteps      | 19169592     |
| train/                  |              |
|    approx_kl            | 0.0153350085 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.687       |
|    explained_variance   | 0.326        |
|    learning_rate        | 7.27e-05     |
|    loss                 | 0.0415       |
|    n_updates            | 5740         |
|    policy_gradient_loss | -0.0175      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 313      |
|    time_elapsed    | 111176   |
|    total_timesteps | 19230720 |
---------------------------------
Eval num_timesteps=19231033, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19231033    |
| train/                  |             |
|    approx_kl            | 0.015406829 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.27e-05    |
|    loss                 | 0.0658      |
|    n_updates            | 5745        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 314      |
|    time_elapsed    | 111585   |
|    total_timesteps | 19292160 |
---------------------------------
Eval num_timesteps=19292474, episode_reward=-0.01 +/- 0.98
Episode length: 29.98 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.01       |
| time/                   |             |
|    total_timesteps      | 19292474    |
| train/                  |             |
|    approx_kl            | 0.015493517 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.26e-05    |
|    loss                 | 0.0878      |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 315      |
|    time_elapsed    | 111952   |
|    total_timesteps | 19353600 |
---------------------------------
Eval num_timesteps=19353915, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 19353915    |
| train/                  |             |
|    approx_kl            | 0.015578709 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.686      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.26e-05    |
|    loss                 | 0.0876      |
|    n_updates            | 5755        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 316      |
|    time_elapsed    | 112324   |
|    total_timesteps | 19415040 |
---------------------------------
Eval num_timesteps=19415356, episode_reward=0.05 +/- 0.98
Episode length: 30.02 +/- 0.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.048      |
| time/                   |            |
|    total_timesteps      | 19415356   |
| train/                  |            |
|    approx_kl            | 0.01523244 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.685     |
|    explained_variance   | 0.301      |
|    learning_rate        | 7.25e-05   |
|    loss                 | 0.0867     |
|    n_updates            | 5760       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 317      |
|    time_elapsed    | 112691   |
|    total_timesteps | 19476480 |
---------------------------------
Eval num_timesteps=19476797, episode_reward=0.05 +/- 0.98
Episode length: 30.03 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 19476797    |
| train/                  |             |
|    approx_kl            | 0.015416462 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.322       |
|    learning_rate        | 7.25e-05    |
|    loss                 | 0.0756      |
|    n_updates            | 5765        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 318      |
|    time_elapsed    | 113136   |
|    total_timesteps | 19537920 |
---------------------------------
Eval num_timesteps=19538238, episode_reward=-0.02 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 19538238    |
| train/                  |             |
|    approx_kl            | 0.015193894 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.321       |
|    learning_rate        | 7.24e-05    |
|    loss                 | 0.0765      |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 319      |
|    time_elapsed    | 113527   |
|    total_timesteps | 19599360 |
---------------------------------
Eval num_timesteps=19599679, episode_reward=0.05 +/- 0.99
Episode length: 30.02 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.05         |
| time/                   |              |
|    total_timesteps      | 19599679     |
| train/                  |              |
|    approx_kl            | 0.0153015265 |
|    clip_fraction        | 0.173        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.683       |
|    explained_variance   | 0.298        |
|    learning_rate        | 7.24e-05     |
|    loss                 | 0.0853       |
|    n_updates            | 5775         |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 320      |
|    time_elapsed    | 113907   |
|    total_timesteps | 19660800 |
---------------------------------
Eval num_timesteps=19661120, episode_reward=0.03 +/- 0.98
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.034      |
| time/                   |            |
|    total_timesteps      | 19661120   |
| train/                  |            |
|    approx_kl            | 0.01515793 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.681     |
|    explained_variance   | 0.319      |
|    learning_rate        | 7.23e-05   |
|    loss                 | 0.0988     |
|    n_updates            | 5780       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 321      |
|    time_elapsed    | 114326   |
|    total_timesteps | 19722240 |
---------------------------------
Eval num_timesteps=19722561, episode_reward=0.11 +/- 0.98
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.108       |
| time/                   |             |
|    total_timesteps      | 19722561    |
| train/                  |             |
|    approx_kl            | 0.015240681 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.22e-05    |
|    loss                 | 0.0792      |
|    n_updates            | 5785        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 322      |
|    time_elapsed    | 114700   |
|    total_timesteps | 19783680 |
---------------------------------
Eval num_timesteps=19784002, episode_reward=0.11 +/- 0.98
Episode length: 30.03 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.11         |
| time/                   |              |
|    total_timesteps      | 19784002     |
| train/                  |              |
|    approx_kl            | 0.0155166825 |
|    clip_fraction        | 0.173        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.681       |
|    explained_variance   | 0.32         |
|    learning_rate        | 7.22e-05     |
|    loss                 | 0.0681       |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.0177      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 323      |
|    time_elapsed    | 115072   |
|    total_timesteps | 19845120 |
---------------------------------
Eval num_timesteps=19845443, episode_reward=0.09 +/- 0.98
Episode length: 30.04 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.09        |
| time/                   |             |
|    total_timesteps      | 19845443    |
| train/                  |             |
|    approx_kl            | 0.015085061 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.682      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.21e-05    |
|    loss                 | 0.0604      |
|    n_updates            | 5795        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 324      |
|    time_elapsed    | 115448   |
|    total_timesteps | 19906560 |
---------------------------------
Eval num_timesteps=19906884, episode_reward=0.10 +/- 0.98
Episode length: 30.07 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.098       |
| time/                   |             |
|    total_timesteps      | 19906884    |
| train/                  |             |
|    approx_kl            | 0.014889121 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7.21e-05    |
|    loss                 | 0.105       |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 325      |
|    time_elapsed    | 115834   |
|    total_timesteps | 19968000 |
---------------------------------
Eval num_timesteps=19968325, episode_reward=0.16 +/- 0.97
Episode length: 30.04 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.158       |
| time/                   |             |
|    total_timesteps      | 19968325    |
| train/                  |             |
|    approx_kl            | 0.015056029 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.683      |
|    explained_variance   | 0.323       |
|    learning_rate        | 7.2e-05     |
|    loss                 | 0.0503      |
|    n_updates            | 5805        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.158
SELFPLAY: new best model, bumping up generation to 9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 326      |
|    time_elapsed    | 116280   |
|    total_timesteps | 20029440 |
---------------------------------
Eval num_timesteps=20029766, episode_reward=-0.05 +/- 0.98
Episode length: 29.94 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.048      |
| time/                   |             |
|    total_timesteps      | 20029766    |
| train/                  |             |
|    approx_kl            | 0.015157403 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.2e-05     |
|    loss                 | 0.0617      |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 327      |
|    time_elapsed    | 116655   |
|    total_timesteps | 20090880 |
---------------------------------
Eval num_timesteps=20091207, episode_reward=-0.04 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.044      |
| time/                   |             |
|    total_timesteps      | 20091207    |
| train/                  |             |
|    approx_kl            | 0.015086828 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.674      |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.19e-05    |
|    loss                 | 0.0791      |
|    n_updates            | 5815        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 328      |
|    time_elapsed    | 117086   |
|    total_timesteps | 20152320 |
---------------------------------
Eval num_timesteps=20152648, episode_reward=-0.02 +/- 0.99
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.016      |
| time/                   |             |
|    total_timesteps      | 20152648    |
| train/                  |             |
|    approx_kl            | 0.015067237 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.672      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.19e-05    |
|    loss                 | 0.0694      |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 329      |
|    time_elapsed    | 117455   |
|    total_timesteps | 20213760 |
---------------------------------
Eval num_timesteps=20214089, episode_reward=0.12 +/- 0.98
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.122       |
| time/                   |             |
|    total_timesteps      | 20214089    |
| train/                  |             |
|    approx_kl            | 0.014592668 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.323       |
|    learning_rate        | 7.18e-05    |
|    loss                 | 0.0537      |
|    n_updates            | 5825        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 330      |
|    time_elapsed    | 117830   |
|    total_timesteps | 20275200 |
---------------------------------
Eval num_timesteps=20275530, episode_reward=0.12 +/- 0.98
Episode length: 30.05 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.118       |
| time/                   |             |
|    total_timesteps      | 20275530    |
| train/                  |             |
|    approx_kl            | 0.015021382 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.332       |
|    learning_rate        | 7.18e-05    |
|    loss                 | 0.073       |
|    n_updates            | 5830        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 331      |
|    time_elapsed    | 118202   |
|    total_timesteps | 20336640 |
---------------------------------
Eval num_timesteps=20336971, episode_reward=-0.03 +/- 0.98
Episode length: 29.91 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.03       |
| time/                   |             |
|    total_timesteps      | 20336971    |
| train/                  |             |
|    approx_kl            | 0.015195944 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.32        |
|    learning_rate        | 7.17e-05    |
|    loss                 | 0.0687      |
|    n_updates            | 5835        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 332      |
|    time_elapsed    | 118579   |
|    total_timesteps | 20398080 |
---------------------------------
Eval num_timesteps=20398412, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.084       |
| time/                   |             |
|    total_timesteps      | 20398412    |
| train/                  |             |
|    approx_kl            | 0.015046009 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.668      |
|    explained_variance   | 0.319       |
|    learning_rate        | 7.16e-05    |
|    loss                 | 0.119       |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 333      |
|    time_elapsed    | 118982   |
|    total_timesteps | 20459520 |
---------------------------------
Eval num_timesteps=20459853, episode_reward=-0.01 +/- 0.98
Episode length: 30.01 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 20459853    |
| train/                  |             |
|    approx_kl            | 0.015251636 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.16e-05    |
|    loss                 | 0.051       |
|    n_updates            | 5845        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 334      |
|    time_elapsed    | 119419   |
|    total_timesteps | 20520960 |
---------------------------------
Eval num_timesteps=20521294, episode_reward=0.05 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.05        |
| time/                   |             |
|    total_timesteps      | 20521294    |
| train/                  |             |
|    approx_kl            | 0.015302369 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.15e-05    |
|    loss                 | 0.0846      |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 171      |
|    iterations      | 335      |
|    time_elapsed    | 119839   |
|    total_timesteps | 20582400 |
---------------------------------
Eval num_timesteps=20582735, episode_reward=0.03 +/- 0.97
Episode length: 30.02 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 20582735    |
| train/                  |             |
|    approx_kl            | 0.015404095 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.661      |
|    explained_variance   | 0.325       |
|    learning_rate        | 7.15e-05    |
|    loss                 | 0.0895      |
|    n_updates            | 5855        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 336      |
|    time_elapsed    | 120219   |
|    total_timesteps | 20643840 |
---------------------------------
Eval num_timesteps=20644176, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 20644176    |
| train/                  |             |
|    approx_kl            | 0.015382199 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.14e-05    |
|    loss                 | 0.0836      |
|    n_updates            | 5860        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 337      |
|    time_elapsed    | 120591   |
|    total_timesteps | 20705280 |
---------------------------------
Eval num_timesteps=20705617, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.052      |
| time/                   |            |
|    total_timesteps      | 20705617   |
| train/                  |            |
|    approx_kl            | 0.01504013 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.662     |
|    explained_variance   | 0.313      |
|    learning_rate        | 7.14e-05   |
|    loss                 | 0.0621     |
|    n_updates            | 5865       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 338      |
|    time_elapsed    | 120963   |
|    total_timesteps | 20766720 |
---------------------------------
Eval num_timesteps=20767058, episode_reward=0.01 +/- 0.98
Episode length: 30.03 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.014       |
| time/                   |             |
|    total_timesteps      | 20767058    |
| train/                  |             |
|    approx_kl            | 0.015071299 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.311       |
|    learning_rate        | 7.13e-05    |
|    loss                 | 0.0677      |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 339      |
|    time_elapsed    | 121337   |
|    total_timesteps | 20828160 |
---------------------------------
Eval num_timesteps=20828499, episode_reward=0.00 +/- 0.98
Episode length: 29.98 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.002       |
| time/                   |             |
|    total_timesteps      | 20828499    |
| train/                  |             |
|    approx_kl            | 0.014437424 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.334       |
|    learning_rate        | 7.13e-05    |
|    loss                 | 0.0791      |
|    n_updates            | 5875        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 340      |
|    time_elapsed    | 121704   |
|    total_timesteps | 20889600 |
---------------------------------
Eval num_timesteps=20889940, episode_reward=0.03 +/- 0.98
Episode length: 29.97 +/- 1.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.03        |
| time/                   |             |
|    total_timesteps      | 20889940    |
| train/                  |             |
|    approx_kl            | 0.015074795 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.306       |
|    learning_rate        | 7.12e-05    |
|    loss                 | 0.119       |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 341      |
|    time_elapsed    | 122095   |
|    total_timesteps | 20951040 |
---------------------------------
Eval num_timesteps=20951381, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 20951381    |
| train/                  |             |
|    approx_kl            | 0.014812457 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.11e-05    |
|    loss                 | 0.102       |
|    n_updates            | 5885        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.04    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 342      |
|    time_elapsed    | 122414   |
|    total_timesteps | 21012480 |
---------------------------------
Eval num_timesteps=21012822, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 21012822    |
| train/                  |             |
|    approx_kl            | 0.014978526 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.11e-05    |
|    loss                 | 0.0762      |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 343      |
|    time_elapsed    | 122735   |
|    total_timesteps | 21073920 |
---------------------------------
Eval num_timesteps=21074263, episode_reward=-0.04 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.04       |
| time/                   |             |
|    total_timesteps      | 21074263    |
| train/                  |             |
|    approx_kl            | 0.014676273 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.315       |
|    learning_rate        | 7.1e-05     |
|    loss                 | 0.0802      |
|    n_updates            | 5895        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 344      |
|    time_elapsed    | 123056   |
|    total_timesteps | 21135360 |
---------------------------------
Eval num_timesteps=21135704, episode_reward=-0.00 +/- 0.98
Episode length: 30.01 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.004      |
| time/                   |             |
|    total_timesteps      | 21135704    |
| train/                  |             |
|    approx_kl            | 0.014875488 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.304       |
|    learning_rate        | 7.1e-05     |
|    loss                 | 0.0984      |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 345      |
|    time_elapsed    | 123379   |
|    total_timesteps | 21196800 |
---------------------------------
Eval num_timesteps=21197145, episode_reward=-0.05 +/- 0.98
Episode length: 30.02 +/- 0.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.048      |
| time/                   |             |
|    total_timesteps      | 21197145    |
| train/                  |             |
|    approx_kl            | 0.014978672 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.312       |
|    learning_rate        | 7.09e-05    |
|    loss                 | 0.0825      |
|    n_updates            | 5905        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 346      |
|    time_elapsed    | 123698   |
|    total_timesteps | 21258240 |
---------------------------------
Eval num_timesteps=21258586, episode_reward=-0.08 +/- 0.98
Episode length: 29.99 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.076      |
| time/                   |             |
|    total_timesteps      | 21258586    |
| train/                  |             |
|    approx_kl            | 0.014922309 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7.09e-05    |
|    loss                 | 0.0726      |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 347      |
|    time_elapsed    | 124016   |
|    total_timesteps | 21319680 |
---------------------------------
Eval num_timesteps=21320027, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.08        |
| time/                   |             |
|    total_timesteps      | 21320027    |
| train/                  |             |
|    approx_kl            | 0.014794475 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.316       |
|    learning_rate        | 7.08e-05    |
|    loss                 | 0.077       |
|    n_updates            | 5915        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 348      |
|    time_elapsed    | 124333   |
|    total_timesteps | 21381120 |
---------------------------------
Eval num_timesteps=21381468, episode_reward=0.05 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.052       |
| time/                   |             |
|    total_timesteps      | 21381468    |
| train/                  |             |
|    approx_kl            | 0.014702203 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.661      |
|    explained_variance   | 0.305       |
|    learning_rate        | 7.08e-05    |
|    loss                 | 0.0794      |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 349      |
|    time_elapsed    | 124650   |
|    total_timesteps | 21442560 |
---------------------------------
Eval num_timesteps=21442909, episode_reward=0.01 +/- 0.98
Episode length: 29.95 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 21442909    |
| train/                  |             |
|    approx_kl            | 0.014870254 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.314       |
|    learning_rate        | 7.07e-05    |
|    loss                 | 0.133       |
|    n_updates            | 5925        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 350      |
|    time_elapsed    | 124967   |
|    total_timesteps | 21504000 |
---------------------------------
Eval num_timesteps=21504350, episode_reward=0.11 +/- 0.98
Episode length: 29.94 +/- 1.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.106       |
| time/                   |             |
|    total_timesteps      | 21504350    |
| train/                  |             |
|    approx_kl            | 0.015278561 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.324       |
|    learning_rate        | 7.06e-05    |
|    loss                 | 0.157       |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 351      |
|    time_elapsed    | 125284   |
|    total_timesteps | 21565440 |
---------------------------------
Eval num_timesteps=21565791, episode_reward=0.06 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.056       |
| time/                   |             |
|    total_timesteps      | 21565791    |
| train/                  |             |
|    approx_kl            | 0.014660213 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.06e-05    |
|    loss                 | 0.0427      |
|    n_updates            | 5935        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 352      |
|    time_elapsed    | 125601   |
|    total_timesteps | 21626880 |
---------------------------------
Eval num_timesteps=21627232, episode_reward=0.09 +/- 0.97
Episode length: 30.01 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.088        |
| time/                   |              |
|    total_timesteps      | 21627232     |
| train/                  |              |
|    approx_kl            | 0.0147427665 |
|    clip_fraction        | 0.171        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.664       |
|    explained_variance   | 0.301        |
|    learning_rate        | 7.05e-05     |
|    loss                 | 0.0474       |
|    n_updates            | 5940         |
|    policy_gradient_loss | -0.0177      |
|    value_loss           | 0.241        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 353      |
|    time_elapsed    | 125920   |
|    total_timesteps | 21688320 |
---------------------------------
Eval num_timesteps=21688673, episode_reward=0.01 +/- 0.98
Episode length: 30.00 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.01        |
| time/                   |             |
|    total_timesteps      | 21688673    |
| train/                  |             |
|    approx_kl            | 0.014473651 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.323       |
|    learning_rate        | 7.05e-05    |
|    loss                 | 0.118       |
|    n_updates            | 5945        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 354      |
|    time_elapsed    | 126239   |
|    total_timesteps | 21749760 |
---------------------------------
Eval num_timesteps=21750114, episode_reward=0.10 +/- 0.97
Episode length: 29.94 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.104       |
| time/                   |             |
|    total_timesteps      | 21750114    |
| train/                  |             |
|    approx_kl            | 0.015007009 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.661      |
|    explained_variance   | 0.318       |
|    learning_rate        | 7.04e-05    |
|    loss                 | 0.132       |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 355      |
|    time_elapsed    | 126560   |
|    total_timesteps | 21811200 |
---------------------------------
Eval num_timesteps=21811555, episode_reward=0.05 +/- 0.98
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 21811555    |
| train/                  |             |
|    approx_kl            | 0.014706322 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.313       |
|    learning_rate        | 7.04e-05    |
|    loss                 | 0.102       |
|    n_updates            | 5955        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 356      |
|    time_elapsed    | 126882   |
|    total_timesteps | 21872640 |
---------------------------------
Eval num_timesteps=21872996, episode_reward=0.04 +/- 0.98
Episode length: 30.00 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 21872996    |
| train/                  |             |
|    approx_kl            | 0.014675486 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.652      |
|    explained_variance   | 0.3         |
|    learning_rate        | 7.03e-05    |
|    loss                 | 0.119       |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 357      |
|    time_elapsed    | 127203   |
|    total_timesteps | 21934080 |
---------------------------------
Eval num_timesteps=21934437, episode_reward=0.04 +/- 0.98
Episode length: 30.02 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.044       |
| time/                   |             |
|    total_timesteps      | 21934437    |
| train/                  |             |
|    approx_kl            | 0.014994641 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.309       |
|    learning_rate        | 7.03e-05    |
|    loss                 | 0.0642      |
|    n_updates            | 5965        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 358      |
|    time_elapsed    | 127520   |
|    total_timesteps | 21995520 |
---------------------------------
Eval num_timesteps=21995878, episode_reward=-0.05 +/- 0.98
Episode length: 29.89 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.054      |
| time/                   |             |
|    total_timesteps      | 21995878    |
| train/                  |             |
|    approx_kl            | 0.014904498 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.307       |
|    learning_rate        | 7.02e-05    |
|    loss                 | 0.0657      |
|    n_updates            | 5970        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 359      |
|    time_elapsed    | 127838   |
|    total_timesteps | 22056960 |
---------------------------------
Eval num_timesteps=22057319, episode_reward=0.03 +/- 0.98
Episode length: 30.03 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.034       |
| time/                   |             |
|    total_timesteps      | 22057319    |
| train/                  |             |
|    approx_kl            | 0.014503286 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.315       |
|    learning_rate        | 7.01e-05    |
|    loss                 | 0.104       |
|    n_updates            | 5975        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 360      |
|    time_elapsed    | 128155   |
|    total_timesteps | 22118400 |
---------------------------------
Eval num_timesteps=22118760, episode_reward=0.08 +/- 0.98
Episode length: 30.02 +/- 0.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.078      |
| time/                   |            |
|    total_timesteps      | 22118760   |
| train/                  |            |
|    approx_kl            | 0.01456124 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.654     |
|    explained_variance   | 0.305      |
|    learning_rate        | 7.01e-05   |
|    loss                 | 0.0883     |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.239      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 361      |
|    time_elapsed    | 128472   |
|    total_timesteps | 22179840 |
---------------------------------
Eval num_timesteps=22180201, episode_reward=0.09 +/- 0.98
Episode length: 30.00 +/- 1.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.094      |
| time/                   |            |
|    total_timesteps      | 22180201   |
| train/                  |            |
|    approx_kl            | 0.01473254 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.658     |
|    explained_variance   | 0.308      |
|    learning_rate        | 7e-05      |
|    loss                 | 0.0609     |
|    n_updates            | 5985       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.242      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 362      |
|    time_elapsed    | 128788   |
|    total_timesteps | 22241280 |
---------------------------------
Eval num_timesteps=22241642, episode_reward=0.07 +/- 0.98
Episode length: 30.03 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 22241642    |
| train/                  |             |
|    approx_kl            | 0.014895331 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.327       |
|    learning_rate        | 7e-05       |
|    loss                 | 0.045       |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 363      |
|    time_elapsed    | 129105   |
|    total_timesteps | 22302720 |
---------------------------------
Eval num_timesteps=22303083, episode_reward=0.09 +/- 0.98
Episode length: 30.02 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.094       |
| time/                   |             |
|    total_timesteps      | 22303083    |
| train/                  |             |
|    approx_kl            | 0.014553682 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.653      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.99e-05    |
|    loss                 | 0.0581      |
|    n_updates            | 5995        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 364      |
|    time_elapsed    | 129426   |
|    total_timesteps | 22364160 |
---------------------------------
Eval num_timesteps=22364524, episode_reward=0.14 +/- 0.97
Episode length: 30.04 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 22364524    |
| train/                  |             |
|    approx_kl            | 0.014773343 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.318       |
|    learning_rate        | 6.99e-05    |
|    loss                 | 0.0856      |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 365      |
|    time_elapsed    | 129784   |
|    total_timesteps | 22425600 |
---------------------------------
Eval num_timesteps=22425965, episode_reward=-0.06 +/- 0.98
Episode length: 29.86 +/- 1.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.064      |
| time/                   |             |
|    total_timesteps      | 22425965    |
| train/                  |             |
|    approx_kl            | 0.014774977 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.307       |
|    learning_rate        | 6.98e-05    |
|    loss                 | 0.0688      |
|    n_updates            | 6005        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 366      |
|    time_elapsed    | 130199   |
|    total_timesteps | 22487040 |
---------------------------------
Eval num_timesteps=22487406, episode_reward=-0.02 +/- 0.98
Episode length: 29.88 +/- 1.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.018      |
| time/                   |             |
|    total_timesteps      | 22487406    |
| train/                  |             |
|    approx_kl            | 0.014741447 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.306       |
|    learning_rate        | 6.98e-05    |
|    loss                 | 0.0739      |
|    n_updates            | 6010        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 367      |
|    time_elapsed    | 130564   |
|    total_timesteps | 22548480 |
---------------------------------
Eval num_timesteps=22548847, episode_reward=-0.05 +/- 0.97
Episode length: 29.95 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.052      |
| time/                   |             |
|    total_timesteps      | 22548847    |
| train/                  |             |
|    approx_kl            | 0.014708121 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.314       |
|    learning_rate        | 6.97e-05    |
|    loss                 | 0.0886      |
|    n_updates            | 6015        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 368      |
|    time_elapsed    | 130975   |
|    total_timesteps | 22609920 |
---------------------------------
Eval num_timesteps=22610288, episode_reward=-0.09 +/- 0.98
Episode length: 29.87 +/- 1.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.9       |
|    mean_reward          | -0.088     |
| time/                   |            |
|    total_timesteps      | 22610288   |
| train/                  |            |
|    approx_kl            | 0.01478655 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.324      |
|    learning_rate        | 6.97e-05   |
|    loss                 | 0.0867     |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.235      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 369      |
|    time_elapsed    | 131340   |
|    total_timesteps | 22671360 |
---------------------------------
Eval num_timesteps=22671729, episode_reward=-0.03 +/- 0.98
Episode length: 29.99 +/- 0.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.028      |
| time/                   |             |
|    total_timesteps      | 22671729    |
| train/                  |             |
|    approx_kl            | 0.014446701 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.324       |
|    learning_rate        | 6.96e-05    |
|    loss                 | 0.134       |
|    n_updates            | 6025        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 370      |
|    time_elapsed    | 131773   |
|    total_timesteps | 22732800 |
---------------------------------
Eval num_timesteps=22733170, episode_reward=0.03 +/- 0.99
Episode length: 29.98 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 22733170    |
| train/                  |             |
|    approx_kl            | 0.014967029 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.326       |
|    learning_rate        | 6.95e-05    |
|    loss                 | 0.0836      |
|    n_updates            | 6030        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 172      |
|    iterations      | 371      |
|    time_elapsed    | 132196   |
|    total_timesteps | 22794240 |
---------------------------------
Eval num_timesteps=22794611, episode_reward=0.03 +/- 0.99
Episode length: 29.97 +/- 1.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.032       |
| time/                   |             |
|    total_timesteps      | 22794611    |
| train/                  |             |
|    approx_kl            | 0.014599546 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.312       |
|    learning_rate        | 6.95e-05    |
|    loss                 | 0.114       |
|    n_updates            | 6035        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 372      |
|    time_elapsed    | 132647   |
|    total_timesteps | 22855680 |
---------------------------------
Eval num_timesteps=22856052, episode_reward=0.00 +/- 0.99
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 22856052    |
| train/                  |             |
|    approx_kl            | 0.014296573 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.327       |
|    learning_rate        | 6.94e-05    |
|    loss                 | 0.0865      |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 373      |
|    time_elapsed    | 133054   |
|    total_timesteps | 22917120 |
---------------------------------
Eval num_timesteps=22917493, episode_reward=-0.00 +/- 0.97
Episode length: 29.96 +/- 1.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.004       |
| time/                   |              |
|    total_timesteps      | 22917493     |
| train/                  |              |
|    approx_kl            | 0.0145158265 |
|    clip_fraction        | 0.165        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.658       |
|    explained_variance   | 0.316        |
|    learning_rate        | 6.94e-05     |
|    loss                 | 0.0694       |
|    n_updates            | 6045         |
|    policy_gradient_loss | -0.0172      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 374      |
|    time_elapsed    | 133525   |
|    total_timesteps | 22978560 |
---------------------------------
Eval num_timesteps=22978934, episode_reward=-0.06 +/- 0.99
Episode length: 29.97 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.058      |
| time/                   |             |
|    total_timesteps      | 22978934    |
| train/                  |             |
|    approx_kl            | 0.015041472 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.319       |
|    learning_rate        | 6.93e-05    |
|    loss                 | 0.0922      |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 375      |
|    time_elapsed    | 133927   |
|    total_timesteps | 23040000 |
---------------------------------
Eval num_timesteps=23040375, episode_reward=0.04 +/- 0.99
Episode length: 29.95 +/- 1.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.04        |
| time/                   |             |
|    total_timesteps      | 23040375    |
| train/                  |             |
|    approx_kl            | 0.014385251 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.93e-05    |
|    loss                 | 0.0798      |
|    n_updates            | 6055        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 376      |
|    time_elapsed    | 134405   |
|    total_timesteps | 23101440 |
---------------------------------
Eval num_timesteps=23101816, episode_reward=0.02 +/- 0.99
Episode length: 29.97 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 23101816    |
| train/                  |             |
|    approx_kl            | 0.014703153 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.317       |
|    learning_rate        | 6.92e-05    |
|    loss                 | 0.0418      |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 377      |
|    time_elapsed    | 134820   |
|    total_timesteps | 23162880 |
---------------------------------
Eval num_timesteps=23163257, episode_reward=0.02 +/- 0.98
Episode length: 29.95 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | 0.018       |
| time/                   |             |
|    total_timesteps      | 23163257    |
| train/                  |             |
|    approx_kl            | 0.014742373 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.32        |
|    learning_rate        | 6.92e-05    |
|    loss                 | 0.0829      |
|    n_updates            | 6065        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 378      |
|    time_elapsed    | 135288   |
|    total_timesteps | 23224320 |
---------------------------------
Eval num_timesteps=23224698, episode_reward=0.02 +/- 0.98
Episode length: 29.99 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 23224698    |
| train/                  |             |
|    approx_kl            | 0.014685605 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.324       |
|    learning_rate        | 6.91e-05    |
|    loss                 | 0.0743      |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 379      |
|    time_elapsed    | 135706   |
|    total_timesteps | 23285760 |
---------------------------------
Eval num_timesteps=23286139, episode_reward=0.09 +/- 0.98
Episode length: 30.01 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 0.086      |
| time/                   |            |
|    total_timesteps      | 23286139   |
| train/                  |            |
|    approx_kl            | 0.01460619 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.656     |
|    explained_variance   | 0.325      |
|    learning_rate        | 6.9e-05    |
|    loss                 | 0.112      |
|    n_updates            | 6075       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 380      |
|    time_elapsed    | 136154   |
|    total_timesteps | 23347200 |
---------------------------------
Eval num_timesteps=23347580, episode_reward=0.10 +/- 0.98
Episode length: 30.04 +/- 0.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.096        |
| time/                   |              |
|    total_timesteps      | 23347580     |
| train/                  |              |
|    approx_kl            | 0.0147876525 |
|    clip_fraction        | 0.168        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.657       |
|    explained_variance   | 0.316        |
|    learning_rate        | 6.9e-05      |
|    loss                 | 0.0606       |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.0173      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 171      |
|    iterations      | 381      |
|    time_elapsed    | 136579   |
|    total_timesteps | 23408640 |
---------------------------------
Eval num_timesteps=23409021, episode_reward=-0.03 +/- 0.99
Episode length: 30.02 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.026     |
| time/                   |            |
|    total_timesteps      | 23409021   |
| train/                  |            |
|    approx_kl            | 0.01456916 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.648     |
|    explained_variance   | 0.305      |
|    learning_rate        | 6.89e-05   |
|    loss                 | 0.0545     |
|    n_updates            | 6085       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 171      |
|    iterations      | 382      |
|    time_elapsed    | 137021   |
|    total_timesteps | 23470080 |
---------------------------------
Eval num_timesteps=23470462, episode_reward=0.06 +/- 0.98
Episode length: 29.96 +/- 1.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.06        |
| time/                   |             |
|    total_timesteps      | 23470462    |
| train/                  |             |
|    approx_kl            | 0.014174852 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.331       |
|    learning_rate        | 6.89e-05    |
|    loss                 | 0.126       |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 383      |
|    time_elapsed    | 137456   |
|    total_timesteps | 23531520 |
---------------------------------
Eval num_timesteps=23531903, episode_reward=0.03 +/- 0.98
Episode length: 29.98 +/- 1.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.028       |
| time/                   |             |
|    total_timesteps      | 23531903    |
| train/                  |             |
|    approx_kl            | 0.014534267 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.88e-05    |
|    loss                 | 0.0506      |
|    n_updates            | 6095        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.03     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 384      |
|    time_elapsed    | 137882   |
|    total_timesteps | 23592960 |
---------------------------------
Eval num_timesteps=23593344, episode_reward=0.07 +/- 0.98
Episode length: 29.99 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 23593344    |
| train/                  |             |
|    approx_kl            | 0.014517859 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.316       |
|    learning_rate        | 6.88e-05    |
|    loss                 | 0.0541      |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 385      |
|    time_elapsed    | 138329   |
|    total_timesteps | 23654400 |
---------------------------------
Eval num_timesteps=23654785, episode_reward=-0.00 +/- 0.98
Episode length: 29.98 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.002      |
| time/                   |             |
|    total_timesteps      | 23654785    |
| train/                  |             |
|    approx_kl            | 0.014168364 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.642      |
|    explained_variance   | 0.316       |
|    learning_rate        | 6.87e-05    |
|    loss                 | 0.112       |
|    n_updates            | 6105        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 386      |
|    time_elapsed    | 138770   |
|    total_timesteps | 23715840 |
---------------------------------
Eval num_timesteps=23716226, episode_reward=0.04 +/- 0.97
Episode length: 29.96 +/- 1.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.038        |
| time/                   |              |
|    total_timesteps      | 23716226     |
| train/                  |              |
|    approx_kl            | 0.0140652545 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.324        |
|    learning_rate        | 6.87e-05     |
|    loss                 | 0.0626       |
|    n_updates            | 6110         |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.238        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 387      |
|    time_elapsed    | 139212   |
|    total_timesteps | 23777280 |
---------------------------------
Eval num_timesteps=23777667, episode_reward=0.03 +/- 0.99
Episode length: 30.02 +/- 0.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.028        |
| time/                   |              |
|    total_timesteps      | 23777667     |
| train/                  |              |
|    approx_kl            | 0.0142207295 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.341        |
|    learning_rate        | 6.86e-05     |
|    loss                 | 0.0578       |
|    n_updates            | 6115         |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 0.233        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 388      |
|    time_elapsed    | 139661   |
|    total_timesteps | 23838720 |
---------------------------------
Eval num_timesteps=23839108, episode_reward=0.08 +/- 0.98
Episode length: 30.00 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.078       |
| time/                   |             |
|    total_timesteps      | 23839108    |
| train/                  |             |
|    approx_kl            | 0.014004255 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.639      |
|    explained_variance   | 0.321       |
|    learning_rate        | 6.85e-05    |
|    loss                 | 0.061       |
|    n_updates            | 6120        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 389      |
|    time_elapsed    | 140089   |
|    total_timesteps | 23900160 |
---------------------------------
Eval num_timesteps=23900549, episode_reward=0.14 +/- 0.98
Episode length: 30.05 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.1        |
|    mean_reward          | 0.144       |
| time/                   |             |
|    total_timesteps      | 23900549    |
| train/                  |             |
|    approx_kl            | 0.014472932 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.639      |
|    explained_variance   | 0.329       |
|    learning_rate        | 6.85e-05    |
|    loss                 | 0.105       |
|    n_updates            | 6125        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.144
SELFPLAY: new best model, bumping up generation to 11
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 390      |
|    time_elapsed    | 140537   |
|    total_timesteps | 23961600 |
---------------------------------
Eval num_timesteps=23961990, episode_reward=0.00 +/- 0.98
Episode length: 29.97 +/- 0.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.002        |
| time/                   |              |
|    total_timesteps      | 23961990     |
| train/                  |              |
|    approx_kl            | 0.0146327475 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.647       |
|    explained_variance   | 0.325        |
|    learning_rate        | 6.84e-05     |
|    loss                 | 0.0679       |
|    n_updates            | 6130         |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 391      |
|    time_elapsed    | 140956   |
|    total_timesteps | 24023040 |
---------------------------------
Eval num_timesteps=24023431, episode_reward=0.11 +/- 0.98
Episode length: 30.00 +/- 0.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 24023431    |
| train/                  |             |
|    approx_kl            | 0.013946053 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.309       |
|    learning_rate        | 6.84e-05    |
|    loss                 | 0.0914      |
|    n_updates            | 6135        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.241       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.08    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 392      |
|    time_elapsed    | 141408   |
|    total_timesteps | 24084480 |
---------------------------------
Eval num_timesteps=24084872, episode_reward=0.07 +/- 0.98
Episode length: 30.01 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 24084872    |
| train/                  |             |
|    approx_kl            | 0.014057559 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.331       |
|    learning_rate        | 6.83e-05    |
|    loss                 | 0.0646      |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 393      |
|    time_elapsed    | 141806   |
|    total_timesteps | 24145920 |
---------------------------------
Eval num_timesteps=24146313, episode_reward=0.06 +/- 0.99
Episode length: 30.03 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.058       |
| time/                   |             |
|    total_timesteps      | 24146313    |
| train/                  |             |
|    approx_kl            | 0.014082391 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.338       |
|    learning_rate        | 6.83e-05    |
|    loss                 | 0.0751      |
|    n_updates            | 6145        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 394      |
|    time_elapsed    | 142280   |
|    total_timesteps | 24207360 |
---------------------------------
Eval num_timesteps=24207754, episode_reward=-0.06 +/- 0.98
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.06       |
| time/                   |             |
|    total_timesteps      | 24207754    |
| train/                  |             |
|    approx_kl            | 0.014121844 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.312       |
|    learning_rate        | 6.82e-05    |
|    loss                 | 0.058       |
|    n_updates            | 6150        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 170      |
|    iterations      | 395      |
|    time_elapsed    | 142704   |
|    total_timesteps | 24268800 |
---------------------------------
Eval num_timesteps=24269195, episode_reward=0.07 +/- 0.98
Episode length: 29.97 +/- 0.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.07        |
| time/                   |             |
|    total_timesteps      | 24269195    |
| train/                  |             |
|    approx_kl            | 0.014075895 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.315       |
|    learning_rate        | 6.82e-05    |
|    loss                 | 0.0368      |
|    n_updates            | 6155        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 396      |
|    time_elapsed    | 143171   |
|    total_timesteps | 24330240 |
---------------------------------
Eval num_timesteps=24330636, episode_reward=0.07 +/- 0.97
Episode length: 29.96 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 24330636    |
| train/                  |             |
|    approx_kl            | 0.014331766 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.81e-05    |
|    loss                 | 0.0837      |
|    n_updates            | 6160        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 397      |
|    time_elapsed    | 143598   |
|    total_timesteps | 24391680 |
---------------------------------
Eval num_timesteps=24392077, episode_reward=-0.01 +/- 0.99
Episode length: 29.98 +/- 0.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.008     |
| time/                   |            |
|    total_timesteps      | 24392077   |
| train/                  |            |
|    approx_kl            | 0.01448957 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.335      |
|    learning_rate        | 6.8e-05    |
|    loss                 | 0.086      |
|    n_updates            | 6165       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.237      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 398      |
|    time_elapsed    | 144057   |
|    total_timesteps | 24453120 |
---------------------------------
Eval num_timesteps=24453518, episode_reward=-0.01 +/- 0.98
Episode length: 29.99 +/- 0.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | -0.006       |
| time/                   |              |
|    total_timesteps      | 24453518     |
| train/                  |              |
|    approx_kl            | 0.0142895095 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.654       |
|    explained_variance   | 0.337        |
|    learning_rate        | 6.8e-05      |
|    loss                 | 0.0952       |
|    n_updates            | 6170         |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 0.239        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 399      |
|    time_elapsed    | 144486   |
|    total_timesteps | 24514560 |
---------------------------------
Eval num_timesteps=24514959, episode_reward=-0.01 +/- 0.99
Episode length: 30.01 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 24514959    |
| train/                  |             |
|    approx_kl            | 0.013856142 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.336       |
|    learning_rate        | 6.79e-05    |
|    loss                 | 0.0953      |
|    n_updates            | 6175        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 400      |
|    time_elapsed    | 144920   |
|    total_timesteps | 24576000 |
---------------------------------
Eval num_timesteps=24576400, episode_reward=0.02 +/- 0.98
Episode length: 30.00 +/- 0.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.024       |
| time/                   |             |
|    total_timesteps      | 24576400    |
| train/                  |             |
|    approx_kl            | 0.013906792 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.336       |
|    learning_rate        | 6.79e-05    |
|    loss                 | 0.0697      |
|    n_updates            | 6180        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 401      |
|    time_elapsed    | 145366   |
|    total_timesteps | 24637440 |
---------------------------------
Eval num_timesteps=24637841, episode_reward=-0.07 +/- 0.98
Episode length: 29.99 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.07       |
| time/                   |             |
|    total_timesteps      | 24637841    |
| train/                  |             |
|    approx_kl            | 0.014482032 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.321       |
|    learning_rate        | 6.78e-05    |
|    loss                 | 0.107       |
|    n_updates            | 6185        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 402      |
|    time_elapsed    | 145784   |
|    total_timesteps | 24698880 |
---------------------------------
Eval num_timesteps=24699282, episode_reward=0.04 +/- 0.98
Episode length: 29.96 +/- 1.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.042       |
| time/                   |             |
|    total_timesteps      | 24699282    |
| train/                  |             |
|    approx_kl            | 0.014364659 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.323       |
|    learning_rate        | 6.78e-05    |
|    loss                 | 0.109       |
|    n_updates            | 6190        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 403      |
|    time_elapsed    | 146254   |
|    total_timesteps | 24760320 |
---------------------------------
Eval num_timesteps=24760723, episode_reward=0.00 +/- 0.99
Episode length: 30.01 +/- 0.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.004       |
| time/                   |             |
|    total_timesteps      | 24760723    |
| train/                  |             |
|    approx_kl            | 0.013792067 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.333       |
|    learning_rate        | 6.77e-05    |
|    loss                 | 0.0376      |
|    n_updates            | 6195        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 404      |
|    time_elapsed    | 146648   |
|    total_timesteps | 24821760 |
---------------------------------
Eval num_timesteps=24822164, episode_reward=-0.02 +/- 0.98
Episode length: 29.97 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.022      |
| time/                   |             |
|    total_timesteps      | 24822164    |
| train/                  |             |
|    approx_kl            | 0.014355223 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.77e-05    |
|    loss                 | 0.0767      |
|    n_updates            | 6200        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.03    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 405      |
|    time_elapsed    | 147136   |
|    total_timesteps | 24883200 |
---------------------------------
Eval num_timesteps=24883605, episode_reward=0.16 +/- 0.97
Episode length: 30.02 +/- 1.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.16        |
| time/                   |             |
|    total_timesteps      | 24883605    |
| train/                  |             |
|    approx_kl            | 0.013608497 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.333       |
|    learning_rate        | 6.76e-05    |
|    loss                 | 0.0795      |
|    n_updates            | 6205        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.238       |
-----------------------------------------
New best mean reward!
SELFPLAY: mean_reward achieved: 0.16
SELFPLAY: new best model, bumping up generation to 12
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 406      |
|    time_elapsed    | 147468   |
|    total_timesteps | 24944640 |
---------------------------------
Eval num_timesteps=24945046, episode_reward=-0.01 +/- 0.99
Episode length: 29.96 +/- 0.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.012      |
| time/                   |             |
|    total_timesteps      | 24945046    |
| train/                  |             |
|    approx_kl            | 0.014124066 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.318       |
|    learning_rate        | 6.75e-05    |
|    loss                 | 0.0542      |
|    n_updates            | 6210        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 407      |
|    time_elapsed    | 147789   |
|    total_timesteps | 25006080 |
---------------------------------
Eval num_timesteps=25006487, episode_reward=-0.08 +/- 0.97
Episode length: 29.98 +/- 0.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.082      |
| time/                   |             |
|    total_timesteps      | 25006487    |
| train/                  |             |
|    approx_kl            | 0.014099171 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.31        |
|    learning_rate        | 6.75e-05    |
|    loss                 | 0.0739      |
|    n_updates            | 6215        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 408      |
|    time_elapsed    | 148111   |
|    total_timesteps | 25067520 |
---------------------------------
Eval num_timesteps=25067928, episode_reward=0.02 +/- 0.99
Episode length: 29.98 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.016       |
| time/                   |             |
|    total_timesteps      | 25067928    |
| train/                  |             |
|    approx_kl            | 0.013853958 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.341       |
|    learning_rate        | 6.74e-05    |
|    loss                 | 0.0479      |
|    n_updates            | 6220        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.233       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 409      |
|    time_elapsed    | 148431   |
|    total_timesteps | 25128960 |
---------------------------------
Eval num_timesteps=25129369, episode_reward=-0.05 +/- 0.99
Episode length: 29.95 +/- 0.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.9        |
|    mean_reward          | -0.054      |
| time/                   |             |
|    total_timesteps      | 25129369    |
| train/                  |             |
|    approx_kl            | 0.013900071 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.335       |
|    learning_rate        | 6.74e-05    |
|    loss                 | 0.131       |
|    n_updates            | 6225        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 410      |
|    time_elapsed    | 148749   |
|    total_timesteps | 25190400 |
---------------------------------
Eval num_timesteps=25190810, episode_reward=0.02 +/- 0.98
Episode length: 29.98 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.02        |
| time/                   |             |
|    total_timesteps      | 25190810    |
| train/                  |             |
|    approx_kl            | 0.013872037 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.325       |
|    learning_rate        | 6.73e-05    |
|    loss                 | 0.0814      |
|    n_updates            | 6230        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 411      |
|    time_elapsed    | 149065   |
|    total_timesteps | 25251840 |
---------------------------------
Eval num_timesteps=25252251, episode_reward=0.11 +/- 0.98
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.114       |
| time/                   |             |
|    total_timesteps      | 25252251    |
| train/                  |             |
|    approx_kl            | 0.013968857 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.322       |
|    learning_rate        | 6.73e-05    |
|    loss                 | 0.0813      |
|    n_updates            | 6235        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 412      |
|    time_elapsed    | 149382   |
|    total_timesteps | 25313280 |
---------------------------------
Eval num_timesteps=25313692, episode_reward=-0.02 +/- 0.98
Episode length: 30.00 +/- 0.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.018     |
| time/                   |            |
|    total_timesteps      | 25313692   |
| train/                  |            |
|    approx_kl            | 0.01364722 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.639     |
|    explained_variance   | 0.318      |
|    learning_rate        | 6.72e-05   |
|    loss                 | 0.064      |
|    n_updates            | 6240       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.241      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 413      |
|    time_elapsed    | 149699   |
|    total_timesteps | 25374720 |
---------------------------------
Eval num_timesteps=25375133, episode_reward=0.05 +/- 0.98
Episode length: 29.97 +/- 0.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.046       |
| time/                   |             |
|    total_timesteps      | 25375133    |
| train/                  |             |
|    approx_kl            | 0.013912547 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.72e-05    |
|    loss                 | 0.0597      |
|    n_updates            | 6245        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 414      |
|    time_elapsed    | 150016   |
|    total_timesteps | 25436160 |
---------------------------------
Eval num_timesteps=25436574, episode_reward=0.05 +/- 0.98
Episode length: 30.01 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30           |
|    mean_reward          | 0.048        |
| time/                   |              |
|    total_timesteps      | 25436574     |
| train/                  |              |
|    approx_kl            | 0.0141609665 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.15         |
|    entropy_loss         | -0.634       |
|    explained_variance   | 0.325        |
|    learning_rate        | 6.71e-05     |
|    loss                 | 0.0428       |
|    n_updates            | 6250         |
|    policy_gradient_loss | -0.0172      |
|    value_loss           | 0.237        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 415      |
|    time_elapsed    | 150333   |
|    total_timesteps | 25497600 |
---------------------------------
Eval num_timesteps=25498015, episode_reward=-0.02 +/- 0.99
Episode length: 29.96 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.02       |
| time/                   |             |
|    total_timesteps      | 25498015    |
| train/                  |             |
|    approx_kl            | 0.014288875 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.635      |
|    explained_variance   | 0.33        |
|    learning_rate        | 6.71e-05    |
|    loss                 | 0.119       |
|    n_updates            | 6255        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 416      |
|    time_elapsed    | 150651   |
|    total_timesteps | 25559040 |
---------------------------------
Eval num_timesteps=25559456, episode_reward=-0.00 +/- 0.98
Episode length: 30.00 +/- 0.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | -0.004     |
| time/                   |            |
|    total_timesteps      | 25559456   |
| train/                  |            |
|    approx_kl            | 0.01416348 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.15       |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.324      |
|    learning_rate        | 6.7e-05    |
|    loss                 | 0.084      |
|    n_updates            | 6260       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.09    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 417      |
|    time_elapsed    | 150970   |
|    total_timesteps | 25620480 |
---------------------------------
Eval num_timesteps=25620897, episode_reward=-0.01 +/- 0.98
Episode length: 30.00 +/- 0.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | -0.014      |
| time/                   |             |
|    total_timesteps      | 25620897    |
| train/                  |             |
|    approx_kl            | 0.013585289 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.632      |
|    explained_variance   | 0.322       |
|    learning_rate        | 6.69e-05    |
|    loss                 | 0.0643      |
|    n_updates            | 6265        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.238       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 169      |
|    iterations      | 418      |
|    time_elapsed    | 151291   |
|    total_timesteps | 25681920 |
---------------------------------
Eval num_timesteps=25682338, episode_reward=0.07 +/- 0.98
Episode length: 30.00 +/- 0.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.068       |
| time/                   |             |
|    total_timesteps      | 25682338    |
| train/                  |             |
|    approx_kl            | 0.014293085 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.328       |
|    learning_rate        | 6.69e-05    |
|    loss                 | 0.0564      |
|    n_updates            | 6270        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.06    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 419      |
|    time_elapsed    | 151613   |
|    total_timesteps | 25743360 |
---------------------------------
Eval num_timesteps=25743779, episode_reward=0.07 +/- 0.99
Episode length: 30.01 +/- 0.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.072       |
| time/                   |             |
|    total_timesteps      | 25743779    |
| train/                  |             |
|    approx_kl            | 0.013686349 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.329       |
|    learning_rate        | 6.68e-05    |
|    loss                 | 0.0443      |
|    n_updates            | 6275        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.237       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 420      |
|    time_elapsed    | 151934   |
|    total_timesteps | 25804800 |
---------------------------------
Eval num_timesteps=25805220, episode_reward=0.02 +/- 0.97
Episode length: 30.00 +/- 0.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 0.022       |
| time/                   |             |
|    total_timesteps      | 25805220    |
| train/                  |             |
|    approx_kl            | 0.013897182 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.15        |
|    entropy_loss         | -0.626      |
|    explained_variance   | 0.33        |
|    learning_rate        | 6.68e-05    |
|    loss                 | 0.0654      |
|    n_updates            | 6280        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.236       |
-----------------------------------------
