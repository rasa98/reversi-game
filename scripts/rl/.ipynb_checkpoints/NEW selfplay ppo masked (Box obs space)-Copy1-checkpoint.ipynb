{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e08b866-f6c3-4b4c-8185-0e847a2568e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/rasa/PycharmProjects/reversi-game/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fdeba6-0cdc-4fa8-8ebb-b6d5634034d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, sync_envs_normalization\n",
    "\n",
    "import stable_baselines3.common.callbacks as callbacks_module\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy as masked_evaluate_policy\n",
    "\n",
    "# Modify the namespace of EvalCallback directly\n",
    "callbacks_module.evaluate_policy = masked_evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "from shutil import copyfile # keep track of generations\n",
    "from collections import OrderedDict\n",
    "\n",
    "from gymnasium.spaces import Discrete, Box, Dict, MultiDiscrete\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "import gymnasium.spaces as spaces\n",
    "from game_logic import Othello\n",
    "import numpy as np\n",
    "import os, math\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23086ab8-3197-4112-9877-fcdb52f351ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SEED = 19\n",
    "NUM_TIMESTEPS = int(30_000_000)\n",
    "EVAL_FREQ = int(20_000)\n",
    "EVAL_EPISODES = int(200)\n",
    "BEST_THRESHOLD = 0.30 # must achieve a mean score above this to replace prev best self\n",
    "\n",
    "RENDER_MODE = False # set this to false if you plan on running for full 1000 trials.\n",
    "\n",
    "LOGDIR = \"ppo_masked/v2/4x_64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "015cd09b-673e-4145-b7b9-2c22763c0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloEnv(gym.Env):   \n",
    "    def __init__(self):\n",
    "        self.game = Othello()\n",
    "        self.agent_turn = 1\n",
    "        shape = self.game.board.shape\n",
    "        self.action_space = Discrete(shape[0] * shape[1])  # sample - [x, y]\n",
    "        # self.observation_space = Dict({\n",
    "        #                                 'board' : Box(0, 2, shape=shape, dtype=int), \n",
    "        #                                 'player': Discrete(2, start=1)\n",
    "        #                               })        \n",
    "        self.observation_space = Box(low=0, high=1, shape=(64*3,), dtype=np.float32)\n",
    "        self.other_agent = None\n",
    "        self.reset_othello_gen = self.reset_othello()    \n",
    "        self.episodes = 0       \n",
    "        \n",
    "\n",
    "    def reset_othello(self):\n",
    "        '''resets game to starting position \n",
    "           and also changes starting player alternatively'''\n",
    "        infinite_player_turn = cycle([1, 2])\n",
    "        while True:\n",
    "            game = Othello()\n",
    "            model_turn = next(infinite_player_turn)\n",
    "            yield game, model_turn\n",
    "    \n",
    "    def change_to_latest_agent(self, agent):\n",
    "        self.other_agent = agent\n",
    "\n",
    "    def get_obs(self):\n",
    "        encoded_board = self.game.get_encoded_state().reshape(-1)\n",
    "        return encoded_board        \n",
    "        \n",
    "    def check_game_ended(self):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        winner = self.game.get_winner()\n",
    "        if winner is not None:\n",
    "            self.episodes += 1\n",
    "            if self.episodes % 10 == 0:\n",
    "                print(f'Ep done - {self.episodes}.')\n",
    "            \n",
    "            done = True\n",
    "            if winner == self.agent_turn:\n",
    "                reward = 1\n",
    "            elif winner == 3 - self.agent_turn: #  other agent turn/figure\n",
    "                reward = -1\n",
    "        return reward, done\n",
    "    \n",
    "    def render(self):  # todo \n",
    "        pass\n",
    "\n",
    "    def close(self):  # todo\n",
    "        pass\n",
    "\n",
    "    def other_agent_play_move(self): \n",
    "        obs = self.get_obs()         \n",
    "        action, _ = self.other_agent.predict(obs, \n",
    "                                             action_masks=self.action_masks(),\n",
    "                                             deterministic=False) \n",
    "        game_action = Othello.get_decoded_field(action)\n",
    "        self.game.play_move(game_action)\n",
    "\n",
    "    def step(self, action):\n",
    "        game_action = Othello.get_decoded_field(action)\n",
    "        self.game.play_move(game_action)\n",
    "\n",
    "        # do self play\n",
    "        while self.game.get_winner() is None and self.game.player_turn != self.agent_turn: #  if game hasnt ended do moves if opponent doesnt have one \n",
    "            self.other_agent_play_move()\n",
    "\n",
    "        reward, done = self.check_game_ended()\n",
    "        info = {}\n",
    "        truncated = False\n",
    "\n",
    "                \n",
    "        # Return step information\n",
    "        return self.get_obs(), reward, done, truncated, info\n",
    "    \n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.game, self.agent_turn = next(self.reset_othello_gen)\n",
    "        if self.agent_turn == 2:\n",
    "            self.other_agent_play_move()\n",
    "        return self.get_obs(), None\n",
    "\n",
    "    def action_masks(self):        \n",
    "        valid_moves = self.game.valid_moves()\n",
    "    \n",
    "        mask = np.zeros(self.game.board.shape, dtype=bool)\n",
    "        \n",
    "        # Set True for each index in the set\n",
    "        for index in valid_moves:\n",
    "            mask[index] = True\n",
    "        mask.flatten()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfdecf41-ef1c-4f12-bbc7-02d06ce2a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayCallback(EvalCallback):\n",
    "    # hacked it to only save new version offrom gymnasium.wrappers import FlattenObservation best model if beats prev self by BEST_THRESHOLD score\n",
    "    # after saving model, resets the best score to be BEST_THRESHOLD\n",
    "    def __init__(self, train_env, eval_env, *args, **kwargs):\n",
    "        super().__init__(eval_env, *args, **kwargs)\n",
    "        self.best_mean_reward = BEST_THRESHOLD\n",
    "        self.generation = 0\n",
    "        self.train_env = train_env\n",
    "        self.eval_env = eval_env\n",
    "    def _on_step(self) -> bool:\n",
    "        # result = super()._on_step() #  eval needs to be masked, its less efficient \n",
    "        result = super()._on_step()\n",
    "        \n",
    "        if result and self.best_mean_reward > BEST_THRESHOLD:\n",
    "            self.generation += 1\n",
    "            print(\"SELFPLAY: mean_reward achieved:\", self.best_mean_reward)\n",
    "            print(\"SELFPLAY: new best model, bumping up generation to\", self.generation)            \n",
    "            source_file = os.path.join(LOGDIR, \"best_model.zip\")\n",
    "            backup_file = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(4)+\".zip\")\n",
    "            copyfile(source_file, backup_file)\n",
    "            self.best_mean_reward = BEST_THRESHOLD\n",
    "            agent = self.model.load(source_file)\n",
    "            self.train_env.unwrapped.change_to_latest_agent(agent)            \n",
    "            self.eval_env.envs[0].unwrapped.change_to_latest_agent(agent)      \n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a200cff4-ce58-4527-a24c-5db541c62ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasa/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/common/maskable/policies.py:78: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n",
      "/home/rasa/venv/ml-venv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:283: UserWarning: Path 'ppo_masked/v2' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 10.\n",
      "Ep done - 20.\n",
      "Ep done - 30.\n",
      "Ep done - 40.\n",
      "Ep done - 50.\n",
      "Ep done - 60.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.6     |\n",
      "|    ep_rew_mean     | -0.159   |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Ep done - 70.\n",
      "Ep done - 80.\n",
      "Ep done - 90.\n",
      "Ep done - 100.\n",
      "Ep done - 110.\n",
      "Ep done - 120.\n",
      "Ep done - 130.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.8        |\n",
      "|    ep_rew_mean          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 508         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012884092 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.98       |\n",
      "|    explained_variance   | -0.33       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0604     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Ep done - 140.\n",
      "Ep done - 150.\n",
      "Ep done - 160.\n",
      "Ep done - 170.\n",
      "Ep done - 180.\n",
      "Ep done - 190.\n",
      "Ep done - 200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014977427 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.98       |\n",
      "|    explained_variance   | -0.186      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0918     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Ep done - 210.\n",
      "Ep done - 220.\n",
      "Ep done - 230.\n",
      "Ep done - 240.\n",
      "Ep done - 250.\n",
      "Ep done - 260.\n",
      "Ep done - 270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.9        |\n",
      "|    ep_rew_mean          | 0.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 463         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014648648 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.97       |\n",
      "|    explained_variance   | -0.292      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0942     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Ep done - 280.\n",
      "Ep done - 290.\n",
      "Ep done - 300.\n",
      "Ep done - 310.\n",
      "Ep done - 320.\n",
      "Ep done - 330.\n",
      "Ep done - 340.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.08      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 449       |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 22        |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0167086 |\n",
      "|    clip_fraction        | 0.173     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.95     |\n",
      "|    explained_variance   | 0.0334    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0715   |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.0688   |\n",
      "|    value_loss           | 0.0981    |\n",
      "---------------------------------------\n",
      "Ep done - 350.\n",
      "Ep done - 360.\n",
      "Ep done - 370.\n",
      "Ep done - 380.\n",
      "Ep done - 390.\n",
      "Ep done - 400.\n",
      "Ep done - 410.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 446         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016366946 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0969     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "Ep done - 420.\n",
      "Ep done - 430.\n",
      "Ep done - 440.\n",
      "Ep done - 450.\n",
      "Ep done - 460.\n",
      "Ep done - 470.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 440         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017781619 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | -0.015      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0718     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Ep done - 480.\n",
      "Ep done - 490.\n",
      "Ep done - 500.\n",
      "Ep done - 510.\n",
      "Ep done - 520.\n",
      "Ep done - 530.\n",
      "Ep done - 540.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | 0.2        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 440        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 37         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01821043 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.89      |\n",
      "|    explained_variance   | 0.135      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0987    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0715    |\n",
      "|    value_loss           | 0.0999     |\n",
      "----------------------------------------\n",
      "Ep done - 550.\n",
      "Ep done - 560.\n",
      "Ep done - 570.\n",
      "Ep done - 580.\n",
      "Ep done - 590.\n",
      "Ep done - 600.\n",
      "Ep done - 610.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.9        |\n",
      "|    ep_rew_mean          | 0.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 439         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020359918 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | -0.0299     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0684     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 0.0931      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 620.\n",
      "Ep done - 630.\n",
      "Ep done - 640.\n",
      "Ep done - 650.\n",
      "Ep done - 660.\n",
      "Ep done - 10.\n",
      "Ep done - 20.\n",
      "Ep done - 30.\n",
      "Ep done - 40.\n",
      "Ep done - 50.\n",
      "Ep done - 60.\n",
      "Ep done - 70.\n",
      "Ep done - 80.\n",
      "Ep done - 90.\n",
      "Ep done - 100.\n",
      "Ep done - 110.\n",
      "Ep done - 120.\n",
      "Ep done - 130.\n",
      "Ep done - 140.\n",
      "Ep done - 150.\n",
      "Ep done - 160.\n",
      "Ep done - 170.\n",
      "Ep done - 180.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m\n\u001b[1;32m     28\u001b[0m env_eval\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39mchange_to_latest_agent(start_model_copy)\n\u001b[1;32m     32\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m SelfPlayCallback(\n\u001b[1;32m     33\u001b[0m     env,\n\u001b[1;32m     34\u001b[0m     env_eval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[1;32m     40\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:526\u001b[0m, in \u001b[0;36mMaskablePPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001b[0m\n\u001b[1;32m    523\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 526\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_masking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:312\u001b[0m, in \u001b[0;36mMaskablePPO.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps, use_masking)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    311\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/stable_baselines3/common/callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mSelfPlayCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_on_step\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# result = super()._on_step() #  eval needs to be masked, its less efficient \u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_mean_reward \u001b[38;5;241m>\u001b[39m BEST_THRESHOLD:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/stable_baselines3/common/callbacks.py:460\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# Reset success rate buffer\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_success_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 460\u001b[0m episode_rewards, episode_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_success_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(episode_rewards, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/common/maskable/evaluation.py:95\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn, use_masking)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_masking:\n\u001b[1;32m     94\u001b[0m     action_masks \u001b[38;5;241m=\u001b[39m get_action_masks(env)\n\u001b[0;32m---> 95\u001b[0m     actions, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    104\u001b[0m         observations,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    105\u001b[0m         state\u001b[38;5;241m=\u001b[39mstates,\n\u001b[1;32m    106\u001b[0m         episode_start\u001b[38;5;241m=\u001b[39mepisode_starts,\n\u001b[1;32m    107\u001b[0m         deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[1;32m    108\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:380\u001b[0m, in \u001b[0;36mMaskablePPO.predict\u001b[0;34m(self, observation, state, episode_start, deterministic, action_masks)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    361\u001b[0m     observation: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     action_masks: Optional[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    366\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/common/maskable/policies.py:290\u001b[0m, in \u001b[0;36mMaskableActorCriticPolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic, action_masks)\u001b[0m\n\u001b[1;32m    287\u001b[0m observation, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 290\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# Convert to numpy\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/common/maskable/policies.py:262\u001b[0m, in \u001b[0;36mMaskableActorCriticPolicy._predict\u001b[0;34m(self, observation, deterministic, action_masks)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    250\u001b[0m     observation: th\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    251\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    252\u001b[0m     action_masks: Optional[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/common/maskable/policies.py:352\u001b[0m, in \u001b[0;36mMaskableActorCriticPolicy.get_distribution\u001b[0;34m(self, obs, action_masks)\u001b[0m\n\u001b[1;32m    350\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distribution\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/common/maskable/distributions.py:159\u001b[0m, in \u001b[0;36mMaskableCategoricalDistribution.apply_masking\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_masking\u001b[39m(\u001b[38;5;28mself\u001b[39m, masks: Optional[np\u001b[38;5;241m.\u001b[39mndarray]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust set distribution parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/sb3_contrib/common/maskable/distributions.py:67\u001b[0m, in \u001b[0;36mMaskableCategorical.apply_masking\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m     64\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_logits\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Reinitialize with updated logits\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# self.probs may already be cached, so we must force an update\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;241m=\u001b[39m logits_to_probs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n",
      "File \u001b[0;32m~/venv/ml-venv/lib/python3.11/site-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m logits\u001b[38;5;241m.\u001b[39mlogsumexp(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(batch_shape, validate_args\u001b[38;5;241m=\u001b[39mvalidate_args)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = OthelloEnv()\n",
    "env = Monitor(env=env)\n",
    "\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "policy_kwargs = {\n",
    "    'net_arch': {\n",
    "        'pi': [64, 64, 64, 64],  # Four hidden layers with 64 units each for the policy network\n",
    "        'vf': [64, 64, 64, 64]   # Four hidden layers with 64 units each for the value network\n",
    "    }\n",
    "}\n",
    "\n",
    "model = MaskablePPO(policy=MaskableActorCriticPolicy, \n",
    "                    env=env, \n",
    "                    device=device,\n",
    "                    verbose=1,\n",
    "                    policy_kwargs=policy_kwargs)\n",
    "starting_model_filepath = LOGDIR + 'random_start_model'\n",
    "# model = MaskablePPO.load(starting_model_filepath, env=env)\n",
    "model.save(starting_model_filepath)\n",
    "\n",
    "start_model_copy = model.load(starting_model_filepath)\n",
    "env.unwrapped.change_to_latest_agent(start_model_copy)\n",
    "\n",
    "\n",
    "env_eval = OthelloEnv()\n",
    "env_eval = Monitor(env=env_eval)\n",
    "\n",
    "env_eval = DummyVecEnv(env_fns=[lambda: env_eval])\n",
    "env_eval.envs[0].unwrapped.change_to_latest_agent(start_model_copy)\n",
    "\n",
    "\n",
    "\n",
    "eval_callback = SelfPlayCallback(\n",
    "    env,\n",
    "    env_eval,\n",
    "    best_model_save_path=LOGDIR,\n",
    "    log_path=LOGDIR,\n",
    "    eval_freq=EVAL_FREQ,\n",
    "    n_eval_episodes=EVAL_EPISODES,\n",
    "    deterministic=False \n",
    "    )\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1076a2e9-f6ec-4ace-a5c7-6d6863935c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('board',\n",
       "              array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 1, 2, 0, 0, 0],\n",
       "                     [0, 0, 0, 2, 1, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0]])),\n",
       "             ('player', 1)])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.unwrapped.get_obs()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "19b6d5cf-09f8-463b-a95d-3a9add321149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obs = spaces.flatten(env.unwrapped.observation_space, obs)\n",
    "new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8ed10-d6d0-4363-9516-89dda14fd7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f7da2c24-2262-4c6c-8464-ae1cb2390730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(22), None)\n",
      "(array(29), None)\n",
      "(array(41), None)\n",
      "(array(28), None)\n",
      "(array(5), None)\n",
      "(array(46), None)\n",
      "(array(24), None)\n",
      "(array(13), None)\n",
      "(array(40), None)\n",
      "(array(59), None)\n",
      "(array(13), None)\n",
      "(array(45), None)\n",
      "(array(18), None)\n",
      "(array(62), None)\n",
      "(array(51), None)\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(model.predict(new_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ede4294c-f6cf-4fff-b4ae-83f2a1c14ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 2, 2, 2, 1, 0, 1, 0, 2, 2, 2, 2, 2,\n",
       "       1, 2, 1, 0, 2, 2, 1, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 0, 0, 0, 1, 0,\n",
       "       2, 2, 1, 1, 0, 2, 1, 0, 2, 2, 2, 1, 2, 0, 0, 0, 2, 0, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc4e49-8b25-4a20-9d99-6e9b04d61b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0969e-6711-4e9a-9ceb-3e6ea3b9d304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5ec1f886-e37f-496d-b6e5-dd33ccd191fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval = OthelloEnv()\n",
    "env_eval = Monitor(env=env_eval)\n",
    "env_eval = FlattenObservation(env_eval)\n",
    "\n",
    "# env_eval = DummyVecEnv(env_fns=[lambda: env_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2b4b5155-d19f-4bc2-91da-bbc09b488fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MaskablePPO.load('history_00000385.zip')\n",
    "model_random = MaskablePPO.load('ppo_masked_selfplay_2/history_00000218.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9b6d8668-3386-434d-97b8-a8ed3ba6ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_eval.envs[0].unwrapped.change_to_latest_agent(model1)\n",
    "env_eval.unwrapped.change_to_latest_agent(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d252cb9d-8f37-4209-9da1-80252d48d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy = masked_evaluate_policy\n",
    "episode_rewards, episode_lengths = evaluate_policy(\n",
    "                model_random,\n",
    "                env_eval,\n",
    "                n_eval_episodes=100,                \n",
    "                deterministic=True,\n",
    "                return_episode_rewards=True,\n",
    "                warn=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3527ceef-0eb6-4e5c-bf97-1e83b354a502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3ab7ebfd-6326-45c0-be8e-0bc89976894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_play(env, model, episodes = 100):     \n",
    "    wins = 0\n",
    "    for episode in range(1, episodes+1):\n",
    "        obs, _ = env.reset()        \n",
    "        done = False\n",
    "        score = 0 \n",
    "        \n",
    "        while not done:\n",
    "            # env.render()\n",
    "            action, _ = model.predict(obs, action_masks=env.unwrapped.action_masks(), deterministic=False)            \n",
    "            obs, reward, done, _, info = env.step(action)            \n",
    "            score+=reward\n",
    "        won = env.game.get_winner() == env.agent_turn\n",
    "        # print(f'Episode:{episode} Score:{score}, play as {env.agent_turn}, won = {won}')\n",
    "        if won:\n",
    "            wins+=1\n",
    "    return wins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dee779e7-d359-4b0a-af2a-6d811403d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_play(env_eval, model_random, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f2f2d5-b1f3-4204-b171-4da8d883c632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e8d7ec47-31cd-4971-88ff-20516a003366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(env_eval.game.board.flatten(), [3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82490061-f7de-4490-9764-80702fd41015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
