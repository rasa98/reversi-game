{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e08b866-f6c3-4b4c-8185-0e847a2568e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# sys.path.append('/home/user/PycharmProjects/reversi-game/')\n",
    "source_dir = os.path.abspath(os.path.join(os.getcwd(), '../../../'))\n",
    "sys.path.append(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c64fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fdeba6-0cdc-4fa8-8ebb-b6d5634034d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "\n",
    "import stable_baselines3.common.callbacks as callbacks_module\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy as masked_evaluate_policy\n",
    "\n",
    "# Modify the namespace of EvalCallback directly\n",
    "callbacks_module.evaluate_policy = masked_evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy, MaskableMultiInputActorCriticPolicy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "from shutil import copyfile # keep track of generations\n",
    "from collections import OrderedDict\n",
    "\n",
    "from gymnasium.spaces import Discrete, Box, Dict, MultiDiscrete\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "import gymnasium.spaces as spaces\n",
    "from game_logic import Othello\n",
    "import numpy as np\n",
    "import os, math\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0045e1-7726-4c4e-bef6-1b2f4743547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23086ab8-3197-4112-9877-fcdb52f351ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SEED = 19\n",
    "NUM_TIMESTEPS = int(30_000_000)\n",
    "EVAL_FREQ = int(10_000)\n",
    "EVAL_EPISODES = int(100)\n",
    "BEST_THRESHOLD = 0.25 # must achieve a mean score above this to replace prev best self\n",
    "\n",
    "RENDER_MODE = False # set this to false if you plan on running for full 1000 trials.\n",
    "\n",
    "LOGDIR = \"models/delete\"\n",
    "# LOGDIR = \"delete_me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "015cd09b-673e-4145-b7b9-2c22763c0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloEnv(gym.Env):   \n",
    "    def __init__(self):\n",
    "        self.game = Othello()\n",
    "        self.agent_turn = 1\n",
    "        shape = self.game.board.shape  \n",
    "        self.action_mapping = self.am(shape)\n",
    "        self.action_space = Discrete(shape[0] * shape[1])  # sample - [x, y]\n",
    "        self.observation_space = Dict({\n",
    "                                        'board' : Box(0, 2, shape=shape, dtype=int),\n",
    "                                        'chips' : MultiDiscrete([65, 65]),\n",
    "                                        'player': Discrete(2, start=1)\n",
    "                                      })        \n",
    "        self.other_agent = None\n",
    "        # self.reset_othello_gen = self.reset_othello()   \n",
    "        \n",
    "        self.episodes = 0    \n",
    "        # self.global_reward = 0\n",
    "    \n",
    "\n",
    "    def am(self, shape):\n",
    "        x, y = shape\n",
    "        return [(n//x, n%x) for n in range(x * y)]\n",
    "        \n",
    "\n",
    "    # def reset_othello(self):\n",
    "    #     '''resets game to starting position \n",
    "    #        and also changes starting player alternatively'''\n",
    "    #     infinite_player_turn = cycle([1]*10 + [2]*10)\n",
    "    #     while True:\n",
    "    #         game = Othello()\n",
    "    #         model_turn = next(infinite_player_turn)\n",
    "    #         yield game, model_turn\n",
    "    \n",
    "    def change_to_latest_agent(self, agent):\n",
    "        self.other_agent = agent\n",
    "\n",
    "    def get_obs(self):        \n",
    "        return OrderedDict({\n",
    "            'board' : self.game.board,\n",
    "            'chips' : np.array(self.game.chips),\n",
    "            'player': self.game.player_turn\n",
    "        })\n",
    "\n",
    "    def get_chips_diff(self): #  from agent perspective\n",
    "        idx = self.agent_turn - 1  # map [1, 2] to [0, 1]\n",
    "        diff = self.game.chips[idx] - self.game.chips[1 - idx]\n",
    "        return diff\n",
    "        \n",
    "    def check_game_ended(self):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        winner = self.game.get_winner()\n",
    "        \n",
    "        if winner is not None:\n",
    "            self.episodes += 1\n",
    "            if self.episodes % 10 == 0:\n",
    "                print(f'Ep done - {self.episodes}.')\n",
    "                # print(f'global_reward -- {self.global_reward}, -- won: {winner == self.agent_turn}')\n",
    "            \n",
    "            done = True\n",
    "            if winner == self.agent_turn:\n",
    "                # reward = max(abs(self.global_reward)*2, 1000)\n",
    "                reward = 1\n",
    "            elif winner == 3 - self.agent_turn: #  other agent turn/figure\n",
    "                # reward = min(-abs(self.global_reward)*2, -1000)\n",
    "                reward = -1\n",
    "        return reward, done\n",
    "    \n",
    "    def render(self):  # todo \n",
    "        pass\n",
    "\n",
    "    def close(self):  # todo\n",
    "        pass\n",
    "\n",
    "    def other_agent_play_move(self): \n",
    "        obs = self.get_obs()\n",
    "        obs = spaces.flatten(self.observation_space, obs)#  need to flatten observation         \n",
    "        action, _ = self.other_agent.predict(obs,\n",
    "                                             action_masks=self.action_masks(),\n",
    "                                             deterministic=False) \n",
    "        game_action = self.action_mapping[action]\n",
    "        self.game.play_move(game_action)\n",
    "\n",
    "    def step(self, action):\n",
    "        # diff_chips_before = self.get_chips_diff()\n",
    "        \n",
    "        game_action = self.action_mapping[action]  \n",
    "        self.game.play_move(game_action)\n",
    "\n",
    "        # inner agent plays\n",
    "        while self.game.get_winner() is None and self.game.player_turn != self.agent_turn: #  if game hasnt ended do moves if opponent doesnt have one \n",
    "            self.other_agent_play_move()\n",
    "\n",
    "        # diff_chips_after = self.get_chips_diff()\n",
    "        \n",
    "        reward, done = self.check_game_ended()\n",
    "\n",
    "        # turn = self.game.turn\n",
    "        # if turn <= 58: #  not sure... feel like at the end theres no more moves to choose and high reward would be bad            \n",
    "        #     factor = (turn // 10) + 1\n",
    "        #     step_reward = factor * (diff_chips_after - diff_chips_before)\n",
    "        #     self.global_reward += step_reward\n",
    "        #     reward += factor * (diff_chips_after - diff_chips_before)\n",
    "\n",
    "        \n",
    "        info = {}\n",
    "        truncated = False\n",
    "\n",
    "                \n",
    "        # Return step information\n",
    "        return self.get_obs(), reward, done, truncated, info\n",
    "    \n",
    "    def reset(self, *args, **kwargs):\n",
    "        # self.global_reward = 0\n",
    "        self.game = Othello() # self.game, self.agent_turn = next(self.reset_othello_gen)\n",
    "        if self.agent_turn == 2:\n",
    "            self.other_agent_play_move()\n",
    "        return self.get_obs(), None\n",
    "\n",
    "    def action_masks(self):        \n",
    "        valid_moves = self.game.valid_moves()\n",
    "    \n",
    "        mask = np.zeros(self.game.board.shape, dtype=bool)\n",
    "        \n",
    "        # Set True for each index in the set\n",
    "        for index in valid_moves:\n",
    "            mask[index] = True\n",
    "        mask.flatten()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d9237-3a89-4fec-9fbb-9c55d0c5c71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2235363-8bce-49ee-a44a-e63a76b1d75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfdecf41-ef1c-4f12-bbc7-02d06ce2a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayCallback(EvalCallback):\n",
    "    # hacked it to only save new version offrom gymnasium.wrappers import FlattenObservation best model if beats prev self by BEST_THRESHOLD score\n",
    "    # after saving model, resets the best score to be BEST_THRESHOLD\n",
    "    def __init__(self, eval_env, *args, **kwargs):\n",
    "        super().__init__(eval_env, *args, **kwargs)\n",
    "        self.best_mean_reward = BEST_THRESHOLD\n",
    "        self.generation = 0        \n",
    "    def _on_step(self) -> bool:\n",
    "        # result = super()._on_step() #  eval needs to be masked, its less efficient \n",
    "        result = super()._on_step()\n",
    "        \n",
    "        if result and self.best_mean_reward > BEST_THRESHOLD:\n",
    "            self.generation += 1\n",
    "            print(\"SELFPLAY: mean_reward achieved:\", self.best_mean_reward)\n",
    "            print(\"SELFPLAY: new best model, bumping up generation to\", self.generation)            \n",
    "            source_file = os.path.join(LOGDIR, \"best_model.zip\")\n",
    "            backup_file = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n",
    "            copyfile(source_file, backup_file)\n",
    "            self.best_mean_reward = BEST_THRESHOLD\n",
    "            agent = self.model.load(source_file)\n",
    "                \n",
    "            self.training_env.env_method(\"change_to_latest_agent\", agent)           \n",
    "            self.eval_env.env_method(\"change_to_latest_agent\", agent) #  .env_method(\"method_name\", args1, args2, kwargs1=kwargs1) \n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927a91a-306e-4da3-9056-9755dd527c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2207e630-3911-417d-be7f-2b47f69cde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OthelloEnv()\n",
    "env = Monitor(env=env)\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd147c-341a-4e53-b705-bc9321831e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8655c3f-2e4f-4f02-8a50-a7b8aaf183fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "#                      net_arch=dict(pi=[128, 64], vf=[64, 64]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "981a14a5-ef60-43eb-ad30-da83838d6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MaskablePPO(policy=MaskableActorCriticPolicy, \n",
    "#                     env=env, \n",
    "#                     verbose=1,\n",
    "#                     # policy_kwargs=policy_kwargs#,\n",
    "#                     # learning_rate=1e-5, \n",
    "#                     # n_steps=6144\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "014f0854-1336-4034-8440-73f239164d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = MaskablePPO.load(\"models/history_00000329\",\n",
    "                         env=env,\n",
    "                         learning_rate=0.007, \n",
    "                         n_steps=6144) # load existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a2087-f33f-4607-82a1-72947acc368f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b92d9-e150-4639-a162-0494a2182340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cf6e676-7c6f-42e7-a0ed-fd7212f436c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_model_filepath = 'models/history_00000329'\n",
    "start_model_copy = MaskablePPO.load(starting_model_filepath)\n",
    "env.unwrapped.change_to_latest_agent(start_model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c367da1b-e3bd-4d8a-8937-8b1a6954e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting_model_filepath = LOGDIR + '/start_model'\n",
    "# model.save(starting_model_filepath)\n",
    "# start_model_copy = model.load(starting_model_filepath)\n",
    "# env.unwrapped.change_to_latest_agent(start_model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9b791-2797-4aaa-a4ba-f82d31be4dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5daf32f-cc99-4e7c-9de0-514fef46a79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d39dc3f-0728-4ac0-bb4c-497201cc7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval = OthelloEnv()\n",
    "env_eval = Monitor(env=env_eval)\n",
    "env_eval = FlattenObservation(env_eval)\n",
    "\n",
    "env_eval = DummyVecEnv(env_fns=[lambda: env_eval])\n",
    "env_eval.envs[0].unwrapped.change_to_latest_agent(start_model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec37f2d-4570-4339-83a2-cb07dbe276b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 7550.\n",
      "Ep done - 7560.\n",
      "Ep done - 7570.\n",
      "Ep done - 7580.\n",
      "Ep done - 7590.\n",
      "Ep done - 7600.\n",
      "Ep done - 7610.\n",
      "Ep done - 7620.\n",
      "Ep done - 7630.\n",
      "Ep done - 7640.\n",
      "Ep done - 7650.\n",
      "Ep done - 7660.\n",
      "Ep done - 7670.\n",
      "Ep done - 7680.\n",
      "Ep done - 7690.\n",
      "Ep done - 7700.\n",
      "Ep done - 7710.\n",
      "Ep done - 7720.\n",
      "Ep done - 7730.\n",
      "Ep done - 7740.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Ep done - 7750.\n",
      "Ep done - 7760.\n",
      "Ep done - 7770.\n",
      "Ep done - 7780.\n",
      "Ep done - 7790.\n",
      "Ep done - 7800.\n",
      "Ep done - 7810.\n",
      "Ep done - 7820.\n",
      "Ep done - 7830.\n",
      "Ep done - 7840.\n",
      "Ep done - 7850.\n",
      "Ep done - 7860.\n",
      "Ep done - 7870.\n",
      "Ep done - 10.\n",
      "Ep done - 20.\n",
      "Ep done - 30.\n",
      "Ep done - 40.\n",
      "Ep done - 50.\n",
      "Ep done - 60.\n",
      "Ep done - 70.\n",
      "Ep done - 80.\n",
      "Ep done - 90.\n",
      "Ep done - 100.\n",
      "Eval num_timesteps=10000, episode_reward=0.11 +/- 0.99\n",
      "Episode length: 30.19 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | 0.11      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 10000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5185341 |\n",
      "|    clip_fraction        | 0.45      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.272    |\n",
      "|    explained_variance   | 0.319     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.156     |\n",
      "|    n_updates            | 19240     |\n",
      "|    policy_gradient_loss | 0.0957    |\n",
      "|    value_loss           | 0.123     |\n",
      "---------------------------------------\n",
      "Ep done - 7880.\n",
      "Ep done - 7890.\n",
      "Ep done - 7900.\n",
      "Ep done - 7910.\n",
      "Ep done - 7920.\n",
      "Ep done - 7930.\n",
      "Ep done - 7940.\n",
      "Ep done - 7950.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 483      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Ep done - 7960.\n",
      "Ep done - 7970.\n",
      "Ep done - 7980.\n",
      "Ep done - 7990.\n",
      "Ep done - 8000.\n",
      "Ep done - 8010.\n",
      "Ep done - 8020.\n",
      "Ep done - 8030.\n",
      "Ep done - 8040.\n",
      "Ep done - 8050.\n",
      "Ep done - 8060.\n",
      "Ep done - 8070.\n",
      "Ep done - 8080.\n",
      "Ep done - 8090.\n",
      "Ep done - 8100.\n",
      "Ep done - 8110.\n",
      "Ep done - 8120.\n",
      "Ep done - 8130.\n",
      "Ep done - 8140.\n",
      "Ep done - 8150.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.37      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 484       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 38        |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2505176 |\n",
      "|    clip_fraction        | 0.398     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.225    |\n",
      "|    explained_variance   | 0.186     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.149     |\n",
      "|    n_updates            | 19250     |\n",
      "|    policy_gradient_loss | 0.125     |\n",
      "|    value_loss           | 0.235     |\n",
      "---------------------------------------\n",
      "Ep done - 8160.\n",
      "Ep done - 8170.\n",
      "Ep done - 8180.\n",
      "Ep done - 8190.\n",
      "Ep done - 8200.\n",
      "Ep done - 110.\n",
      "Ep done - 120.\n",
      "Ep done - 130.\n",
      "Ep done - 140.\n",
      "Ep done - 150.\n",
      "Ep done - 160.\n",
      "Ep done - 170.\n",
      "Ep done - 180.\n",
      "Ep done - 190.\n",
      "Ep done - 200.\n",
      "Eval num_timesteps=20000, episode_reward=0.15 +/- 0.96\n",
      "Episode length: 30.12 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.15       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71680766 |\n",
      "|    clip_fraction        | 0.295      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.168     |\n",
      "|    explained_variance   | 0.158      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.154      |\n",
      "|    n_updates            | 19260      |\n",
      "|    policy_gradient_loss | 0.0636     |\n",
      "|    value_loss           | 0.211      |\n",
      "----------------------------------------\n",
      "Ep done - 8210.\n",
      "Ep done - 8220.\n",
      "Ep done - 8230.\n",
      "Ep done - 8240.\n",
      "Ep done - 8250.\n",
      "Ep done - 8260.\n",
      "Ep done - 8270.\n",
      "Ep done - 8280.\n",
      "Ep done - 8290.\n",
      "Ep done - 8300.\n",
      "Ep done - 8310.\n",
      "Ep done - 8320.\n",
      "Ep done - 8330.\n",
      "Ep done - 8340.\n",
      "Ep done - 8350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.27     |\n",
      "| time/              |          |\n",
      "|    fps             | 447      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Ep done - 8360.\n",
      "Ep done - 8370.\n",
      "Ep done - 8380.\n",
      "Ep done - 8390.\n",
      "Ep done - 8400.\n",
      "Ep done - 8410.\n",
      "Ep done - 8420.\n",
      "Ep done - 8430.\n",
      "Ep done - 8440.\n",
      "Ep done - 8450.\n",
      "Ep done - 8460.\n",
      "Ep done - 8470.\n",
      "Ep done - 8480.\n",
      "Ep done - 8490.\n",
      "Ep done - 8500.\n",
      "Ep done - 8510.\n",
      "Ep done - 8520.\n",
      "Ep done - 8530.\n",
      "Ep done - 210.\n",
      "Ep done - 220.\n",
      "Ep done - 230.\n",
      "Ep done - 240.\n",
      "Ep done - 250.\n",
      "Ep done - 260.\n",
      "Ep done - 270.\n",
      "Ep done - 280.\n",
      "Ep done - 290.\n",
      "Ep done - 300.\n",
      "Eval num_timesteps=30000, episode_reward=0.49 +/- 0.84\n",
      "Episode length: 30.17 +/- 0.55\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | 0.49      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 30000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4268379 |\n",
      "|    clip_fraction        | 0.214     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.125    |\n",
      "|    explained_variance   | 0.215     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.12      |\n",
      "|    n_updates            | 19270     |\n",
      "|    policy_gradient_loss | 0.0468    |\n",
      "|    value_loss           | 0.235     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.49\n",
      "SELFPLAY: new best model, bumping up generation to 1\n",
      "Ep done - 8540.\n",
      "Ep done - 8550.\n",
      "Ep done - 8560.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 424      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Ep done - 8570.\n",
      "Ep done - 8580.\n",
      "Ep done - 8590.\n",
      "Ep done - 8600.\n",
      "Ep done - 8610.\n",
      "Ep done - 8620.\n",
      "Ep done - 8630.\n",
      "Ep done - 8640.\n",
      "Ep done - 8650.\n",
      "Ep done - 8660.\n",
      "Ep done - 8670.\n",
      "Ep done - 8680.\n",
      "Ep done - 8690.\n",
      "Ep done - 8700.\n",
      "Ep done - 8710.\n",
      "Ep done - 8720.\n",
      "Ep done - 8730.\n",
      "Ep done - 8740.\n",
      "Ep done - 8750.\n",
      "Ep done - 8760.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | -0.25      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 432        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 85         |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78176457 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.109     |\n",
      "|    explained_variance   | 0.168      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0798     |\n",
      "|    n_updates            | 19280      |\n",
      "|    policy_gradient_loss | 0.0514     |\n",
      "|    value_loss           | 0.172      |\n",
      "----------------------------------------\n",
      "Ep done - 8770.\n",
      "Ep done - 8780.\n",
      "Ep done - 8790.\n",
      "Ep done - 8800.\n",
      "Ep done - 8810.\n",
      "Ep done - 8820.\n",
      "Ep done - 8830.\n",
      "Ep done - 8840.\n",
      "Ep done - 8850.\n",
      "Ep done - 8860.\n",
      "Ep done - 8870.\n",
      "Ep done - 310.\n",
      "Ep done - 320.\n",
      "Ep done - 330.\n",
      "Ep done - 340.\n",
      "Ep done - 350.\n",
      "Ep done - 360.\n",
      "Ep done - 370.\n",
      "Ep done - 380.\n",
      "Ep done - 390.\n",
      "Ep done - 400.\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.99\n",
      "Episode length: 30.08 +/- 0.42\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 40000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7402169 |\n",
      "|    clip_fraction        | 0.32      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.173    |\n",
      "|    explained_variance   | -0.0177   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.158     |\n",
      "|    n_updates            | 19290     |\n",
      "|    policy_gradient_loss | 0.0459    |\n",
      "|    value_loss           | 0.212     |\n",
      "---------------------------------------\n",
      "Ep done - 8880.\n",
      "Ep done - 8890.\n",
      "Ep done - 8900.\n",
      "Ep done - 8910.\n",
      "Ep done - 8920.\n",
      "Ep done - 8930.\n",
      "Ep done - 8940.\n",
      "Ep done - 8950.\n",
      "Ep done - 8960.\n",
      "Ep done - 8970.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 420      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Ep done - 8980.\n",
      "Ep done - 8990.\n",
      "Ep done - 9000.\n",
      "Ep done - 9010.\n",
      "Ep done - 9020.\n",
      "Ep done - 9030.\n",
      "Ep done - 9040.\n",
      "Ep done - 9050.\n",
      "Ep done - 9060.\n",
      "Ep done - 9070.\n",
      "Ep done - 9080.\n",
      "Ep done - 9090.\n",
      "Ep done - 9100.\n",
      "Ep done - 9110.\n",
      "Ep done - 9120.\n",
      "Ep done - 9130.\n",
      "Ep done - 9140.\n",
      "Ep done - 9150.\n",
      "Ep done - 9160.\n",
      "Ep done - 9170.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.2       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 424       |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 115       |\n",
      "|    total_timesteps      | 49152     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3417349 |\n",
      "|    clip_fraction        | 0.193     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.11     |\n",
      "|    explained_variance   | -8.19e-05 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0945    |\n",
      "|    n_updates            | 19300     |\n",
      "|    policy_gradient_loss | 0.018     |\n",
      "|    value_loss           | 0.234     |\n",
      "---------------------------------------\n",
      "Ep done - 9180.\n",
      "Ep done - 9190.\n",
      "Ep done - 9200.\n",
      "Ep done - 410.\n",
      "Ep done - 420.\n",
      "Ep done - 430.\n",
      "Ep done - 440.\n",
      "Ep done - 450.\n",
      "Ep done - 460.\n",
      "Ep done - 470.\n",
      "Ep done - 480.\n",
      "Ep done - 490.\n",
      "Ep done - 500.\n",
      "Eval num_timesteps=50000, episode_reward=0.69 +/- 0.72\n",
      "Episode length: 30.42 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.4       |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13415992 |\n",
      "|    clip_fraction        | 0.108      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0676    |\n",
      "|    explained_variance   | 0.2        |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.135      |\n",
      "|    n_updates            | 19310      |\n",
      "|    policy_gradient_loss | 0.0125     |\n",
      "|    value_loss           | 0.208      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.69\n",
      "SELFPLAY: new best model, bumping up generation to 2\n",
      "Ep done - 9210.\n",
      "Ep done - 9220.\n",
      "Ep done - 9230.\n",
      "Ep done - 9240.\n",
      "Ep done - 9250.\n",
      "Ep done - 9260.\n",
      "Ep done - 9270.\n",
      "Ep done - 9280.\n",
      "Ep done - 9290.\n",
      "Ep done - 9300.\n",
      "Ep done - 9310.\n",
      "Ep done - 9320.\n",
      "Ep done - 9330.\n",
      "Ep done - 9340.\n",
      "Ep done - 9350.\n",
      "Ep done - 9360.\n",
      "Ep done - 9370.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.12     |\n",
      "| time/              |          |\n",
      "|    fps             | 417      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Ep done - 9380.\n",
      "Ep done - 9390.\n",
      "Ep done - 9400.\n",
      "Ep done - 9410.\n",
      "Ep done - 9420.\n",
      "Ep done - 9430.\n",
      "Ep done - 9440.\n",
      "Ep done - 9450.\n",
      "Ep done - 9460.\n",
      "Ep done - 9470.\n",
      "Ep done - 9480.\n",
      "Ep done - 9490.\n",
      "Ep done - 9500.\n",
      "Ep done - 9510.\n",
      "Ep done - 9520.\n",
      "Ep done - 9530.\n",
      "Ep done - 510.\n",
      "Ep done - 520.\n",
      "Ep done - 530.\n",
      "Ep done - 540.\n",
      "Ep done - 550.\n",
      "Ep done - 560.\n",
      "Ep done - 570.\n",
      "Ep done - 580.\n",
      "Ep done - 590.\n",
      "Ep done - 600.\n",
      "Eval num_timesteps=60000, episode_reward=0.39 +/- 0.92\n",
      "Episode length: 30.58 +/- 0.83\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.6       |\n",
      "|    mean_reward          | 0.39       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 60000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39938995 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0921    |\n",
      "|    explained_variance   | 0.0648     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0744     |\n",
      "|    n_updates            | 19320      |\n",
      "|    policy_gradient_loss | 0.0129     |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.39\n",
      "SELFPLAY: new best model, bumping up generation to 3\n",
      "Ep done - 9540.\n",
      "Ep done - 9550.\n",
      "Ep done - 9560.\n",
      "Ep done - 9570.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | 0.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 408      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Ep done - 9580.\n",
      "Ep done - 9590.\n",
      "Ep done - 9600.\n",
      "Ep done - 9610.\n",
      "Ep done - 9620.\n",
      "Ep done - 9630.\n",
      "Ep done - 9640.\n",
      "Ep done - 9650.\n",
      "Ep done - 9660.\n",
      "Ep done - 9670.\n",
      "Ep done - 9680.\n",
      "Ep done - 9690.\n",
      "Ep done - 9700.\n",
      "Ep done - 9710.\n",
      "Ep done - 9720.\n",
      "Ep done - 9730.\n",
      "Ep done - 9740.\n",
      "Ep done - 9750.\n",
      "Ep done - 9760.\n",
      "Ep done - 9770.\n",
      "Ep done - 9780.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.6       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 413       |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 163       |\n",
      "|    total_timesteps      | 67584     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3196883 |\n",
      "|    clip_fraction        | 0.151     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0895   |\n",
      "|    explained_variance   | 0.174     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0643    |\n",
      "|    n_updates            | 19330     |\n",
      "|    policy_gradient_loss | 0.0244    |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "Ep done - 9790.\n",
      "Ep done - 9800.\n",
      "Ep done - 9810.\n",
      "Ep done - 9820.\n",
      "Ep done - 9830.\n",
      "Ep done - 9840.\n",
      "Ep done - 9850.\n",
      "Ep done - 9860.\n",
      "Ep done - 610.\n",
      "Ep done - 620.\n",
      "Ep done - 630.\n",
      "Ep done - 640.\n",
      "Ep done - 650.\n",
      "Ep done - 660.\n",
      "Ep done - 670.\n",
      "Ep done - 680.\n",
      "Ep done - 690.\n",
      "Ep done - 700.\n",
      "Eval num_timesteps=70000, episode_reward=0.34 +/- 0.94\n",
      "Episode length: 30.11 +/- 0.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.34       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.36102977 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.071     |\n",
      "|    explained_variance   | 0.0803     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.246      |\n",
      "|    n_updates            | 19340      |\n",
      "|    policy_gradient_loss | 0.0288     |\n",
      "|    value_loss           | 0.136      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.34\n",
      "SELFPLAY: new best model, bumping up generation to 4\n",
      "Ep done - 9870.\n",
      "Ep done - 9880.\n",
      "Ep done - 9890.\n",
      "Ep done - 9900.\n",
      "Ep done - 9910.\n",
      "Ep done - 9920.\n",
      "Ep done - 9930.\n",
      "Ep done - 9940.\n",
      "Ep done - 9950.\n",
      "Ep done - 9960.\n",
      "Ep done - 9970.\n",
      "Ep done - 9980.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 407      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Ep done - 9990.\n",
      "Ep done - 10000.\n",
      "Ep done - 10010.\n",
      "Ep done - 10020.\n",
      "Ep done - 10030.\n",
      "Ep done - 10040.\n",
      "Ep done - 10050.\n",
      "Ep done - 10060.\n",
      "Ep done - 10070.\n",
      "Ep done - 10080.\n",
      "Ep done - 10090.\n",
      "Ep done - 10100.\n",
      "Ep done - 10110.\n",
      "Ep done - 10120.\n",
      "Ep done - 10130.\n",
      "Ep done - 10140.\n",
      "Ep done - 10150.\n",
      "Ep done - 10160.\n",
      "Ep done - 10170.\n",
      "Ep done - 10180.\n",
      "Ep done - 10190.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 413        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 193        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.24927671 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0766    |\n",
      "|    explained_variance   | 0.218      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.04       |\n",
      "|    n_updates            | 19350      |\n",
      "|    policy_gradient_loss | 0.00187    |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "Ep done - 710.\n",
      "Ep done - 720.\n",
      "Ep done - 730.\n",
      "Ep done - 740.\n",
      "Ep done - 750.\n",
      "Ep done - 760.\n",
      "Ep done - 770.\n",
      "Ep done - 780.\n",
      "Ep done - 790.\n",
      "Ep done - 800.\n",
      "Eval num_timesteps=80000, episode_reward=0.52 +/- 0.85\n",
      "Episode length: 30.21 +/- 0.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66239494 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0491    |\n",
      "|    explained_variance   | 0.391      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0254     |\n",
      "|    n_updates            | 19360      |\n",
      "|    policy_gradient_loss | 0.0089     |\n",
      "|    value_loss           | 0.0695     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.52\n",
      "SELFPLAY: new best model, bumping up generation to 5\n",
      "Ep done - 10200.\n",
      "Ep done - 10210.\n",
      "Ep done - 10220.\n",
      "Ep done - 10230.\n",
      "Ep done - 10240.\n",
      "Ep done - 10250.\n",
      "Ep done - 10260.\n",
      "Ep done - 10270.\n",
      "Ep done - 10280.\n",
      "Ep done - 10290.\n",
      "Ep done - 10300.\n",
      "Ep done - 10310.\n",
      "Ep done - 10320.\n",
      "Ep done - 10330.\n",
      "Ep done - 10340.\n",
      "Ep done - 10350.\n",
      "Ep done - 10360.\n",
      "Ep done - 10370.\n",
      "Ep done - 10380.\n",
      "Ep done - 10390.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.49     |\n",
      "| time/              |          |\n",
      "|    fps             | 409      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 210      |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Ep done - 10400.\n",
      "Ep done - 10410.\n",
      "Ep done - 10420.\n",
      "Ep done - 10430.\n",
      "Ep done - 10440.\n",
      "Ep done - 10450.\n",
      "Ep done - 10460.\n",
      "Ep done - 10470.\n",
      "Ep done - 10480.\n",
      "Ep done - 10490.\n",
      "Ep done - 10500.\n",
      "Ep done - 10510.\n",
      "Ep done - 10520.\n",
      "Ep done - 810.\n",
      "Ep done - 820.\n",
      "Ep done - 830.\n",
      "Ep done - 840.\n",
      "Ep done - 850.\n",
      "Ep done - 860.\n",
      "Ep done - 870.\n",
      "Ep done - 880.\n",
      "Ep done - 890.\n",
      "Ep done - 900.\n",
      "Eval num_timesteps=90000, episode_reward=0.32 +/- 0.95\n",
      "Episode length: 30.10 +/- 0.41\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.32      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 90000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.8279717 |\n",
      "|    clip_fraction        | 0.305     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0435   |\n",
      "|    explained_variance   | 0.556     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.112     |\n",
      "|    n_updates            | 19370     |\n",
      "|    policy_gradient_loss | 0.00543   |\n",
      "|    value_loss           | 0.114     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.32\n",
      "SELFPLAY: new best model, bumping up generation to 6\n",
      "Ep done - 10530.\n",
      "Ep done - 10540.\n",
      "Ep done - 10550.\n",
      "Ep done - 10560.\n",
      "Ep done - 10570.\n",
      "Ep done - 10580.\n",
      "Ep done - 10590.\n",
      "Ep done - 10600.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 406      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Ep done - 10610.\n",
      "Ep done - 10620.\n",
      "Ep done - 10630.\n",
      "Ep done - 10640.\n",
      "Ep done - 10650.\n",
      "Ep done - 10660.\n",
      "Ep done - 10670.\n",
      "Ep done - 10680.\n",
      "Ep done - 10690.\n",
      "Ep done - 10700.\n",
      "Ep done - 10710.\n",
      "Ep done - 10720.\n",
      "Ep done - 10730.\n",
      "Ep done - 10740.\n",
      "Ep done - 10750.\n",
      "Ep done - 10760.\n",
      "Ep done - 10770.\n",
      "Ep done - 10780.\n",
      "Ep done - 10790.\n",
      "Ep done - 10800.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.05      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 410       |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 239       |\n",
      "|    total_timesteps      | 98304     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3929229 |\n",
      "|    clip_fraction        | 0.143     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0522   |\n",
      "|    explained_variance   | 0.0651    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0475    |\n",
      "|    n_updates            | 19380     |\n",
      "|    policy_gradient_loss | 0.0276    |\n",
      "|    value_loss           | 0.227     |\n",
      "---------------------------------------\n",
      "Ep done - 10810.\n",
      "Ep done - 10820.\n",
      "Ep done - 10830.\n",
      "Ep done - 10840.\n",
      "Ep done - 10850.\n",
      "Ep done - 10860.\n",
      "Ep done - 910.\n",
      "Ep done - 920.\n",
      "Ep done - 930.\n",
      "Ep done - 940.\n",
      "Ep done - 950.\n",
      "Ep done - 960.\n",
      "Ep done - 970.\n",
      "Ep done - 980.\n",
      "Ep done - 990.\n",
      "Ep done - 1000.\n",
      "Eval num_timesteps=100000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.08 +/- 0.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23903625 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0527    |\n",
      "|    explained_variance   | 0.486      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.023      |\n",
      "|    n_updates            | 19390      |\n",
      "|    policy_gradient_loss | 0.0109     |\n",
      "|    value_loss           | 0.0739     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 7\n",
      "Ep done - 10870.\n",
      "Ep done - 10880.\n",
      "Ep done - 10890.\n",
      "Ep done - 10900.\n",
      "Ep done - 10910.\n",
      "Ep done - 10920.\n",
      "Ep done - 10930.\n",
      "Ep done - 10940.\n",
      "Ep done - 10950.\n",
      "Ep done - 10960.\n",
      "Ep done - 10970.\n",
      "Ep done - 10980.\n",
      "Ep done - 10990.\n",
      "Ep done - 11000.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 406      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 256      |\n",
      "|    total_timesteps | 104448   |\n",
      "---------------------------------\n",
      "Ep done - 11010.\n",
      "Ep done - 11020.\n",
      "Ep done - 11030.\n",
      "Ep done - 11040.\n",
      "Ep done - 11050.\n",
      "Ep done - 11060.\n",
      "Ep done - 11070.\n",
      "Ep done - 11080.\n",
      "Ep done - 11090.\n",
      "Ep done - 11100.\n",
      "Ep done - 11110.\n",
      "Ep done - 11120.\n",
      "Ep done - 11130.\n",
      "Ep done - 11140.\n",
      "Ep done - 11150.\n",
      "Ep done - 11160.\n",
      "Ep done - 11170.\n",
      "Ep done - 11180.\n",
      "Ep done - 11190.\n",
      "Ep done - 1010.\n",
      "Ep done - 1020.\n",
      "Ep done - 1030.\n",
      "Ep done - 1040.\n",
      "Ep done - 1050.\n",
      "Ep done - 1060.\n",
      "Ep done - 1070.\n",
      "Ep done - 1080.\n",
      "Ep done - 1090.\n",
      "Ep done - 1100.\n",
      "Eval num_timesteps=110000, episode_reward=0.28 +/- 0.96\n",
      "Episode length: 30.27 +/- 0.44\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | 0.28      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 110000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8560567 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0456   |\n",
      "|    explained_variance   | 0.214     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0584    |\n",
      "|    n_updates            | 19400     |\n",
      "|    policy_gradient_loss | 0.0117    |\n",
      "|    value_loss           | 0.125     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.28\n",
      "SELFPLAY: new best model, bumping up generation to 8\n",
      "Ep done - 11200.\n",
      "Ep done - 11210.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 403      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 274      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Ep done - 11220.\n",
      "Ep done - 11230.\n",
      "Ep done - 11240.\n",
      "Ep done - 11250.\n",
      "Ep done - 11260.\n",
      "Ep done - 11270.\n",
      "Ep done - 11280.\n",
      "Ep done - 11290.\n",
      "Ep done - 11300.\n",
      "Ep done - 11310.\n",
      "Ep done - 11320.\n",
      "Ep done - 11330.\n",
      "Ep done - 11340.\n",
      "Ep done - 11350.\n",
      "Ep done - 11360.\n",
      "Ep done - 11370.\n",
      "Ep done - 11380.\n",
      "Ep done - 11390.\n",
      "Ep done - 11400.\n",
      "Ep done - 11410.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.96     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 407       |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 286       |\n",
      "|    total_timesteps      | 116736    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2813038 |\n",
      "|    clip_fraction        | 0.0729    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0385   |\n",
      "|    explained_variance   | 0.498     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0745    |\n",
      "|    n_updates            | 19410     |\n",
      "|    policy_gradient_loss | 0.00259   |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Ep done - 11420.\n",
      "Ep done - 11430.\n",
      "Ep done - 11440.\n",
      "Ep done - 11450.\n",
      "Ep done - 11460.\n",
      "Ep done - 11470.\n",
      "Ep done - 11480.\n",
      "Ep done - 11490.\n",
      "Ep done - 11500.\n",
      "Ep done - 11510.\n",
      "Ep done - 11520.\n",
      "Ep done - 1110.\n",
      "Ep done - 1120.\n",
      "Ep done - 1130.\n",
      "Ep done - 1140.\n",
      "Ep done - 1150.\n",
      "Ep done - 1160.\n",
      "Ep done - 1170.\n",
      "Ep done - 1180.\n",
      "Ep done - 1190.\n",
      "Ep done - 1200.\n",
      "Eval num_timesteps=120000, episode_reward=0.70 +/- 0.71\n",
      "Episode length: 30.42 +/- 0.51\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.4      |\n",
      "|    mean_reward          | 0.7       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 120000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9482423 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0297   |\n",
      "|    explained_variance   | -0.917    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0202   |\n",
      "|    n_updates            | 19420     |\n",
      "|    policy_gradient_loss | -0.0257   |\n",
      "|    value_loss           | 0.0142    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 9\n",
      "Ep done - 11530.\n",
      "Ep done - 11540.\n",
      "Ep done - 11550.\n",
      "Ep done - 11560.\n",
      "Ep done - 11570.\n",
      "Ep done - 11580.\n",
      "Ep done - 11590.\n",
      "Ep done - 11600.\n",
      "Ep done - 11610.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 403      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 304      |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Ep done - 11620.\n",
      "Ep done - 11630.\n",
      "Ep done - 11640.\n",
      "Ep done - 11650.\n",
      "Ep done - 11660.\n",
      "Ep done - 11670.\n",
      "Ep done - 11680.\n",
      "Ep done - 11690.\n",
      "Ep done - 11700.\n",
      "Ep done - 11710.\n",
      "Ep done - 11720.\n",
      "Ep done - 11730.\n",
      "Ep done - 11740.\n",
      "Ep done - 11750.\n",
      "Ep done - 11760.\n",
      "Ep done - 11770.\n",
      "Ep done - 11780.\n",
      "Ep done - 11790.\n",
      "Ep done - 11800.\n",
      "Ep done - 11810.\n",
      "Ep done - 11820.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.6      |\n",
      "|    ep_rew_mean          | 0.1       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 404       |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 318       |\n",
      "|    total_timesteps      | 129024    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6959146 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0532   |\n",
      "|    explained_variance   | -0.356    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.126     |\n",
      "|    n_updates            | 19430     |\n",
      "|    policy_gradient_loss | 0.184     |\n",
      "|    value_loss           | 0.107     |\n",
      "---------------------------------------\n",
      "Ep done - 11830.\n",
      "Ep done - 11840.\n",
      "Ep done - 11850.\n",
      "Ep done - 1210.\n",
      "Ep done - 1220.\n",
      "Ep done - 1230.\n",
      "Ep done - 1240.\n",
      "Ep done - 1250.\n",
      "Ep done - 1260.\n",
      "Ep done - 1270.\n",
      "Ep done - 1280.\n",
      "Ep done - 1290.\n",
      "Ep done - 1300.\n",
      "Eval num_timesteps=130000, episode_reward=0.74 +/- 0.67\n",
      "Episode length: 29.80 +/- 0.40\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.8      |\n",
      "|    mean_reward          | 0.74      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 130000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0615176 |\n",
      "|    clip_fraction        | 0.118     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.03     |\n",
      "|    explained_variance   | -0.267    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0503    |\n",
      "|    n_updates            | 19440     |\n",
      "|    policy_gradient_loss | 0.0155    |\n",
      "|    value_loss           | 0.211     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.74\n",
      "SELFPLAY: new best model, bumping up generation to 10\n",
      "Ep done - 11860.\n",
      "Ep done - 11870.\n",
      "Ep done - 11880.\n",
      "Ep done - 11890.\n",
      "Ep done - 11900.\n",
      "Ep done - 11910.\n",
      "Ep done - 11920.\n",
      "Ep done - 11930.\n",
      "Ep done - 11940.\n",
      "Ep done - 11950.\n",
      "Ep done - 11960.\n",
      "Ep done - 11970.\n",
      "Ep done - 11980.\n",
      "Ep done - 11990.\n",
      "Ep done - 12000.\n",
      "Ep done - 12010.\n",
      "Ep done - 12020.\n",
      "Ep done - 12030.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 402      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 336      |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Ep done - 12040.\n",
      "Ep done - 12050.\n",
      "Ep done - 12060.\n",
      "Ep done - 12070.\n",
      "Ep done - 12080.\n",
      "Ep done - 12090.\n",
      "Ep done - 12100.\n",
      "Ep done - 12110.\n",
      "Ep done - 12120.\n",
      "Ep done - 12130.\n",
      "Ep done - 12140.\n",
      "Ep done - 12150.\n",
      "Ep done - 12160.\n",
      "Ep done - 12170.\n",
      "Ep done - 12180.\n",
      "Ep done - 12190.\n",
      "Ep done - 1310.\n",
      "Ep done - 1320.\n",
      "Ep done - 1330.\n",
      "Ep done - 1340.\n",
      "Ep done - 1350.\n",
      "Ep done - 1360.\n",
      "Ep done - 1370.\n",
      "Ep done - 1380.\n",
      "Ep done - 1390.\n",
      "Ep done - 1400.\n",
      "Eval num_timesteps=140000, episode_reward=-0.34 +/- 0.94\n",
      "Episode length: 29.67 +/- 0.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.7       |\n",
      "|    mean_reward          | -0.34      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 140000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14291744 |\n",
      "|    clip_fraction        | 0.0595     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0252    |\n",
      "|    explained_variance   | 0.202      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0865     |\n",
      "|    n_updates            | 19450      |\n",
      "|    policy_gradient_loss | 0.0646     |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "Ep done - 12200.\n",
      "Ep done - 12210.\n",
      "Ep done - 12220.\n",
      "Ep done - 12230.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | -0.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 400      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Ep done - 12240.\n",
      "Ep done - 12250.\n",
      "Ep done - 12260.\n",
      "Ep done - 12270.\n",
      "Ep done - 12280.\n",
      "Ep done - 12290.\n",
      "Ep done - 12300.\n",
      "Ep done - 12310.\n",
      "Ep done - 12320.\n",
      "Ep done - 12330.\n",
      "Ep done - 12340.\n",
      "Ep done - 12350.\n",
      "Ep done - 12360.\n",
      "Ep done - 12370.\n",
      "Ep done - 12380.\n",
      "Ep done - 12390.\n",
      "Ep done - 12400.\n",
      "Ep done - 12410.\n",
      "Ep done - 12420.\n",
      "Ep done - 12430.\n",
      "Ep done - 12440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.6        |\n",
      "|    ep_rew_mean          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 401         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 367         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073566996 |\n",
      "|    clip_fraction        | 0.0369      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0216     |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 19460       |\n",
      "|    policy_gradient_loss | 0.000252    |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Ep done - 12450.\n",
      "Ep done - 12460.\n",
      "Ep done - 12470.\n",
      "Ep done - 12480.\n",
      "Ep done - 12490.\n",
      "Ep done - 12500.\n",
      "Ep done - 12510.\n",
      "Ep done - 12520.\n",
      "Ep done - 12530.\n",
      "Ep done - 1410.\n",
      "Ep done - 1420.\n",
      "Ep done - 1430.\n",
      "Ep done - 1440.\n",
      "Ep done - 1450.\n",
      "Ep done - 1460.\n",
      "Ep done - 1470.\n",
      "Ep done - 1480.\n",
      "Ep done - 1490.\n",
      "Ep done - 1500.\n",
      "Eval num_timesteps=150000, episode_reward=-0.06 +/- 1.00\n",
      "Episode length: 29.57 +/- 0.53\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.6       |\n",
      "|    mean_reward          | -0.06      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 150000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45669237 |\n",
      "|    clip_fraction        | 0.0531     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0139    |\n",
      "|    explained_variance   | 0.598      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0723     |\n",
      "|    n_updates            | 19470      |\n",
      "|    policy_gradient_loss | 0.00505    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "Ep done - 12540.\n",
      "Ep done - 12550.\n",
      "Ep done - 12560.\n",
      "Ep done - 12570.\n",
      "Ep done - 12580.\n",
      "Ep done - 12590.\n",
      "Ep done - 12600.\n",
      "Ep done - 12610.\n",
      "Ep done - 12620.\n",
      "Ep done - 12630.\n",
      "Ep done - 12640.\n",
      "Ep done - 12650.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.6     |\n",
      "|    ep_rew_mean     | 0.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 385      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 398      |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "Ep done - 12660.\n",
      "Ep done - 12670.\n",
      "Ep done - 12680.\n",
      "Ep done - 12690.\n",
      "Ep done - 12700.\n",
      "Ep done - 12710.\n",
      "Ep done - 12720.\n",
      "Ep done - 12730.\n",
      "Ep done - 12740.\n",
      "Ep done - 12750.\n",
      "Ep done - 12760.\n",
      "Ep done - 12770.\n",
      "Ep done - 12780.\n",
      "Ep done - 12790.\n",
      "Ep done - 12800.\n",
      "Ep done - 12810.\n",
      "Ep done - 12820.\n",
      "Ep done - 12830.\n",
      "Ep done - 12840.\n",
      "Ep done - 12850.\n",
      "Ep done - 12860.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.8      |\n",
      "|    ep_rew_mean          | 0.01      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 386       |\n",
      "|    iterations           | 26        |\n",
      "|    time_elapsed         | 413       |\n",
      "|    total_timesteps      | 159744    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1622206 |\n",
      "|    clip_fraction        | 0.0926    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0182   |\n",
      "|    explained_variance   | 0.0482    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0582    |\n",
      "|    n_updates            | 19480     |\n",
      "|    policy_gradient_loss | -0.00802  |\n",
      "|    value_loss           | 0.184     |\n",
      "---------------------------------------\n",
      "Ep done - 1510.\n",
      "Ep done - 1520.\n",
      "Ep done - 1530.\n",
      "Ep done - 1540.\n",
      "Ep done - 1550.\n",
      "Ep done - 1560.\n",
      "Ep done - 1570.\n",
      "Ep done - 1580.\n",
      "Ep done - 1590.\n",
      "Ep done - 1600.\n",
      "Eval num_timesteps=160000, episode_reward=-0.08 +/- 1.00\n",
      "Episode length: 29.84 +/- 0.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.8       |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18405616 |\n",
      "|    clip_fraction        | 0.0364     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0148    |\n",
      "|    explained_variance   | -0.534     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0604     |\n",
      "|    n_updates            | 19490      |\n",
      "|    policy_gradient_loss | -0.00463   |\n",
      "|    value_loss           | 0.136      |\n",
      "----------------------------------------\n",
      "Ep done - 12870.\n",
      "Ep done - 12880.\n",
      "Ep done - 12890.\n",
      "Ep done - 12900.\n",
      "Ep done - 12910.\n",
      "Ep done - 12920.\n",
      "Ep done - 12930.\n",
      "Ep done - 12940.\n",
      "Ep done - 12950.\n",
      "Ep done - 12960.\n",
      "Ep done - 12970.\n",
      "Ep done - 12980.\n",
      "Ep done - 12990.\n",
      "Ep done - 13000.\n",
      "Ep done - 13010.\n",
      "Ep done - 13020.\n",
      "Ep done - 13030.\n",
      "Ep done - 13040.\n",
      "Ep done - 13050.\n",
      "Ep done - 13060.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | -0.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 379      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 165888   |\n",
      "---------------------------------\n",
      "Ep done - 13070.\n",
      "Ep done - 13080.\n",
      "Ep done - 13090.\n",
      "Ep done - 13100.\n",
      "Ep done - 13110.\n",
      "Ep done - 13120.\n",
      "Ep done - 13130.\n",
      "Ep done - 13140.\n",
      "Ep done - 13150.\n",
      "Ep done - 13160.\n",
      "Ep done - 13170.\n",
      "Ep done - 13180.\n",
      "Ep done - 13190.\n",
      "Ep done - 13200.\n",
      "Ep done - 1610.\n",
      "Ep done - 1620.\n",
      "Ep done - 1630.\n",
      "Ep done - 1640.\n",
      "Ep done - 1650.\n",
      "Ep done - 1660.\n",
      "Ep done - 1670.\n",
      "Ep done - 1680.\n",
      "Ep done - 1690.\n",
      "Ep done - 1700.\n",
      "Eval num_timesteps=170000, episode_reward=0.56 +/- 0.71\n",
      "Episode length: 29.89 +/- 0.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.9       |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 170000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21157335 |\n",
      "|    clip_fraction        | 0.0472     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0154    |\n",
      "|    explained_variance   | 0.52       |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0595     |\n",
      "|    n_updates            | 19500      |\n",
      "|    policy_gradient_loss | 0.00333    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 11\n",
      "Ep done - 13210.\n",
      "Ep done - 13220.\n",
      "Ep done - 13230.\n",
      "Ep done - 13240.\n",
      "Ep done - 13250.\n",
      "Ep done - 13260.\n",
      "Ep done - 13270.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.6     |\n",
      "|    ep_rew_mean     | -0.14    |\n",
      "| time/              |          |\n",
      "|    fps             | 378      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 455      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Ep done - 13280.\n",
      "Ep done - 13290.\n",
      "Ep done - 13300.\n",
      "Ep done - 13310.\n",
      "Ep done - 13320.\n",
      "Ep done - 13330.\n",
      "Ep done - 13340.\n",
      "Ep done - 13350.\n",
      "Ep done - 13360.\n",
      "Ep done - 13370.\n",
      "Ep done - 13380.\n",
      "Ep done - 13390.\n",
      "Ep done - 13400.\n",
      "Ep done - 13410.\n",
      "Ep done - 13420.\n",
      "Ep done - 13430.\n",
      "Ep done - 13440.\n",
      "Ep done - 13450.\n",
      "Ep done - 13460.\n",
      "Ep done - 13470.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.7      |\n",
      "|    ep_rew_mean          | 0.3       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 380       |\n",
      "|    iterations           | 29        |\n",
      "|    time_elapsed         | 467       |\n",
      "|    total_timesteps      | 178176    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7306719 |\n",
      "|    clip_fraction        | 0.0859    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0251   |\n",
      "|    explained_variance   | -0.242    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0641    |\n",
      "|    n_updates            | 19510     |\n",
      "|    policy_gradient_loss | -0.00877  |\n",
      "|    value_loss           | 0.174     |\n",
      "---------------------------------------\n",
      "Ep done - 13480.\n",
      "Ep done - 13490.\n",
      "Ep done - 13500.\n",
      "Ep done - 13510.\n",
      "Ep done - 13520.\n",
      "Ep done - 13530.\n",
      "Ep done - 13540.\n",
      "Ep done - 1710.\n",
      "Ep done - 1720.\n",
      "Ep done - 1730.\n",
      "Ep done - 1740.\n",
      "Ep done - 1750.\n",
      "Ep done - 1760.\n",
      "Ep done - 1770.\n",
      "Ep done - 1780.\n",
      "Ep done - 1790.\n",
      "Ep done - 1800.\n",
      "Eval num_timesteps=180000, episode_reward=0.06 +/- 0.85\n",
      "Episode length: 30.00 +/- 0.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.06       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 180000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.98243666 |\n",
      "|    clip_fraction        | 0.0775     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0186    |\n",
      "|    explained_variance   | 0.212      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00494    |\n",
      "|    n_updates            | 19520      |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    value_loss           | 0.0306     |\n",
      "----------------------------------------\n",
      "Ep done - 13550.\n",
      "Ep done - 13560.\n",
      "Ep done - 13570.\n",
      "Ep done - 13580.\n",
      "Ep done - 13590.\n",
      "Ep done - 13600.\n",
      "Ep done - 13610.\n",
      "Ep done - 13620.\n",
      "Ep done - 13630.\n",
      "Ep done - 13640.\n",
      "Ep done - 13650.\n",
      "Ep done - 13660.\n",
      "Ep done - 13670.\n",
      "Ep done - 13680.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.11     |\n",
      "| time/              |          |\n",
      "|    fps             | 379      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 485      |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n",
      "Ep done - 13690.\n",
      "Ep done - 13700.\n",
      "Ep done - 13710.\n",
      "Ep done - 13720.\n",
      "Ep done - 13730.\n",
      "Ep done - 13740.\n",
      "Ep done - 13750.\n",
      "Ep done - 13760.\n",
      "Ep done - 13770.\n",
      "Ep done - 13780.\n",
      "Ep done - 13790.\n",
      "Ep done - 13800.\n",
      "Ep done - 13810.\n",
      "Ep done - 13820.\n",
      "Ep done - 13830.\n",
      "Ep done - 13840.\n",
      "Ep done - 13850.\n",
      "Ep done - 13860.\n",
      "Ep done - 13870.\n",
      "Ep done - 1810.\n",
      "Ep done - 1820.\n",
      "Ep done - 1830.\n",
      "Ep done - 1840.\n",
      "Ep done - 1850.\n",
      "Ep done - 1860.\n",
      "Ep done - 1870.\n",
      "Ep done - 1880.\n",
      "Ep done - 1890.\n",
      "Ep done - 1900.\n",
      "Eval num_timesteps=190000, episode_reward=0.88 +/- 0.47\n",
      "Episode length: 30.27 +/- 0.51\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | 0.88      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 190000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.7654433 |\n",
      "|    clip_fraction        | 0.295     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0217   |\n",
      "|    explained_variance   | -1.1      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0295    |\n",
      "|    n_updates            | 19530     |\n",
      "|    policy_gradient_loss | -0.00455  |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.88\n",
      "SELFPLAY: new best model, bumping up generation to 12\n",
      "Ep done - 13880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.74     |\n",
      "| time/              |          |\n",
      "|    fps             | 378      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 503      |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "Ep done - 13890.\n",
      "Ep done - 13900.\n",
      "Ep done - 13910.\n",
      "Ep done - 13920.\n",
      "Ep done - 13930.\n",
      "Ep done - 13940.\n",
      "Ep done - 13950.\n",
      "Ep done - 13960.\n",
      "Ep done - 13970.\n",
      "Ep done - 13980.\n",
      "Ep done - 13990.\n",
      "Ep done - 14000.\n",
      "Ep done - 14010.\n",
      "Ep done - 14020.\n",
      "Ep done - 14030.\n",
      "Ep done - 14040.\n",
      "Ep done - 14050.\n",
      "Ep done - 14060.\n",
      "Ep done - 14070.\n",
      "Ep done - 14080.\n",
      "Ep done - 14090.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.21     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 380       |\n",
      "|    iterations           | 32        |\n",
      "|    time_elapsed         | 516       |\n",
      "|    total_timesteps      | 196608    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 5.8002377 |\n",
      "|    clip_fraction        | 0.246     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0329   |\n",
      "|    explained_variance   | 0.131     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0368    |\n",
      "|    n_updates            | 19540     |\n",
      "|    policy_gradient_loss | 0.0102    |\n",
      "|    value_loss           | 0.0564    |\n",
      "---------------------------------------\n",
      "Ep done - 14100.\n",
      "Ep done - 14110.\n",
      "Ep done - 14120.\n",
      "Ep done - 14130.\n",
      "Ep done - 14140.\n",
      "Ep done - 14150.\n",
      "Ep done - 14160.\n",
      "Ep done - 14170.\n",
      "Ep done - 14180.\n",
      "Ep done - 14190.\n",
      "Ep done - 14200.\n",
      "Ep done - 1910.\n",
      "Ep done - 1920.\n",
      "Ep done - 1930.\n",
      "Ep done - 1940.\n",
      "Ep done - 1950.\n",
      "Ep done - 1960.\n",
      "Ep done - 1970.\n",
      "Ep done - 1980.\n",
      "Ep done - 1990.\n",
      "Ep done - 2000.\n",
      "Eval num_timesteps=200000, episode_reward=0.09 +/- 0.99\n",
      "Episode length: 30.25 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | 0.09      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 200000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 6.1819463 |\n",
      "|    clip_fraction        | 0.322     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0488   |\n",
      "|    explained_variance   | 0.454     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0181    |\n",
      "|    n_updates            | 19550     |\n",
      "|    policy_gradient_loss | 0.00514   |\n",
      "|    value_loss           | 0.0913    |\n",
      "---------------------------------------\n",
      "Ep done - 14210.\n",
      "Ep done - 14220.\n",
      "Ep done - 14230.\n",
      "Ep done - 14240.\n",
      "Ep done - 14250.\n",
      "Ep done - 14260.\n",
      "Ep done - 14270.\n",
      "Ep done - 14280.\n",
      "Ep done - 14290.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 379      |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 534      |\n",
      "|    total_timesteps | 202752   |\n",
      "---------------------------------\n",
      "Ep done - 14300.\n",
      "Ep done - 14310.\n",
      "Ep done - 14320.\n",
      "Ep done - 14330.\n",
      "Ep done - 14340.\n",
      "Ep done - 14350.\n",
      "Ep done - 14360.\n",
      "Ep done - 14370.\n",
      "Ep done - 14380.\n",
      "Ep done - 14390.\n",
      "Ep done - 14400.\n",
      "Ep done - 14410.\n",
      "Ep done - 14420.\n",
      "Ep done - 14430.\n",
      "Ep done - 14440.\n",
      "Ep done - 14450.\n",
      "Ep done - 14460.\n",
      "Ep done - 14470.\n",
      "Ep done - 14480.\n",
      "Ep done - 14490.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.34      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 380       |\n",
      "|    iterations           | 34        |\n",
      "|    time_elapsed         | 548       |\n",
      "|    total_timesteps      | 208896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8507929 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0423   |\n",
      "|    explained_variance   | 0.582     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.156     |\n",
      "|    n_updates            | 19560     |\n",
      "|    policy_gradient_loss | 0.0105    |\n",
      "|    value_loss           | 0.114     |\n",
      "---------------------------------------\n",
      "Ep done - 14500.\n",
      "Ep done - 14510.\n",
      "Ep done - 14520.\n",
      "Ep done - 14530.\n",
      "Ep done - 2010.\n",
      "Ep done - 2020.\n",
      "Ep done - 2030.\n",
      "Ep done - 2040.\n",
      "Ep done - 2050.\n",
      "Ep done - 2060.\n",
      "Ep done - 2070.\n",
      "Ep done - 2080.\n",
      "Ep done - 2090.\n",
      "Ep done - 2100.\n",
      "Eval num_timesteps=210000, episode_reward=0.67 +/- 0.63\n",
      "Episode length: 30.36 +/- 0.48\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.4      |\n",
      "|    mean_reward          | 0.67      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 210000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6341624 |\n",
      "|    clip_fraction        | 0.124     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0459   |\n",
      "|    explained_variance   | 0.616     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.000839 |\n",
      "|    n_updates            | 19570     |\n",
      "|    policy_gradient_loss | -0.0057   |\n",
      "|    value_loss           | 0.081     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.67\n",
      "SELFPLAY: new best model, bumping up generation to 13\n",
      "Ep done - 14540.\n",
      "Ep done - 14550.\n",
      "Ep done - 14560.\n",
      "Ep done - 14570.\n",
      "Ep done - 14580.\n",
      "Ep done - 14590.\n",
      "Ep done - 14600.\n",
      "Ep done - 14610.\n",
      "Ep done - 14620.\n",
      "Ep done - 14630.\n",
      "Ep done - 14640.\n",
      "Ep done - 14650.\n",
      "Ep done - 14660.\n",
      "Ep done - 14670.\n",
      "Ep done - 14680.\n",
      "Ep done - 14690.\n",
      "Ep done - 14700.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 379      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 566      |\n",
      "|    total_timesteps | 215040   |\n",
      "---------------------------------\n",
      "Ep done - 14710.\n",
      "Ep done - 14720.\n",
      "Ep done - 14730.\n",
      "Ep done - 14740.\n",
      "Ep done - 14750.\n",
      "Ep done - 14760.\n",
      "Ep done - 14770.\n",
      "Ep done - 14780.\n",
      "Ep done - 14790.\n",
      "Ep done - 14800.\n",
      "Ep done - 14810.\n",
      "Ep done - 14820.\n",
      "Ep done - 14830.\n",
      "Ep done - 14840.\n",
      "Ep done - 14850.\n",
      "Ep done - 14860.\n",
      "Ep done - 2110.\n",
      "Ep done - 2120.\n",
      "Ep done - 2130.\n",
      "Ep done - 2140.\n",
      "Ep done - 2150.\n",
      "Ep done - 2160.\n",
      "Ep done - 2170.\n",
      "Ep done - 2180.\n",
      "Ep done - 2190.\n",
      "Ep done - 2200.\n",
      "Eval num_timesteps=220000, episode_reward=-0.35 +/- 0.93\n",
      "Episode length: 30.20 +/- 0.57\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | -0.35     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 220000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0432181 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0538   |\n",
      "|    explained_variance   | 0.384     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0873    |\n",
      "|    n_updates            | 19580     |\n",
      "|    policy_gradient_loss | 0.014     |\n",
      "|    value_loss           | 0.161     |\n",
      "---------------------------------------\n",
      "Ep done - 14870.\n",
      "Ep done - 14880.\n",
      "Ep done - 14890.\n",
      "Ep done - 14900.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 379      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 583      |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "Ep done - 14910.\n",
      "Ep done - 14920.\n",
      "Ep done - 14930.\n",
      "Ep done - 14940.\n",
      "Ep done - 14950.\n",
      "Ep done - 14960.\n",
      "Ep done - 14970.\n",
      "Ep done - 14980.\n",
      "Ep done - 14990.\n",
      "Ep done - 15000.\n",
      "Ep done - 15010.\n",
      "Ep done - 15020.\n",
      "Ep done - 15030.\n",
      "Ep done - 15040.\n",
      "Ep done - 15050.\n",
      "Ep done - 15060.\n",
      "Ep done - 15070.\n",
      "Ep done - 15080.\n",
      "Ep done - 15090.\n",
      "Ep done - 15100.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.05       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 596        |\n",
      "|    total_timesteps      | 227328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73718816 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0821    |\n",
      "|    explained_variance   | -0.149     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0931     |\n",
      "|    n_updates            | 19590      |\n",
      "|    policy_gradient_loss | 0.0336     |\n",
      "|    value_loss           | 0.204      |\n",
      "----------------------------------------\n",
      "Ep done - 15110.\n",
      "Ep done - 15120.\n",
      "Ep done - 15130.\n",
      "Ep done - 15140.\n",
      "Ep done - 15150.\n",
      "Ep done - 15160.\n",
      "Ep done - 15170.\n",
      "Ep done - 15180.\n",
      "Ep done - 15190.\n",
      "Ep done - 2210.\n",
      "Ep done - 2220.\n",
      "Ep done - 2230.\n",
      "Ep done - 2240.\n",
      "Ep done - 2250.\n",
      "Ep done - 2260.\n",
      "Ep done - 2270.\n",
      "Ep done - 2280.\n",
      "Ep done - 2290.\n",
      "Ep done - 2300.\n",
      "Eval num_timesteps=230000, episode_reward=0.33 +/- 0.90\n",
      "Episode length: 29.84 +/- 0.86\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.8       |\n",
      "|    mean_reward          | 0.33       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 230000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55680263 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0445    |\n",
      "|    explained_variance   | 0.0507     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0898     |\n",
      "|    n_updates            | 19600      |\n",
      "|    policy_gradient_loss | 0.00222    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.33\n",
      "SELFPLAY: new best model, bumping up generation to 14\n",
      "Ep done - 15200.\n",
      "Ep done - 15210.\n",
      "Ep done - 15220.\n",
      "Ep done - 15230.\n",
      "Ep done - 15240.\n",
      "Ep done - 15250.\n",
      "Ep done - 15260.\n",
      "Ep done - 15270.\n",
      "Ep done - 15280.\n",
      "Ep done - 15290.\n",
      "Ep done - 15300.\n",
      "Ep done - 15310.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 380      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 613      |\n",
      "|    total_timesteps | 233472   |\n",
      "---------------------------------\n",
      "Ep done - 15320.\n",
      "Ep done - 15330.\n",
      "Ep done - 15340.\n",
      "Ep done - 15350.\n",
      "Ep done - 15360.\n",
      "Ep done - 15370.\n",
      "Ep done - 15380.\n",
      "Ep done - 15390.\n",
      "Ep done - 15400.\n",
      "Ep done - 15410.\n",
      "Ep done - 15420.\n",
      "Ep done - 15430.\n",
      "Ep done - 15440.\n",
      "Ep done - 15450.\n",
      "Ep done - 15460.\n",
      "Ep done - 15470.\n",
      "Ep done - 15480.\n",
      "Ep done - 15490.\n",
      "Ep done - 15500.\n",
      "Ep done - 15510.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.5      |\n",
      "|    ep_rew_mean          | 0.09      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 382       |\n",
      "|    iterations           | 39        |\n",
      "|    time_elapsed         | 626       |\n",
      "|    total_timesteps      | 239616    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7569933 |\n",
      "|    clip_fraction        | 0.13      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.038    |\n",
      "|    explained_variance   | 0.272     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0554    |\n",
      "|    n_updates            | 19610     |\n",
      "|    policy_gradient_loss | 0.00895   |\n",
      "|    value_loss           | 0.177     |\n",
      "---------------------------------------\n",
      "Ep done - 15520.\n",
      "Ep done - 2310.\n",
      "Ep done - 2320.\n",
      "Ep done - 2330.\n",
      "Ep done - 2340.\n",
      "Ep done - 2350.\n",
      "Ep done - 2360.\n",
      "Ep done - 2370.\n",
      "Ep done - 2380.\n",
      "Ep done - 2390.\n",
      "Ep done - 2400.\n",
      "Eval num_timesteps=240000, episode_reward=0.25 +/- 0.96\n",
      "Episode length: 30.50 +/- 0.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.5       |\n",
      "|    mean_reward          | 0.25       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 240000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.99168414 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0349    |\n",
      "|    explained_variance   | -0.0637    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0261     |\n",
      "|    n_updates            | 19620      |\n",
      "|    policy_gradient_loss | 0.00486    |\n",
      "|    value_loss           | 0.149      |\n",
      "----------------------------------------\n",
      "Ep done - 15530.\n",
      "Ep done - 15540.\n",
      "Ep done - 15550.\n",
      "Ep done - 15560.\n",
      "Ep done - 15570.\n",
      "Ep done - 15580.\n",
      "Ep done - 15590.\n",
      "Ep done - 15600.\n",
      "Ep done - 15610.\n",
      "Ep done - 15620.\n",
      "Ep done - 15630.\n",
      "Ep done - 15640.\n",
      "Ep done - 15650.\n",
      "Ep done - 15660.\n",
      "Ep done - 15670.\n",
      "Ep done - 15680.\n",
      "Ep done - 15690.\n",
      "Ep done - 15700.\n",
      "Ep done - 15710.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.6     |\n",
      "|    ep_rew_mean     | 0.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 380      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 645      |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Ep done - 15720.\n",
      "Ep done - 15730.\n",
      "Ep done - 15740.\n",
      "Ep done - 15750.\n",
      "Ep done - 15760.\n",
      "Ep done - 15770.\n",
      "Ep done - 15780.\n",
      "Ep done - 15790.\n",
      "Ep done - 15800.\n",
      "Ep done - 15810.\n",
      "Ep done - 15820.\n",
      "Ep done - 15830.\n",
      "Ep done - 15840.\n",
      "Ep done - 15850.\n",
      "Ep done - 2410.\n",
      "Ep done - 2420.\n",
      "Ep done - 2430.\n",
      "Ep done - 2440.\n",
      "Ep done - 2450.\n",
      "Ep done - 2460.\n",
      "Ep done - 2470.\n",
      "Ep done - 2480.\n",
      "Ep done - 2490.\n",
      "Ep done - 2500.\n",
      "Eval num_timesteps=250000, episode_reward=-0.12 +/- 0.99\n",
      "Episode length: 30.30 +/- 0.52\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | -0.12     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 250000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9945411 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0292   |\n",
      "|    explained_variance   | 0.476     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0384    |\n",
      "|    n_updates            | 19630     |\n",
      "|    policy_gradient_loss | 0.0212    |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "Ep done - 15860.\n",
      "Ep done - 15870.\n",
      "Ep done - 15880.\n",
      "Ep done - 15890.\n",
      "Ep done - 15900.\n",
      "Ep done - 15910.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | -0.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 378      |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 665      |\n",
      "|    total_timesteps | 251904   |\n",
      "---------------------------------\n",
      "Ep done - 15920.\n",
      "Ep done - 15930.\n",
      "Ep done - 15940.\n",
      "Ep done - 15950.\n",
      "Ep done - 15960.\n",
      "Ep done - 15970.\n",
      "Ep done - 15980.\n",
      "Ep done - 15990.\n",
      "Ep done - 16000.\n",
      "Ep done - 16010.\n",
      "Ep done - 16020.\n",
      "Ep done - 16030.\n",
      "Ep done - 16040.\n",
      "Ep done - 16050.\n",
      "Ep done - 16060.\n",
      "Ep done - 16070.\n",
      "Ep done - 16080.\n",
      "Ep done - 16090.\n",
      "Ep done - 16100.\n",
      "Ep done - 16110.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | 0.04       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 681        |\n",
      "|    total_timesteps      | 258048     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.89014894 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0203    |\n",
      "|    explained_variance   | 0.59       |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00751   |\n",
      "|    n_updates            | 19640      |\n",
      "|    policy_gradient_loss | 0.0189     |\n",
      "|    value_loss           | 0.0478     |\n",
      "----------------------------------------\n",
      "Ep done - 16120.\n",
      "Ep done - 16130.\n",
      "Ep done - 16140.\n",
      "Ep done - 16150.\n",
      "Ep done - 16160.\n",
      "Ep done - 16170.\n",
      "Ep done - 16180.\n",
      "Ep done - 2510.\n",
      "Ep done - 2520.\n",
      "Ep done - 2530.\n",
      "Ep done - 2540.\n",
      "Ep done - 2550.\n",
      "Ep done - 2560.\n",
      "Ep done - 2570.\n",
      "Ep done - 2580.\n",
      "Ep done - 2590.\n",
      "Ep done - 2600.\n",
      "Eval num_timesteps=260000, episode_reward=0.26 +/- 0.97\n",
      "Episode length: 30.65 +/- 0.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.6       |\n",
      "|    mean_reward          | 0.26       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 260000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30660704 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0473    |\n",
      "|    explained_variance   | 0.66       |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.087      |\n",
      "|    n_updates            | 19650      |\n",
      "|    policy_gradient_loss | 0.00981    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.26\n",
      "SELFPLAY: new best model, bumping up generation to 15\n",
      "Ep done - 16190.\n",
      "Ep done - 16200.\n",
      "Ep done - 16210.\n",
      "Ep done - 16220.\n",
      "Ep done - 16230.\n",
      "Ep done - 16240.\n",
      "Ep done - 16250.\n",
      "Ep done - 16260.\n",
      "Ep done - 16270.\n",
      "Ep done - 16280.\n",
      "Ep done - 16290.\n",
      "Ep done - 16300.\n",
      "Ep done - 16310.\n",
      "Ep done - 16320.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 377      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 699      |\n",
      "|    total_timesteps | 264192   |\n",
      "---------------------------------\n",
      "Ep done - 16330.\n",
      "Ep done - 16340.\n",
      "Ep done - 16350.\n",
      "Ep done - 16360.\n",
      "Ep done - 16370.\n",
      "Ep done - 16380.\n",
      "Ep done - 16390.\n",
      "Ep done - 16400.\n",
      "Ep done - 16410.\n",
      "Ep done - 16420.\n",
      "Ep done - 16430.\n",
      "Ep done - 16440.\n",
      "Ep done - 16450.\n",
      "Ep done - 16460.\n",
      "Ep done - 16470.\n",
      "Ep done - 16480.\n",
      "Ep done - 16490.\n",
      "Ep done - 16500.\n",
      "Ep done - 16510.\n",
      "Ep done - 2610.\n",
      "Ep done - 2620.\n",
      "Ep done - 2630.\n",
      "Ep done - 2640.\n",
      "Ep done - 2650.\n",
      "Ep done - 2660.\n",
      "Ep done - 2670.\n",
      "Ep done - 2680.\n",
      "Ep done - 2690.\n",
      "Ep done - 2700.\n",
      "Eval num_timesteps=270000, episode_reward=-0.47 +/- 0.88\n",
      "Episode length: 30.09 +/- 0.29\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.47     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 270000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2925959 |\n",
      "|    clip_fraction        | 0.172     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.037    |\n",
      "|    explained_variance   | 0.28      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0897    |\n",
      "|    n_updates            | 19660     |\n",
      "|    policy_gradient_loss | 0.00601   |\n",
      "|    value_loss           | 0.17      |\n",
      "---------------------------------------\n",
      "Ep done - 16520.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 377      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 716      |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "Ep done - 16530.\n",
      "Ep done - 16540.\n",
      "Ep done - 16550.\n",
      "Ep done - 16560.\n",
      "Ep done - 16570.\n",
      "Ep done - 16580.\n",
      "Ep done - 16590.\n",
      "Ep done - 16600.\n",
      "Ep done - 16610.\n",
      "Ep done - 16620.\n",
      "Ep done - 16630.\n",
      "Ep done - 16640.\n",
      "Ep done - 16650.\n",
      "Ep done - 16660.\n",
      "Ep done - 16670.\n",
      "Ep done - 16680.\n",
      "Ep done - 16690.\n",
      "Ep done - 16700.\n",
      "Ep done - 16710.\n",
      "Ep done - 16720.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.6      |\n",
      "|    ep_rew_mean          | 0.65      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 378       |\n",
      "|    iterations           | 45        |\n",
      "|    time_elapsed         | 729       |\n",
      "|    total_timesteps      | 276480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2662771 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0296   |\n",
      "|    explained_variance   | 0.268     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0682    |\n",
      "|    n_updates            | 19670     |\n",
      "|    policy_gradient_loss | 0.0121    |\n",
      "|    value_loss           | 0.0957    |\n",
      "---------------------------------------\n",
      "Ep done - 16730.\n",
      "Ep done - 16740.\n",
      "Ep done - 16750.\n",
      "Ep done - 16760.\n",
      "Ep done - 16770.\n",
      "Ep done - 16780.\n",
      "Ep done - 16790.\n",
      "Ep done - 16800.\n",
      "Ep done - 16810.\n",
      "Ep done - 16820.\n",
      "Ep done - 16830.\n",
      "Ep done - 16840.\n",
      "Ep done - 2710.\n",
      "Ep done - 2720.\n",
      "Ep done - 2730.\n",
      "Ep done - 2740.\n",
      "Ep done - 2750.\n",
      "Ep done - 2760.\n",
      "Ep done - 2770.\n",
      "Ep done - 2780.\n",
      "Ep done - 2790.\n",
      "Ep done - 2800.\n",
      "Eval num_timesteps=280000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.36 +/- 0.50\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.4     |\n",
      "|    mean_reward          | 0.98     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 280000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.821294 |\n",
      "|    clip_fraction        | 0.0912   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0181  |\n",
      "|    explained_variance   | 0.094    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0761   |\n",
      "|    n_updates            | 19680    |\n",
      "|    policy_gradient_loss | -0.00996 |\n",
      "|    value_loss           | 0.112    |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 16\n",
      "Ep done - 16850.\n",
      "Ep done - 16860.\n",
      "Ep done - 16870.\n",
      "Ep done - 16880.\n",
      "Ep done - 16890.\n",
      "Ep done - 16900.\n",
      "Ep done - 16910.\n",
      "Ep done - 16920.\n",
      "Ep done - 16930.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 377      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 747      |\n",
      "|    total_timesteps | 282624   |\n",
      "---------------------------------\n",
      "Ep done - 16940.\n",
      "Ep done - 16950.\n",
      "Ep done - 16960.\n",
      "Ep done - 16970.\n",
      "Ep done - 16980.\n",
      "Ep done - 16990.\n",
      "Ep done - 17000.\n",
      "Ep done - 17010.\n",
      "Ep done - 17020.\n",
      "Ep done - 17030.\n",
      "Ep done - 17040.\n",
      "Ep done - 17050.\n",
      "Ep done - 17060.\n",
      "Ep done - 17070.\n",
      "Ep done - 17080.\n",
      "Ep done - 17090.\n",
      "Ep done - 17100.\n",
      "Ep done - 17110.\n",
      "Ep done - 17120.\n",
      "Ep done - 17130.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | 0.63       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 761        |\n",
      "|    total_timesteps      | 288768     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86781096 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0312    |\n",
      "|    explained_variance   | 0.251      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.0105    |\n",
      "|    n_updates            | 19690      |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    value_loss           | 0.0607     |\n",
      "----------------------------------------\n",
      "Ep done - 17140.\n",
      "Ep done - 17150.\n",
      "Ep done - 17160.\n",
      "Ep done - 17170.\n",
      "Ep done - 2810.\n",
      "Ep done - 2820.\n",
      "Ep done - 2830.\n",
      "Ep done - 2840.\n",
      "Ep done - 2850.\n",
      "Ep done - 2860.\n",
      "Ep done - 2870.\n",
      "Ep done - 2880.\n",
      "Ep done - 2890.\n",
      "Ep done - 2900.\n",
      "Eval num_timesteps=290000, episode_reward=0.53 +/- 0.84\n",
      "Episode length: 30.02 +/- 0.28\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.53      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 290000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6184222 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0234   |\n",
      "|    explained_variance   | 0.138     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0539    |\n",
      "|    n_updates            | 19700     |\n",
      "|    policy_gradient_loss | 0.0174    |\n",
      "|    value_loss           | 0.0995    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.53\n",
      "SELFPLAY: new best model, bumping up generation to 17\n",
      "Ep done - 17180.\n",
      "Ep done - 17190.\n",
      "Ep done - 17200.\n",
      "Ep done - 17210.\n",
      "Ep done - 17220.\n",
      "Ep done - 17230.\n",
      "Ep done - 17240.\n",
      "Ep done - 17250.\n",
      "Ep done - 17260.\n",
      "Ep done - 17270.\n",
      "Ep done - 17280.\n",
      "Ep done - 17290.\n",
      "Ep done - 17300.\n",
      "Ep done - 17310.\n",
      "Ep done - 17320.\n",
      "Ep done - 17330.\n",
      "Ep done - 17340.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.2     |\n",
      "|    ep_rew_mean     | 0.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 378      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 778      |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Ep done - 17350.\n",
      "Ep done - 17360.\n",
      "Ep done - 17370.\n",
      "Ep done - 17380.\n",
      "Ep done - 17390.\n",
      "Ep done - 17400.\n",
      "Ep done - 17410.\n",
      "Ep done - 17420.\n",
      "Ep done - 17430.\n",
      "Ep done - 17440.\n",
      "Ep done - 17450.\n",
      "Ep done - 17460.\n",
      "Ep done - 17470.\n",
      "Ep done - 17480.\n",
      "Ep done - 17490.\n",
      "Ep done - 17500.\n",
      "Ep done - 17510.\n",
      "Ep done - 2910.\n",
      "Ep done - 2920.\n",
      "Ep done - 2930.\n",
      "Ep done - 2940.\n",
      "Ep done - 2950.\n",
      "Ep done - 2960.\n",
      "Ep done - 2970.\n",
      "Ep done - 2980.\n",
      "Ep done - 2990.\n",
      "Ep done - 3000.\n",
      "Eval num_timesteps=300000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 29.52 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.5       |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 300000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42213234 |\n",
      "|    clip_fraction        | 0.0585     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0087    |\n",
      "|    explained_variance   | -0.479     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0541     |\n",
      "|    n_updates            | 19710      |\n",
      "|    policy_gradient_loss | 0.00272    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 18\n",
      "Ep done - 17520.\n",
      "Ep done - 17530.\n",
      "Ep done - 17540.\n",
      "Ep done - 17550.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | 0.78     |\n",
      "| time/              |          |\n",
      "|    fps             | 377      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 796      |\n",
      "|    total_timesteps | 301056   |\n",
      "---------------------------------\n",
      "Ep done - 17560.\n",
      "Ep done - 17570.\n",
      "Ep done - 17580.\n",
      "Ep done - 17590.\n",
      "Ep done - 17600.\n",
      "Ep done - 17610.\n",
      "Ep done - 17620.\n",
      "Ep done - 17630.\n",
      "Ep done - 17640.\n",
      "Ep done - 17650.\n",
      "Ep done - 17660.\n",
      "Ep done - 17670.\n",
      "Ep done - 17680.\n",
      "Ep done - 17690.\n",
      "Ep done - 17700.\n",
      "Ep done - 17710.\n",
      "Ep done - 17720.\n",
      "Ep done - 17730.\n",
      "Ep done - 17740.\n",
      "Ep done - 17750.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | -0.14     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 377       |\n",
      "|    iterations           | 50        |\n",
      "|    time_elapsed         | 813       |\n",
      "|    total_timesteps      | 307200    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4319686 |\n",
      "|    clip_fraction        | 0.0251    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00543  |\n",
      "|    explained_variance   | 0.0447    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.185     |\n",
      "|    n_updates            | 19720     |\n",
      "|    policy_gradient_loss | 0.000488  |\n",
      "|    value_loss           | 0.0687    |\n",
      "---------------------------------------\n",
      "Ep done - 17760.\n",
      "Ep done - 17770.\n",
      "Ep done - 17780.\n",
      "Ep done - 17790.\n",
      "Ep done - 17800.\n",
      "Ep done - 17810.\n",
      "Ep done - 17820.\n",
      "Ep done - 17830.\n",
      "Ep done - 17840.\n",
      "Ep done - 3010.\n",
      "Ep done - 3020.\n",
      "Ep done - 3030.\n",
      "Ep done - 3040.\n",
      "Ep done - 3050.\n",
      "Ep done - 3060.\n",
      "Ep done - 3070.\n",
      "Ep done - 3080.\n",
      "Ep done - 3090.\n",
      "Ep done - 3100.\n",
      "Eval num_timesteps=310000, episode_reward=0.01 +/- 0.99\n",
      "Episode length: 30.03 +/- 0.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.01      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 310000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8747172 |\n",
      "|    clip_fraction        | 0.231     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.035    |\n",
      "|    explained_variance   | -0.187    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.107     |\n",
      "|    n_updates            | 19730     |\n",
      "|    policy_gradient_loss | 0.033     |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "Ep done - 17850.\n",
      "Ep done - 17860.\n",
      "Ep done - 17870.\n",
      "Ep done - 17880.\n",
      "Ep done - 17890.\n",
      "Ep done - 17900.\n",
      "Ep done - 17910.\n",
      "Ep done - 17920.\n",
      "Ep done - 17930.\n",
      "Ep done - 17940.\n",
      "Ep done - 17950.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 373      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 837      |\n",
      "|    total_timesteps | 313344   |\n",
      "---------------------------------\n",
      "Ep done - 17960.\n",
      "Ep done - 17970.\n",
      "Ep done - 17980.\n",
      "Ep done - 17990.\n",
      "Ep done - 18000.\n",
      "Ep done - 18010.\n",
      "Ep done - 18020.\n",
      "Ep done - 18030.\n",
      "Ep done - 18040.\n",
      "Ep done - 18050.\n",
      "Ep done - 18060.\n",
      "Ep done - 18070.\n",
      "Ep done - 18080.\n",
      "Ep done - 18090.\n",
      "Ep done - 18100.\n",
      "Ep done - 18110.\n",
      "Ep done - 18120.\n",
      "Ep done - 18130.\n",
      "Ep done - 18140.\n",
      "Ep done - 18150.\n",
      "Ep done - 18160.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.8       |\n",
      "|    ep_rew_mean          | 0.38       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 851        |\n",
      "|    total_timesteps      | 319488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59114563 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0331    |\n",
      "|    explained_variance   | -0.481     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0527     |\n",
      "|    n_updates            | 19740      |\n",
      "|    policy_gradient_loss | 0.00969    |\n",
      "|    value_loss           | 0.122      |\n",
      "----------------------------------------\n",
      "Ep done - 18170.\n",
      "Ep done - 18180.\n",
      "Ep done - 3110.\n",
      "Ep done - 3120.\n",
      "Ep done - 3130.\n",
      "Ep done - 3140.\n",
      "Ep done - 3150.\n",
      "Ep done - 3160.\n",
      "Ep done - 3170.\n",
      "Ep done - 3180.\n",
      "Ep done - 3190.\n",
      "Ep done - 3200.\n",
      "Eval num_timesteps=320000, episode_reward=-0.25 +/- 0.92\n",
      "Episode length: 30.05 +/- 0.36\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.25     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 320000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.0606017 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0261   |\n",
      "|    explained_variance   | -0.825    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00209  |\n",
      "|    n_updates            | 19750     |\n",
      "|    policy_gradient_loss | 0.0333    |\n",
      "|    value_loss           | 0.127     |\n",
      "---------------------------------------\n",
      "Ep done - 18190.\n",
      "Ep done - 18200.\n",
      "Ep done - 18210.\n",
      "Ep done - 18220.\n",
      "Ep done - 18230.\n",
      "Ep done - 18240.\n",
      "Ep done - 18250.\n",
      "Ep done - 18260.\n",
      "Ep done - 18270.\n",
      "Ep done - 18280.\n",
      "Ep done - 18290.\n",
      "Ep done - 18300.\n",
      "Ep done - 18310.\n",
      "Ep done - 18320.\n",
      "Ep done - 18330.\n",
      "Ep done - 18340.\n",
      "Ep done - 18350.\n",
      "Ep done - 18360.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.35    |\n",
      "| time/              |          |\n",
      "|    fps             | 373      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 872      |\n",
      "|    total_timesteps | 325632   |\n",
      "---------------------------------\n",
      "Ep done - 18370.\n",
      "Ep done - 18380.\n",
      "Ep done - 18390.\n",
      "Ep done - 18400.\n",
      "Ep done - 18410.\n",
      "Ep done - 18420.\n",
      "Ep done - 18430.\n",
      "Ep done - 18440.\n",
      "Ep done - 18450.\n",
      "Ep done - 18460.\n",
      "Ep done - 18470.\n",
      "Ep done - 18480.\n",
      "Ep done - 18490.\n",
      "Ep done - 18500.\n",
      "Ep done - 18510.\n",
      "Ep done - 3210.\n",
      "Ep done - 3220.\n",
      "Ep done - 3230.\n",
      "Ep done - 3240.\n",
      "Ep done - 3250.\n",
      "Ep done - 3260.\n",
      "Ep done - 3270.\n",
      "Ep done - 3280.\n",
      "Ep done - 3290.\n",
      "Ep done - 3300.\n",
      "Eval num_timesteps=330000, episode_reward=-0.46 +/- 0.89\n",
      "Episode length: 30.25 +/- 0.43\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 330000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3069669 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.023    |\n",
      "|    explained_variance   | -0.151    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0715    |\n",
      "|    n_updates            | 19760     |\n",
      "|    policy_gradient_loss | 0.00512   |\n",
      "|    value_loss           | 0.176     |\n",
      "---------------------------------------\n",
      "Ep done - 18520.\n",
      "Ep done - 18530.\n",
      "Ep done - 18540.\n",
      "Ep done - 18550.\n",
      "Ep done - 18560.\n",
      "Ep done - 18570.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | -0.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 371      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 893      |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "Ep done - 18580.\n",
      "Ep done - 18590.\n",
      "Ep done - 18600.\n",
      "Ep done - 18610.\n",
      "Ep done - 18620.\n",
      "Ep done - 18630.\n",
      "Ep done - 18640.\n",
      "Ep done - 18650.\n",
      "Ep done - 18660.\n",
      "Ep done - 18670.\n",
      "Ep done - 18680.\n",
      "Ep done - 18690.\n",
      "Ep done - 18700.\n",
      "Ep done - 18710.\n",
      "Ep done - 18720.\n",
      "Ep done - 18730.\n",
      "Ep done - 18740.\n",
      "Ep done - 18750.\n",
      "Ep done - 18760.\n",
      "Ep done - 18770.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.21     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 371       |\n",
      "|    iterations           | 55        |\n",
      "|    time_elapsed         | 908       |\n",
      "|    total_timesteps      | 337920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1068987 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0297   |\n",
      "|    explained_variance   | -0.13     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0895    |\n",
      "|    n_updates            | 19770     |\n",
      "|    policy_gradient_loss | -0.00867  |\n",
      "|    value_loss           | 0.21      |\n",
      "---------------------------------------\n",
      "Ep done - 18780.\n",
      "Ep done - 18790.\n",
      "Ep done - 18800.\n",
      "Ep done - 18810.\n",
      "Ep done - 18820.\n",
      "Ep done - 18830.\n",
      "Ep done - 18840.\n",
      "Ep done - 3310.\n",
      "Ep done - 3320.\n",
      "Ep done - 3330.\n",
      "Ep done - 3340.\n",
      "Ep done - 3350.\n",
      "Ep done - 3360.\n",
      "Ep done - 3370.\n",
      "Ep done - 3380.\n",
      "Ep done - 3390.\n",
      "Ep done - 3400.\n",
      "Eval num_timesteps=340000, episode_reward=0.29 +/- 0.95\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.29      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 340000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4396478 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0291   |\n",
      "|    explained_variance   | -0.13     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0824    |\n",
      "|    n_updates            | 19780     |\n",
      "|    policy_gradient_loss | 0.0325    |\n",
      "|    value_loss           | 0.225     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.29\n",
      "SELFPLAY: new best model, bumping up generation to 19\n",
      "Ep done - 18850.\n",
      "Ep done - 18860.\n",
      "Ep done - 18870.\n",
      "Ep done - 18880.\n",
      "Ep done - 18890.\n",
      "Ep done - 18900.\n",
      "Ep done - 18910.\n",
      "Ep done - 18920.\n",
      "Ep done - 18930.\n",
      "Ep done - 18940.\n",
      "Ep done - 18950.\n",
      "Ep done - 18960.\n",
      "Ep done - 18970.\n",
      "Ep done - 18980.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 370      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 928      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Ep done - 18990.\n",
      "Ep done - 19000.\n",
      "Ep done - 19010.\n",
      "Ep done - 19020.\n",
      "Ep done - 19030.\n",
      "Ep done - 19040.\n",
      "Ep done - 19050.\n",
      "Ep done - 19060.\n",
      "Ep done - 19070.\n",
      "Ep done - 19080.\n",
      "Ep done - 19090.\n",
      "Ep done - 19100.\n",
      "Ep done - 19110.\n",
      "Ep done - 19120.\n",
      "Ep done - 19130.\n",
      "Ep done - 19140.\n",
      "Ep done - 19150.\n",
      "Ep done - 19160.\n",
      "Ep done - 19170.\n",
      "Ep done - 3410.\n",
      "Ep done - 3420.\n",
      "Ep done - 3430.\n",
      "Ep done - 3440.\n",
      "Ep done - 3450.\n",
      "Ep done - 3460.\n",
      "Ep done - 3470.\n",
      "Ep done - 3480.\n",
      "Ep done - 3490.\n",
      "Ep done - 3500.\n",
      "Eval num_timesteps=350000, episode_reward=0.76 +/- 0.65\n",
      "Episode length: 30.16 +/- 0.37\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.2     |\n",
      "|    mean_reward          | 0.76     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 350000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.730931 |\n",
      "|    clip_fraction        | 0.14     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0336  |\n",
      "|    explained_variance   | 0.0925   |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0769   |\n",
      "|    n_updates            | 19790    |\n",
      "|    policy_gradient_loss | 0.0016   |\n",
      "|    value_loss           | 0.135    |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 20\n",
      "Ep done - 19180.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 369      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 947      |\n",
      "|    total_timesteps | 350208   |\n",
      "---------------------------------\n",
      "Ep done - 19190.\n",
      "Ep done - 19200.\n",
      "Ep done - 19210.\n",
      "Ep done - 19220.\n",
      "Ep done - 19230.\n",
      "Ep done - 19240.\n",
      "Ep done - 19250.\n",
      "Ep done - 19260.\n",
      "Ep done - 19270.\n",
      "Ep done - 19280.\n",
      "Ep done - 19290.\n",
      "Ep done - 19300.\n",
      "Ep done - 19310.\n",
      "Ep done - 19320.\n",
      "Ep done - 19330.\n",
      "Ep done - 19340.\n",
      "Ep done - 19350.\n",
      "Ep done - 19360.\n",
      "Ep done - 19370.\n",
      "Ep done - 19380.\n",
      "Ep done - 19390.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.8      |\n",
      "|    ep_rew_mean          | -0.62     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 369       |\n",
      "|    iterations           | 58        |\n",
      "|    time_elapsed         | 963       |\n",
      "|    total_timesteps      | 356352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0885216 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0224   |\n",
      "|    explained_variance   | 0.0954    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0011   |\n",
      "|    n_updates            | 19800     |\n",
      "|    policy_gradient_loss | 0.00375   |\n",
      "|    value_loss           | 0.094     |\n",
      "---------------------------------------\n",
      "Ep done - 19400.\n",
      "Ep done - 19410.\n",
      "Ep done - 19420.\n",
      "Ep done - 19430.\n",
      "Ep done - 19440.\n",
      "Ep done - 19450.\n",
      "Ep done - 19460.\n",
      "Ep done - 19470.\n",
      "Ep done - 19480.\n",
      "Ep done - 19490.\n",
      "Ep done - 19500.\n",
      "Ep done - 19510.\n",
      "Ep done - 3510.\n",
      "Ep done - 3520.\n",
      "Ep done - 3530.\n",
      "Ep done - 3540.\n",
      "Ep done - 3550.\n",
      "Ep done - 3560.\n",
      "Ep done - 3570.\n",
      "Ep done - 3580.\n",
      "Ep done - 3590.\n",
      "Ep done - 3600.\n",
      "Eval num_timesteps=360000, episode_reward=0.52 +/- 0.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.52      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 360000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7462345 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0202   |\n",
      "|    explained_variance   | 0.104     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0307    |\n",
      "|    n_updates            | 19810     |\n",
      "|    policy_gradient_loss | 0.00606   |\n",
      "|    value_loss           | 0.0718    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.52\n",
      "SELFPLAY: new best model, bumping up generation to 21\n",
      "Ep done - 19520.\n",
      "Ep done - 19530.\n",
      "Ep done - 19540.\n",
      "Ep done - 19550.\n",
      "Ep done - 19560.\n",
      "Ep done - 19570.\n",
      "Ep done - 19580.\n",
      "Ep done - 19590.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 368      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 983      |\n",
      "|    total_timesteps | 362496   |\n",
      "---------------------------------\n",
      "Ep done - 19600.\n",
      "Ep done - 19610.\n",
      "Ep done - 19620.\n",
      "Ep done - 19630.\n",
      "Ep done - 19640.\n",
      "Ep done - 19650.\n",
      "Ep done - 19660.\n",
      "Ep done - 19670.\n",
      "Ep done - 19680.\n",
      "Ep done - 19690.\n",
      "Ep done - 19700.\n",
      "Ep done - 19710.\n",
      "Ep done - 19720.\n",
      "Ep done - 19730.\n",
      "Ep done - 19740.\n",
      "Ep done - 19750.\n",
      "Ep done - 19760.\n",
      "Ep done - 19770.\n",
      "Ep done - 19780.\n",
      "Ep done - 19790.\n",
      "Ep done - 19800.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.7       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 368       |\n",
      "|    iterations           | 60        |\n",
      "|    time_elapsed         | 999       |\n",
      "|    total_timesteps      | 368640    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5894857 |\n",
      "|    clip_fraction        | 0.0954    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0222   |\n",
      "|    explained_variance   | 0.276     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0793    |\n",
      "|    n_updates            | 19820     |\n",
      "|    policy_gradient_loss | 0.0307    |\n",
      "|    value_loss           | 0.15      |\n",
      "---------------------------------------\n",
      "Ep done - 19810.\n",
      "Ep done - 19820.\n",
      "Ep done - 19830.\n",
      "Ep done - 19840.\n",
      "Ep done - 3610.\n",
      "Ep done - 3620.\n",
      "Ep done - 3630.\n",
      "Ep done - 3640.\n",
      "Ep done - 3650.\n",
      "Ep done - 3660.\n",
      "Ep done - 3670.\n",
      "Ep done - 3680.\n",
      "Ep done - 3690.\n",
      "Ep done - 3700.\n",
      "Eval num_timesteps=370000, episode_reward=0.69 +/- 0.72\n",
      "Episode length: 29.89 +/- 0.37\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.9      |\n",
      "|    mean_reward          | 0.69      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 370000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8981771 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0254   |\n",
      "|    explained_variance   | 0.286     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0272    |\n",
      "|    n_updates            | 19830     |\n",
      "|    policy_gradient_loss | 0.215     |\n",
      "|    value_loss           | 0.0999    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.69\n",
      "SELFPLAY: new best model, bumping up generation to 22\n",
      "Ep done - 19850.\n",
      "Ep done - 19860.\n",
      "Ep done - 19870.\n",
      "Ep done - 19880.\n",
      "Ep done - 19890.\n",
      "Ep done - 19900.\n",
      "Ep done - 19910.\n",
      "Ep done - 19920.\n",
      "Ep done - 19930.\n",
      "Ep done - 19940.\n",
      "Ep done - 19950.\n",
      "Ep done - 19960.\n",
      "Ep done - 19970.\n",
      "Ep done - 19980.\n",
      "Ep done - 19990.\n",
      "Ep done - 20000.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 1019     |\n",
      "|    total_timesteps | 374784   |\n",
      "---------------------------------\n",
      "Ep done - 20010.\n",
      "Ep done - 20020.\n",
      "Ep done - 20030.\n",
      "Ep done - 20040.\n",
      "Ep done - 20050.\n",
      "Ep done - 20060.\n",
      "Ep done - 20070.\n",
      "Ep done - 20080.\n",
      "Ep done - 20090.\n",
      "Ep done - 20100.\n",
      "Ep done - 20110.\n",
      "Ep done - 20120.\n",
      "Ep done - 20130.\n",
      "Ep done - 20140.\n",
      "Ep done - 20150.\n",
      "Ep done - 20160.\n",
      "Ep done - 20170.\n",
      "Ep done - 3710.\n",
      "Ep done - 3720.\n",
      "Ep done - 3730.\n",
      "Ep done - 3740.\n",
      "Ep done - 3750.\n",
      "Ep done - 3760.\n",
      "Ep done - 3770.\n",
      "Ep done - 3780.\n",
      "Ep done - 3790.\n",
      "Ep done - 3800.\n",
      "Eval num_timesteps=380000, episode_reward=-0.15 +/- 0.95\n",
      "Episode length: 29.85 +/- 0.70\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.9      |\n",
      "|    mean_reward          | -0.15     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 380000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9237879 |\n",
      "|    clip_fraction        | 0.0669    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0139   |\n",
      "|    explained_variance   | -0.3      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0884    |\n",
      "|    n_updates            | 19840     |\n",
      "|    policy_gradient_loss | 0.00483   |\n",
      "|    value_loss           | 0.229     |\n",
      "---------------------------------------\n",
      "Ep done - 20180.\n",
      "Ep done - 20190.\n",
      "Ep done - 20200.\n",
      "Ep done - 20210.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 1039     |\n",
      "|    total_timesteps | 380928   |\n",
      "---------------------------------\n",
      "Ep done - 20220.\n",
      "Ep done - 20230.\n",
      "Ep done - 20240.\n",
      "Ep done - 20250.\n",
      "Ep done - 20260.\n",
      "Ep done - 20270.\n",
      "Ep done - 20280.\n",
      "Ep done - 20290.\n",
      "Ep done - 20300.\n",
      "Ep done - 20310.\n",
      "Ep done - 20320.\n",
      "Ep done - 20330.\n",
      "Ep done - 20340.\n",
      "Ep done - 20350.\n",
      "Ep done - 20360.\n",
      "Ep done - 20370.\n",
      "Ep done - 20380.\n",
      "Ep done - 20390.\n",
      "Ep done - 20400.\n",
      "Ep done - 20410.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.33       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 1054       |\n",
      "|    total_timesteps      | 387072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.76716614 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0295    |\n",
      "|    explained_variance   | -0.383     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0168     |\n",
      "|    n_updates            | 19850      |\n",
      "|    policy_gradient_loss | 0.011      |\n",
      "|    value_loss           | 0.139      |\n",
      "----------------------------------------\n",
      "Ep done - 20420.\n",
      "Ep done - 20430.\n",
      "Ep done - 20440.\n",
      "Ep done - 20450.\n",
      "Ep done - 20460.\n",
      "Ep done - 20470.\n",
      "Ep done - 20480.\n",
      "Ep done - 20490.\n",
      "Ep done - 20500.\n",
      "Ep done - 20510.\n",
      "Ep done - 3810.\n",
      "Ep done - 3820.\n",
      "Ep done - 3830.\n",
      "Ep done - 3840.\n",
      "Ep done - 3850.\n",
      "Ep done - 3860.\n",
      "Ep done - 3870.\n",
      "Ep done - 3880.\n",
      "Ep done - 3890.\n",
      "Ep done - 3900.\n",
      "Eval num_timesteps=390000, episode_reward=0.23 +/- 0.94\n",
      "Episode length: 30.12 +/- 0.35\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.23      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 390000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5249138 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0226   |\n",
      "|    explained_variance   | -0.079    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0895    |\n",
      "|    n_updates            | 19860     |\n",
      "|    policy_gradient_loss | 0.0185    |\n",
      "|    value_loss           | 0.212     |\n",
      "---------------------------------------\n",
      "Ep done - 20520.\n",
      "Ep done - 20530.\n",
      "Ep done - 20540.\n",
      "Ep done - 20550.\n",
      "Ep done - 20560.\n",
      "Ep done - 20570.\n",
      "Ep done - 20580.\n",
      "Ep done - 20590.\n",
      "Ep done - 20600.\n",
      "Ep done - 20610.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 1074     |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Ep done - 20620.\n",
      "Ep done - 20630.\n",
      "Ep done - 20640.\n",
      "Ep done - 20650.\n",
      "Ep done - 20660.\n",
      "Ep done - 20670.\n",
      "Ep done - 20680.\n",
      "Ep done - 20690.\n",
      "Ep done - 20700.\n",
      "Ep done - 20710.\n",
      "Ep done - 20720.\n",
      "Ep done - 20730.\n",
      "Ep done - 20740.\n",
      "Ep done - 20750.\n",
      "Ep done - 20760.\n",
      "Ep done - 20770.\n",
      "Ep done - 20780.\n",
      "Ep done - 20790.\n",
      "Ep done - 20800.\n",
      "Ep done - 20810.\n",
      "Ep done - 20820.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.72      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 366       |\n",
      "|    iterations           | 65        |\n",
      "|    time_elapsed         | 1089      |\n",
      "|    total_timesteps      | 399360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6035699 |\n",
      "|    clip_fraction        | 0.082     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0199   |\n",
      "|    explained_variance   | 0.0877    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.144     |\n",
      "|    n_updates            | 19870     |\n",
      "|    policy_gradient_loss | 0.0124    |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "Ep done - 20830.\n",
      "Ep done - 20840.\n",
      "Ep done - 3910.\n",
      "Ep done - 3920.\n",
      "Ep done - 3930.\n",
      "Ep done - 3940.\n",
      "Ep done - 3950.\n",
      "Ep done - 3960.\n",
      "Ep done - 3970.\n",
      "Ep done - 3980.\n",
      "Ep done - 3990.\n",
      "Ep done - 4000.\n",
      "Eval num_timesteps=400000, episode_reward=0.40 +/- 0.89\n",
      "Episode length: 30.47 +/- 0.70\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.5       |\n",
      "|    mean_reward          | 0.4        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 400000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47404733 |\n",
      "|    clip_fraction        | 0.0515     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00605   |\n",
      "|    explained_variance   | 0.233      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0561     |\n",
      "|    n_updates            | 19880      |\n",
      "|    policy_gradient_loss | -0.00423   |\n",
      "|    value_loss           | 0.1        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.4\n",
      "SELFPLAY: new best model, bumping up generation to 23\n",
      "Ep done - 20850.\n",
      "Ep done - 20860.\n",
      "Ep done - 20870.\n",
      "Ep done - 20880.\n",
      "Ep done - 20890.\n",
      "Ep done - 20900.\n",
      "Ep done - 20910.\n",
      "Ep done - 20920.\n",
      "Ep done - 20930.\n",
      "Ep done - 20940.\n",
      "Ep done - 20950.\n",
      "Ep done - 20960.\n",
      "Ep done - 20970.\n",
      "Ep done - 20980.\n",
      "Ep done - 20990.\n",
      "Ep done - 21000.\n",
      "Ep done - 21010.\n",
      "Ep done - 21020.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 1109     |\n",
      "|    total_timesteps | 405504   |\n",
      "---------------------------------\n",
      "Ep done - 21030.\n",
      "Ep done - 21040.\n",
      "Ep done - 21050.\n",
      "Ep done - 21060.\n",
      "Ep done - 21070.\n",
      "Ep done - 21080.\n",
      "Ep done - 21090.\n",
      "Ep done - 21100.\n",
      "Ep done - 21110.\n",
      "Ep done - 21120.\n",
      "Ep done - 21130.\n",
      "Ep done - 21140.\n",
      "Ep done - 21150.\n",
      "Ep done - 21160.\n",
      "Ep done - 21170.\n",
      "Ep done - 4010.\n",
      "Ep done - 4020.\n",
      "Ep done - 4030.\n",
      "Ep done - 4040.\n",
      "Ep done - 4050.\n",
      "Ep done - 4060.\n",
      "Ep done - 4070.\n",
      "Ep done - 4080.\n",
      "Ep done - 4090.\n",
      "Ep done - 4100.\n",
      "Eval num_timesteps=410000, episode_reward=0.99 +/- 0.10\n",
      "Episode length: 30.65 +/- 0.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.6       |\n",
      "|    mean_reward          | 0.99       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 410000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38935828 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0337    |\n",
      "|    explained_variance   | 0.218      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0471     |\n",
      "|    n_updates            | 19890      |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.99\n",
      "SELFPLAY: new best model, bumping up generation to 24\n",
      "Ep done - 21180.\n",
      "Ep done - 21190.\n",
      "Ep done - 21200.\n",
      "Ep done - 21210.\n",
      "Ep done - 21220.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 1127     |\n",
      "|    total_timesteps | 411648   |\n",
      "---------------------------------\n",
      "Ep done - 21230.\n",
      "Ep done - 21240.\n",
      "Ep done - 21250.\n",
      "Ep done - 21260.\n",
      "Ep done - 21270.\n",
      "Ep done - 21280.\n",
      "Ep done - 21290.\n",
      "Ep done - 21300.\n",
      "Ep done - 21310.\n",
      "Ep done - 21320.\n",
      "Ep done - 21330.\n",
      "Ep done - 21340.\n",
      "Ep done - 21350.\n",
      "Ep done - 21360.\n",
      "Ep done - 21370.\n",
      "Ep done - 21380.\n",
      "Ep done - 21390.\n",
      "Ep done - 21400.\n",
      "Ep done - 21410.\n",
      "Ep done - 21420.\n",
      "Ep done - 21430.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.3      |\n",
      "|    ep_rew_mean          | 0.59      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 365       |\n",
      "|    iterations           | 68        |\n",
      "|    time_elapsed         | 1141      |\n",
      "|    total_timesteps      | 417792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9271975 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0215   |\n",
      "|    explained_variance   | -0.278    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00127  |\n",
      "|    n_updates            | 19900     |\n",
      "|    policy_gradient_loss | 0.0217    |\n",
      "|    value_loss           | 0.0835    |\n",
      "---------------------------------------\n",
      "Ep done - 21440.\n",
      "Ep done - 21450.\n",
      "Ep done - 21460.\n",
      "Ep done - 21470.\n",
      "Ep done - 21480.\n",
      "Ep done - 21490.\n",
      "Ep done - 21500.\n",
      "Ep done - 4110.\n",
      "Ep done - 4120.\n",
      "Ep done - 4130.\n",
      "Ep done - 4140.\n",
      "Ep done - 4150.\n",
      "Ep done - 4160.\n",
      "Ep done - 4170.\n",
      "Ep done - 4180.\n",
      "Ep done - 4190.\n",
      "Ep done - 4200.\n",
      "Eval num_timesteps=420000, episode_reward=0.04 +/- 1.00\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.04      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 420000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.8828795 |\n",
      "|    clip_fraction        | 0.185     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0287   |\n",
      "|    explained_variance   | -0.597    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0117    |\n",
      "|    n_updates            | 19910     |\n",
      "|    policy_gradient_loss | -0.00308  |\n",
      "|    value_loss           | 0.0763    |\n",
      "---------------------------------------\n",
      "Ep done - 21510.\n",
      "Ep done - 21520.\n",
      "Ep done - 21530.\n",
      "Ep done - 21540.\n",
      "Ep done - 21550.\n",
      "Ep done - 21560.\n",
      "Ep done - 21570.\n",
      "Ep done - 21580.\n",
      "Ep done - 21590.\n",
      "Ep done - 21600.\n",
      "Ep done - 21610.\n",
      "Ep done - 21620.\n",
      "Ep done - 21630.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 1159     |\n",
      "|    total_timesteps | 423936   |\n",
      "---------------------------------\n",
      "Ep done - 21640.\n",
      "Ep done - 21650.\n",
      "Ep done - 21660.\n",
      "Ep done - 21670.\n",
      "Ep done - 21680.\n",
      "Ep done - 21690.\n",
      "Ep done - 21700.\n",
      "Ep done - 21710.\n",
      "Ep done - 21720.\n",
      "Ep done - 21730.\n",
      "Ep done - 21740.\n",
      "Ep done - 21750.\n",
      "Ep done - 21760.\n",
      "Ep done - 21770.\n",
      "Ep done - 21780.\n",
      "Ep done - 21790.\n",
      "Ep done - 21800.\n",
      "Ep done - 21810.\n",
      "Ep done - 21820.\n",
      "Ep done - 21830.\n",
      "Ep done - 4210.\n",
      "Ep done - 4220.\n",
      "Ep done - 4230.\n",
      "Ep done - 4240.\n",
      "Ep done - 4250.\n",
      "Ep done - 4260.\n",
      "Ep done - 4270.\n",
      "Ep done - 4280.\n",
      "Ep done - 4290.\n",
      "Ep done - 4300.\n",
      "Eval num_timesteps=430000, episode_reward=0.91 +/- 0.40\n",
      "Episode length: 30.03 +/- 0.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.91       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 430000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52721554 |\n",
      "|    clip_fraction        | 0.0915     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0232    |\n",
      "|    explained_variance   | -0.57      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00308    |\n",
      "|    n_updates            | 19920      |\n",
      "|    policy_gradient_loss | -0.00979   |\n",
      "|    value_loss           | 0.0882     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.91\n",
      "SELFPLAY: new best model, bumping up generation to 25\n",
      "Ep done - 21840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.92     |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1176     |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "Ep done - 21850.\n",
      "Ep done - 21860.\n",
      "Ep done - 21870.\n",
      "Ep done - 21880.\n",
      "Ep done - 21890.\n",
      "Ep done - 21900.\n",
      "Ep done - 21910.\n",
      "Ep done - 21920.\n",
      "Ep done - 21930.\n",
      "Ep done - 21940.\n",
      "Ep done - 21950.\n",
      "Ep done - 21960.\n",
      "Ep done - 21970.\n",
      "Ep done - 21980.\n",
      "Ep done - 21990.\n",
      "Ep done - 22000.\n",
      "Ep done - 22010.\n",
      "Ep done - 22020.\n",
      "Ep done - 22030.\n",
      "Ep done - 22040.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.8      |\n",
      "|    ep_rew_mean          | 0.32      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 366       |\n",
      "|    iterations           | 71        |\n",
      "|    time_elapsed         | 1189      |\n",
      "|    total_timesteps      | 436224    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5253824 |\n",
      "|    clip_fraction        | 0.172     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0363   |\n",
      "|    explained_variance   | -0.765    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000622  |\n",
      "|    n_updates            | 19930     |\n",
      "|    policy_gradient_loss | 0.00573   |\n",
      "|    value_loss           | 0.0439    |\n",
      "---------------------------------------\n",
      "Ep done - 22050.\n",
      "Ep done - 22060.\n",
      "Ep done - 22070.\n",
      "Ep done - 22080.\n",
      "Ep done - 22090.\n",
      "Ep done - 22100.\n",
      "Ep done - 22110.\n",
      "Ep done - 22120.\n",
      "Ep done - 22130.\n",
      "Ep done - 22140.\n",
      "Ep done - 22150.\n",
      "Ep done - 22160.\n",
      "Ep done - 22170.\n",
      "Ep done - 4310.\n",
      "Ep done - 4320.\n",
      "Ep done - 4330.\n",
      "Ep done - 4340.\n",
      "Ep done - 4350.\n",
      "Ep done - 4360.\n",
      "Ep done - 4370.\n",
      "Ep done - 4380.\n",
      "Ep done - 4390.\n",
      "Ep done - 4400.\n",
      "Eval num_timesteps=440000, episode_reward=-0.32 +/- 0.94\n",
      "Episode length: 30.20 +/- 0.40\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.2     |\n",
      "|    mean_reward          | -0.32    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 440000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 3.182783 |\n",
      "|    clip_fraction        | 0.215    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0124  |\n",
      "|    explained_variance   | 0.00467  |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0059   |\n",
      "|    n_updates            | 19940    |\n",
      "|    policy_gradient_loss | -0.00103 |\n",
      "|    value_loss           | 0.0873   |\n",
      "--------------------------------------\n",
      "Ep done - 22180.\n",
      "Ep done - 22190.\n",
      "Ep done - 22200.\n",
      "Ep done - 22210.\n",
      "Ep done - 22220.\n",
      "Ep done - 22230.\n",
      "Ep done - 22240.\n",
      "Ep done - 22250.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1209     |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Ep done - 22260.\n",
      "Ep done - 22270.\n",
      "Ep done - 22280.\n",
      "Ep done - 22290.\n",
      "Ep done - 22300.\n",
      "Ep done - 22310.\n",
      "Ep done - 22320.\n",
      "Ep done - 22330.\n",
      "Ep done - 22340.\n",
      "Ep done - 22350.\n",
      "Ep done - 22360.\n",
      "Ep done - 22370.\n",
      "Ep done - 22380.\n",
      "Ep done - 22390.\n",
      "Ep done - 22400.\n",
      "Ep done - 22410.\n",
      "Ep done - 22420.\n",
      "Ep done - 22430.\n",
      "Ep done - 22440.\n",
      "Ep done - 22450.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.4       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 366       |\n",
      "|    iterations           | 73        |\n",
      "|    time_elapsed         | 1223      |\n",
      "|    total_timesteps      | 448512    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6972931 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.027    |\n",
      "|    explained_variance   | -0.281    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0223    |\n",
      "|    n_updates            | 19950     |\n",
      "|    policy_gradient_loss | -0.00132  |\n",
      "|    value_loss           | 0.143     |\n",
      "---------------------------------------\n",
      "Ep done - 22460.\n",
      "Ep done - 22470.\n",
      "Ep done - 22480.\n",
      "Ep done - 22490.\n",
      "Ep done - 22500.\n",
      "Ep done - 4410.\n",
      "Ep done - 4420.\n",
      "Ep done - 4430.\n",
      "Ep done - 4440.\n",
      "Ep done - 4450.\n",
      "Ep done - 4460.\n",
      "Ep done - 4470.\n",
      "Ep done - 4480.\n",
      "Ep done - 4490.\n",
      "Ep done - 4500.\n",
      "Eval num_timesteps=450000, episode_reward=0.82 +/- 0.57\n",
      "Episode length: 30.09 +/- 0.35\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.1     |\n",
      "|    mean_reward          | 0.82     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 450000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.669359 |\n",
      "|    clip_fraction        | 0.096    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0152  |\n",
      "|    explained_variance   | 0.0091   |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0402   |\n",
      "|    n_updates            | 19960    |\n",
      "|    policy_gradient_loss | 0.0026   |\n",
      "|    value_loss           | 0.174    |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.82\n",
      "SELFPLAY: new best model, bumping up generation to 26\n",
      "Ep done - 22510.\n",
      "Ep done - 22520.\n",
      "Ep done - 22530.\n",
      "Ep done - 22540.\n",
      "Ep done - 22550.\n",
      "Ep done - 22560.\n",
      "Ep done - 22570.\n",
      "Ep done - 22580.\n",
      "Ep done - 22590.\n",
      "Ep done - 22600.\n",
      "Ep done - 22610.\n",
      "Ep done - 22620.\n",
      "Ep done - 22630.\n",
      "Ep done - 22640.\n",
      "Ep done - 22650.\n",
      "Ep done - 22660.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1240     |\n",
      "|    total_timesteps | 454656   |\n",
      "---------------------------------\n",
      "Ep done - 22670.\n",
      "Ep done - 22680.\n",
      "Ep done - 22690.\n",
      "Ep done - 22700.\n",
      "Ep done - 22710.\n",
      "Ep done - 22720.\n",
      "Ep done - 22730.\n",
      "Ep done - 22740.\n",
      "Ep done - 22750.\n",
      "Ep done - 22760.\n",
      "Ep done - 22770.\n",
      "Ep done - 22780.\n",
      "Ep done - 22790.\n",
      "Ep done - 22800.\n",
      "Ep done - 22810.\n",
      "Ep done - 22820.\n",
      "Ep done - 22830.\n",
      "Ep done - 4510.\n",
      "Ep done - 4520.\n",
      "Ep done - 4530.\n",
      "Ep done - 4540.\n",
      "Ep done - 4550.\n",
      "Ep done - 4560.\n",
      "Ep done - 4570.\n",
      "Ep done - 4580.\n",
      "Ep done - 4590.\n",
      "Ep done - 4600.\n",
      "Eval num_timesteps=460000, episode_reward=-0.82 +/- 0.55\n",
      "Episode length: 30.06 +/- 0.60\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.82     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 460000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7597923 |\n",
      "|    clip_fraction        | 0.143     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0263   |\n",
      "|    explained_variance   | -0.0612   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0406    |\n",
      "|    n_updates            | 19970     |\n",
      "|    policy_gradient_loss | -0.005    |\n",
      "|    value_loss           | 0.0569    |\n",
      "---------------------------------------\n",
      "Ep done - 22840.\n",
      "Ep done - 22850.\n",
      "Ep done - 22860.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 1259     |\n",
      "|    total_timesteps | 460800   |\n",
      "---------------------------------\n",
      "Ep done - 22870.\n",
      "Ep done - 22880.\n",
      "Ep done - 22890.\n",
      "Ep done - 22900.\n",
      "Ep done - 22910.\n",
      "Ep done - 22920.\n",
      "Ep done - 22930.\n",
      "Ep done - 22940.\n",
      "Ep done - 22950.\n",
      "Ep done - 22960.\n",
      "Ep done - 22970.\n",
      "Ep done - 22980.\n",
      "Ep done - 22990.\n",
      "Ep done - 23000.\n",
      "Ep done - 23010.\n",
      "Ep done - 23020.\n",
      "Ep done - 23030.\n",
      "Ep done - 23040.\n",
      "Ep done - 23050.\n",
      "Ep done - 23060.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 31.1     |\n",
      "|    ep_rew_mean          | 0.31     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 366      |\n",
      "|    iterations           | 76       |\n",
      "|    time_elapsed         | 1272     |\n",
      "|    total_timesteps      | 466944   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.545709 |\n",
      "|    clip_fraction        | 0.171    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.016   |\n",
      "|    explained_variance   | -0.429   |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0505   |\n",
      "|    n_updates            | 19980    |\n",
      "|    policy_gradient_loss | 0.047    |\n",
      "|    value_loss           | 0.145    |\n",
      "--------------------------------------\n",
      "Ep done - 23070.\n",
      "Ep done - 23080.\n",
      "Ep done - 23090.\n",
      "Ep done - 23100.\n",
      "Ep done - 23110.\n",
      "Ep done - 23120.\n",
      "Ep done - 23130.\n",
      "Ep done - 23140.\n",
      "Ep done - 23150.\n",
      "Ep done - 23160.\n",
      "Ep done - 4610.\n",
      "Ep done - 4620.\n",
      "Ep done - 4630.\n",
      "Ep done - 4640.\n",
      "Ep done - 4650.\n",
      "Ep done - 4660.\n",
      "Ep done - 4670.\n",
      "Ep done - 4680.\n",
      "Ep done - 4690.\n",
      "Ep done - 4700.\n",
      "Eval num_timesteps=470000, episode_reward=0.47 +/- 0.73\n",
      "Episode length: 30.54 +/- 0.61\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.5      |\n",
      "|    mean_reward          | 0.47      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 470000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5512505 |\n",
      "|    clip_fraction        | 0.106     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0117   |\n",
      "|    explained_variance   | -0.653    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.126     |\n",
      "|    n_updates            | 19990     |\n",
      "|    policy_gradient_loss | -0.00027  |\n",
      "|    value_loss           | 0.189     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.47\n",
      "SELFPLAY: new best model, bumping up generation to 27\n",
      "Ep done - 23170.\n",
      "Ep done - 23180.\n",
      "Ep done - 23190.\n",
      "Ep done - 23200.\n",
      "Ep done - 23210.\n",
      "Ep done - 23220.\n",
      "Ep done - 23230.\n",
      "Ep done - 23240.\n",
      "Ep done - 23250.\n",
      "Ep done - 23260.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.41    |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 1291     |\n",
      "|    total_timesteps | 473088   |\n",
      "---------------------------------\n",
      "Ep done - 23270.\n",
      "Ep done - 23280.\n",
      "Ep done - 23290.\n",
      "Ep done - 23300.\n",
      "Ep done - 23310.\n",
      "Ep done - 23320.\n",
      "Ep done - 23330.\n",
      "Ep done - 23340.\n",
      "Ep done - 23350.\n",
      "Ep done - 23360.\n",
      "Ep done - 23370.\n",
      "Ep done - 23380.\n",
      "Ep done - 23390.\n",
      "Ep done - 23400.\n",
      "Ep done - 23410.\n",
      "Ep done - 23420.\n",
      "Ep done - 23430.\n",
      "Ep done - 23440.\n",
      "Ep done - 23450.\n",
      "Ep done - 23460.\n",
      "Ep done - 23470.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.88     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 366       |\n",
      "|    iterations           | 78        |\n",
      "|    time_elapsed         | 1307      |\n",
      "|    total_timesteps      | 479232    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8329273 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.019    |\n",
      "|    explained_variance   | -0.25     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.074     |\n",
      "|    n_updates            | 20000     |\n",
      "|    policy_gradient_loss | 0.015     |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "Ep done - 23480.\n",
      "Ep done - 23490.\n",
      "Ep done - 4710.\n",
      "Ep done - 4720.\n",
      "Ep done - 4730.\n",
      "Ep done - 4740.\n",
      "Ep done - 4750.\n",
      "Ep done - 4760.\n",
      "Ep done - 4770.\n",
      "Ep done - 4780.\n",
      "Ep done - 4790.\n",
      "Ep done - 4800.\n",
      "Eval num_timesteps=480000, episode_reward=-0.92 +/- 0.39\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.92     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 480000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0284835 |\n",
      "|    clip_fraction        | 0.1       |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0108   |\n",
      "|    explained_variance   | -0.306    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0126   |\n",
      "|    n_updates            | 20010     |\n",
      "|    policy_gradient_loss | -0.0176   |\n",
      "|    value_loss           | 0.0362    |\n",
      "---------------------------------------\n",
      "Ep done - 23500.\n",
      "Ep done - 23510.\n",
      "Ep done - 23520.\n",
      "Ep done - 23530.\n",
      "Ep done - 23540.\n",
      "Ep done - 23550.\n",
      "Ep done - 23560.\n",
      "Ep done - 23570.\n",
      "Ep done - 23580.\n",
      "Ep done - 23590.\n",
      "Ep done - 23600.\n",
      "Ep done - 23610.\n",
      "Ep done - 23620.\n",
      "Ep done - 23630.\n",
      "Ep done - 23640.\n",
      "Ep done - 23650.\n",
      "Ep done - 23660.\n",
      "Ep done - 23670.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 1328     |\n",
      "|    total_timesteps | 485376   |\n",
      "---------------------------------\n",
      "Ep done - 23680.\n",
      "Ep done - 23690.\n",
      "Ep done - 23700.\n",
      "Ep done - 23710.\n",
      "Ep done - 23720.\n",
      "Ep done - 23730.\n",
      "Ep done - 23740.\n",
      "Ep done - 23750.\n",
      "Ep done - 23760.\n",
      "Ep done - 23770.\n",
      "Ep done - 23780.\n",
      "Ep done - 23790.\n",
      "Ep done - 23800.\n",
      "Ep done - 23810.\n",
      "Ep done - 23820.\n",
      "Ep done - 23830.\n",
      "Ep done - 4810.\n",
      "Ep done - 4820.\n",
      "Ep done - 4830.\n",
      "Ep done - 4840.\n",
      "Ep done - 4850.\n",
      "Ep done - 4860.\n",
      "Ep done - 4870.\n",
      "Ep done - 4880.\n",
      "Ep done - 4890.\n",
      "Ep done - 4900.\n",
      "Eval num_timesteps=490000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 490000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73257464 |\n",
      "|    clip_fraction        | 0.0805     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00464   |\n",
      "|    explained_variance   | 0.464      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0452     |\n",
      "|    n_updates            | 20020      |\n",
      "|    policy_gradient_loss | 0.0148     |\n",
      "|    value_loss           | 0.0537     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 28\n",
      "Ep done - 23840.\n",
      "Ep done - 23850.\n",
      "Ep done - 23860.\n",
      "Ep done - 23870.\n",
      "Ep done - 23880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 1346     |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Ep done - 23890.\n",
      "Ep done - 23900.\n",
      "Ep done - 23910.\n",
      "Ep done - 23920.\n",
      "Ep done - 23930.\n",
      "Ep done - 23940.\n",
      "Ep done - 23950.\n",
      "Ep done - 23960.\n",
      "Ep done - 23970.\n",
      "Ep done - 23980.\n",
      "Ep done - 23990.\n",
      "Ep done - 24000.\n",
      "Ep done - 24010.\n",
      "Ep done - 24020.\n",
      "Ep done - 24030.\n",
      "Ep done - 24040.\n",
      "Ep done - 24050.\n",
      "Ep done - 24060.\n",
      "Ep done - 24070.\n",
      "Ep done - 24080.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.3      |\n",
      "|    ep_rew_mean          | -0.67     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 365       |\n",
      "|    iterations           | 81        |\n",
      "|    time_elapsed         | 1363      |\n",
      "|    total_timesteps      | 497664    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.5735786 |\n",
      "|    clip_fraction        | 0.0861    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0038   |\n",
      "|    explained_variance   | -0.269    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0566    |\n",
      "|    n_updates            | 20030     |\n",
      "|    policy_gradient_loss | -0.00935  |\n",
      "|    value_loss           | 0.123     |\n",
      "---------------------------------------\n",
      "Ep done - 24090.\n",
      "Ep done - 24100.\n",
      "Ep done - 24110.\n",
      "Ep done - 24120.\n",
      "Ep done - 24130.\n",
      "Ep done - 24140.\n",
      "Ep done - 24150.\n",
      "Ep done - 24160.\n",
      "Ep done - 4910.\n",
      "Ep done - 4920.\n",
      "Ep done - 4930.\n",
      "Ep done - 4940.\n",
      "Ep done - 4950.\n",
      "Ep done - 4960.\n",
      "Ep done - 4970.\n",
      "Ep done - 4980.\n",
      "Ep done - 4990.\n",
      "Ep done - 5000.\n",
      "Eval num_timesteps=500000, episode_reward=0.56 +/- 0.79\n",
      "Episode length: 30.46 +/- 0.64\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.5      |\n",
      "|    mean_reward          | 0.56      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 500000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.6861632 |\n",
      "|    clip_fraction        | 0.204     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00284  |\n",
      "|    explained_variance   | -0.828    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0984    |\n",
      "|    n_updates            | 20040     |\n",
      "|    policy_gradient_loss | 0.0359    |\n",
      "|    value_loss           | 0.126     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 29\n",
      "Ep done - 24170.\n",
      "Ep done - 24180.\n",
      "Ep done - 24190.\n",
      "Ep done - 24200.\n",
      "Ep done - 24210.\n",
      "Ep done - 24220.\n",
      "Ep done - 24230.\n",
      "Ep done - 24240.\n",
      "Ep done - 24250.\n",
      "Ep done - 24260.\n",
      "Ep done - 24270.\n",
      "Ep done - 24280.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.6     |\n",
      "|    ep_rew_mean     | 0.79     |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 1382     |\n",
      "|    total_timesteps | 503808   |\n",
      "---------------------------------\n",
      "Ep done - 24290.\n",
      "Ep done - 24300.\n",
      "Ep done - 24310.\n",
      "Ep done - 24320.\n",
      "Ep done - 24330.\n",
      "Ep done - 24340.\n",
      "Ep done - 24350.\n",
      "Ep done - 24360.\n",
      "Ep done - 24370.\n",
      "Ep done - 24380.\n",
      "Ep done - 24390.\n",
      "Ep done - 24400.\n",
      "Ep done - 24410.\n",
      "Ep done - 24420.\n",
      "Ep done - 24430.\n",
      "Ep done - 24440.\n",
      "Ep done - 24450.\n",
      "Ep done - 24460.\n",
      "Ep done - 24470.\n",
      "Ep done - 24480.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30.1     |\n",
      "|    ep_rew_mean          | 0.96     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 365      |\n",
      "|    iterations           | 83       |\n",
      "|    time_elapsed         | 1396     |\n",
      "|    total_timesteps      | 509952   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 6.650394 |\n",
      "|    clip_fraction        | 0.251    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0197  |\n",
      "|    explained_variance   | -0.325   |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0702   |\n",
      "|    n_updates            | 20050    |\n",
      "|    policy_gradient_loss | -0.00651 |\n",
      "|    value_loss           | 0.173    |\n",
      "--------------------------------------\n",
      "Ep done - 24490.\n",
      "Ep done - 5010.\n",
      "Ep done - 5020.\n",
      "Ep done - 5030.\n",
      "Ep done - 5040.\n",
      "Ep done - 5050.\n",
      "Ep done - 5060.\n",
      "Ep done - 5070.\n",
      "Ep done - 5080.\n",
      "Ep done - 5090.\n",
      "Ep done - 5100.\n",
      "Eval num_timesteps=510000, episode_reward=0.11 +/- 0.99\n",
      "Episode length: 30.05 +/- 0.30\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.11      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 510000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3893089 |\n",
      "|    clip_fraction        | 0.118     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00827  |\n",
      "|    explained_variance   | 0.242     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00712   |\n",
      "|    n_updates            | 20060     |\n",
      "|    policy_gradient_loss | -0.00357  |\n",
      "|    value_loss           | 0.0527    |\n",
      "---------------------------------------\n",
      "Ep done - 24500.\n",
      "Ep done - 24510.\n",
      "Ep done - 24520.\n",
      "Ep done - 24530.\n",
      "Ep done - 24540.\n",
      "Ep done - 24550.\n",
      "Ep done - 24560.\n",
      "Ep done - 24570.\n",
      "Ep done - 24580.\n",
      "Ep done - 24590.\n",
      "Ep done - 24600.\n",
      "Ep done - 24610.\n",
      "Ep done - 24620.\n",
      "Ep done - 24630.\n",
      "Ep done - 24640.\n",
      "Ep done - 24650.\n",
      "Ep done - 24660.\n",
      "Ep done - 24670.\n",
      "Ep done - 24680.\n",
      "Ep done - 24690.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1416     |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "Ep done - 24700.\n",
      "Ep done - 24710.\n",
      "Ep done - 24720.\n",
      "Ep done - 24730.\n",
      "Ep done - 24740.\n",
      "Ep done - 24750.\n",
      "Ep done - 24760.\n",
      "Ep done - 24770.\n",
      "Ep done - 24780.\n",
      "Ep done - 24790.\n",
      "Ep done - 24800.\n",
      "Ep done - 24810.\n",
      "Ep done - 24820.\n",
      "Ep done - 5110.\n",
      "Ep done - 5120.\n",
      "Ep done - 5130.\n",
      "Ep done - 5140.\n",
      "Ep done - 5150.\n",
      "Ep done - 5160.\n",
      "Ep done - 5170.\n",
      "Ep done - 5180.\n",
      "Ep done - 5190.\n",
      "Ep done - 5200.\n",
      "Eval num_timesteps=520000, episode_reward=0.90 +/- 0.44\n",
      "Episode length: 30.02 +/- 0.14\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.9       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 520000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7424663 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0101   |\n",
      "|    explained_variance   | -0.199    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.178     |\n",
      "|    n_updates            | 20070     |\n",
      "|    policy_gradient_loss | 0.00931   |\n",
      "|    value_loss           | 0.278     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.9\n",
      "SELFPLAY: new best model, bumping up generation to 30\n",
      "Ep done - 24830.\n",
      "Ep done - 24840.\n",
      "Ep done - 24850.\n",
      "Ep done - 24860.\n",
      "Ep done - 24870.\n",
      "Ep done - 24880.\n",
      "Ep done - 24890.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 1436     |\n",
      "|    total_timesteps | 522240   |\n",
      "---------------------------------\n",
      "Ep done - 24900.\n",
      "Ep done - 24910.\n",
      "Ep done - 24920.\n",
      "Ep done - 24930.\n",
      "Ep done - 24940.\n",
      "Ep done - 24950.\n",
      "Ep done - 24960.\n",
      "Ep done - 24970.\n",
      "Ep done - 24980.\n",
      "Ep done - 24990.\n",
      "Ep done - 25000.\n",
      "Ep done - 25010.\n",
      "Ep done - 25020.\n",
      "Ep done - 25030.\n",
      "Ep done - 25040.\n",
      "Ep done - 25050.\n",
      "Ep done - 25060.\n",
      "Ep done - 25070.\n",
      "Ep done - 25080.\n",
      "Ep done - 25090.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.8       |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 363        |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 1452       |\n",
      "|    total_timesteps      | 528384     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50844413 |\n",
      "|    clip_fraction        | 0.0688     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00568   |\n",
      "|    explained_variance   | -0.01      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.046      |\n",
      "|    n_updates            | 20080      |\n",
      "|    policy_gradient_loss | -0.00155   |\n",
      "|    value_loss           | 0.145      |\n",
      "----------------------------------------\n",
      "Ep done - 25100.\n",
      "Ep done - 25110.\n",
      "Ep done - 25120.\n",
      "Ep done - 25130.\n",
      "Ep done - 25140.\n",
      "Ep done - 5210.\n",
      "Ep done - 5220.\n",
      "Ep done - 5230.\n",
      "Ep done - 5240.\n",
      "Ep done - 5250.\n",
      "Ep done - 5260.\n",
      "Ep done - 5270.\n",
      "Ep done - 5280.\n",
      "Ep done - 5290.\n",
      "Ep done - 5300.\n",
      "Eval num_timesteps=530000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.97 +/- 0.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 31         |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 530000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34005094 |\n",
      "|    clip_fraction        | 0.0348     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00222   |\n",
      "|    explained_variance   | -1.2       |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00954    |\n",
      "|    n_updates            | 20090      |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 31\n",
      "Ep done - 25150.\n",
      "Ep done - 25160.\n",
      "Ep done - 25170.\n",
      "Ep done - 25180.\n",
      "Ep done - 25190.\n",
      "Ep done - 25200.\n",
      "Ep done - 25210.\n",
      "Ep done - 25220.\n",
      "Ep done - 25230.\n",
      "Ep done - 25240.\n",
      "Ep done - 25250.\n",
      "Ep done - 25260.\n",
      "Ep done - 25270.\n",
      "Ep done - 25280.\n",
      "Ep done - 25290.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.8     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 1471     |\n",
      "|    total_timesteps | 534528   |\n",
      "---------------------------------\n",
      "Ep done - 25300.\n",
      "Ep done - 25310.\n",
      "Ep done - 25320.\n",
      "Ep done - 25330.\n",
      "Ep done - 25340.\n",
      "Ep done - 25350.\n",
      "Ep done - 25360.\n",
      "Ep done - 25370.\n",
      "Ep done - 25380.\n",
      "Ep done - 25390.\n",
      "Ep done - 25400.\n",
      "Ep done - 25410.\n",
      "Ep done - 25420.\n",
      "Ep done - 25430.\n",
      "Ep done - 25440.\n",
      "Ep done - 25450.\n",
      "Ep done - 25460.\n",
      "Ep done - 25470.\n",
      "Ep done - 5310.\n",
      "Ep done - 5320.\n",
      "Ep done - 5330.\n",
      "Ep done - 5340.\n",
      "Ep done - 5350.\n",
      "Ep done - 5360.\n",
      "Ep done - 5370.\n",
      "Ep done - 5380.\n",
      "Ep done - 5390.\n",
      "Ep done - 5400.\n",
      "Eval num_timesteps=540000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.23 +/- 0.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.98       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 540000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84002453 |\n",
      "|    clip_fraction        | 0.0838     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00174   |\n",
      "|    explained_variance   | -0.0092    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0439     |\n",
      "|    n_updates            | 20100      |\n",
      "|    policy_gradient_loss | -0.000276  |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 32\n",
      "Ep done - 25480.\n",
      "Ep done - 25490.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1492     |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Ep done - 25500.\n",
      "Ep done - 25510.\n",
      "Ep done - 25520.\n",
      "Ep done - 25530.\n",
      "Ep done - 25540.\n",
      "Ep done - 25550.\n",
      "Ep done - 25560.\n",
      "Ep done - 25570.\n",
      "Ep done - 25580.\n",
      "Ep done - 25590.\n",
      "Ep done - 25600.\n",
      "Ep done - 25610.\n",
      "Ep done - 25620.\n",
      "Ep done - 25630.\n",
      "Ep done - 25640.\n",
      "Ep done - 25650.\n",
      "Ep done - 25660.\n",
      "Ep done - 25670.\n",
      "Ep done - 25680.\n",
      "Ep done - 25690.\n",
      "Ep done - 25700.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 361        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 1512       |\n",
      "|    total_timesteps      | 546816     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35557517 |\n",
      "|    clip_fraction        | 0.0358     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00226   |\n",
      "|    explained_variance   | -0.338     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.0105    |\n",
      "|    n_updates            | 20110      |\n",
      "|    policy_gradient_loss | -0.00826   |\n",
      "|    value_loss           | 0.00454    |\n",
      "----------------------------------------\n",
      "Ep done - 25710.\n",
      "Ep done - 25720.\n",
      "Ep done - 25730.\n",
      "Ep done - 25740.\n",
      "Ep done - 25750.\n",
      "Ep done - 25760.\n",
      "Ep done - 25770.\n",
      "Ep done - 25780.\n",
      "Ep done - 25790.\n",
      "Ep done - 25800.\n",
      "Ep done - 25810.\n",
      "Ep done - 5410.\n",
      "Ep done - 5420.\n",
      "Ep done - 5430.\n",
      "Ep done - 5440.\n",
      "Ep done - 5450.\n",
      "Ep done - 5460.\n",
      "Ep done - 5470.\n",
      "Ep done - 5480.\n",
      "Ep done - 5490.\n",
      "Ep done - 5500.\n",
      "Eval num_timesteps=550000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.03 +/- 0.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 550000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46009263 |\n",
      "|    clip_fraction        | 0.0314     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00301   |\n",
      "|    explained_variance   | -0.112     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0467     |\n",
      "|    n_updates            | 20120      |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.6\n",
      "SELFPLAY: new best model, bumping up generation to 33\n",
      "Ep done - 25820.\n",
      "Ep done - 25830.\n",
      "Ep done - 25840.\n",
      "Ep done - 25850.\n",
      "Ep done - 25860.\n",
      "Ep done - 25870.\n",
      "Ep done - 25880.\n",
      "Ep done - 25890.\n",
      "Ep done - 25900.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 0.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 360      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1533     |\n",
      "|    total_timesteps | 552960   |\n",
      "---------------------------------\n",
      "Ep done - 25910.\n",
      "Ep done - 25920.\n",
      "Ep done - 25930.\n",
      "Ep done - 25940.\n",
      "Ep done - 25950.\n",
      "Ep done - 25960.\n",
      "Ep done - 25970.\n",
      "Ep done - 25980.\n",
      "Ep done - 25990.\n",
      "Ep done - 26000.\n",
      "Ep done - 26010.\n",
      "Ep done - 26020.\n",
      "Ep done - 26030.\n",
      "Ep done - 26040.\n",
      "Ep done - 26050.\n",
      "Ep done - 26060.\n",
      "Ep done - 26070.\n",
      "Ep done - 26080.\n",
      "Ep done - 26090.\n",
      "Ep done - 26100.\n",
      "Ep done - 26110.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.66      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 360       |\n",
      "|    iterations           | 91        |\n",
      "|    time_elapsed         | 1549      |\n",
      "|    total_timesteps      | 559104    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5106108 |\n",
      "|    clip_fraction        | 0.0326    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000605 |\n",
      "|    explained_variance   | 0.0681    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00559   |\n",
      "|    n_updates            | 20130     |\n",
      "|    policy_gradient_loss | -0.00833  |\n",
      "|    value_loss           | 0.0538    |\n",
      "---------------------------------------\n",
      "Ep done - 26120.\n",
      "Ep done - 26130.\n",
      "Ep done - 26140.\n",
      "Ep done - 5510.\n",
      "Ep done - 5520.\n",
      "Ep done - 5530.\n",
      "Ep done - 5540.\n",
      "Ep done - 5550.\n",
      "Ep done - 5560.\n",
      "Ep done - 5570.\n",
      "Ep done - 5580.\n",
      "Ep done - 5590.\n",
      "Ep done - 5600.\n",
      "Eval num_timesteps=560000, episode_reward=0.83 +/- 0.53\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 560000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7664438 |\n",
      "|    clip_fraction        | 0.0419    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00231  |\n",
      "|    explained_variance   | 0.507     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0315   |\n",
      "|    n_updates            | 20140     |\n",
      "|    policy_gradient_loss | -0.00542  |\n",
      "|    value_loss           | 0.0529    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.83\n",
      "SELFPLAY: new best model, bumping up generation to 34\n",
      "Ep done - 26150.\n",
      "Ep done - 26160.\n",
      "Ep done - 26170.\n",
      "Ep done - 26180.\n",
      "Ep done - 26190.\n",
      "Ep done - 26200.\n",
      "Ep done - 26210.\n",
      "Ep done - 26220.\n",
      "Ep done - 26230.\n",
      "Ep done - 26240.\n",
      "Ep done - 26250.\n",
      "Ep done - 26260.\n",
      "Ep done - 26270.\n",
      "Ep done - 26280.\n",
      "Ep done - 26290.\n",
      "Ep done - 26300.\n",
      "Ep done - 26310.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 359      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 1570     |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "Ep done - 26320.\n",
      "Ep done - 26330.\n",
      "Ep done - 26340.\n",
      "Ep done - 26350.\n",
      "Ep done - 26360.\n",
      "Ep done - 26370.\n",
      "Ep done - 26380.\n",
      "Ep done - 26390.\n",
      "Ep done - 26400.\n",
      "Ep done - 26410.\n",
      "Ep done - 26420.\n",
      "Ep done - 26430.\n",
      "Ep done - 26440.\n",
      "Ep done - 26450.\n",
      "Ep done - 26460.\n",
      "Ep done - 26470.\n",
      "Ep done - 5610.\n",
      "Ep done - 5620.\n",
      "Ep done - 5630.\n",
      "Ep done - 5640.\n",
      "Ep done - 5650.\n",
      "Ep done - 5660.\n",
      "Ep done - 5670.\n",
      "Ep done - 5680.\n",
      "Ep done - 5690.\n",
      "Ep done - 5700.\n",
      "Eval num_timesteps=570000, episode_reward=0.93 +/- 0.32\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.93      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 570000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0740405 |\n",
      "|    clip_fraction        | 0.059     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00549  |\n",
      "|    explained_variance   | 0.178     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0337    |\n",
      "|    n_updates            | 20150     |\n",
      "|    policy_gradient_loss | 0.00226   |\n",
      "|    value_loss           | 0.0852    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.93\n",
      "SELFPLAY: new best model, bumping up generation to 35\n",
      "Ep done - 26480.\n",
      "Ep done - 26490.\n",
      "Ep done - 26500.\n",
      "Ep done - 26510.\n",
      "Ep done - 26520.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 359      |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 1590     |\n",
      "|    total_timesteps | 571392   |\n",
      "---------------------------------\n",
      "Ep done - 26530.\n",
      "Ep done - 26540.\n",
      "Ep done - 26550.\n",
      "Ep done - 26560.\n",
      "Ep done - 26570.\n",
      "Ep done - 26580.\n",
      "Ep done - 26590.\n",
      "Ep done - 26600.\n",
      "Ep done - 26610.\n",
      "Ep done - 26620.\n",
      "Ep done - 26630.\n",
      "Ep done - 26640.\n",
      "Ep done - 26650.\n",
      "Ep done - 26660.\n",
      "Ep done - 26670.\n",
      "Ep done - 26680.\n",
      "Ep done - 26690.\n",
      "Ep done - 26700.\n",
      "Ep done - 26710.\n",
      "Ep done - 26720.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.77      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 359       |\n",
      "|    iterations           | 94        |\n",
      "|    time_elapsed         | 1606      |\n",
      "|    total_timesteps      | 577536    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5864544 |\n",
      "|    clip_fraction        | 0.061     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0133   |\n",
      "|    explained_variance   | 0.227     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0399    |\n",
      "|    n_updates            | 20160     |\n",
      "|    policy_gradient_loss | 0.0039    |\n",
      "|    value_loss           | 0.0927    |\n",
      "---------------------------------------\n",
      "Ep done - 26730.\n",
      "Ep done - 26740.\n",
      "Ep done - 26750.\n",
      "Ep done - 26760.\n",
      "Ep done - 26770.\n",
      "Ep done - 26780.\n",
      "Ep done - 26790.\n",
      "Ep done - 26800.\n",
      "Ep done - 26810.\n",
      "Ep done - 5710.\n",
      "Ep done - 5720.\n",
      "Ep done - 5730.\n",
      "Ep done - 5740.\n",
      "Ep done - 5750.\n",
      "Ep done - 5760.\n",
      "Ep done - 5770.\n",
      "Ep done - 5780.\n",
      "Ep done - 5790.\n",
      "Ep done - 5800.\n",
      "Eval num_timesteps=580000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 580000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49236545 |\n",
      "|    clip_fraction        | 0.0417     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00981   |\n",
      "|    explained_variance   | -0.648     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0125     |\n",
      "|    n_updates            | 20170      |\n",
      "|    policy_gradient_loss | 0.000972   |\n",
      "|    value_loss           | 0.0619     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 36\n",
      "Ep done - 26820.\n",
      "Ep done - 26830.\n",
      "Ep done - 26840.\n",
      "Ep done - 26850.\n",
      "Ep done - 26860.\n",
      "Ep done - 26870.\n",
      "Ep done - 26880.\n",
      "Ep done - 26890.\n",
      "Ep done - 26900.\n",
      "Ep done - 26910.\n",
      "Ep done - 26920.\n",
      "Ep done - 26930.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.93    |\n",
      "| time/              |          |\n",
      "|    fps             | 358      |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 1626     |\n",
      "|    total_timesteps | 583680   |\n",
      "---------------------------------\n",
      "Ep done - 26940.\n",
      "Ep done - 26950.\n",
      "Ep done - 26960.\n",
      "Ep done - 26970.\n",
      "Ep done - 26980.\n",
      "Ep done - 26990.\n",
      "Ep done - 27000.\n",
      "Ep done - 27010.\n",
      "Ep done - 27020.\n",
      "Ep done - 27030.\n",
      "Ep done - 27040.\n",
      "Ep done - 27050.\n",
      "Ep done - 27060.\n",
      "Ep done - 27070.\n",
      "Ep done - 27080.\n",
      "Ep done - 27090.\n",
      "Ep done - 27100.\n",
      "Ep done - 27110.\n",
      "Ep done - 27120.\n",
      "Ep done - 27130.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | -0.68    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 359      |\n",
      "|    iterations           | 96       |\n",
      "|    time_elapsed         | 1642     |\n",
      "|    total_timesteps      | 589824   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.607316 |\n",
      "|    clip_fraction        | 0.0739   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00539 |\n",
      "|    explained_variance   | 0.207    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0482   |\n",
      "|    n_updates            | 20180    |\n",
      "|    policy_gradient_loss | -0.00433 |\n",
      "|    value_loss           | 0.116    |\n",
      "--------------------------------------\n",
      "Ep done - 27140.\n",
      "Ep done - 5810.\n",
      "Ep done - 5820.\n",
      "Ep done - 5830.\n",
      "Ep done - 5840.\n",
      "Ep done - 5850.\n",
      "Ep done - 5860.\n",
      "Ep done - 5870.\n",
      "Ep done - 5880.\n",
      "Ep done - 5890.\n",
      "Ep done - 5900.\n",
      "Eval num_timesteps=590000, episode_reward=-0.36 +/- 0.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | -0.36       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 590000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089215904 |\n",
      "|    clip_fraction        | 0.00617     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000956   |\n",
      "|    explained_variance   | -0.229      |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0709      |\n",
      "|    n_updates            | 20190       |\n",
      "|    policy_gradient_loss | 0.0008      |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Ep done - 27150.\n",
      "Ep done - 27160.\n",
      "Ep done - 27170.\n",
      "Ep done - 27180.\n",
      "Ep done - 27190.\n",
      "Ep done - 27200.\n",
      "Ep done - 27210.\n",
      "Ep done - 27220.\n",
      "Ep done - 27230.\n",
      "Ep done - 27240.\n",
      "Ep done - 27250.\n",
      "Ep done - 27260.\n",
      "Ep done - 27270.\n",
      "Ep done - 27280.\n",
      "Ep done - 27290.\n",
      "Ep done - 27300.\n",
      "Ep done - 27310.\n",
      "Ep done - 27320.\n",
      "Ep done - 27330.\n",
      "Ep done - 27340.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 358      |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 1662     |\n",
      "|    total_timesteps | 595968   |\n",
      "---------------------------------\n",
      "Ep done - 27350.\n",
      "Ep done - 27360.\n",
      "Ep done - 27370.\n",
      "Ep done - 27380.\n",
      "Ep done - 27390.\n",
      "Ep done - 27400.\n",
      "Ep done - 27410.\n",
      "Ep done - 27420.\n",
      "Ep done - 27430.\n",
      "Ep done - 27440.\n",
      "Ep done - 27450.\n",
      "Ep done - 27460.\n",
      "Ep done - 27470.\n",
      "Ep done - 5910.\n",
      "Ep done - 5920.\n",
      "Ep done - 5930.\n",
      "Ep done - 5940.\n",
      "Ep done - 5950.\n",
      "Ep done - 5960.\n",
      "Ep done - 5970.\n",
      "Ep done - 5980.\n",
      "Ep done - 5990.\n",
      "Ep done - 6000.\n",
      "Eval num_timesteps=600000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.53      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 600000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7101364 |\n",
      "|    clip_fraction        | 0.0546    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00494  |\n",
      "|    explained_variance   | -0.0536   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0538    |\n",
      "|    n_updates            | 20200     |\n",
      "|    policy_gradient_loss | -0.00702  |\n",
      "|    value_loss           | 0.184     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.53\n",
      "SELFPLAY: new best model, bumping up generation to 37\n",
      "Ep done - 27480.\n",
      "Ep done - 27490.\n",
      "Ep done - 27500.\n",
      "Ep done - 27510.\n",
      "Ep done - 27520.\n",
      "Ep done - 27530.\n",
      "Ep done - 27540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 357      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1683     |\n",
      "|    total_timesteps | 602112   |\n",
      "---------------------------------\n",
      "Ep done - 27550.\n",
      "Ep done - 27560.\n",
      "Ep done - 27570.\n",
      "Ep done - 27580.\n",
      "Ep done - 27590.\n",
      "Ep done - 27600.\n",
      "Ep done - 27610.\n",
      "Ep done - 27620.\n",
      "Ep done - 27630.\n",
      "Ep done - 27640.\n",
      "Ep done - 27650.\n",
      "Ep done - 27660.\n",
      "Ep done - 27670.\n",
      "Ep done - 27680.\n",
      "Ep done - 27690.\n",
      "Ep done - 27700.\n",
      "Ep done - 27710.\n",
      "Ep done - 27720.\n",
      "Ep done - 27730.\n",
      "Ep done - 27740.\n",
      "Ep done - 27750.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.51     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 358       |\n",
      "|    iterations           | 99        |\n",
      "|    time_elapsed         | 1698      |\n",
      "|    total_timesteps      | 608256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4999808 |\n",
      "|    clip_fraction        | 0.0821    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0043   |\n",
      "|    explained_variance   | -0.235    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.155     |\n",
      "|    n_updates            | 20210     |\n",
      "|    policy_gradient_loss | 0.015     |\n",
      "|    value_loss           | 0.272     |\n",
      "---------------------------------------\n",
      "Ep done - 27760.\n",
      "Ep done - 27770.\n",
      "Ep done - 27780.\n",
      "Ep done - 27790.\n",
      "Ep done - 27800.\n",
      "Ep done - 27810.\n",
      "Ep done - 6010.\n",
      "Ep done - 6020.\n",
      "Ep done - 6030.\n",
      "Ep done - 6040.\n",
      "Ep done - 6050.\n",
      "Ep done - 6060.\n",
      "Ep done - 6070.\n",
      "Ep done - 6080.\n",
      "Ep done - 6090.\n",
      "Ep done - 6100.\n",
      "Eval num_timesteps=610000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 610000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88645965 |\n",
      "|    clip_fraction        | 0.0523     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000562  |\n",
      "|    explained_variance   | -0.235     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0845     |\n",
      "|    n_updates            | 20220      |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    value_loss           | 0.203      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.6\n",
      "SELFPLAY: new best model, bumping up generation to 38\n",
      "Ep done - 27820.\n",
      "Ep done - 27830.\n",
      "Ep done - 27840.\n",
      "Ep done - 27850.\n",
      "Ep done - 27860.\n",
      "Ep done - 27870.\n",
      "Ep done - 27880.\n",
      "Ep done - 27890.\n",
      "Ep done - 27900.\n",
      "Ep done - 27910.\n",
      "Ep done - 27920.\n",
      "Ep done - 27930.\n",
      "Ep done - 27940.\n",
      "Ep done - 27950.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.5     |\n",
      "|    ep_rew_mean     | 0.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 357      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 1718     |\n",
      "|    total_timesteps | 614400   |\n",
      "---------------------------------\n",
      "Ep done - 27960.\n",
      "Ep done - 27970.\n",
      "Ep done - 27980.\n",
      "Ep done - 27990.\n",
      "Ep done - 28000.\n",
      "Ep done - 28010.\n",
      "Ep done - 28020.\n",
      "Ep done - 28030.\n",
      "Ep done - 28040.\n",
      "Ep done - 28050.\n",
      "Ep done - 28060.\n",
      "Ep done - 28070.\n",
      "Ep done - 28080.\n",
      "Ep done - 28090.\n",
      "Ep done - 28100.\n",
      "Ep done - 28110.\n",
      "Ep done - 28120.\n",
      "Ep done - 28130.\n",
      "Ep done - 28140.\n",
      "Ep done - 6110.\n",
      "Ep done - 6120.\n",
      "Ep done - 6130.\n",
      "Ep done - 6140.\n",
      "Ep done - 6150.\n",
      "Ep done - 6160.\n",
      "Ep done - 6170.\n",
      "Ep done - 6180.\n",
      "Ep done - 6190.\n",
      "Ep done - 6200.\n",
      "Eval num_timesteps=620000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.16 +/- 0.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.98       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 620000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29890886 |\n",
      "|    clip_fraction        | 0.00799    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000521  |\n",
      "|    explained_variance   | -0.311     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0863     |\n",
      "|    n_updates            | 20230      |\n",
      "|    policy_gradient_loss | -0.000375  |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 39\n",
      "Ep done - 28150.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 357      |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 1738     |\n",
      "|    total_timesteps | 620544   |\n",
      "---------------------------------\n",
      "Ep done - 28160.\n",
      "Ep done - 28170.\n",
      "Ep done - 28180.\n",
      "Ep done - 28190.\n",
      "Ep done - 28200.\n",
      "Ep done - 28210.\n",
      "Ep done - 28220.\n",
      "Ep done - 28230.\n",
      "Ep done - 28240.\n",
      "Ep done - 28250.\n",
      "Ep done - 28260.\n",
      "Ep done - 28270.\n",
      "Ep done - 28280.\n",
      "Ep done - 28290.\n",
      "Ep done - 28300.\n",
      "Ep done - 28310.\n",
      "Ep done - 28320.\n",
      "Ep done - 28330.\n",
      "Ep done - 28340.\n",
      "Ep done - 28350.\n",
      "Ep done - 28360.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | -0.32      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 102        |\n",
      "|    time_elapsed         | 1753       |\n",
      "|    total_timesteps      | 626688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33915243 |\n",
      "|    clip_fraction        | 0.0262     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00106   |\n",
      "|    explained_variance   | 0.223      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0504     |\n",
      "|    n_updates            | 20240      |\n",
      "|    policy_gradient_loss | 0.000354   |\n",
      "|    value_loss           | 0.114      |\n",
      "----------------------------------------\n",
      "Ep done - 28370.\n",
      "Ep done - 28380.\n",
      "Ep done - 28390.\n",
      "Ep done - 28400.\n",
      "Ep done - 28410.\n",
      "Ep done - 28420.\n",
      "Ep done - 28430.\n",
      "Ep done - 28440.\n",
      "Ep done - 28450.\n",
      "Ep done - 28460.\n",
      "Ep done - 28470.\n",
      "Ep done - 6210.\n",
      "Ep done - 6220.\n",
      "Ep done - 6230.\n",
      "Ep done - 6240.\n",
      "Ep done - 6250.\n",
      "Ep done - 6260.\n",
      "Ep done - 6270.\n",
      "Ep done - 6280.\n",
      "Ep done - 6290.\n",
      "Ep done - 6300.\n",
      "Eval num_timesteps=630000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.13 +/- 0.34\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 630000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4189539 |\n",
      "|    clip_fraction        | 0.0634    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00426  |\n",
      "|    explained_variance   | -0.309    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.106     |\n",
      "|    n_updates            | 20250     |\n",
      "|    policy_gradient_loss | -0.00784  |\n",
      "|    value_loss           | 0.252     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 40\n",
      "Ep done - 28480.\n",
      "Ep done - 28490.\n",
      "Ep done - 28500.\n",
      "Ep done - 28510.\n",
      "Ep done - 28520.\n",
      "Ep done - 28530.\n",
      "Ep done - 28540.\n",
      "Ep done - 28550.\n",
      "Ep done - 28560.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.9     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 356      |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 1774     |\n",
      "|    total_timesteps | 632832   |\n",
      "---------------------------------\n",
      "Ep done - 28570.\n",
      "Ep done - 28580.\n",
      "Ep done - 28590.\n",
      "Ep done - 28600.\n",
      "Ep done - 28610.\n",
      "Ep done - 28620.\n",
      "Ep done - 28630.\n",
      "Ep done - 28640.\n",
      "Ep done - 28650.\n",
      "Ep done - 28660.\n",
      "Ep done - 28670.\n",
      "Ep done - 28680.\n",
      "Ep done - 28690.\n",
      "Ep done - 28700.\n",
      "Ep done - 28710.\n",
      "Ep done - 28720.\n",
      "Ep done - 28730.\n",
      "Ep done - 28740.\n",
      "Ep done - 28750.\n",
      "Ep done - 28760.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 31         |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 104        |\n",
      "|    time_elapsed         | 1790       |\n",
      "|    total_timesteps      | 638976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20149867 |\n",
      "|    clip_fraction        | 0.0195     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00442   |\n",
      "|    explained_variance   | -1.53      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00167    |\n",
      "|    n_updates            | 20260      |\n",
      "|    policy_gradient_loss | -0.000462  |\n",
      "|    value_loss           | 0.0213     |\n",
      "----------------------------------------\n",
      "Ep done - 28770.\n",
      "Ep done - 28780.\n",
      "Ep done - 28790.\n",
      "Ep done - 6310.\n",
      "Ep done - 6320.\n",
      "Ep done - 6330.\n",
      "Ep done - 6340.\n",
      "Ep done - 6350.\n",
      "Ep done - 6360.\n",
      "Ep done - 6370.\n",
      "Ep done - 6380.\n",
      "Ep done - 6390.\n",
      "Ep done - 6400.\n",
      "Eval num_timesteps=640000, episode_reward=0.04 +/- 1.00\n",
      "Episode length: 29.99 +/- 0.97\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.04      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 640000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6038011 |\n",
      "|    clip_fraction        | 0.0288    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0165   |\n",
      "|    explained_variance   | 0.736     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.004    |\n",
      "|    n_updates            | 20270     |\n",
      "|    policy_gradient_loss | -0.00297  |\n",
      "|    value_loss           | 0.00242   |\n",
      "---------------------------------------\n",
      "Ep done - 28800.\n",
      "Ep done - 28810.\n",
      "Ep done - 28820.\n",
      "Ep done - 28830.\n",
      "Ep done - 28840.\n",
      "Ep done - 28850.\n",
      "Ep done - 28860.\n",
      "Ep done - 28870.\n",
      "Ep done - 28880.\n",
      "Ep done - 28890.\n",
      "Ep done - 28900.\n",
      "Ep done - 28910.\n",
      "Ep done - 28920.\n",
      "Ep done - 28930.\n",
      "Ep done - 28940.\n",
      "Ep done - 28950.\n",
      "Ep done - 28960.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 355      |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 1812     |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Ep done - 28970.\n",
      "Ep done - 28980.\n",
      "Ep done - 28990.\n",
      "Ep done - 29000.\n",
      "Ep done - 29010.\n",
      "Ep done - 29020.\n",
      "Ep done - 29030.\n",
      "Ep done - 29040.\n",
      "Ep done - 29050.\n",
      "Ep done - 29060.\n",
      "Ep done - 29070.\n",
      "Ep done - 29080.\n",
      "Ep done - 29090.\n",
      "Ep done - 29100.\n",
      "Ep done - 29110.\n",
      "Ep done - 29120.\n",
      "Ep done - 6410.\n",
      "Ep done - 6420.\n",
      "Ep done - 6430.\n",
      "Ep done - 6440.\n",
      "Ep done - 6450.\n",
      "Ep done - 6460.\n",
      "Ep done - 6470.\n",
      "Ep done - 6480.\n",
      "Ep done - 6490.\n",
      "Ep done - 6500.\n",
      "Eval num_timesteps=650000, episode_reward=0.72 +/- 0.69\n",
      "Episode length: 30.86 +/- 0.35\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.9      |\n",
      "|    mean_reward          | 0.72      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 650000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1852345 |\n",
      "|    clip_fraction        | 0.0974    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000673 |\n",
      "|    explained_variance   | -0.244    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0943    |\n",
      "|    n_updates            | 20280     |\n",
      "|    policy_gradient_loss | -0.00825  |\n",
      "|    value_loss           | 0.271     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.72\n",
      "SELFPLAY: new best model, bumping up generation to 41\n",
      "Ep done - 29130.\n",
      "Ep done - 29140.\n",
      "Ep done - 29150.\n",
      "Ep done - 29160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.5     |\n",
      "|    ep_rew_mean     | -0.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 355      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 1832     |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "Ep done - 29170.\n",
      "Ep done - 29180.\n",
      "Ep done - 29190.\n",
      "Ep done - 29200.\n",
      "Ep done - 29210.\n",
      "Ep done - 29220.\n",
      "Ep done - 29230.\n",
      "Ep done - 29240.\n",
      "Ep done - 29250.\n",
      "Ep done - 29260.\n",
      "Ep done - 29270.\n",
      "Ep done - 29280.\n",
      "Ep done - 29290.\n",
      "Ep done - 29300.\n",
      "Ep done - 29310.\n",
      "Ep done - 29320.\n",
      "Ep done - 29330.\n",
      "Ep done - 29340.\n",
      "Ep done - 29350.\n",
      "Ep done - 29360.\n",
      "Ep done - 29370.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 107        |\n",
      "|    time_elapsed         | 1847       |\n",
      "|    total_timesteps      | 657408     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45028082 |\n",
      "|    clip_fraction        | 0.0289     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000135  |\n",
      "|    explained_variance   | -0.175     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0754     |\n",
      "|    n_updates            | 20290      |\n",
      "|    policy_gradient_loss | 0.00556    |\n",
      "|    value_loss           | 0.202      |\n",
      "----------------------------------------\n",
      "Ep done - 29380.\n",
      "Ep done - 29390.\n",
      "Ep done - 29400.\n",
      "Ep done - 29410.\n",
      "Ep done - 29420.\n",
      "Ep done - 29430.\n",
      "Ep done - 29440.\n",
      "Ep done - 29450.\n",
      "Ep done - 6510.\n",
      "Ep done - 6520.\n",
      "Ep done - 6530.\n",
      "Ep done - 6540.\n",
      "Ep done - 6550.\n",
      "Ep done - 6560.\n",
      "Ep done - 6570.\n",
      "Ep done - 6580.\n",
      "Ep done - 6590.\n",
      "Ep done - 6600.\n",
      "Eval num_timesteps=660000, episode_reward=-0.98 +/- 0.14\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 660000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.0781548 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000541 |\n",
      "|    explained_variance   | -0.0592   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0323   |\n",
      "|    n_updates            | 20300     |\n",
      "|    policy_gradient_loss | -0.0191   |\n",
      "|    value_loss           | 0.0233    |\n",
      "---------------------------------------\n",
      "Ep done - 29460.\n",
      "Ep done - 29470.\n",
      "Ep done - 29480.\n",
      "Ep done - 29490.\n",
      "Ep done - 29500.\n",
      "Ep done - 29510.\n",
      "Ep done - 29520.\n",
      "Ep done - 29530.\n",
      "Ep done - 29540.\n",
      "Ep done - 29550.\n",
      "Ep done - 29560.\n",
      "Ep done - 29570.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 355      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1868     |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "Ep done - 29580.\n",
      "Ep done - 29590.\n",
      "Ep done - 29600.\n",
      "Ep done - 29610.\n",
      "Ep done - 29620.\n",
      "Ep done - 29630.\n",
      "Ep done - 29640.\n",
      "Ep done - 29650.\n",
      "Ep done - 29660.\n",
      "Ep done - 29670.\n",
      "Ep done - 29680.\n",
      "Ep done - 29690.\n",
      "Ep done - 29700.\n",
      "Ep done - 29710.\n",
      "Ep done - 29720.\n",
      "Ep done - 29730.\n",
      "Ep done - 29740.\n",
      "Ep done - 29750.\n",
      "Ep done - 29760.\n",
      "Ep done - 29770.\n",
      "Ep done - 29780.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.95     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 355       |\n",
      "|    iterations           | 109       |\n",
      "|    time_elapsed         | 1883      |\n",
      "|    total_timesteps      | 669696    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4521974 |\n",
      "|    clip_fraction        | 0.0326    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000615 |\n",
      "|    explained_variance   | -0.328    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0393    |\n",
      "|    n_updates            | 20310     |\n",
      "|    policy_gradient_loss | -0.00533  |\n",
      "|    value_loss           | 0.0307    |\n",
      "---------------------------------------\n",
      "Ep done - 29790.\n",
      "Ep done - 6610.\n",
      "Ep done - 6620.\n",
      "Ep done - 6630.\n",
      "Ep done - 6640.\n",
      "Ep done - 6650.\n",
      "Ep done - 6660.\n",
      "Ep done - 6670.\n",
      "Ep done - 6680.\n",
      "Ep done - 6690.\n",
      "Ep done - 6700.\n",
      "Eval num_timesteps=670000, episode_reward=0.99 +/- 0.10\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.99      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 670000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.115904  |\n",
      "|    clip_fraction        | 0.105     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000448 |\n",
      "|    explained_variance   | 0.267     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00542  |\n",
      "|    n_updates            | 20320     |\n",
      "|    policy_gradient_loss | 0.00468   |\n",
      "|    value_loss           | 0.0161    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.99\n",
      "SELFPLAY: new best model, bumping up generation to 42\n",
      "Ep done - 29800.\n",
      "Ep done - 29810.\n",
      "Ep done - 29820.\n",
      "Ep done - 29830.\n",
      "Ep done - 29840.\n",
      "Ep done - 29850.\n",
      "Ep done - 29860.\n",
      "Ep done - 29870.\n",
      "Ep done - 29880.\n",
      "Ep done - 29890.\n",
      "Ep done - 29900.\n",
      "Ep done - 29910.\n",
      "Ep done - 29920.\n",
      "Ep done - 29930.\n",
      "Ep done - 29940.\n",
      "Ep done - 29950.\n",
      "Ep done - 29960.\n",
      "Ep done - 29970.\n",
      "Ep done - 29980.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | 0.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1904     |\n",
      "|    total_timesteps | 675840   |\n",
      "---------------------------------\n",
      "Ep done - 29990.\n",
      "Ep done - 30000.\n",
      "Ep done - 30010.\n",
      "Ep done - 30020.\n",
      "Ep done - 30030.\n",
      "Ep done - 30040.\n",
      "Ep done - 30050.\n",
      "Ep done - 30060.\n",
      "Ep done - 30070.\n",
      "Ep done - 30080.\n",
      "Ep done - 30090.\n",
      "Ep done - 30100.\n",
      "Ep done - 30110.\n",
      "Ep done - 30120.\n",
      "Ep done - 6710.\n",
      "Ep done - 6720.\n",
      "Ep done - 6730.\n",
      "Ep done - 6740.\n",
      "Ep done - 6750.\n",
      "Ep done - 6760.\n",
      "Ep done - 6770.\n",
      "Ep done - 6780.\n",
      "Ep done - 6790.\n",
      "Ep done - 6800.\n",
      "Eval num_timesteps=680000, episode_reward=-0.66 +/- 0.75\n",
      "Episode length: 30.00 +/- 0.35\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.66     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 680000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0986391 |\n",
      "|    clip_fraction        | 0.0743    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00402  |\n",
      "|    explained_variance   | -0.568    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0169    |\n",
      "|    n_updates            | 20330     |\n",
      "|    policy_gradient_loss | 0.0187    |\n",
      "|    value_loss           | 0.0723    |\n",
      "---------------------------------------\n",
      "Ep done - 30130.\n",
      "Ep done - 30140.\n",
      "Ep done - 30150.\n",
      "Ep done - 30160.\n",
      "Ep done - 30170.\n",
      "Ep done - 30180.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 111      |\n",
      "|    time_elapsed    | 1925     |\n",
      "|    total_timesteps | 681984   |\n",
      "---------------------------------\n",
      "Ep done - 30190.\n",
      "Ep done - 30200.\n",
      "Ep done - 30210.\n",
      "Ep done - 30220.\n",
      "Ep done - 30230.\n",
      "Ep done - 30240.\n",
      "Ep done - 30250.\n",
      "Ep done - 30260.\n",
      "Ep done - 30270.\n",
      "Ep done - 30280.\n",
      "Ep done - 30290.\n",
      "Ep done - 30300.\n",
      "Ep done - 30310.\n",
      "Ep done - 30320.\n",
      "Ep done - 30330.\n",
      "Ep done - 30340.\n",
      "Ep done - 30350.\n",
      "Ep done - 30360.\n",
      "Ep done - 30370.\n",
      "Ep done - 30380.\n",
      "Ep done - 30390.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 29.9     |\n",
      "|    ep_rew_mean          | -0.74    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 354      |\n",
      "|    iterations           | 112      |\n",
      "|    time_elapsed         | 1942     |\n",
      "|    total_timesteps      | 688128   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 4.109246 |\n",
      "|    clip_fraction        | 0.194    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0041  |\n",
      "|    explained_variance   | -0.848   |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0416   |\n",
      "|    n_updates            | 20340    |\n",
      "|    policy_gradient_loss | 0.0112   |\n",
      "|    value_loss           | 0.227    |\n",
      "--------------------------------------\n",
      "Ep done - 30400.\n",
      "Ep done - 30410.\n",
      "Ep done - 30420.\n",
      "Ep done - 30430.\n",
      "Ep done - 30440.\n",
      "Ep done - 30450.\n",
      "Ep done - 6810.\n",
      "Ep done - 6820.\n",
      "Ep done - 6830.\n",
      "Ep done - 6840.\n",
      "Ep done - 6850.\n",
      "Ep done - 6860.\n",
      "Ep done - 6870.\n",
      "Ep done - 6880.\n",
      "Ep done - 6890.\n",
      "Ep done - 6900.\n",
      "Eval num_timesteps=690000, episode_reward=0.14 +/- 0.99\n",
      "Episode length: 29.91 +/- 0.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.9       |\n",
      "|    mean_reward          | 0.14       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 690000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.99060845 |\n",
      "|    clip_fraction        | 0.0679     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00477   |\n",
      "|    explained_variance   | -8.23e-06  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.11       |\n",
      "|    n_updates            | 20350      |\n",
      "|    policy_gradient_loss | 0.00132    |\n",
      "|    value_loss           | 0.192      |\n",
      "----------------------------------------\n",
      "Ep done - 30460.\n",
      "Ep done - 30470.\n",
      "Ep done - 30480.\n",
      "Ep done - 30490.\n",
      "Ep done - 30500.\n",
      "Ep done - 30510.\n",
      "Ep done - 30520.\n",
      "Ep done - 30530.\n",
      "Ep done - 30540.\n",
      "Ep done - 30550.\n",
      "Ep done - 30560.\n",
      "Ep done - 30570.\n",
      "Ep done - 30580.\n",
      "Ep done - 30590.\n",
      "Ep done - 30600.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 0.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 1962     |\n",
      "|    total_timesteps | 694272   |\n",
      "---------------------------------\n",
      "Ep done - 30610.\n",
      "Ep done - 30620.\n",
      "Ep done - 30630.\n",
      "Ep done - 30640.\n",
      "Ep done - 30650.\n",
      "Ep done - 30660.\n",
      "Ep done - 30670.\n",
      "Ep done - 30680.\n",
      "Ep done - 30690.\n",
      "Ep done - 30700.\n",
      "Ep done - 30710.\n",
      "Ep done - 30720.\n",
      "Ep done - 30730.\n",
      "Ep done - 30740.\n",
      "Ep done - 30750.\n",
      "Ep done - 30760.\n",
      "Ep done - 30770.\n",
      "Ep done - 30780.\n",
      "Ep done - 30790.\n",
      "Ep done - 6910.\n",
      "Ep done - 6920.\n",
      "Ep done - 6930.\n",
      "Ep done - 6940.\n",
      "Ep done - 6950.\n",
      "Ep done - 6960.\n",
      "Ep done - 6970.\n",
      "Ep done - 6980.\n",
      "Ep done - 6990.\n",
      "Ep done - 7000.\n",
      "Eval num_timesteps=700000, episode_reward=0.17 +/- 0.98\n",
      "Episode length: 29.78 +/- 0.46\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.8      |\n",
      "|    mean_reward          | 0.17      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 700000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2469498 |\n",
      "|    clip_fraction        | 0.0199    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00391  |\n",
      "|    explained_variance   | 9.54e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.123     |\n",
      "|    n_updates            | 20360     |\n",
      "|    policy_gradient_loss | -1.44e-05 |\n",
      "|    value_loss           | 0.293     |\n",
      "---------------------------------------\n",
      "Ep done - 30800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | -0.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1984     |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "Ep done - 30810.\n",
      "Ep done - 30820.\n",
      "Ep done - 30830.\n",
      "Ep done - 30840.\n",
      "Ep done - 30850.\n",
      "Ep done - 30860.\n",
      "Ep done - 30870.\n",
      "Ep done - 30880.\n",
      "Ep done - 30890.\n",
      "Ep done - 30900.\n",
      "Ep done - 30910.\n",
      "Ep done - 30920.\n",
      "Ep done - 30930.\n",
      "Ep done - 30940.\n",
      "Ep done - 30950.\n",
      "Ep done - 30960.\n",
      "Ep done - 30970.\n",
      "Ep done - 30980.\n",
      "Ep done - 30990.\n",
      "Ep done - 31000.\n",
      "Ep done - 31010.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.7       |\n",
      "|    ep_rew_mean          | -0.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 115        |\n",
      "|    time_elapsed         | 2000       |\n",
      "|    total_timesteps      | 706560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16884172 |\n",
      "|    clip_fraction        | 0.0112     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00479   |\n",
      "|    explained_variance   | 0.06       |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.133      |\n",
      "|    n_updates            | 20370      |\n",
      "|    policy_gradient_loss | -0.000607  |\n",
      "|    value_loss           | 0.279      |\n",
      "----------------------------------------\n",
      "Ep done - 31020.\n",
      "Ep done - 31030.\n",
      "Ep done - 31040.\n",
      "Ep done - 31050.\n",
      "Ep done - 31060.\n",
      "Ep done - 31070.\n",
      "Ep done - 31080.\n",
      "Ep done - 31090.\n",
      "Ep done - 31100.\n",
      "Ep done - 31110.\n",
      "Ep done - 31120.\n",
      "Ep done - 31130.\n",
      "Ep done - 7010.\n",
      "Ep done - 7020.\n",
      "Ep done - 7030.\n",
      "Ep done - 7040.\n",
      "Ep done - 7050.\n",
      "Ep done - 7060.\n",
      "Ep done - 7070.\n",
      "Ep done - 7080.\n",
      "Ep done - 7090.\n",
      "Ep done - 7100.\n",
      "Eval num_timesteps=710000, episode_reward=-0.12 +/- 0.93\n",
      "Episode length: 29.64 +/- 0.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.6       |\n",
      "|    mean_reward          | -0.12      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 710000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12173041 |\n",
      "|    clip_fraction        | 0.0143     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00165   |\n",
      "|    explained_variance   | 0.137      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.111      |\n",
      "|    n_updates            | 20380      |\n",
      "|    policy_gradient_loss | 0.00266    |\n",
      "|    value_loss           | 0.264      |\n",
      "----------------------------------------\n",
      "Ep done - 31140.\n",
      "Ep done - 31150.\n",
      "Ep done - 31160.\n",
      "Ep done - 31170.\n",
      "Ep done - 31180.\n",
      "Ep done - 31190.\n",
      "Ep done - 31200.\n",
      "Ep done - 31210.\n",
      "Ep done - 31220.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | -0.13    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 2020     |\n",
      "|    total_timesteps | 712704   |\n",
      "---------------------------------\n",
      "Ep done - 31230.\n",
      "Ep done - 31240.\n",
      "Ep done - 31250.\n",
      "Ep done - 31260.\n",
      "Ep done - 31270.\n",
      "Ep done - 31280.\n",
      "Ep done - 31290.\n",
      "Ep done - 31300.\n",
      "Ep done - 31310.\n",
      "Ep done - 31320.\n",
      "Ep done - 31330.\n",
      "Ep done - 31340.\n",
      "Ep done - 31350.\n",
      "Ep done - 31360.\n",
      "Ep done - 31370.\n",
      "Ep done - 31380.\n",
      "Ep done - 31390.\n",
      "Ep done - 31400.\n",
      "Ep done - 31410.\n",
      "Ep done - 31420.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.6      |\n",
      "|    ep_rew_mean          | 0.24      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 352       |\n",
      "|    iterations           | 117       |\n",
      "|    time_elapsed         | 2037      |\n",
      "|    total_timesteps      | 718848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6287839 |\n",
      "|    clip_fraction        | 0.023     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | 0.153     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0902    |\n",
      "|    n_updates            | 20390     |\n",
      "|    policy_gradient_loss | -0.00183  |\n",
      "|    value_loss           | 0.246     |\n",
      "---------------------------------------\n",
      "Ep done - 31430.\n",
      "Ep done - 31440.\n",
      "Ep done - 31450.\n",
      "Ep done - 31460.\n",
      "Ep done - 7110.\n",
      "Ep done - 7120.\n",
      "Ep done - 7130.\n",
      "Ep done - 7140.\n",
      "Ep done - 7150.\n",
      "Ep done - 7160.\n",
      "Ep done - 7170.\n",
      "Ep done - 7180.\n",
      "Ep done - 7190.\n",
      "Ep done - 7200.\n",
      "Eval num_timesteps=720000, episode_reward=0.40 +/- 0.91\n",
      "Episode length: 29.95 +/- 0.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.9       |\n",
      "|    mean_reward          | 0.4        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 720000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09555963 |\n",
      "|    clip_fraction        | 0.00832    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00106   |\n",
      "|    explained_variance   | 0.294      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.101      |\n",
      "|    n_updates            | 20400      |\n",
      "|    policy_gradient_loss | -0.00122   |\n",
      "|    value_loss           | 0.254      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.4\n",
      "SELFPLAY: new best model, bumping up generation to 43\n",
      "Ep done - 31470.\n",
      "Ep done - 31480.\n",
      "Ep done - 31490.\n",
      "Ep done - 31500.\n",
      "Ep done - 31510.\n",
      "Ep done - 31520.\n",
      "Ep done - 31530.\n",
      "Ep done - 31540.\n",
      "Ep done - 31550.\n",
      "Ep done - 31560.\n",
      "Ep done - 31570.\n",
      "Ep done - 31580.\n",
      "Ep done - 31590.\n",
      "Ep done - 31600.\n",
      "Ep done - 31610.\n",
      "Ep done - 31620.\n",
      "Ep done - 31630.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 2057     |\n",
      "|    total_timesteps | 724992   |\n",
      "---------------------------------\n",
      "Ep done - 31640.\n",
      "Ep done - 31650.\n",
      "Ep done - 31660.\n",
      "Ep done - 31670.\n",
      "Ep done - 31680.\n",
      "Ep done - 31690.\n",
      "Ep done - 31700.\n",
      "Ep done - 31710.\n",
      "Ep done - 31720.\n",
      "Ep done - 31730.\n",
      "Ep done - 31740.\n",
      "Ep done - 31750.\n",
      "Ep done - 31760.\n",
      "Ep done - 31770.\n",
      "Ep done - 31780.\n",
      "Ep done - 31790.\n",
      "Ep done - 31800.\n",
      "Ep done - 7210.\n",
      "Ep done - 7220.\n",
      "Ep done - 7230.\n",
      "Ep done - 7240.\n",
      "Ep done - 7250.\n",
      "Ep done - 7260.\n",
      "Ep done - 7270.\n",
      "Ep done - 7280.\n",
      "Ep done - 7290.\n",
      "Ep done - 7300.\n",
      "Eval num_timesteps=730000, episode_reward=-0.48 +/- 0.88\n",
      "Episode length: 30.01 +/- 0.10\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.48    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 730000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.203784 |\n",
      "|    clip_fraction        | 0.119    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00571 |\n",
      "|    explained_variance   | 0.0199   |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.156    |\n",
      "|    n_updates            | 20410    |\n",
      "|    policy_gradient_loss | 1.48e-05 |\n",
      "|    value_loss           | 0.225    |\n",
      "--------------------------------------\n",
      "Ep done - 31810.\n",
      "Ep done - 31820.\n",
      "Ep done - 31830.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 2077     |\n",
      "|    total_timesteps | 731136   |\n",
      "---------------------------------\n",
      "Ep done - 31840.\n",
      "Ep done - 31850.\n",
      "Ep done - 31860.\n",
      "Ep done - 31870.\n",
      "Ep done - 31880.\n",
      "Ep done - 31890.\n",
      "Ep done - 31900.\n",
      "Ep done - 31910.\n",
      "Ep done - 31920.\n",
      "Ep done - 31930.\n",
      "Ep done - 31940.\n",
      "Ep done - 31950.\n",
      "Ep done - 31960.\n",
      "Ep done - 31970.\n",
      "Ep done - 31980.\n",
      "Ep done - 31990.\n",
      "Ep done - 32000.\n",
      "Ep done - 32010.\n",
      "Ep done - 32020.\n",
      "Ep done - 32030.\n",
      "Ep done - 32040.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.42       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 120        |\n",
      "|    time_elapsed         | 2092       |\n",
      "|    total_timesteps      | 737280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49595392 |\n",
      "|    clip_fraction        | 0.0459     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00136   |\n",
      "|    explained_variance   | -0.0467    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0903     |\n",
      "|    n_updates            | 20420      |\n",
      "|    policy_gradient_loss | -0.00544   |\n",
      "|    value_loss           | 0.247      |\n",
      "----------------------------------------\n",
      "Ep done - 32050.\n",
      "Ep done - 32060.\n",
      "Ep done - 32070.\n",
      "Ep done - 32080.\n",
      "Ep done - 32090.\n",
      "Ep done - 32100.\n",
      "Ep done - 32110.\n",
      "Ep done - 32120.\n",
      "Ep done - 32130.\n",
      "Ep done - 7310.\n",
      "Ep done - 7320.\n",
      "Ep done - 7330.\n",
      "Ep done - 7340.\n",
      "Ep done - 7350.\n",
      "Ep done - 7360.\n",
      "Ep done - 7370.\n",
      "Ep done - 7380.\n",
      "Ep done - 7390.\n",
      "Ep done - 7400.\n",
      "Eval num_timesteps=740000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 30.01 +/- 0.10\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.2     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 740000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 5.211926 |\n",
      "|    clip_fraction        | 0.134    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00422 |\n",
      "|    explained_variance   | 0.1      |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0981   |\n",
      "|    n_updates            | 20430    |\n",
      "|    policy_gradient_loss | -0.0132  |\n",
      "|    value_loss           | 0.223    |\n",
      "--------------------------------------\n",
      "Ep done - 32140.\n",
      "Ep done - 32150.\n",
      "Ep done - 32160.\n",
      "Ep done - 32170.\n",
      "Ep done - 32180.\n",
      "Ep done - 32190.\n",
      "Ep done - 32200.\n",
      "Ep done - 32210.\n",
      "Ep done - 32220.\n",
      "Ep done - 32230.\n",
      "Ep done - 32240.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 2114     |\n",
      "|    total_timesteps | 743424   |\n",
      "---------------------------------\n",
      "Ep done - 32250.\n",
      "Ep done - 32260.\n",
      "Ep done - 32270.\n",
      "Ep done - 32280.\n",
      "Ep done - 32290.\n",
      "Ep done - 32300.\n",
      "Ep done - 32310.\n",
      "Ep done - 32320.\n",
      "Ep done - 32330.\n",
      "Ep done - 32340.\n",
      "Ep done - 32350.\n",
      "Ep done - 32360.\n",
      "Ep done - 32370.\n",
      "Ep done - 32380.\n",
      "Ep done - 32390.\n",
      "Ep done - 32400.\n",
      "Ep done - 32410.\n",
      "Ep done - 32420.\n",
      "Ep done - 32430.\n",
      "Ep done - 32440.\n",
      "Ep done - 32450.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.22      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 351       |\n",
      "|    iterations           | 122       |\n",
      "|    time_elapsed         | 2130      |\n",
      "|    total_timesteps      | 749568    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0490317 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0134   |\n",
      "|    explained_variance   | 0.334     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0639    |\n",
      "|    n_updates            | 20440     |\n",
      "|    policy_gradient_loss | 0.00156   |\n",
      "|    value_loss           | 0.222     |\n",
      "---------------------------------------\n",
      "Ep done - 32460.\n",
      "Ep done - 7410.\n",
      "Ep done - 7420.\n",
      "Ep done - 7430.\n",
      "Ep done - 7440.\n",
      "Ep done - 7450.\n",
      "Ep done - 7460.\n",
      "Ep done - 7470.\n",
      "Ep done - 7480.\n",
      "Ep done - 7490.\n",
      "Ep done - 7500.\n",
      "Eval num_timesteps=750000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.30 +/- 0.46\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.3      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 750000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.9516823 |\n",
      "|    clip_fraction        | 0.223     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.011    |\n",
      "|    explained_variance   | -0.507    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0815    |\n",
      "|    n_updates            | 20450     |\n",
      "|    policy_gradient_loss | 0.0434    |\n",
      "|    value_loss           | 0.324     |\n",
      "---------------------------------------\n",
      "Ep done - 32470.\n",
      "Ep done - 32480.\n",
      "Ep done - 32490.\n",
      "Ep done - 32500.\n",
      "Ep done - 32510.\n",
      "Ep done - 32520.\n",
      "Ep done - 32530.\n",
      "Ep done - 32540.\n",
      "Ep done - 32550.\n",
      "Ep done - 32560.\n",
      "Ep done - 32570.\n",
      "Ep done - 32580.\n",
      "Ep done - 32590.\n",
      "Ep done - 32600.\n",
      "Ep done - 32610.\n",
      "Ep done - 32620.\n",
      "Ep done - 32630.\n",
      "Ep done - 32640.\n",
      "Ep done - 32650.\n",
      "Ep done - 32660.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.3     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 2151     |\n",
      "|    total_timesteps | 755712   |\n",
      "---------------------------------\n",
      "Ep done - 32670.\n",
      "Ep done - 32680.\n",
      "Ep done - 32690.\n",
      "Ep done - 32700.\n",
      "Ep done - 32710.\n",
      "Ep done - 32720.\n",
      "Ep done - 32730.\n",
      "Ep done - 32740.\n",
      "Ep done - 32750.\n",
      "Ep done - 32760.\n",
      "Ep done - 32770.\n",
      "Ep done - 32780.\n",
      "Ep done - 32790.\n",
      "Ep done - 32800.\n",
      "Ep done - 7510.\n",
      "Ep done - 7520.\n",
      "Ep done - 7530.\n",
      "Ep done - 7540.\n",
      "Ep done - 7550.\n",
      "Ep done - 7560.\n",
      "Ep done - 7570.\n",
      "Ep done - 7580.\n",
      "Ep done - 7590.\n",
      "Ep done - 7600.\n",
      "Eval num_timesteps=760000, episode_reward=-0.27 +/- 0.96\n",
      "Episode length: 30.10 +/- 0.30\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.27     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 760000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.6409264 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00922  |\n",
      "|    explained_variance   | -0.182    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00361  |\n",
      "|    n_updates            | 20460     |\n",
      "|    policy_gradient_loss | -0.0185   |\n",
      "|    value_loss           | 0.0209    |\n",
      "---------------------------------------\n",
      "Ep done - 32810.\n",
      "Ep done - 32820.\n",
      "Ep done - 32830.\n",
      "Ep done - 32840.\n",
      "Ep done - 32850.\n",
      "Ep done - 32860.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 2171     |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "Ep done - 32870.\n",
      "Ep done - 32880.\n",
      "Ep done - 32890.\n",
      "Ep done - 32900.\n",
      "Ep done - 32910.\n",
      "Ep done - 32920.\n",
      "Ep done - 32930.\n",
      "Ep done - 32940.\n",
      "Ep done - 32950.\n",
      "Ep done - 32960.\n",
      "Ep done - 32970.\n",
      "Ep done - 32980.\n",
      "Ep done - 32990.\n",
      "Ep done - 33000.\n",
      "Ep done - 33010.\n",
      "Ep done - 33020.\n",
      "Ep done - 33030.\n",
      "Ep done - 33040.\n",
      "Ep done - 33050.\n",
      "Ep done - 33060.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.9      |\n",
      "|    ep_rew_mean          | 0.13      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 351       |\n",
      "|    iterations           | 125       |\n",
      "|    time_elapsed         | 2187      |\n",
      "|    total_timesteps      | 768000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4405969 |\n",
      "|    clip_fraction        | 0.118     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00663  |\n",
      "|    explained_variance   | 0.00987   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0337    |\n",
      "|    n_updates            | 20470     |\n",
      "|    policy_gradient_loss | 0.00439   |\n",
      "|    value_loss           | 0.121     |\n",
      "---------------------------------------\n",
      "Ep done - 33070.\n",
      "Ep done - 33080.\n",
      "Ep done - 33090.\n",
      "Ep done - 33100.\n",
      "Ep done - 33110.\n",
      "Ep done - 33120.\n",
      "Ep done - 33130.\n",
      "Ep done - 7610.\n",
      "Ep done - 7620.\n",
      "Ep done - 7630.\n",
      "Ep done - 7640.\n",
      "Ep done - 7650.\n",
      "Ep done - 7660.\n",
      "Ep done - 7670.\n",
      "Ep done - 7680.\n",
      "Ep done - 7690.\n",
      "Ep done - 7700.\n",
      "Eval num_timesteps=770000, episode_reward=-0.26 +/- 0.97\n",
      "Episode length: 29.65 +/- 0.88\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.6      |\n",
      "|    mean_reward          | -0.26     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 770000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.0626457 |\n",
      "|    clip_fraction        | 0.119     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00317  |\n",
      "|    explained_variance   | -0.903    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00954   |\n",
      "|    n_updates            | 20480     |\n",
      "|    policy_gradient_loss | -0.0148   |\n",
      "|    value_loss           | 0.068     |\n",
      "---------------------------------------\n",
      "Ep done - 33140.\n",
      "Ep done - 33150.\n",
      "Ep done - 33160.\n",
      "Ep done - 33170.\n",
      "Ep done - 33180.\n",
      "Ep done - 33190.\n",
      "Ep done - 33200.\n",
      "Ep done - 33210.\n",
      "Ep done - 33220.\n",
      "Ep done - 33230.\n",
      "Ep done - 33240.\n",
      "Ep done - 33250.\n",
      "Ep done - 33260.\n",
      "Ep done - 33270.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.6     |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 2210     |\n",
      "|    total_timesteps | 774144   |\n",
      "---------------------------------\n",
      "Ep done - 33280.\n",
      "Ep done - 33290.\n",
      "Ep done - 33300.\n",
      "Ep done - 33310.\n",
      "Ep done - 33320.\n",
      "Ep done - 33330.\n",
      "Ep done - 33340.\n",
      "Ep done - 33350.\n",
      "Ep done - 33360.\n",
      "Ep done - 33370.\n",
      "Ep done - 33380.\n",
      "Ep done - 33390.\n",
      "Ep done - 33400.\n",
      "Ep done - 33410.\n",
      "Ep done - 33420.\n",
      "Ep done - 33430.\n",
      "Ep done - 33440.\n",
      "Ep done - 33450.\n",
      "Ep done - 33460.\n",
      "Ep done - 7710.\n",
      "Ep done - 7720.\n",
      "Ep done - 7730.\n",
      "Ep done - 7740.\n",
      "Ep done - 7750.\n",
      "Ep done - 7760.\n",
      "Ep done - 7770.\n",
      "Ep done - 7780.\n",
      "Ep done - 7790.\n",
      "Ep done - 7800.\n",
      "Eval num_timesteps=780000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.20 +/- 0.40\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 780000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5003298 |\n",
      "|    clip_fraction        | 0.0377    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00345  |\n",
      "|    explained_variance   | 0.0873    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0551    |\n",
      "|    n_updates            | 20490     |\n",
      "|    policy_gradient_loss | -0.000298 |\n",
      "|    value_loss           | 0.144     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 44\n",
      "Ep done - 33470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 2232     |\n",
      "|    total_timesteps | 780288   |\n",
      "---------------------------------\n",
      "Ep done - 33480.\n",
      "Ep done - 33490.\n",
      "Ep done - 33500.\n",
      "Ep done - 33510.\n",
      "Ep done - 33520.\n",
      "Ep done - 33530.\n",
      "Ep done - 33540.\n",
      "Ep done - 33550.\n",
      "Ep done - 33560.\n",
      "Ep done - 33570.\n",
      "Ep done - 33580.\n",
      "Ep done - 33590.\n",
      "Ep done - 33600.\n",
      "Ep done - 33610.\n",
      "Ep done - 33620.\n",
      "Ep done - 33630.\n",
      "Ep done - 33640.\n",
      "Ep done - 33650.\n",
      "Ep done - 33660.\n",
      "Ep done - 33670.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.3      |\n",
      "|    ep_rew_mean          | 0.02      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 349       |\n",
      "|    iterations           | 128       |\n",
      "|    time_elapsed         | 2249      |\n",
      "|    total_timesteps      | 786432    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6489673 |\n",
      "|    clip_fraction        | 0.0509    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00309  |\n",
      "|    explained_variance   | -0.801    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0352    |\n",
      "|    n_updates            | 20500     |\n",
      "|    policy_gradient_loss | 0.000616  |\n",
      "|    value_loss           | 0.0404    |\n",
      "---------------------------------------\n",
      "Ep done - 33680.\n",
      "Ep done - 33690.\n",
      "Ep done - 33700.\n",
      "Ep done - 33710.\n",
      "Ep done - 33720.\n",
      "Ep done - 33730.\n",
      "Ep done - 33740.\n",
      "Ep done - 33750.\n",
      "Ep done - 33760.\n",
      "Ep done - 33770.\n",
      "Ep done - 33780.\n",
      "Ep done - 33790.\n",
      "Ep done - 7810.\n",
      "Ep done - 7820.\n",
      "Ep done - 7830.\n",
      "Ep done - 7840.\n",
      "Ep done - 7850.\n",
      "Ep done - 7860.\n",
      "Ep done - 7870.\n",
      "Ep done - 7880.\n",
      "Ep done - 7890.\n",
      "Ep done - 7900.\n",
      "Eval num_timesteps=790000, episode_reward=0.93 +/- 0.35\n",
      "Episode length: 30.60 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.6      |\n",
      "|    mean_reward          | 0.93      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 790000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9461134 |\n",
      "|    clip_fraction        | 0.109     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0101   |\n",
      "|    explained_variance   | -0.379    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.145     |\n",
      "|    n_updates            | 20510     |\n",
      "|    policy_gradient_loss | 0.0147    |\n",
      "|    value_loss           | 0.277     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.93\n",
      "SELFPLAY: new best model, bumping up generation to 45\n",
      "Ep done - 33800.\n",
      "Ep done - 33810.\n",
      "Ep done - 33820.\n",
      "Ep done - 33830.\n",
      "Ep done - 33840.\n",
      "Ep done - 33850.\n",
      "Ep done - 33860.\n",
      "Ep done - 33870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.1     |\n",
      "|    ep_rew_mean     | 0.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 2270     |\n",
      "|    total_timesteps | 792576   |\n",
      "---------------------------------\n",
      "Ep done - 33880.\n",
      "Ep done - 33890.\n",
      "Ep done - 33900.\n",
      "Ep done - 33910.\n",
      "Ep done - 33920.\n",
      "Ep done - 33930.\n",
      "Ep done - 33940.\n",
      "Ep done - 33950.\n",
      "Ep done - 33960.\n",
      "Ep done - 33970.\n",
      "Ep done - 33980.\n",
      "Ep done - 33990.\n",
      "Ep done - 34000.\n",
      "Ep done - 34010.\n",
      "Ep done - 34020.\n",
      "Ep done - 34030.\n",
      "Ep done - 34040.\n",
      "Ep done - 34050.\n",
      "Ep done - 34060.\n",
      "Ep done - 34070.\n",
      "Ep done - 34080.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.6      |\n",
      "|    ep_rew_mean          | -0.79     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 349       |\n",
      "|    iterations           | 130       |\n",
      "|    time_elapsed         | 2286      |\n",
      "|    total_timesteps      | 798720    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.9725132 |\n",
      "|    clip_fraction        | 0.192     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0398   |\n",
      "|    explained_variance   | -0.401    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0302    |\n",
      "|    n_updates            | 20520     |\n",
      "|    policy_gradient_loss | -0.0114   |\n",
      "|    value_loss           | 0.108     |\n",
      "---------------------------------------\n",
      "Ep done - 34090.\n",
      "Ep done - 34100.\n",
      "Ep done - 34110.\n",
      "Ep done - 34120.\n",
      "Ep done - 7910.\n",
      "Ep done - 7920.\n",
      "Ep done - 7930.\n",
      "Ep done - 7940.\n",
      "Ep done - 7950.\n",
      "Ep done - 7960.\n",
      "Ep done - 7970.\n",
      "Ep done - 7980.\n",
      "Ep done - 7990.\n",
      "Ep done - 8000.\n",
      "Eval num_timesteps=800000, episode_reward=0.72 +/- 0.69\n",
      "Episode length: 29.87 +/- 0.34\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.9      |\n",
      "|    mean_reward          | 0.72      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 800000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7035185 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0104   |\n",
      "|    explained_variance   | -0.243    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00914  |\n",
      "|    n_updates            | 20530     |\n",
      "|    policy_gradient_loss | 0.00376   |\n",
      "|    value_loss           | 0.0544    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.72\n",
      "SELFPLAY: new best model, bumping up generation to 46\n",
      "Ep done - 34130.\n",
      "Ep done - 34140.\n",
      "Ep done - 34150.\n",
      "Ep done - 34160.\n",
      "Ep done - 34170.\n",
      "Ep done - 34180.\n",
      "Ep done - 34190.\n",
      "Ep done - 34200.\n",
      "Ep done - 34210.\n",
      "Ep done - 34220.\n",
      "Ep done - 34230.\n",
      "Ep done - 34240.\n",
      "Ep done - 34250.\n",
      "Ep done - 34260.\n",
      "Ep done - 34270.\n",
      "Ep done - 34280.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.6     |\n",
      "|    ep_rew_mean     | 0.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 2306     |\n",
      "|    total_timesteps | 804864   |\n",
      "---------------------------------\n",
      "Ep done - 34290.\n",
      "Ep done - 34300.\n",
      "Ep done - 34310.\n",
      "Ep done - 34320.\n",
      "Ep done - 34330.\n",
      "Ep done - 34340.\n",
      "Ep done - 34350.\n",
      "Ep done - 34360.\n",
      "Ep done - 34370.\n",
      "Ep done - 34380.\n",
      "Ep done - 34390.\n",
      "Ep done - 34400.\n",
      "Ep done - 34410.\n",
      "Ep done - 34420.\n",
      "Ep done - 34430.\n",
      "Ep done - 34440.\n",
      "Ep done - 34450.\n",
      "Ep done - 8010.\n",
      "Ep done - 8020.\n",
      "Ep done - 8030.\n",
      "Ep done - 8040.\n",
      "Ep done - 8050.\n",
      "Ep done - 8060.\n",
      "Ep done - 8070.\n",
      "Ep done - 8080.\n",
      "Ep done - 8090.\n",
      "Ep done - 8100.\n",
      "Eval num_timesteps=810000, episode_reward=0.38 +/- 0.92\n",
      "Episode length: 30.66 +/- 0.47\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.7      |\n",
      "|    mean_reward          | 0.38      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 810000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3775505 |\n",
      "|    clip_fraction        | 0.0912    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00759  |\n",
      "|    explained_variance   | 0.196     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0043   |\n",
      "|    n_updates            | 20540     |\n",
      "|    policy_gradient_loss | -0.0101   |\n",
      "|    value_loss           | 0.0736    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.38\n",
      "SELFPLAY: new best model, bumping up generation to 47\n",
      "Ep done - 34460.\n",
      "Ep done - 34470.\n",
      "Ep done - 34480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.7     |\n",
      "|    ep_rew_mean     | 0.54     |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 2327     |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Ep done - 34490.\n",
      "Ep done - 34500.\n",
      "Ep done - 34510.\n",
      "Ep done - 34520.\n",
      "Ep done - 34530.\n",
      "Ep done - 34540.\n",
      "Ep done - 34550.\n",
      "Ep done - 34560.\n",
      "Ep done - 34570.\n",
      "Ep done - 34580.\n",
      "Ep done - 34590.\n",
      "Ep done - 34600.\n",
      "Ep done - 34610.\n",
      "Ep done - 34620.\n",
      "Ep done - 34630.\n",
      "Ep done - 34640.\n",
      "Ep done - 34650.\n",
      "Ep done - 34660.\n",
      "Ep done - 34670.\n",
      "Ep done - 34680.\n",
      "Ep done - 34690.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.9       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 348       |\n",
      "|    iterations           | 133       |\n",
      "|    time_elapsed         | 2344      |\n",
      "|    total_timesteps      | 817152    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3917323 |\n",
      "|    clip_fraction        | 0.118     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0126   |\n",
      "|    explained_variance   | -0.941    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0686    |\n",
      "|    n_updates            | 20550     |\n",
      "|    policy_gradient_loss | -0.00375  |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Ep done - 34700.\n",
      "Ep done - 34710.\n",
      "Ep done - 34720.\n",
      "Ep done - 34730.\n",
      "Ep done - 34740.\n",
      "Ep done - 34750.\n",
      "Ep done - 34760.\n",
      "Ep done - 34770.\n",
      "Ep done - 34780.\n",
      "Ep done - 8110.\n",
      "Ep done - 8120.\n",
      "Ep done - 8130.\n",
      "Ep done - 8140.\n",
      "Ep done - 8150.\n",
      "Ep done - 8160.\n",
      "Ep done - 8170.\n",
      "Ep done - 8180.\n",
      "Ep done - 8190.\n",
      "Ep done - 8200.\n",
      "Eval num_timesteps=820000, episode_reward=0.95 +/- 0.30\n",
      "Episode length: 30.02 +/- 0.14\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.95      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 820000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.2716975 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0116   |\n",
      "|    explained_variance   | 0.038     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0169    |\n",
      "|    n_updates            | 20560     |\n",
      "|    policy_gradient_loss | -0.00389  |\n",
      "|    value_loss           | 0.0418    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.95\n",
      "SELFPLAY: new best model, bumping up generation to 48\n",
      "Ep done - 34790.\n",
      "Ep done - 34800.\n",
      "Ep done - 34810.\n",
      "Ep done - 34820.\n",
      "Ep done - 34830.\n",
      "Ep done - 34840.\n",
      "Ep done - 34850.\n",
      "Ep done - 34860.\n",
      "Ep done - 34870.\n",
      "Ep done - 34880.\n",
      "Ep done - 34890.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | -0.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 2364     |\n",
      "|    total_timesteps | 823296   |\n",
      "---------------------------------\n",
      "Ep done - 34900.\n",
      "Ep done - 34910.\n",
      "Ep done - 34920.\n",
      "Ep done - 34930.\n",
      "Ep done - 34940.\n",
      "Ep done - 34950.\n",
      "Ep done - 34960.\n",
      "Ep done - 34970.\n",
      "Ep done - 34980.\n",
      "Ep done - 34990.\n",
      "Ep done - 35000.\n",
      "Ep done - 35010.\n",
      "Ep done - 35020.\n",
      "Ep done - 35030.\n",
      "Ep done - 35040.\n",
      "Ep done - 35050.\n",
      "Ep done - 35060.\n",
      "Ep done - 35070.\n",
      "Ep done - 35080.\n",
      "Ep done - 35090.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.5      |\n",
      "|    ep_rew_mean          | -0.56     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 348       |\n",
      "|    iterations           | 135       |\n",
      "|    time_elapsed         | 2381      |\n",
      "|    total_timesteps      | 829440    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1134615 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0143   |\n",
      "|    explained_variance   | 0.382     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0577    |\n",
      "|    n_updates            | 20570     |\n",
      "|    policy_gradient_loss | 0.00412   |\n",
      "|    value_loss           | 0.115     |\n",
      "---------------------------------------\n",
      "Ep done - 35100.\n",
      "Ep done - 35110.\n",
      "Ep done - 8210.\n",
      "Ep done - 8220.\n",
      "Ep done - 8230.\n",
      "Ep done - 8240.\n",
      "Ep done - 8250.\n",
      "Ep done - 8260.\n",
      "Ep done - 8270.\n",
      "Ep done - 8280.\n",
      "Ep done - 8290.\n",
      "Ep done - 8300.\n",
      "Eval num_timesteps=830000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 29.56 +/- 0.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.6       |\n",
      "|    mean_reward          | -0.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 830000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56797594 |\n",
      "|    clip_fraction        | 0.0821     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0158    |\n",
      "|    explained_variance   | 0.222      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.0127    |\n",
      "|    n_updates            | 20580      |\n",
      "|    policy_gradient_loss | 0.0151     |\n",
      "|    value_loss           | 0.0536     |\n",
      "----------------------------------------\n",
      "Ep done - 35120.\n",
      "Ep done - 35130.\n",
      "Ep done - 35140.\n",
      "Ep done - 35150.\n",
      "Ep done - 35160.\n",
      "Ep done - 35170.\n",
      "Ep done - 35180.\n",
      "Ep done - 35190.\n",
      "Ep done - 35200.\n",
      "Ep done - 35210.\n",
      "Ep done - 35220.\n",
      "Ep done - 35230.\n",
      "Ep done - 35240.\n",
      "Ep done - 35250.\n",
      "Ep done - 35260.\n",
      "Ep done - 35270.\n",
      "Ep done - 35280.\n",
      "Ep done - 35290.\n",
      "Ep done - 35300.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | -0.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 2401     |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Ep done - 35310.\n",
      "Ep done - 35320.\n",
      "Ep done - 35330.\n",
      "Ep done - 35340.\n",
      "Ep done - 35350.\n",
      "Ep done - 35360.\n",
      "Ep done - 35370.\n",
      "Ep done - 35380.\n",
      "Ep done - 35390.\n",
      "Ep done - 35400.\n",
      "Ep done - 35410.\n",
      "Ep done - 35420.\n",
      "Ep done - 35430.\n",
      "Ep done - 35440.\n",
      "Ep done - 35450.\n",
      "Ep done - 8310.\n",
      "Ep done - 8320.\n",
      "Ep done - 8330.\n",
      "Ep done - 8340.\n",
      "Ep done - 8350.\n",
      "Ep done - 8360.\n",
      "Ep done - 8370.\n",
      "Ep done - 8380.\n",
      "Ep done - 8390.\n",
      "Ep done - 8400.\n",
      "Eval num_timesteps=840000, episode_reward=0.78 +/- 0.63\n",
      "Episode length: 30.04 +/- 0.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.78      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 840000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2847524 |\n",
      "|    clip_fraction        | 0.0657    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0157   |\n",
      "|    explained_variance   | 0.612     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0355    |\n",
      "|    n_updates            | 20590     |\n",
      "|    policy_gradient_loss | 0.00678   |\n",
      "|    value_loss           | 0.0789    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.78\n",
      "SELFPLAY: new best model, bumping up generation to 49\n",
      "Ep done - 35460.\n",
      "Ep done - 35470.\n",
      "Ep done - 35480.\n",
      "Ep done - 35490.\n",
      "Ep done - 35500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 2423     |\n",
      "|    total_timesteps | 841728   |\n",
      "---------------------------------\n",
      "Ep done - 35510.\n",
      "Ep done - 35520.\n",
      "Ep done - 35530.\n",
      "Ep done - 35540.\n",
      "Ep done - 35550.\n",
      "Ep done - 35560.\n",
      "Ep done - 35570.\n",
      "Ep done - 35580.\n",
      "Ep done - 35590.\n",
      "Ep done - 35600.\n",
      "Ep done - 35610.\n",
      "Ep done - 35620.\n",
      "Ep done - 35630.\n",
      "Ep done - 35640.\n",
      "Ep done - 35650.\n",
      "Ep done - 35660.\n",
      "Ep done - 35670.\n",
      "Ep done - 35680.\n",
      "Ep done - 35690.\n",
      "Ep done - 35700.\n",
      "Ep done - 35710.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.79      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 347       |\n",
      "|    iterations           | 138       |\n",
      "|    time_elapsed         | 2439      |\n",
      "|    total_timesteps      | 847872    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1664836 |\n",
      "|    clip_fraction        | 0.12      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0179   |\n",
      "|    explained_variance   | -0.198    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.013    |\n",
      "|    n_updates            | 20600     |\n",
      "|    policy_gradient_loss | 0.0163    |\n",
      "|    value_loss           | 0.108     |\n",
      "---------------------------------------\n",
      "Ep done - 35720.\n",
      "Ep done - 35730.\n",
      "Ep done - 35740.\n",
      "Ep done - 35750.\n",
      "Ep done - 35760.\n",
      "Ep done - 35770.\n",
      "Ep done - 35780.\n",
      "Ep done - 8410.\n",
      "Ep done - 8420.\n",
      "Ep done - 8430.\n",
      "Ep done - 8440.\n",
      "Ep done - 8450.\n",
      "Ep done - 8460.\n",
      "Ep done - 8470.\n",
      "Ep done - 8480.\n",
      "Ep done - 8490.\n",
      "Ep done - 8500.\n",
      "Eval num_timesteps=850000, episode_reward=-0.88 +/- 0.43\n",
      "Episode length: 30.08 +/- 0.27\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.88     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 850000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.7396634 |\n",
      "|    clip_fraction        | 0.254     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0219   |\n",
      "|    explained_variance   | -2.36     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0178   |\n",
      "|    n_updates            | 20610     |\n",
      "|    policy_gradient_loss | -0.0247   |\n",
      "|    value_loss           | 0.0796    |\n",
      "---------------------------------------\n",
      "Ep done - 35790.\n",
      "Ep done - 35800.\n",
      "Ep done - 35810.\n",
      "Ep done - 35820.\n",
      "Ep done - 35830.\n",
      "Ep done - 35840.\n",
      "Ep done - 35850.\n",
      "Ep done - 35860.\n",
      "Ep done - 35870.\n",
      "Ep done - 35880.\n",
      "Ep done - 35890.\n",
      "Ep done - 35900.\n",
      "Ep done - 35910.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.91    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 2459     |\n",
      "|    total_timesteps | 854016   |\n",
      "---------------------------------\n",
      "Ep done - 35920.\n",
      "Ep done - 35930.\n",
      "Ep done - 35940.\n",
      "Ep done - 35950.\n",
      "Ep done - 35960.\n",
      "Ep done - 35970.\n",
      "Ep done - 35980.\n",
      "Ep done - 35990.\n",
      "Ep done - 36000.\n",
      "Ep done - 36010.\n",
      "Ep done - 36020.\n",
      "Ep done - 36030.\n",
      "Ep done - 36040.\n",
      "Ep done - 36050.\n",
      "Ep done - 36060.\n",
      "Ep done - 36070.\n",
      "Ep done - 36080.\n",
      "Ep done - 36090.\n",
      "Ep done - 36100.\n",
      "Ep done - 36110.\n",
      "Ep done - 8510.\n",
      "Ep done - 8520.\n",
      "Ep done - 8530.\n",
      "Ep done - 8540.\n",
      "Ep done - 8550.\n",
      "Ep done - 8560.\n",
      "Ep done - 8570.\n",
      "Ep done - 8580.\n",
      "Ep done - 8590.\n",
      "Ep done - 8600.\n",
      "Eval num_timesteps=860000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 860000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9637856 |\n",
      "|    clip_fraction        | 0.0816    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00506  |\n",
      "|    explained_variance   | -1.72     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00422   |\n",
      "|    n_updates            | 20620     |\n",
      "|    policy_gradient_loss | 0.000349  |\n",
      "|    value_loss           | 0.0349    |\n",
      "---------------------------------------\n",
      "Ep done - 36120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 2479     |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "Ep done - 36130.\n",
      "Ep done - 36140.\n",
      "Ep done - 36150.\n",
      "Ep done - 36160.\n",
      "Ep done - 36170.\n",
      "Ep done - 36180.\n",
      "Ep done - 36190.\n",
      "Ep done - 36200.\n",
      "Ep done - 36210.\n",
      "Ep done - 36220.\n",
      "Ep done - 36230.\n",
      "Ep done - 36240.\n",
      "Ep done - 36250.\n",
      "Ep done - 36260.\n",
      "Ep done - 36270.\n",
      "Ep done - 36280.\n",
      "Ep done - 36290.\n",
      "Ep done - 36300.\n",
      "Ep done - 36310.\n",
      "Ep done - 36320.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | -1       |\n",
      "| time/                   |          |\n",
      "|    fps                  | 347      |\n",
      "|    iterations           | 141      |\n",
      "|    time_elapsed         | 2495     |\n",
      "|    total_timesteps      | 866304   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 4.703146 |\n",
      "|    clip_fraction        | 0.138    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00472 |\n",
      "|    explained_variance   | 0.782    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | -0.0266  |\n",
      "|    n_updates            | 20630    |\n",
      "|    policy_gradient_loss | -0.0055  |\n",
      "|    value_loss           | 0.00255  |\n",
      "--------------------------------------\n",
      "Ep done - 36330.\n",
      "Ep done - 36340.\n",
      "Ep done - 36350.\n",
      "Ep done - 36360.\n",
      "Ep done - 36370.\n",
      "Ep done - 36380.\n",
      "Ep done - 36390.\n",
      "Ep done - 36400.\n",
      "Ep done - 36410.\n",
      "Ep done - 36420.\n",
      "Ep done - 36430.\n",
      "Ep done - 36440.\n",
      "Ep done - 8610.\n",
      "Ep done - 8620.\n",
      "Ep done - 8630.\n",
      "Ep done - 8640.\n",
      "Ep done - 8650.\n",
      "Ep done - 8660.\n",
      "Ep done - 8670.\n",
      "Ep done - 8680.\n",
      "Ep done - 8690.\n",
      "Ep done - 8700.\n",
      "Eval num_timesteps=870000, episode_reward=-0.93 +/- 0.35\n",
      "Episode length: 30.26 +/- 0.46\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | -0.93     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 870000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.2927759 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00601  |\n",
      "|    explained_variance   | 0.0557    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00786  |\n",
      "|    n_updates            | 20640     |\n",
      "|    policy_gradient_loss | 0.0136    |\n",
      "|    value_loss           | 0.00547   |\n",
      "---------------------------------------\n",
      "Ep done - 36450.\n",
      "Ep done - 36460.\n",
      "Ep done - 36470.\n",
      "Ep done - 36480.\n",
      "Ep done - 36490.\n",
      "Ep done - 36500.\n",
      "Ep done - 36510.\n",
      "Ep done - 36520.\n",
      "Ep done - 36530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 2515     |\n",
      "|    total_timesteps | 872448   |\n",
      "---------------------------------\n",
      "Ep done - 36540.\n",
      "Ep done - 36550.\n",
      "Ep done - 36560.\n",
      "Ep done - 36570.\n",
      "Ep done - 36580.\n",
      "Ep done - 36590.\n",
      "Ep done - 36600.\n",
      "Ep done - 36610.\n",
      "Ep done - 36620.\n",
      "Ep done - 36630.\n",
      "Ep done - 36640.\n",
      "Ep done - 36650.\n",
      "Ep done - 36660.\n",
      "Ep done - 36670.\n",
      "Ep done - 36680.\n",
      "Ep done - 36690.\n",
      "Ep done - 36700.\n",
      "Ep done - 36710.\n",
      "Ep done - 36720.\n",
      "Ep done - 36730.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 347       |\n",
      "|    iterations           | 143       |\n",
      "|    time_elapsed         | 2531      |\n",
      "|    total_timesteps      | 878592    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7260077 |\n",
      "|    clip_fraction        | 0.0752    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00762  |\n",
      "|    explained_variance   | -2.35     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.019     |\n",
      "|    n_updates            | 20650     |\n",
      "|    policy_gradient_loss | -0.00587  |\n",
      "|    value_loss           | 0.00912   |\n",
      "---------------------------------------\n",
      "Ep done - 36740.\n",
      "Ep done - 36750.\n",
      "Ep done - 36760.\n",
      "Ep done - 36770.\n",
      "Ep done - 36780.\n",
      "Ep done - 8710.\n",
      "Ep done - 8720.\n",
      "Ep done - 8730.\n",
      "Ep done - 8740.\n",
      "Ep done - 8750.\n",
      "Ep done - 8760.\n",
      "Ep done - 8770.\n",
      "Ep done - 8780.\n",
      "Ep done - 8790.\n",
      "Ep done - 8800.\n",
      "Eval num_timesteps=880000, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.96 +/- 0.24\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 31        |\n",
      "|    mean_reward          | 0.94      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 880000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5754799 |\n",
      "|    clip_fraction        | 0.105     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00271  |\n",
      "|    explained_variance   | -1.4      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00338   |\n",
      "|    n_updates            | 20660     |\n",
      "|    policy_gradient_loss | -0.00558  |\n",
      "|    value_loss           | 0.0598    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.94\n",
      "SELFPLAY: new best model, bumping up generation to 50\n",
      "Ep done - 36790.\n",
      "Ep done - 36800.\n",
      "Ep done - 36810.\n",
      "Ep done - 36820.\n",
      "Ep done - 36830.\n",
      "Ep done - 36840.\n",
      "Ep done - 36850.\n",
      "Ep done - 36860.\n",
      "Ep done - 36870.\n",
      "Ep done - 36880.\n",
      "Ep done - 36890.\n",
      "Ep done - 36900.\n",
      "Ep done - 36910.\n",
      "Ep done - 36920.\n",
      "Ep done - 36930.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 2551     |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Ep done - 36940.\n",
      "Ep done - 36950.\n",
      "Ep done - 36960.\n",
      "Ep done - 36970.\n",
      "Ep done - 36980.\n",
      "Ep done - 36990.\n",
      "Ep done - 37000.\n",
      "Ep done - 37010.\n",
      "Ep done - 37020.\n",
      "Ep done - 37030.\n",
      "Ep done - 37040.\n",
      "Ep done - 37050.\n",
      "Ep done - 37060.\n",
      "Ep done - 37070.\n",
      "Ep done - 37080.\n",
      "Ep done - 37090.\n",
      "Ep done - 37100.\n",
      "Ep done - 37110.\n",
      "Ep done - 8810.\n",
      "Ep done - 8820.\n",
      "Ep done - 8830.\n",
      "Ep done - 8840.\n",
      "Ep done - 8850.\n",
      "Ep done - 8860.\n",
      "Ep done - 8870.\n",
      "Ep done - 8880.\n",
      "Ep done - 8890.\n",
      "Ep done - 8900.\n",
      "Eval num_timesteps=890000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | 1        |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 890000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.765545 |\n",
      "|    clip_fraction        | 0.11     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00615 |\n",
      "|    explained_variance   | 0.224    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0463   |\n",
      "|    n_updates            | 20670    |\n",
      "|    policy_gradient_loss | 0.0164   |\n",
      "|    value_loss           | 0.157    |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 51\n",
      "Ep done - 37120.\n",
      "Ep done - 37130.\n",
      "Ep done - 37140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 2572     |\n",
      "|    total_timesteps | 890880   |\n",
      "---------------------------------\n",
      "Ep done - 37150.\n",
      "Ep done - 37160.\n",
      "Ep done - 37170.\n",
      "Ep done - 37180.\n",
      "Ep done - 37190.\n",
      "Ep done - 37200.\n",
      "Ep done - 37210.\n",
      "Ep done - 37220.\n",
      "Ep done - 37230.\n",
      "Ep done - 37240.\n",
      "Ep done - 37250.\n",
      "Ep done - 37260.\n",
      "Ep done - 37270.\n",
      "Ep done - 37280.\n",
      "Ep done - 37290.\n",
      "Ep done - 37300.\n",
      "Ep done - 37310.\n",
      "Ep done - 37320.\n",
      "Ep done - 37330.\n",
      "Ep done - 37340.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.9      |\n",
      "|    ep_rew_mean          | 0.91      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 346       |\n",
      "|    iterations           | 146       |\n",
      "|    time_elapsed         | 2587      |\n",
      "|    total_timesteps      | 897024    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3733796 |\n",
      "|    clip_fraction        | 0.0305    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00493  |\n",
      "|    explained_variance   | -0.19     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0301    |\n",
      "|    n_updates            | 20680     |\n",
      "|    policy_gradient_loss | -0.00745  |\n",
      "|    value_loss           | 0.0528    |\n",
      "---------------------------------------\n",
      "Ep done - 37350.\n",
      "Ep done - 37360.\n",
      "Ep done - 37370.\n",
      "Ep done - 37380.\n",
      "Ep done - 37390.\n",
      "Ep done - 37400.\n",
      "Ep done - 37410.\n",
      "Ep done - 37420.\n",
      "Ep done - 37430.\n",
      "Ep done - 37440.\n",
      "Ep done - 8910.\n",
      "Ep done - 8920.\n",
      "Ep done - 8930.\n",
      "Ep done - 8940.\n",
      "Ep done - 8950.\n",
      "Ep done - 8960.\n",
      "Ep done - 8970.\n",
      "Ep done - 8980.\n",
      "Ep done - 8990.\n",
      "Ep done - 9000.\n",
      "Eval num_timesteps=900000, episode_reward=0.43 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.43       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 900000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41219434 |\n",
      "|    clip_fraction        | 0.042      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00192   |\n",
      "|    explained_variance   | 0.209      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0127     |\n",
      "|    n_updates            | 20690      |\n",
      "|    policy_gradient_loss | -0.00212   |\n",
      "|    value_loss           | 0.05       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.43\n",
      "SELFPLAY: new best model, bumping up generation to 52\n",
      "Ep done - 37450.\n",
      "Ep done - 37460.\n",
      "Ep done - 37470.\n",
      "Ep done - 37480.\n",
      "Ep done - 37490.\n",
      "Ep done - 37500.\n",
      "Ep done - 37510.\n",
      "Ep done - 37520.\n",
      "Ep done - 37530.\n",
      "Ep done - 37540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 2608     |\n",
      "|    total_timesteps | 903168   |\n",
      "---------------------------------\n",
      "Ep done - 37550.\n",
      "Ep done - 37560.\n",
      "Ep done - 37570.\n",
      "Ep done - 37580.\n",
      "Ep done - 37590.\n",
      "Ep done - 37600.\n",
      "Ep done - 37610.\n",
      "Ep done - 37620.\n",
      "Ep done - 37630.\n",
      "Ep done - 37640.\n",
      "Ep done - 37650.\n",
      "Ep done - 37660.\n",
      "Ep done - 37670.\n",
      "Ep done - 37680.\n",
      "Ep done - 37690.\n",
      "Ep done - 37700.\n",
      "Ep done - 37710.\n",
      "Ep done - 37720.\n",
      "Ep done - 37730.\n",
      "Ep done - 37740.\n",
      "Ep done - 37750.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.96      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 346       |\n",
      "|    iterations           | 148       |\n",
      "|    time_elapsed         | 2624      |\n",
      "|    total_timesteps      | 909312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7011733 |\n",
      "|    clip_fraction        | 0.033     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00353  |\n",
      "|    explained_variance   | -0.0124   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0142    |\n",
      "|    n_updates            | 20700     |\n",
      "|    policy_gradient_loss | -0.0094   |\n",
      "|    value_loss           | 0.0901    |\n",
      "---------------------------------------\n",
      "Ep done - 37760.\n",
      "Ep done - 37770.\n",
      "Ep done - 9010.\n",
      "Ep done - 9020.\n",
      "Ep done - 9030.\n",
      "Ep done - 9040.\n",
      "Ep done - 9050.\n",
      "Ep done - 9060.\n",
      "Ep done - 9070.\n",
      "Ep done - 9080.\n",
      "Ep done - 9090.\n",
      "Ep done - 9100.\n",
      "Eval num_timesteps=910000, episode_reward=-0.76 +/- 0.65\n",
      "Episode length: 29.12 +/- 0.32\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.1      |\n",
      "|    mean_reward          | -0.76     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 910000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3103601 |\n",
      "|    clip_fraction        | 0.0461    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00319  |\n",
      "|    explained_variance   | -0.00536  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00693   |\n",
      "|    n_updates            | 20710     |\n",
      "|    policy_gradient_loss | -0.00638  |\n",
      "|    value_loss           | 0.0354    |\n",
      "---------------------------------------\n",
      "Ep done - 37780.\n",
      "Ep done - 37790.\n",
      "Ep done - 37800.\n",
      "Ep done - 37810.\n",
      "Ep done - 37820.\n",
      "Ep done - 37830.\n",
      "Ep done - 37840.\n",
      "Ep done - 37850.\n",
      "Ep done - 37860.\n",
      "Ep done - 37870.\n",
      "Ep done - 37880.\n",
      "Ep done - 37890.\n",
      "Ep done - 37900.\n",
      "Ep done - 37910.\n",
      "Ep done - 37920.\n",
      "Ep done - 37930.\n",
      "Ep done - 37940.\n",
      "Ep done - 37950.\n",
      "Ep done - 37960.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.1     |\n",
      "|    ep_rew_mean     | -0.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 2644     |\n",
      "|    total_timesteps | 915456   |\n",
      "---------------------------------\n",
      "Ep done - 37970.\n",
      "Ep done - 37980.\n",
      "Ep done - 37990.\n",
      "Ep done - 38000.\n",
      "Ep done - 38010.\n",
      "Ep done - 38020.\n",
      "Ep done - 38030.\n",
      "Ep done - 38040.\n",
      "Ep done - 38050.\n",
      "Ep done - 38060.\n",
      "Ep done - 38070.\n",
      "Ep done - 38080.\n",
      "Ep done - 38090.\n",
      "Ep done - 38100.\n",
      "Ep done - 38110.\n",
      "Ep done - 9110.\n",
      "Ep done - 9120.\n",
      "Ep done - 9130.\n",
      "Ep done - 9140.\n",
      "Ep done - 9150.\n",
      "Ep done - 9160.\n",
      "Ep done - 9170.\n",
      "Ep done - 9180.\n",
      "Ep done - 9190.\n",
      "Ep done - 9200.\n",
      "Eval num_timesteps=920000, episode_reward=-0.86 +/- 0.51\n",
      "Episode length: 30.01 +/- 0.22\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.86    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 920000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 3.35944  |\n",
      "|    clip_fraction        | 0.135    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00136 |\n",
      "|    explained_variance   | -0.44    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | -0.00491 |\n",
      "|    n_updates            | 20720    |\n",
      "|    policy_gradient_loss | -0.0223  |\n",
      "|    value_loss           | 0.0697   |\n",
      "--------------------------------------\n",
      "Ep done - 38120.\n",
      "Ep done - 38130.\n",
      "Ep done - 38140.\n",
      "Ep done - 38150.\n",
      "Ep done - 38160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.88    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 2664     |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "Ep done - 38170.\n",
      "Ep done - 38180.\n",
      "Ep done - 38190.\n",
      "Ep done - 38200.\n",
      "Ep done - 38210.\n",
      "Ep done - 38220.\n",
      "Ep done - 38230.\n",
      "Ep done - 38240.\n",
      "Ep done - 38250.\n",
      "Ep done - 38260.\n",
      "Ep done - 38270.\n",
      "Ep done - 38280.\n",
      "Ep done - 38290.\n",
      "Ep done - 38300.\n",
      "Ep done - 38310.\n",
      "Ep done - 38320.\n",
      "Ep done - 38330.\n",
      "Ep done - 38340.\n",
      "Ep done - 38350.\n",
      "Ep done - 38360.\n",
      "Ep done - 38370.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -0.86       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 151         |\n",
      "|    time_elapsed         | 2680        |\n",
      "|    total_timesteps      | 927744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002536293 |\n",
      "|    clip_fraction        | 0.000456    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000332   |\n",
      "|    explained_variance   | -0.267      |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0334      |\n",
      "|    n_updates            | 20730       |\n",
      "|    policy_gradient_loss | 0.000244    |\n",
      "|    value_loss           | 0.0642      |\n",
      "-----------------------------------------\n",
      "Ep done - 38380.\n",
      "Ep done - 38390.\n",
      "Ep done - 38400.\n",
      "Ep done - 38410.\n",
      "Ep done - 38420.\n",
      "Ep done - 38430.\n",
      "Ep done - 38440.\n",
      "Ep done - 9210.\n",
      "Ep done - 9220.\n",
      "Ep done - 9230.\n",
      "Ep done - 9240.\n",
      "Ep done - 9250.\n",
      "Ep done - 9260.\n",
      "Ep done - 9270.\n",
      "Ep done - 9280.\n",
      "Ep done - 9290.\n",
      "Ep done - 9300.\n",
      "Eval num_timesteps=930000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 930000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9617771 |\n",
      "|    clip_fraction        | 0.116     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00231  |\n",
      "|    explained_variance   | 0.649     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0165   |\n",
      "|    n_updates            | 20740     |\n",
      "|    policy_gradient_loss | -0.00263  |\n",
      "|    value_loss           | 0.0558    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 53\n",
      "Ep done - 38450.\n",
      "Ep done - 38460.\n",
      "Ep done - 38470.\n",
      "Ep done - 38480.\n",
      "Ep done - 38490.\n",
      "Ep done - 38500.\n",
      "Ep done - 38510.\n",
      "Ep done - 38520.\n",
      "Ep done - 38530.\n",
      "Ep done - 38540.\n",
      "Ep done - 38550.\n",
      "Ep done - 38560.\n",
      "Ep done - 38570.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.9     |\n",
      "|    ep_rew_mean     | 0.96     |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 2700     |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Ep done - 38580.\n",
      "Ep done - 38590.\n",
      "Ep done - 38600.\n",
      "Ep done - 38610.\n",
      "Ep done - 38620.\n",
      "Ep done - 38630.\n",
      "Ep done - 38640.\n",
      "Ep done - 38650.\n",
      "Ep done - 38660.\n",
      "Ep done - 38670.\n",
      "Ep done - 38680.\n",
      "Ep done - 38690.\n",
      "Ep done - 38700.\n",
      "Ep done - 38710.\n",
      "Ep done - 38720.\n",
      "Ep done - 38730.\n",
      "Ep done - 38740.\n",
      "Ep done - 38750.\n",
      "Ep done - 38760.\n",
      "Ep done - 38770.\n",
      "Ep done - 9310.\n",
      "Ep done - 9320.\n",
      "Ep done - 9330.\n",
      "Ep done - 9340.\n",
      "Ep done - 9350.\n",
      "Ep done - 9360.\n",
      "Ep done - 9370.\n",
      "Ep done - 9380.\n",
      "Ep done - 9390.\n",
      "Ep done - 9400.\n",
      "Eval num_timesteps=940000, episode_reward=0.87 +/- 0.36\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.87      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 940000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 13.284694 |\n",
      "|    clip_fraction        | 0.259     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00341  |\n",
      "|    explained_variance   | -2.81     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0529   |\n",
      "|    n_updates            | 20750     |\n",
      "|    policy_gradient_loss | -0.0368   |\n",
      "|    value_loss           | 0.0368    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.87\n",
      "SELFPLAY: new best model, bumping up generation to 54\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 2721     |\n",
      "|    total_timesteps | 940032   |\n",
      "---------------------------------\n",
      "Ep done - 38780.\n",
      "Ep done - 38790.\n",
      "Ep done - 38800.\n",
      "Ep done - 38810.\n",
      "Ep done - 38820.\n",
      "Ep done - 38830.\n",
      "Ep done - 38840.\n",
      "Ep done - 38850.\n",
      "Ep done - 38860.\n",
      "Ep done - 38870.\n",
      "Ep done - 38880.\n",
      "Ep done - 38890.\n",
      "Ep done - 38900.\n",
      "Ep done - 38910.\n",
      "Ep done - 38920.\n",
      "Ep done - 38930.\n",
      "Ep done - 38940.\n",
      "Ep done - 38950.\n",
      "Ep done - 38960.\n",
      "Ep done - 38970.\n",
      "Ep done - 38980.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.06     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 345       |\n",
      "|    iterations           | 154       |\n",
      "|    time_elapsed         | 2737      |\n",
      "|    total_timesteps      | 946176    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.9636438 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00338  |\n",
      "|    explained_variance   | -1.03     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.000405 |\n",
      "|    n_updates            | 20760     |\n",
      "|    policy_gradient_loss | -0.0071   |\n",
      "|    value_loss           | 0.0293    |\n",
      "---------------------------------------\n",
      "Ep done - 38990.\n",
      "Ep done - 39000.\n",
      "Ep done - 39010.\n",
      "Ep done - 39020.\n",
      "Ep done - 39030.\n",
      "Ep done - 39040.\n",
      "Ep done - 39050.\n",
      "Ep done - 39060.\n",
      "Ep done - 39070.\n",
      "Ep done - 39080.\n",
      "Ep done - 39090.\n",
      "Ep done - 39100.\n",
      "Ep done - 9410.\n",
      "Ep done - 9420.\n",
      "Ep done - 9430.\n",
      "Ep done - 9440.\n",
      "Ep done - 9450.\n",
      "Ep done - 9460.\n",
      "Ep done - 9470.\n",
      "Ep done - 9480.\n",
      "Ep done - 9490.\n",
      "Ep done - 9500.\n",
      "Eval num_timesteps=950000, episode_reward=-0.08 +/- 1.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 950000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01879933 |\n",
      "|    clip_fraction        | 0.00397    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000929  |\n",
      "|    explained_variance   | -0.165     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0708     |\n",
      "|    n_updates            | 20770      |\n",
      "|    policy_gradient_loss | 0.000236   |\n",
      "|    value_loss           | 0.207      |\n",
      "----------------------------------------\n",
      "Ep done - 39110.\n",
      "Ep done - 39120.\n",
      "Ep done - 39130.\n",
      "Ep done - 39140.\n",
      "Ep done - 39150.\n",
      "Ep done - 39160.\n",
      "Ep done - 39170.\n",
      "Ep done - 39180.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 2757     |\n",
      "|    total_timesteps | 952320   |\n",
      "---------------------------------\n",
      "Ep done - 39190.\n",
      "Ep done - 39200.\n",
      "Ep done - 39210.\n",
      "Ep done - 39220.\n",
      "Ep done - 39230.\n",
      "Ep done - 39240.\n",
      "Ep done - 39250.\n",
      "Ep done - 39260.\n",
      "Ep done - 39270.\n",
      "Ep done - 39280.\n",
      "Ep done - 39290.\n",
      "Ep done - 39300.\n",
      "Ep done - 39310.\n",
      "Ep done - 39320.\n",
      "Ep done - 39330.\n",
      "Ep done - 39340.\n",
      "Ep done - 39350.\n",
      "Ep done - 39360.\n",
      "Ep done - 39370.\n",
      "Ep done - 39380.\n",
      "Ep done - 39390.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.02       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 156        |\n",
      "|    time_elapsed         | 2776       |\n",
      "|    total_timesteps      | 958464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29723716 |\n",
      "|    clip_fraction        | 0.0285     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000965  |\n",
      "|    explained_variance   | 0.491      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0841     |\n",
      "|    n_updates            | 20780      |\n",
      "|    policy_gradient_loss | 0.00552    |\n",
      "|    value_loss           | 0.181      |\n",
      "----------------------------------------\n",
      "Ep done - 39400.\n",
      "Ep done - 39410.\n",
      "Ep done - 39420.\n",
      "Ep done - 39430.\n",
      "Ep done - 39440.\n",
      "Ep done - 9510.\n",
      "Ep done - 9520.\n",
      "Ep done - 9530.\n",
      "Ep done - 9540.\n",
      "Ep done - 9550.\n",
      "Ep done - 9560.\n",
      "Ep done - 9570.\n",
      "Ep done - 9580.\n",
      "Ep done - 9590.\n",
      "Ep done - 9600.\n",
      "Eval num_timesteps=960000, episode_reward=0.04 +/- 1.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 0.04        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026119137 |\n",
      "|    clip_fraction        | 0.00794     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.00118    |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 20790       |\n",
      "|    policy_gradient_loss | 9.24e-05    |\n",
      "|    value_loss           | 0.236       |\n",
      "-----------------------------------------\n",
      "Ep done - 39450.\n",
      "Ep done - 39460.\n",
      "Ep done - 39470.\n",
      "Ep done - 39480.\n",
      "Ep done - 39490.\n",
      "Ep done - 39500.\n",
      "Ep done - 39510.\n",
      "Ep done - 39520.\n",
      "Ep done - 39530.\n",
      "Ep done - 39540.\n",
      "Ep done - 39550.\n",
      "Ep done - 39560.\n",
      "Ep done - 39570.\n",
      "Ep done - 39580.\n",
      "Ep done - 39590.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 343      |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 2805     |\n",
      "|    total_timesteps | 964608   |\n",
      "---------------------------------\n",
      "Ep done - 39600.\n",
      "Ep done - 39610.\n",
      "Ep done - 39620.\n",
      "Ep done - 39630.\n",
      "Ep done - 39640.\n",
      "Ep done - 39650.\n",
      "Ep done - 39660.\n",
      "Ep done - 39670.\n",
      "Ep done - 39680.\n",
      "Ep done - 39690.\n",
      "Ep done - 39700.\n",
      "Ep done - 39710.\n",
      "Ep done - 39720.\n",
      "Ep done - 39730.\n",
      "Ep done - 39740.\n",
      "Ep done - 39750.\n",
      "Ep done - 39760.\n",
      "Ep done - 39770.\n",
      "Ep done - 9610.\n",
      "Ep done - 9620.\n",
      "Ep done - 9630.\n",
      "Ep done - 9640.\n",
      "Ep done - 9650.\n",
      "Ep done - 9660.\n",
      "Ep done - 9670.\n",
      "Ep done - 9680.\n",
      "Ep done - 9690.\n",
      "Ep done - 9700.\n",
      "Eval num_timesteps=970000, episode_reward=0.46 +/- 0.85\n",
      "Episode length: 29.84 +/- 0.37\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.8      |\n",
      "|    mean_reward          | 0.46      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 970000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4585363 |\n",
      "|    clip_fraction        | 0.0278    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00296  |\n",
      "|    explained_variance   | 0.586     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0992    |\n",
      "|    n_updates            | 20800     |\n",
      "|    policy_gradient_loss | -0.000828 |\n",
      "|    value_loss           | 0.221     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.46\n",
      "SELFPLAY: new best model, bumping up generation to 55\n",
      "Ep done - 39780.\n",
      "Ep done - 39790.\n",
      "Ep done - 39800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 0.15     |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 2833     |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "Ep done - 39810.\n",
      "Ep done - 39820.\n",
      "Ep done - 39830.\n",
      "Ep done - 39840.\n",
      "Ep done - 39850.\n",
      "Ep done - 39860.\n",
      "Ep done - 39870.\n",
      "Ep done - 39880.\n",
      "Ep done - 39890.\n",
      "Ep done - 39900.\n",
      "Ep done - 39910.\n",
      "Ep done - 39920.\n",
      "Ep done - 39930.\n",
      "Ep done - 39940.\n",
      "Ep done - 39950.\n",
      "Ep done - 39960.\n",
      "Ep done - 39970.\n",
      "Ep done - 39980.\n",
      "Ep done - 39990.\n",
      "Ep done - 40000.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0         |\n",
      "| time/                   |           |\n",
      "|    fps                  | 342       |\n",
      "|    iterations           | 159       |\n",
      "|    time_elapsed         | 2852      |\n",
      "|    total_timesteps      | 976896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7958907 |\n",
      "|    clip_fraction        | 0.0867    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00378  |\n",
      "|    explained_variance   | -0.882    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0825    |\n",
      "|    n_updates            | 20810     |\n",
      "|    policy_gradient_loss | -0.00544  |\n",
      "|    value_loss           | 0.19      |\n",
      "---------------------------------------\n",
      "Ep done - 40010.\n",
      "Ep done - 40020.\n",
      "Ep done - 40030.\n",
      "Ep done - 40040.\n",
      "Ep done - 40050.\n",
      "Ep done - 40060.\n",
      "Ep done - 40070.\n",
      "Ep done - 40080.\n",
      "Ep done - 40090.\n",
      "Ep done - 40100.\n",
      "Ep done - 9710.\n",
      "Ep done - 9720.\n",
      "Ep done - 9730.\n",
      "Ep done - 9740.\n",
      "Ep done - 9750.\n",
      "Ep done - 9760.\n",
      "Ep done - 9770.\n",
      "Ep done - 9780.\n",
      "Ep done - 9790.\n",
      "Ep done - 9800.\n",
      "Eval num_timesteps=980000, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.94      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 980000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.2851639 |\n",
      "|    clip_fraction        | 0.266     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0137   |\n",
      "|    explained_variance   | 0.0275    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0487    |\n",
      "|    n_updates            | 20820     |\n",
      "|    policy_gradient_loss | 0.035     |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.94\n",
      "SELFPLAY: new best model, bumping up generation to 56\n",
      "Ep done - 40110.\n",
      "Ep done - 40120.\n",
      "Ep done - 40130.\n",
      "Ep done - 40140.\n",
      "Ep done - 40150.\n",
      "Ep done - 40160.\n",
      "Ep done - 40170.\n",
      "Ep done - 40180.\n",
      "Ep done - 40190.\n",
      "Ep done - 40200.\n",
      "Ep done - 40210.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 0.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 2875     |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Ep done - 40220.\n",
      "Ep done - 40230.\n",
      "Ep done - 40240.\n",
      "Ep done - 40250.\n",
      "Ep done - 40260.\n",
      "Ep done - 40270.\n",
      "Ep done - 40280.\n",
      "Ep done - 40290.\n",
      "Ep done - 40300.\n",
      "Ep done - 40310.\n",
      "Ep done - 40320.\n",
      "Ep done - 40330.\n",
      "Ep done - 40340.\n",
      "Ep done - 40350.\n",
      "Ep done - 40360.\n",
      "Ep done - 40370.\n",
      "Ep done - 40380.\n",
      "Ep done - 40390.\n",
      "Ep done - 40400.\n",
      "Ep done - 40410.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | -0.62     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 341       |\n",
      "|    iterations           | 161       |\n",
      "|    time_elapsed         | 2892      |\n",
      "|    total_timesteps      | 989184    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8067468 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0103   |\n",
      "|    explained_variance   | -0.315    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0153    |\n",
      "|    n_updates            | 20830     |\n",
      "|    policy_gradient_loss | 0.00251   |\n",
      "|    value_loss           | 0.0859    |\n",
      "---------------------------------------\n",
      "Ep done - 40420.\n",
      "Ep done - 40430.\n",
      "Ep done - 40440.\n",
      "Ep done - 9810.\n",
      "Ep done - 9820.\n",
      "Ep done - 9830.\n",
      "Ep done - 9840.\n",
      "Ep done - 9850.\n",
      "Ep done - 9860.\n",
      "Ep done - 9870.\n",
      "Ep done - 9880.\n",
      "Ep done - 9890.\n",
      "Ep done - 9900.\n",
      "Eval num_timesteps=990000, episode_reward=0.78 +/- 0.63\n",
      "Episode length: 30.09 +/- 0.29\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.78      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 990000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3699595 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0146   |\n",
      "|    explained_variance   | -0.152    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0172   |\n",
      "|    n_updates            | 20840     |\n",
      "|    policy_gradient_loss | -0.0157   |\n",
      "|    value_loss           | 0.122     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.78\n",
      "SELFPLAY: new best model, bumping up generation to 57\n",
      "Ep done - 40450.\n",
      "Ep done - 40460.\n",
      "Ep done - 40470.\n",
      "Ep done - 40480.\n",
      "Ep done - 40490.\n",
      "Ep done - 40500.\n",
      "Ep done - 40510.\n",
      "Ep done - 40520.\n",
      "Ep done - 40530.\n",
      "Ep done - 40540.\n",
      "Ep done - 40550.\n",
      "Ep done - 40560.\n",
      "Ep done - 40570.\n",
      "Ep done - 40580.\n",
      "Ep done - 40590.\n",
      "Ep done - 40600.\n",
      "Ep done - 40610.\n",
      "Ep done - 40620.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2913     |\n",
      "|    total_timesteps | 995328   |\n",
      "---------------------------------\n",
      "Ep done - 40630.\n",
      "Ep done - 40640.\n",
      "Ep done - 40650.\n",
      "Ep done - 40660.\n",
      "Ep done - 40670.\n",
      "Ep done - 40680.\n",
      "Ep done - 40690.\n",
      "Ep done - 40700.\n",
      "Ep done - 40710.\n",
      "Ep done - 40720.\n",
      "Ep done - 40730.\n",
      "Ep done - 40740.\n",
      "Ep done - 40750.\n",
      "Ep done - 40760.\n",
      "Ep done - 40770.\n",
      "Ep done - 9910.\n",
      "Ep done - 9920.\n",
      "Ep done - 9930.\n",
      "Ep done - 9940.\n",
      "Ep done - 9950.\n",
      "Ep done - 9960.\n",
      "Ep done - 9970.\n",
      "Ep done - 9980.\n",
      "Ep done - 9990.\n",
      "Ep done - 10000.\n",
      "Eval num_timesteps=1000000, episode_reward=0.86 +/- 0.51\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1000000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0709877 |\n",
      "|    clip_fraction        | 0.059     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00148  |\n",
      "|    explained_variance   | -0.36     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00948  |\n",
      "|    n_updates            | 20850     |\n",
      "|    policy_gradient_loss | 0.00127   |\n",
      "|    value_loss           | 0.0915    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.86\n",
      "SELFPLAY: new best model, bumping up generation to 58\n",
      "Ep done - 40780.\n",
      "Ep done - 40790.\n",
      "Ep done - 40800.\n",
      "Ep done - 40810.\n",
      "Ep done - 40820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 2937     |\n",
      "|    total_timesteps | 1001472  |\n",
      "---------------------------------\n",
      "Ep done - 40830.\n",
      "Ep done - 40840.\n",
      "Ep done - 40850.\n",
      "Ep done - 40860.\n",
      "Ep done - 40870.\n",
      "Ep done - 40880.\n",
      "Ep done - 40890.\n",
      "Ep done - 40900.\n",
      "Ep done - 40910.\n",
      "Ep done - 40920.\n",
      "Ep done - 40930.\n",
      "Ep done - 40940.\n",
      "Ep done - 40950.\n",
      "Ep done - 40960.\n",
      "Ep done - 40970.\n",
      "Ep done - 40980.\n",
      "Ep done - 40990.\n",
      "Ep done - 41000.\n",
      "Ep done - 41010.\n",
      "Ep done - 41020.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.25      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 341       |\n",
      "|    iterations           | 164       |\n",
      "|    time_elapsed         | 2951      |\n",
      "|    total_timesteps      | 1007616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.4662914 |\n",
      "|    clip_fraction        | 0.112     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00504  |\n",
      "|    explained_variance   | -0.68     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0755    |\n",
      "|    n_updates            | 20860     |\n",
      "|    policy_gradient_loss | -0.00606  |\n",
      "|    value_loss           | 0.177     |\n",
      "---------------------------------------\n",
      "Ep done - 41030.\n",
      "Ep done - 41040.\n",
      "Ep done - 41050.\n",
      "Ep done - 41060.\n",
      "Ep done - 41070.\n",
      "Ep done - 41080.\n",
      "Ep done - 41090.\n",
      "Ep done - 41100.\n",
      "Ep done - 10010.\n",
      "Ep done - 10020.\n",
      "Ep done - 10030.\n",
      "Ep done - 10040.\n",
      "Ep done - 10050.\n",
      "Ep done - 10060.\n",
      "Ep done - 10070.\n",
      "Ep done - 10080.\n",
      "Ep done - 10090.\n",
      "Ep done - 10100.\n",
      "Eval num_timesteps=1010000, episode_reward=0.86 +/- 0.51\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1010000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8994798 |\n",
      "|    clip_fraction        | 0.0831    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000953 |\n",
      "|    explained_variance   | -0.128    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.092     |\n",
      "|    n_updates            | 20870     |\n",
      "|    policy_gradient_loss | -0.00647  |\n",
      "|    value_loss           | 0.184     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.86\n",
      "SELFPLAY: new best model, bumping up generation to 59\n",
      "Ep done - 41110.\n",
      "Ep done - 41120.\n",
      "Ep done - 41130.\n",
      "Ep done - 41140.\n",
      "Ep done - 41150.\n",
      "Ep done - 41160.\n",
      "Ep done - 41170.\n",
      "Ep done - 41180.\n",
      "Ep done - 41190.\n",
      "Ep done - 41200.\n",
      "Ep done - 41210.\n",
      "Ep done - 41220.\n",
      "Ep done - 41230.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 2969     |\n",
      "|    total_timesteps | 1013760  |\n",
      "---------------------------------\n",
      "Ep done - 41240.\n",
      "Ep done - 41250.\n",
      "Ep done - 41260.\n",
      "Ep done - 41270.\n",
      "Ep done - 41280.\n",
      "Ep done - 41290.\n",
      "Ep done - 41300.\n",
      "Ep done - 41310.\n",
      "Ep done - 41320.\n",
      "Ep done - 41330.\n",
      "Ep done - 41340.\n",
      "Ep done - 41350.\n",
      "Ep done - 41360.\n",
      "Ep done - 41370.\n",
      "Ep done - 41380.\n",
      "Ep done - 41390.\n",
      "Ep done - 41400.\n",
      "Ep done - 41410.\n",
      "Ep done - 41420.\n",
      "Ep done - 41430.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.92      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 341       |\n",
      "|    iterations           | 166       |\n",
      "|    time_elapsed         | 2986      |\n",
      "|    total_timesteps      | 1019904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1517253 |\n",
      "|    clip_fraction        | 0.0823    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00103  |\n",
      "|    explained_variance   | 0.304     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00467   |\n",
      "|    n_updates            | 20880     |\n",
      "|    policy_gradient_loss | -0.0118   |\n",
      "|    value_loss           | 0.0333    |\n",
      "---------------------------------------\n",
      "Ep done - 41440.\n",
      "Ep done - 10110.\n",
      "Ep done - 10120.\n",
      "Ep done - 10130.\n",
      "Ep done - 10140.\n",
      "Ep done - 10150.\n",
      "Ep done - 10160.\n",
      "Ep done - 10170.\n",
      "Ep done - 10180.\n",
      "Ep done - 10190.\n",
      "Ep done - 10200.\n",
      "Eval num_timesteps=1020000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.94 +/- 0.24\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.9      |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1020000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0895478 |\n",
      "|    clip_fraction        | 0.0861    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00767  |\n",
      "|    explained_variance   | 0.0293    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0131   |\n",
      "|    n_updates            | 20890     |\n",
      "|    policy_gradient_loss | 0.113     |\n",
      "|    value_loss           | 0.0215    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 60\n",
      "Ep done - 41450.\n",
      "Ep done - 41460.\n",
      "Ep done - 41470.\n",
      "Ep done - 41480.\n",
      "Ep done - 41490.\n",
      "Ep done - 41500.\n",
      "Ep done - 41510.\n",
      "Ep done - 41520.\n",
      "Ep done - 41530.\n",
      "Ep done - 41540.\n",
      "Ep done - 41550.\n",
      "Ep done - 41560.\n",
      "Ep done - 41570.\n",
      "Ep done - 41580.\n",
      "Ep done - 41590.\n",
      "Ep done - 41600.\n",
      "Ep done - 41610.\n",
      "Ep done - 41620.\n",
      "Ep done - 41630.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.9     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 3004     |\n",
      "|    total_timesteps | 1026048  |\n",
      "---------------------------------\n",
      "Ep done - 41640.\n",
      "Ep done - 41650.\n",
      "Ep done - 41660.\n",
      "Ep done - 41670.\n",
      "Ep done - 41680.\n",
      "Ep done - 41690.\n",
      "Ep done - 41700.\n",
      "Ep done - 41710.\n",
      "Ep done - 41720.\n",
      "Ep done - 41730.\n",
      "Ep done - 41740.\n",
      "Ep done - 41750.\n",
      "Ep done - 41760.\n",
      "Ep done - 10210.\n",
      "Ep done - 10220.\n",
      "Ep done - 10230.\n",
      "Ep done - 10240.\n",
      "Ep done - 10250.\n",
      "Ep done - 10260.\n",
      "Ep done - 10270.\n",
      "Ep done - 10280.\n",
      "Ep done - 10290.\n",
      "Ep done - 10300.\n",
      "Eval num_timesteps=1030000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.59 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.6      |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1030000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 11.022702 |\n",
      "|    clip_fraction        | 0.187     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00223  |\n",
      "|    explained_variance   | 0.551     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0354   |\n",
      "|    n_updates            | 20900     |\n",
      "|    policy_gradient_loss | -0.0177   |\n",
      "|    value_loss           | 0.00272   |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 61\n",
      "Ep done - 41770.\n",
      "Ep done - 41780.\n",
      "Ep done - 41790.\n",
      "Ep done - 41800.\n",
      "Ep done - 41810.\n",
      "Ep done - 41820.\n",
      "Ep done - 41830.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.93     |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 3023     |\n",
      "|    total_timesteps | 1032192  |\n",
      "---------------------------------\n",
      "Ep done - 41840.\n",
      "Ep done - 41850.\n",
      "Ep done - 41860.\n",
      "Ep done - 41870.\n",
      "Ep done - 41880.\n",
      "Ep done - 41890.\n",
      "Ep done - 41900.\n",
      "Ep done - 41910.\n",
      "Ep done - 41920.\n",
      "Ep done - 41930.\n",
      "Ep done - 41940.\n",
      "Ep done - 41950.\n",
      "Ep done - 41960.\n",
      "Ep done - 41970.\n",
      "Ep done - 41980.\n",
      "Ep done - 41990.\n",
      "Ep done - 42000.\n",
      "Ep done - 42010.\n",
      "Ep done - 42020.\n",
      "Ep done - 42030.\n",
      "Ep done - 42040.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.05      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 341       |\n",
      "|    iterations           | 169       |\n",
      "|    time_elapsed         | 3037      |\n",
      "|    total_timesteps      | 1038336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.2437935 |\n",
      "|    clip_fraction        | 0.0968    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00233  |\n",
      "|    explained_variance   | 0.0209    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0099   |\n",
      "|    n_updates            | 20910     |\n",
      "|    policy_gradient_loss | -0.00679  |\n",
      "|    value_loss           | 0.028     |\n",
      "---------------------------------------\n",
      "Ep done - 42050.\n",
      "Ep done - 42060.\n",
      "Ep done - 42070.\n",
      "Ep done - 42080.\n",
      "Ep done - 42090.\n",
      "Ep done - 10310.\n",
      "Ep done - 10320.\n",
      "Ep done - 10330.\n",
      "Ep done - 10340.\n",
      "Ep done - 10350.\n",
      "Ep done - 10360.\n",
      "Ep done - 10370.\n",
      "Ep done - 10380.\n",
      "Ep done - 10390.\n",
      "Ep done - 10400.\n",
      "Eval num_timesteps=1040000, episode_reward=0.95 +/- 0.30\n",
      "Episode length: 30.04 +/- 0.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.95       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1040000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60306054 |\n",
      "|    clip_fraction        | 0.0661     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00427   |\n",
      "|    explained_variance   | -0.0731    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0855     |\n",
      "|    n_updates            | 20920      |\n",
      "|    policy_gradient_loss | 0.00296    |\n",
      "|    value_loss           | 0.246      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.95\n",
      "SELFPLAY: new best model, bumping up generation to 62\n",
      "Ep done - 42100.\n",
      "Ep done - 42110.\n",
      "Ep done - 42120.\n",
      "Ep done - 42130.\n",
      "Ep done - 42140.\n",
      "Ep done - 42150.\n",
      "Ep done - 42160.\n",
      "Ep done - 42170.\n",
      "Ep done - 42180.\n",
      "Ep done - 42190.\n",
      "Ep done - 42200.\n",
      "Ep done - 42210.\n",
      "Ep done - 42220.\n",
      "Ep done - 42230.\n",
      "Ep done - 42240.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.6     |\n",
      "|    ep_rew_mean     | 0.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 3058     |\n",
      "|    total_timesteps | 1044480  |\n",
      "---------------------------------\n",
      "Ep done - 42250.\n",
      "Ep done - 42260.\n",
      "Ep done - 42270.\n",
      "Ep done - 42280.\n",
      "Ep done - 42290.\n",
      "Ep done - 42300.\n",
      "Ep done - 42310.\n",
      "Ep done - 42320.\n",
      "Ep done - 42330.\n",
      "Ep done - 42340.\n",
      "Ep done - 42350.\n",
      "Ep done - 42360.\n",
      "Ep done - 42370.\n",
      "Ep done - 42380.\n",
      "Ep done - 42390.\n",
      "Ep done - 42400.\n",
      "Ep done - 42410.\n",
      "Ep done - 42420.\n",
      "Ep done - 10410.\n",
      "Ep done - 10420.\n",
      "Ep done - 10430.\n",
      "Ep done - 10440.\n",
      "Ep done - 10450.\n",
      "Ep done - 10460.\n",
      "Ep done - 10470.\n",
      "Ep done - 10480.\n",
      "Ep done - 10490.\n",
      "Ep done - 10500.\n",
      "Eval num_timesteps=1050000, episode_reward=-0.25 +/- 0.96\n",
      "Episode length: 30.00 +/- 0.32\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.25     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1050000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 6.0017624 |\n",
      "|    clip_fraction        | 0.21      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00343  |\n",
      "|    explained_variance   | -0.0704   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.117     |\n",
      "|    n_updates            | 20930     |\n",
      "|    policy_gradient_loss | 0.0018    |\n",
      "|    value_loss           | 0.139     |\n",
      "---------------------------------------\n",
      "Ep done - 42430.\n",
      "Ep done - 42440.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 3077     |\n",
      "|    total_timesteps | 1050624  |\n",
      "---------------------------------\n",
      "Ep done - 42450.\n",
      "Ep done - 42460.\n",
      "Ep done - 42470.\n",
      "Ep done - 42480.\n",
      "Ep done - 42490.\n",
      "Ep done - 42500.\n",
      "Ep done - 42510.\n",
      "Ep done - 42520.\n",
      "Ep done - 42530.\n",
      "Ep done - 42540.\n",
      "Ep done - 42550.\n",
      "Ep done - 42560.\n",
      "Ep done - 42570.\n",
      "Ep done - 42580.\n",
      "Ep done - 42590.\n",
      "Ep done - 42600.\n",
      "Ep done - 42610.\n",
      "Ep done - 42620.\n",
      "Ep done - 42630.\n",
      "Ep done - 42640.\n",
      "Ep done - 42650.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30.1     |\n",
      "|    ep_rew_mean          | 0.1      |\n",
      "| time/                   |          |\n",
      "|    fps                  | 341      |\n",
      "|    iterations           | 172      |\n",
      "|    time_elapsed         | 3091     |\n",
      "|    total_timesteps      | 1056768  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.249617 |\n",
      "|    clip_fraction        | 0.0774   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00476 |\n",
      "|    explained_variance   | -0.196   |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.143    |\n",
      "|    n_updates            | 20940    |\n",
      "|    policy_gradient_loss | 0.00679  |\n",
      "|    value_loss           | 0.239    |\n",
      "--------------------------------------\n",
      "Ep done - 42660.\n",
      "Ep done - 42670.\n",
      "Ep done - 42680.\n",
      "Ep done - 42690.\n",
      "Ep done - 42700.\n",
      "Ep done - 42710.\n",
      "Ep done - 42720.\n",
      "Ep done - 42730.\n",
      "Ep done - 42740.\n",
      "Ep done - 42750.\n",
      "Ep done - 10510.\n",
      "Ep done - 10520.\n",
      "Ep done - 10530.\n",
      "Ep done - 10540.\n",
      "Ep done - 10550.\n",
      "Ep done - 10560.\n",
      "Ep done - 10570.\n",
      "Ep done - 10580.\n",
      "Ep done - 10590.\n",
      "Ep done - 10600.\n",
      "Eval num_timesteps=1060000, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.89 +/- 0.31\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.9      |\n",
      "|    mean_reward          | 0.92      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1060000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8193403 |\n",
      "|    clip_fraction        | 0.18      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0258   |\n",
      "|    explained_variance   | -0.548    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0669    |\n",
      "|    n_updates            | 20950     |\n",
      "|    policy_gradient_loss | 0.01      |\n",
      "|    value_loss           | 0.16      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.92\n",
      "SELFPLAY: new best model, bumping up generation to 63\n",
      "Ep done - 42760.\n",
      "Ep done - 42770.\n",
      "Ep done - 42780.\n",
      "Ep done - 42790.\n",
      "Ep done - 42800.\n",
      "Ep done - 42810.\n",
      "Ep done - 42820.\n",
      "Ep done - 42830.\n",
      "Ep done - 42840.\n",
      "Ep done - 42850.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 173      |\n",
      "|    time_elapsed    | 3110     |\n",
      "|    total_timesteps | 1062912  |\n",
      "---------------------------------\n",
      "Ep done - 42860.\n",
      "Ep done - 42870.\n",
      "Ep done - 42880.\n",
      "Ep done - 42890.\n",
      "Ep done - 42900.\n",
      "Ep done - 42910.\n",
      "Ep done - 42920.\n",
      "Ep done - 42930.\n",
      "Ep done - 42940.\n",
      "Ep done - 42950.\n",
      "Ep done - 42960.\n",
      "Ep done - 42970.\n",
      "Ep done - 42980.\n",
      "Ep done - 42990.\n",
      "Ep done - 43000.\n",
      "Ep done - 43010.\n",
      "Ep done - 43020.\n",
      "Ep done - 43030.\n",
      "Ep done - 43040.\n",
      "Ep done - 43050.\n",
      "Ep done - 43060.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.64      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 174        |\n",
      "|    time_elapsed         | 3124       |\n",
      "|    total_timesteps      | 1069056    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.28627607 |\n",
      "|    clip_fraction        | 0.0439     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00868   |\n",
      "|    explained_variance   | 0.0447     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0906     |\n",
      "|    n_updates            | 20960      |\n",
      "|    policy_gradient_loss | 0.000213   |\n",
      "|    value_loss           | 0.199      |\n",
      "----------------------------------------\n",
      "Ep done - 43070.\n",
      "Ep done - 43080.\n",
      "Ep done - 43090.\n",
      "Ep done - 10610.\n",
      "Ep done - 10620.\n",
      "Ep done - 10630.\n",
      "Ep done - 10640.\n",
      "Ep done - 10650.\n",
      "Ep done - 10660.\n",
      "Ep done - 10670.\n",
      "Ep done - 10680.\n",
      "Ep done - 10690.\n",
      "Ep done - 10700.\n",
      "Eval num_timesteps=1070000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1070000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4659685 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00314  |\n",
      "|    explained_variance   | 0.233     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0307    |\n",
      "|    n_updates            | 20970     |\n",
      "|    policy_gradient_loss | -0.00469  |\n",
      "|    value_loss           | 0.126     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 64\n",
      "Ep done - 43100.\n",
      "Ep done - 43110.\n",
      "Ep done - 43120.\n",
      "Ep done - 43130.\n",
      "Ep done - 43140.\n",
      "Ep done - 43150.\n",
      "Ep done - 43160.\n",
      "Ep done - 43170.\n",
      "Ep done - 43180.\n",
      "Ep done - 43190.\n",
      "Ep done - 43200.\n",
      "Ep done - 43210.\n",
      "Ep done - 43220.\n",
      "Ep done - 43230.\n",
      "Ep done - 43240.\n",
      "Ep done - 43250.\n",
      "Ep done - 43260.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 175      |\n",
      "|    time_elapsed    | 3141     |\n",
      "|    total_timesteps | 1075200  |\n",
      "---------------------------------\n",
      "Ep done - 43270.\n",
      "Ep done - 43280.\n",
      "Ep done - 43290.\n",
      "Ep done - 43300.\n",
      "Ep done - 43310.\n",
      "Ep done - 43320.\n",
      "Ep done - 43330.\n",
      "Ep done - 43340.\n",
      "Ep done - 43350.\n",
      "Ep done - 43360.\n",
      "Ep done - 43370.\n",
      "Ep done - 43380.\n",
      "Ep done - 43390.\n",
      "Ep done - 43400.\n",
      "Ep done - 43410.\n",
      "Ep done - 43420.\n",
      "Ep done - 10710.\n",
      "Ep done - 10720.\n",
      "Ep done - 10730.\n",
      "Ep done - 10740.\n",
      "Ep done - 10750.\n",
      "Ep done - 10760.\n",
      "Ep done - 10770.\n",
      "Ep done - 10780.\n",
      "Ep done - 10790.\n",
      "Ep done - 10800.\n",
      "Eval num_timesteps=1080000, episode_reward=0.76 +/- 0.65\n",
      "Episode length: 29.98 +/- 0.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.76      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1080000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.1316195 |\n",
      "|    clip_fraction        | 0.0798    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000827 |\n",
      "|    explained_variance   | -3.34     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0033   |\n",
      "|    n_updates            | 20980     |\n",
      "|    policy_gradient_loss | -0.0171   |\n",
      "|    value_loss           | 0.0308    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 65\n",
      "Ep done - 43430.\n",
      "Ep done - 43440.\n",
      "Ep done - 43450.\n",
      "Ep done - 43460.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.81     |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 3159     |\n",
      "|    total_timesteps | 1081344  |\n",
      "---------------------------------\n",
      "Ep done - 43470.\n",
      "Ep done - 43480.\n",
      "Ep done - 43490.\n",
      "Ep done - 43500.\n",
      "Ep done - 43510.\n",
      "Ep done - 43520.\n",
      "Ep done - 43530.\n",
      "Ep done - 43540.\n",
      "Ep done - 43550.\n",
      "Ep done - 43560.\n",
      "Ep done - 43570.\n",
      "Ep done - 43580.\n",
      "Ep done - 43590.\n",
      "Ep done - 43600.\n",
      "Ep done - 43610.\n",
      "Ep done - 43620.\n",
      "Ep done - 43630.\n",
      "Ep done - 43640.\n",
      "Ep done - 43650.\n",
      "Ep done - 43660.\n",
      "Ep done - 43670.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.72       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 177        |\n",
      "|    time_elapsed         | 3174       |\n",
      "|    total_timesteps      | 1087488    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40395823 |\n",
      "|    clip_fraction        | 0.0143     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00127   |\n",
      "|    explained_variance   | -0.0224    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0613     |\n",
      "|    n_updates            | 20990      |\n",
      "|    policy_gradient_loss | -0.00532   |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "Ep done - 43680.\n",
      "Ep done - 43690.\n",
      "Ep done - 43700.\n",
      "Ep done - 43710.\n",
      "Ep done - 43720.\n",
      "Ep done - 43730.\n",
      "Ep done - 43740.\n",
      "Ep done - 43750.\n",
      "Ep done - 10810.\n",
      "Ep done - 10820.\n",
      "Ep done - 10830.\n",
      "Ep done - 10840.\n",
      "Ep done - 10850.\n",
      "Ep done - 10860.\n",
      "Ep done - 10870.\n",
      "Ep done - 10880.\n",
      "Ep done - 10890.\n",
      "Ep done - 10900.\n",
      "Eval num_timesteps=1090000, episode_reward=0.70 +/- 0.71\n",
      "Episode length: 29.88 +/- 0.32\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.9       |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1090000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26017705 |\n",
      "|    clip_fraction        | 0.0092     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00102   |\n",
      "|    explained_variance   | 0.0362     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0567     |\n",
      "|    n_updates            | 21000      |\n",
      "|    policy_gradient_loss | -0.00468   |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 66\n",
      "Ep done - 43760.\n",
      "Ep done - 43770.\n",
      "Ep done - 43780.\n",
      "Ep done - 43790.\n",
      "Ep done - 43800.\n",
      "Ep done - 43810.\n",
      "Ep done - 43820.\n",
      "Ep done - 43830.\n",
      "Ep done - 43840.\n",
      "Ep done - 43850.\n",
      "Ep done - 43860.\n",
      "Ep done - 43870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 3196     |\n",
      "|    total_timesteps | 1093632  |\n",
      "---------------------------------\n",
      "Ep done - 43880.\n",
      "Ep done - 43890.\n",
      "Ep done - 43900.\n",
      "Ep done - 43910.\n",
      "Ep done - 43920.\n",
      "Ep done - 43930.\n",
      "Ep done - 43940.\n",
      "Ep done - 43950.\n",
      "Ep done - 43960.\n",
      "Ep done - 43970.\n",
      "Ep done - 43980.\n",
      "Ep done - 43990.\n",
      "Ep done - 44000.\n",
      "Ep done - 44010.\n",
      "Ep done - 44020.\n",
      "Ep done - 44030.\n",
      "Ep done - 44040.\n",
      "Ep done - 44050.\n",
      "Ep done - 44060.\n",
      "Ep done - 44070.\n",
      "Ep done - 44080.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.72       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 179        |\n",
      "|    time_elapsed         | 3213       |\n",
      "|    total_timesteps      | 1099776    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14271428 |\n",
      "|    clip_fraction        | 0.0197     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00239   |\n",
      "|    explained_variance   | 0.00291    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.109      |\n",
      "|    n_updates            | 21010      |\n",
      "|    policy_gradient_loss | 0.00159    |\n",
      "|    value_loss           | 0.202      |\n",
      "----------------------------------------\n",
      "Ep done - 10910.\n",
      "Ep done - 10920.\n",
      "Ep done - 10930.\n",
      "Ep done - 10940.\n",
      "Ep done - 10950.\n",
      "Ep done - 10960.\n",
      "Ep done - 10970.\n",
      "Ep done - 10980.\n",
      "Ep done - 10990.\n",
      "Ep done - 11000.\n",
      "Eval num_timesteps=1100000, episode_reward=-0.72 +/- 0.66\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.72      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1100000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68669504 |\n",
      "|    clip_fraction        | 0.0301     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00157   |\n",
      "|    explained_variance   | 0.0158     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0584     |\n",
      "|    n_updates            | 21020      |\n",
      "|    policy_gradient_loss | 0.00145    |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "Ep done - 44090.\n",
      "Ep done - 44100.\n",
      "Ep done - 44110.\n",
      "Ep done - 44120.\n",
      "Ep done - 44130.\n",
      "Ep done - 44140.\n",
      "Ep done - 44150.\n",
      "Ep done - 44160.\n",
      "Ep done - 44170.\n",
      "Ep done - 44180.\n",
      "Ep done - 44190.\n",
      "Ep done - 44200.\n",
      "Ep done - 44210.\n",
      "Ep done - 44220.\n",
      "Ep done - 44230.\n",
      "Ep done - 44240.\n",
      "Ep done - 44250.\n",
      "Ep done - 44260.\n",
      "Ep done - 44270.\n",
      "Ep done - 44280.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 3233     |\n",
      "|    total_timesteps | 1105920  |\n",
      "---------------------------------\n",
      "Ep done - 44290.\n",
      "Ep done - 44300.\n",
      "Ep done - 44310.\n",
      "Ep done - 44320.\n",
      "Ep done - 44330.\n",
      "Ep done - 44340.\n",
      "Ep done - 44350.\n",
      "Ep done - 44360.\n",
      "Ep done - 44370.\n",
      "Ep done - 44380.\n",
      "Ep done - 44390.\n",
      "Ep done - 44400.\n",
      "Ep done - 44410.\n",
      "Ep done - 44420.\n",
      "Ep done - 11010.\n",
      "Ep done - 11020.\n",
      "Ep done - 11030.\n",
      "Ep done - 11040.\n",
      "Ep done - 11050.\n",
      "Ep done - 11060.\n",
      "Ep done - 11070.\n",
      "Ep done - 11080.\n",
      "Ep done - 11090.\n",
      "Ep done - 11100.\n",
      "Eval num_timesteps=1110000, episode_reward=0.99 +/- 0.10\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.99      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1110000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.7994134 |\n",
      "|    clip_fraction        | 0.125     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000594 |\n",
      "|    explained_variance   | -0.156    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0158    |\n",
      "|    n_updates            | 21030     |\n",
      "|    policy_gradient_loss | -0.0114   |\n",
      "|    value_loss           | 0.129     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.99\n",
      "SELFPLAY: new best model, bumping up generation to 67\n",
      "Ep done - 44430.\n",
      "Ep done - 44440.\n",
      "Ep done - 44450.\n",
      "Ep done - 44460.\n",
      "Ep done - 44470.\n",
      "Ep done - 44480.\n",
      "Ep done - 44490.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.96     |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 3255     |\n",
      "|    total_timesteps | 1112064  |\n",
      "---------------------------------\n",
      "Ep done - 44500.\n",
      "Ep done - 44510.\n",
      "Ep done - 44520.\n",
      "Ep done - 44530.\n",
      "Ep done - 44540.\n",
      "Ep done - 44550.\n",
      "Ep done - 44560.\n",
      "Ep done - 44570.\n",
      "Ep done - 44580.\n",
      "Ep done - 44590.\n",
      "Ep done - 44600.\n",
      "Ep done - 44610.\n",
      "Ep done - 44620.\n",
      "Ep done - 44630.\n",
      "Ep done - 44640.\n",
      "Ep done - 44650.\n",
      "Ep done - 44660.\n",
      "Ep done - 44670.\n",
      "Ep done - 44680.\n",
      "Ep done - 44690.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.3      |\n",
      "|    ep_rew_mean          | 0.35      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 341       |\n",
      "|    iterations           | 182       |\n",
      "|    time_elapsed         | 3273      |\n",
      "|    total_timesteps      | 1118208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 6.6215997 |\n",
      "|    clip_fraction        | 0.109     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00132  |\n",
      "|    explained_variance   | -1.6      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0224   |\n",
      "|    n_updates            | 21040     |\n",
      "|    policy_gradient_loss | -0.013    |\n",
      "|    value_loss           | 0.0324    |\n",
      "---------------------------------------\n",
      "Ep done - 44700.\n",
      "Ep done - 44710.\n",
      "Ep done - 44720.\n",
      "Ep done - 44730.\n",
      "Ep done - 44740.\n",
      "Ep done - 44750.\n",
      "Ep done - 11110.\n",
      "Ep done - 11120.\n",
      "Ep done - 11130.\n",
      "Ep done - 11140.\n",
      "Ep done - 11150.\n",
      "Ep done - 11160.\n",
      "Ep done - 11170.\n",
      "Ep done - 11180.\n",
      "Ep done - 11190.\n",
      "Ep done - 11200.\n",
      "Eval num_timesteps=1120000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.66 +/- 0.47\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.7      |\n",
      "|    mean_reward          | 0.66      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1120000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2200426 |\n",
      "|    clip_fraction        | 0.125     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00778  |\n",
      "|    explained_variance   | -0.132    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.112     |\n",
      "|    n_updates            | 21050     |\n",
      "|    policy_gradient_loss | 0.00466   |\n",
      "|    value_loss           | 0.263     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.66\n",
      "SELFPLAY: new best model, bumping up generation to 68\n",
      "Ep done - 44760.\n",
      "Ep done - 44770.\n",
      "Ep done - 44780.\n",
      "Ep done - 44790.\n",
      "Ep done - 44800.\n",
      "Ep done - 44810.\n",
      "Ep done - 44820.\n",
      "Ep done - 44830.\n",
      "Ep done - 44840.\n",
      "Ep done - 44850.\n",
      "Ep done - 44860.\n",
      "Ep done - 44870.\n",
      "Ep done - 44880.\n",
      "Ep done - 44890.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.8     |\n",
      "|    ep_rew_mean     | 0.92     |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 183      |\n",
      "|    time_elapsed    | 3295     |\n",
      "|    total_timesteps | 1124352  |\n",
      "---------------------------------\n",
      "Ep done - 44900.\n",
      "Ep done - 44910.\n",
      "Ep done - 44920.\n",
      "Ep done - 44930.\n",
      "Ep done - 44940.\n",
      "Ep done - 44950.\n",
      "Ep done - 44960.\n",
      "Ep done - 44970.\n",
      "Ep done - 44980.\n",
      "Ep done - 44990.\n",
      "Ep done - 45000.\n",
      "Ep done - 45010.\n",
      "Ep done - 45020.\n",
      "Ep done - 45030.\n",
      "Ep done - 45040.\n",
      "Ep done - 45050.\n",
      "Ep done - 45060.\n",
      "Ep done - 45070.\n",
      "Ep done - 45080.\n",
      "Ep done - 11210.\n",
      "Ep done - 11220.\n",
      "Ep done - 11230.\n",
      "Ep done - 11240.\n",
      "Ep done - 11250.\n",
      "Ep done - 11260.\n",
      "Ep done - 11270.\n",
      "Ep done - 11280.\n",
      "Ep done - 11290.\n",
      "Ep done - 11300.\n",
      "Eval num_timesteps=1130000, episode_reward=-0.40 +/- 0.91\n",
      "Episode length: 30.02 +/- 0.14\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.4     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 1130000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.016138 |\n",
      "|    clip_fraction        | 0.149    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0247  |\n",
      "|    explained_variance   | 0.0255   |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0205   |\n",
      "|    n_updates            | 21060    |\n",
      "|    policy_gradient_loss | -0.00478 |\n",
      "|    value_loss           | 0.0995   |\n",
      "--------------------------------------\n",
      "Ep done - 45090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.19    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 3319     |\n",
      "|    total_timesteps | 1130496  |\n",
      "---------------------------------\n",
      "Ep done - 45100.\n",
      "Ep done - 45110.\n",
      "Ep done - 45120.\n",
      "Ep done - 45130.\n",
      "Ep done - 45140.\n",
      "Ep done - 45150.\n",
      "Ep done - 45160.\n",
      "Ep done - 45170.\n",
      "Ep done - 45180.\n",
      "Ep done - 45190.\n",
      "Ep done - 45200.\n",
      "Ep done - 45210.\n",
      "Ep done - 45220.\n",
      "Ep done - 45230.\n",
      "Ep done - 45240.\n",
      "Ep done - 45250.\n",
      "Ep done - 45260.\n",
      "Ep done - 45270.\n",
      "Ep done - 45280.\n",
      "Ep done - 45290.\n",
      "Ep done - 45300.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.9        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 185        |\n",
      "|    time_elapsed         | 3341       |\n",
      "|    total_timesteps      | 1136640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75713515 |\n",
      "|    clip_fraction        | 0.0671     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0104    |\n",
      "|    explained_variance   | -0.113     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0435     |\n",
      "|    n_updates            | 21070      |\n",
      "|    policy_gradient_loss | -0.00902   |\n",
      "|    value_loss           | 0.259      |\n",
      "----------------------------------------\n",
      "Ep done - 45310.\n",
      "Ep done - 45320.\n",
      "Ep done - 45330.\n",
      "Ep done - 45340.\n",
      "Ep done - 45350.\n",
      "Ep done - 45360.\n",
      "Ep done - 45370.\n",
      "Ep done - 45380.\n",
      "Ep done - 45390.\n",
      "Ep done - 45400.\n",
      "Ep done - 45410.\n",
      "Ep done - 11310.\n",
      "Ep done - 11320.\n",
      "Ep done - 11330.\n",
      "Ep done - 11340.\n",
      "Ep done - 11350.\n",
      "Ep done - 11360.\n",
      "Ep done - 11370.\n",
      "Ep done - 11380.\n",
      "Ep done - 11390.\n",
      "Ep done - 11400.\n",
      "Eval num_timesteps=1140000, episode_reward=-0.70 +/- 0.71\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 1140000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.15609  |\n",
      "|    clip_fraction        | 0.106    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00582 |\n",
      "|    explained_variance   | -1.77    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.017    |\n",
      "|    n_updates            | 21080    |\n",
      "|    policy_gradient_loss | -0.00109 |\n",
      "|    value_loss           | 0.0984   |\n",
      "--------------------------------------\n",
      "Ep done - 45420.\n",
      "Ep done - 45430.\n",
      "Ep done - 45440.\n",
      "Ep done - 45450.\n",
      "Ep done - 45460.\n",
      "Ep done - 45470.\n",
      "Ep done - 45480.\n",
      "Ep done - 45490.\n",
      "Ep done - 45500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 3361     |\n",
      "|    total_timesteps | 1142784  |\n",
      "---------------------------------\n",
      "Ep done - 45510.\n",
      "Ep done - 45520.\n",
      "Ep done - 45530.\n",
      "Ep done - 45540.\n",
      "Ep done - 45550.\n",
      "Ep done - 45560.\n",
      "Ep done - 45570.\n",
      "Ep done - 45580.\n",
      "Ep done - 45590.\n",
      "Ep done - 45600.\n",
      "Ep done - 45610.\n",
      "Ep done - 45620.\n",
      "Ep done - 45630.\n",
      "Ep done - 45640.\n",
      "Ep done - 45650.\n",
      "Ep done - 45660.\n",
      "Ep done - 45670.\n",
      "Ep done - 45680.\n",
      "Ep done - 45690.\n",
      "Ep done - 45700.\n",
      "Ep done - 45710.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 340       |\n",
      "|    iterations           | 187       |\n",
      "|    time_elapsed         | 3377      |\n",
      "|    total_timesteps      | 1148928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.8786964 |\n",
      "|    clip_fraction        | 0.123     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00219  |\n",
      "|    explained_variance   | -0.582    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000779  |\n",
      "|    n_updates            | 21090     |\n",
      "|    policy_gradient_loss | -0.0102   |\n",
      "|    value_loss           | 0.118     |\n",
      "---------------------------------------\n",
      "Ep done - 45720.\n",
      "Ep done - 45730.\n",
      "Ep done - 45740.\n",
      "Ep done - 11410.\n",
      "Ep done - 11420.\n",
      "Ep done - 11430.\n",
      "Ep done - 11440.\n",
      "Ep done - 11450.\n",
      "Ep done - 11460.\n",
      "Ep done - 11470.\n",
      "Ep done - 11480.\n",
      "Ep done - 11490.\n",
      "Ep done - 11500.\n",
      "Eval num_timesteps=1150000, episode_reward=-0.47 +/- 0.70\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.47     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1150000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8683649 |\n",
      "|    clip_fraction        | 0.086     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.016    |\n",
      "|    explained_variance   | 0.156     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0447    |\n",
      "|    n_updates            | 21100     |\n",
      "|    policy_gradient_loss | -0.0108   |\n",
      "|    value_loss           | 0.142     |\n",
      "---------------------------------------\n",
      "Ep done - 45750.\n",
      "Ep done - 45760.\n",
      "Ep done - 45770.\n",
      "Ep done - 45780.\n",
      "Ep done - 45790.\n",
      "Ep done - 45800.\n",
      "Ep done - 45810.\n",
      "Ep done - 45820.\n",
      "Ep done - 45830.\n",
      "Ep done - 45840.\n",
      "Ep done - 45850.\n",
      "Ep done - 45860.\n",
      "Ep done - 45870.\n",
      "Ep done - 45880.\n",
      "Ep done - 45890.\n",
      "Ep done - 45900.\n",
      "Ep done - 45910.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 3401     |\n",
      "|    total_timesteps | 1155072  |\n",
      "---------------------------------\n",
      "Ep done - 45920.\n",
      "Ep done - 45930.\n",
      "Ep done - 45940.\n",
      "Ep done - 45950.\n",
      "Ep done - 45960.\n",
      "Ep done - 45970.\n",
      "Ep done - 45980.\n",
      "Ep done - 45990.\n",
      "Ep done - 46000.\n",
      "Ep done - 46010.\n",
      "Ep done - 46020.\n",
      "Ep done - 46030.\n",
      "Ep done - 46040.\n",
      "Ep done - 46050.\n",
      "Ep done - 46060.\n",
      "Ep done - 46070.\n",
      "Ep done - 46080.\n",
      "Ep done - 11510.\n",
      "Ep done - 11520.\n",
      "Ep done - 11530.\n",
      "Ep done - 11540.\n",
      "Ep done - 11550.\n",
      "Ep done - 11560.\n",
      "Ep done - 11570.\n",
      "Ep done - 11580.\n",
      "Ep done - 11590.\n",
      "Ep done - 11600.\n",
      "Eval num_timesteps=1160000, episode_reward=-0.63 +/- 0.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.63     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1160000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.4270594 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0104   |\n",
      "|    explained_variance   | 0.151     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0327    |\n",
      "|    n_updates            | 21110     |\n",
      "|    policy_gradient_loss | -0.0175   |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "Ep done - 46090.\n",
      "Ep done - 46100.\n",
      "Ep done - 46110.\n",
      "Ep done - 46120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 189      |\n",
      "|    time_elapsed    | 3427     |\n",
      "|    total_timesteps | 1161216  |\n",
      "---------------------------------\n",
      "Ep done - 46130.\n",
      "Ep done - 46140.\n",
      "Ep done - 46150.\n",
      "Ep done - 46160.\n",
      "Ep done - 46170.\n",
      "Ep done - 46180.\n",
      "Ep done - 46190.\n",
      "Ep done - 46200.\n",
      "Ep done - 46210.\n",
      "Ep done - 46220.\n",
      "Ep done - 46230.\n",
      "Ep done - 46240.\n",
      "Ep done - 46250.\n",
      "Ep done - 46260.\n",
      "Ep done - 46270.\n",
      "Ep done - 46280.\n",
      "Ep done - 46290.\n",
      "Ep done - 46300.\n",
      "Ep done - 46310.\n",
      "Ep done - 46320.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.68     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 338       |\n",
      "|    iterations           | 190       |\n",
      "|    time_elapsed         | 3444      |\n",
      "|    total_timesteps      | 1167360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.3545039 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00495  |\n",
      "|    explained_variance   | 0.101     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0418    |\n",
      "|    n_updates            | 21120     |\n",
      "|    policy_gradient_loss | -0.0135   |\n",
      "|    value_loss           | 0.17      |\n",
      "---------------------------------------\n",
      "Ep done - 46330.\n",
      "Ep done - 46340.\n",
      "Ep done - 46350.\n",
      "Ep done - 46360.\n",
      "Ep done - 46370.\n",
      "Ep done - 46380.\n",
      "Ep done - 46390.\n",
      "Ep done - 46400.\n",
      "Ep done - 46410.\n",
      "Ep done - 11610.\n",
      "Ep done - 11620.\n",
      "Ep done - 11630.\n",
      "Ep done - 11640.\n",
      "Ep done - 11650.\n",
      "Ep done - 11660.\n",
      "Ep done - 11670.\n",
      "Ep done - 11680.\n",
      "Ep done - 11690.\n",
      "Ep done - 11700.\n",
      "Eval num_timesteps=1170000, episode_reward=-0.51 +/- 0.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.51      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1170000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94769984 |\n",
      "|    clip_fraction        | 0.0527     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000211  |\n",
      "|    explained_variance   | 0.251      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.205      |\n",
      "|    n_updates            | 21130      |\n",
      "|    policy_gradient_loss | 0.00725    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "Ep done - 46420.\n",
      "Ep done - 46430.\n",
      "Ep done - 46440.\n",
      "Ep done - 46450.\n",
      "Ep done - 46460.\n",
      "Ep done - 46470.\n",
      "Ep done - 46480.\n",
      "Ep done - 46490.\n",
      "Ep done - 46500.\n",
      "Ep done - 46510.\n",
      "Ep done - 46520.\n",
      "Ep done - 46530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 3466     |\n",
      "|    total_timesteps | 1173504  |\n",
      "---------------------------------\n",
      "Ep done - 46540.\n",
      "Ep done - 46550.\n",
      "Ep done - 46560.\n",
      "Ep done - 46570.\n",
      "Ep done - 46580.\n",
      "Ep done - 46590.\n",
      "Ep done - 46600.\n",
      "Ep done - 46610.\n",
      "Ep done - 46620.\n",
      "Ep done - 46630.\n",
      "Ep done - 46640.\n",
      "Ep done - 46650.\n",
      "Ep done - 46660.\n",
      "Ep done - 46670.\n",
      "Ep done - 46680.\n",
      "Ep done - 46690.\n",
      "Ep done - 46700.\n",
      "Ep done - 46710.\n",
      "Ep done - 46720.\n",
      "Ep done - 46730.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.27      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 338       |\n",
      "|    iterations           | 192       |\n",
      "|    time_elapsed         | 3486      |\n",
      "|    total_timesteps      | 1179648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3866638 |\n",
      "|    clip_fraction        | 0.0429    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000263 |\n",
      "|    explained_variance   | 0.00932   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0561    |\n",
      "|    n_updates            | 21140     |\n",
      "|    policy_gradient_loss | 0.00144   |\n",
      "|    value_loss           | 0.169     |\n",
      "---------------------------------------\n",
      "Ep done - 46740.\n",
      "Ep done - 11710.\n",
      "Ep done - 11720.\n",
      "Ep done - 11730.\n",
      "Ep done - 11740.\n",
      "Ep done - 11750.\n",
      "Ep done - 11760.\n",
      "Ep done - 11770.\n",
      "Ep done - 11780.\n",
      "Ep done - 11790.\n",
      "Ep done - 11800.\n",
      "Eval num_timesteps=1180000, episode_reward=-0.96 +/- 0.28\n",
      "Episode length: 28.42 +/- 0.71\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 28.4      |\n",
      "|    mean_reward          | -0.96     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1180000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.6826365 |\n",
      "|    clip_fraction        | 0.091     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00145  |\n",
      "|    explained_variance   | 0.0722    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.112     |\n",
      "|    n_updates            | 21150     |\n",
      "|    policy_gradient_loss | 0.00985   |\n",
      "|    value_loss           | 0.208     |\n",
      "---------------------------------------\n",
      "Ep done - 46750.\n",
      "Ep done - 46760.\n",
      "Ep done - 46770.\n",
      "Ep done - 46780.\n",
      "Ep done - 46790.\n",
      "Ep done - 46800.\n",
      "Ep done - 46810.\n",
      "Ep done - 46820.\n",
      "Ep done - 46830.\n",
      "Ep done - 46840.\n",
      "Ep done - 46850.\n",
      "Ep done - 46860.\n",
      "Ep done - 46870.\n",
      "Ep done - 46880.\n",
      "Ep done - 46890.\n",
      "Ep done - 46900.\n",
      "Ep done - 46910.\n",
      "Ep done - 46920.\n",
      "Ep done - 46930.\n",
      "Ep done - 46940.\n",
      "Ep done - 46950.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.5     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 193      |\n",
      "|    time_elapsed    | 3510     |\n",
      "|    total_timesteps | 1185792  |\n",
      "---------------------------------\n",
      "Ep done - 46960.\n",
      "Ep done - 46970.\n",
      "Ep done - 46980.\n",
      "Ep done - 46990.\n",
      "Ep done - 47000.\n",
      "Ep done - 47010.\n",
      "Ep done - 47020.\n",
      "Ep done - 47030.\n",
      "Ep done - 47040.\n",
      "Ep done - 47050.\n",
      "Ep done - 47060.\n",
      "Ep done - 47070.\n",
      "Ep done - 47080.\n",
      "Ep done - 47090.\n",
      "Ep done - 11810.\n",
      "Ep done - 11820.\n",
      "Ep done - 11830.\n",
      "Ep done - 11840.\n",
      "Ep done - 11850.\n",
      "Ep done - 11860.\n",
      "Ep done - 11870.\n",
      "Ep done - 11880.\n",
      "Ep done - 11890.\n",
      "Ep done - 11900.\n",
      "Eval num_timesteps=1190000, episode_reward=-0.99 +/- 0.10\n",
      "Episode length: 29.96 +/- 0.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.99     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1190000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.4017963 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0034   |\n",
      "|    explained_variance   | -0.773    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00428  |\n",
      "|    n_updates            | 21160     |\n",
      "|    policy_gradient_loss | -0.0141   |\n",
      "|    value_loss           | 0.0643    |\n",
      "---------------------------------------\n",
      "Ep done - 47100.\n",
      "Ep done - 47110.\n",
      "Ep done - 47120.\n",
      "Ep done - 47130.\n",
      "Ep done - 47140.\n",
      "Ep done - 47150.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 3531     |\n",
      "|    total_timesteps | 1191936  |\n",
      "---------------------------------\n",
      "Ep done - 47160.\n",
      "Ep done - 47170.\n",
      "Ep done - 47180.\n",
      "Ep done - 47190.\n",
      "Ep done - 47200.\n",
      "Ep done - 47210.\n",
      "Ep done - 47220.\n",
      "Ep done - 47230.\n",
      "Ep done - 47240.\n",
      "Ep done - 47250.\n",
      "Ep done - 47260.\n",
      "Ep done - 47270.\n",
      "Ep done - 47280.\n",
      "Ep done - 47290.\n",
      "Ep done - 47300.\n",
      "Ep done - 47310.\n",
      "Ep done - 47320.\n",
      "Ep done - 47330.\n",
      "Ep done - 47340.\n",
      "Ep done - 47350.\n",
      "Ep done - 47360.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 337       |\n",
      "|    iterations           | 195       |\n",
      "|    time_elapsed         | 3545      |\n",
      "|    total_timesteps      | 1198080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5944139 |\n",
      "|    clip_fraction        | 0.0883    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0026   |\n",
      "|    explained_variance   | -0.0479   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0157   |\n",
      "|    n_updates            | 21170     |\n",
      "|    policy_gradient_loss | 0.00933   |\n",
      "|    value_loss           | 0.00792   |\n",
      "---------------------------------------\n",
      "Ep done - 47370.\n",
      "Ep done - 47380.\n",
      "Ep done - 47390.\n",
      "Ep done - 47400.\n",
      "Ep done - 47410.\n",
      "Ep done - 47420.\n",
      "Ep done - 11910.\n",
      "Ep done - 11920.\n",
      "Ep done - 11930.\n",
      "Ep done - 11940.\n",
      "Ep done - 11950.\n",
      "Ep done - 11960.\n",
      "Ep done - 11970.\n",
      "Ep done - 11980.\n",
      "Ep done - 11990.\n",
      "Ep done - 12000.\n",
      "Eval num_timesteps=1200000, episode_reward=-0.31 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.31     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1200000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4613466 |\n",
      "|    clip_fraction        | 0.102     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00131  |\n",
      "|    explained_variance   | 0.734     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0133   |\n",
      "|    n_updates            | 21180     |\n",
      "|    policy_gradient_loss | 0.0651    |\n",
      "|    value_loss           | 0.00388   |\n",
      "---------------------------------------\n",
      "Ep done - 47430.\n",
      "Ep done - 47440.\n",
      "Ep done - 47450.\n",
      "Ep done - 47460.\n",
      "Ep done - 47470.\n",
      "Ep done - 47480.\n",
      "Ep done - 47490.\n",
      "Ep done - 47500.\n",
      "Ep done - 47510.\n",
      "Ep done - 47520.\n",
      "Ep done - 47530.\n",
      "Ep done - 47540.\n",
      "Ep done - 47550.\n",
      "Ep done - 47560.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.14    |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 3565     |\n",
      "|    total_timesteps | 1204224  |\n",
      "---------------------------------\n",
      "Ep done - 47570.\n",
      "Ep done - 47580.\n",
      "Ep done - 47590.\n",
      "Ep done - 47600.\n",
      "Ep done - 47610.\n",
      "Ep done - 47620.\n",
      "Ep done - 47630.\n",
      "Ep done - 47640.\n",
      "Ep done - 47650.\n",
      "Ep done - 47660.\n",
      "Ep done - 47670.\n",
      "Ep done - 47680.\n",
      "Ep done - 47690.\n",
      "Ep done - 47700.\n",
      "Ep done - 47710.\n",
      "Ep done - 47720.\n",
      "Ep done - 47730.\n",
      "Ep done - 47740.\n",
      "Ep done - 47750.\n",
      "Ep done - 47760.\n",
      "Ep done - 12010.\n",
      "Ep done - 12020.\n",
      "Ep done - 12030.\n",
      "Ep done - 12040.\n",
      "Ep done - 12050.\n",
      "Ep done - 12060.\n",
      "Ep done - 12070.\n",
      "Ep done - 12080.\n",
      "Ep done - 12090.\n",
      "Ep done - 12100.\n",
      "Eval num_timesteps=1210000, episode_reward=-0.96 +/- 0.28\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.96     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1210000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7891113 |\n",
      "|    clip_fraction        | 0.0283    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00309  |\n",
      "|    explained_variance   | -0.129    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.121     |\n",
      "|    n_updates            | 21190     |\n",
      "|    policy_gradient_loss | -0.00111  |\n",
      "|    value_loss           | 0.313     |\n",
      "---------------------------------------\n",
      "Ep done - 47770.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 197      |\n",
      "|    time_elapsed    | 3586     |\n",
      "|    total_timesteps | 1210368  |\n",
      "---------------------------------\n",
      "Ep done - 47780.\n",
      "Ep done - 47790.\n",
      "Ep done - 47800.\n",
      "Ep done - 47810.\n",
      "Ep done - 47820.\n",
      "Ep done - 47830.\n",
      "Ep done - 47840.\n",
      "Ep done - 47850.\n",
      "Ep done - 47860.\n",
      "Ep done - 47870.\n",
      "Ep done - 47880.\n",
      "Ep done - 47890.\n",
      "Ep done - 47900.\n",
      "Ep done - 47910.\n",
      "Ep done - 47920.\n",
      "Ep done - 47930.\n",
      "Ep done - 47940.\n",
      "Ep done - 47950.\n",
      "Ep done - 47960.\n",
      "Ep done - 47970.\n",
      "Ep done - 47980.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.6      |\n",
      "|    ep_rew_mean          | -0.97     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 337       |\n",
      "|    iterations           | 198       |\n",
      "|    time_elapsed         | 3603      |\n",
      "|    total_timesteps      | 1216512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.2913885 |\n",
      "|    clip_fraction        | 0.0981    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00216  |\n",
      "|    explained_variance   | -1.07e-06 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0156    |\n",
      "|    n_updates            | 21200     |\n",
      "|    policy_gradient_loss | -0.00835  |\n",
      "|    value_loss           | 0.0366    |\n",
      "---------------------------------------\n",
      "Ep done - 47990.\n",
      "Ep done - 48000.\n",
      "Ep done - 48010.\n",
      "Ep done - 48020.\n",
      "Ep done - 48030.\n",
      "Ep done - 48040.\n",
      "Ep done - 48050.\n",
      "Ep done - 48060.\n",
      "Ep done - 48070.\n",
      "Ep done - 48080.\n",
      "Ep done - 48090.\n",
      "Ep done - 12110.\n",
      "Ep done - 12120.\n",
      "Ep done - 12130.\n",
      "Ep done - 12140.\n",
      "Ep done - 12150.\n",
      "Ep done - 12160.\n",
      "Ep done - 12170.\n",
      "Ep done - 12180.\n",
      "Ep done - 12190.\n",
      "Ep done - 12200.\n",
      "Eval num_timesteps=1220000, episode_reward=-0.08 +/- 1.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1220000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67082053 |\n",
      "|    clip_fraction        | 0.0264     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00104   |\n",
      "|    explained_variance   | -1.31e-06  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.0039    |\n",
      "|    n_updates            | 21210      |\n",
      "|    policy_gradient_loss | -0.00523   |\n",
      "|    value_loss           | 0.0274     |\n",
      "----------------------------------------\n",
      "Ep done - 48100.\n",
      "Ep done - 48110.\n",
      "Ep done - 48120.\n",
      "Ep done - 48130.\n",
      "Ep done - 48140.\n",
      "Ep done - 48150.\n",
      "Ep done - 48160.\n",
      "Ep done - 48170.\n",
      "Ep done - 48180.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 199      |\n",
      "|    time_elapsed    | 3625     |\n",
      "|    total_timesteps | 1222656  |\n",
      "---------------------------------\n",
      "Ep done - 48190.\n",
      "Ep done - 48200.\n",
      "Ep done - 48210.\n",
      "Ep done - 48220.\n",
      "Ep done - 48230.\n",
      "Ep done - 48240.\n",
      "Ep done - 48250.\n",
      "Ep done - 48260.\n",
      "Ep done - 48270.\n",
      "Ep done - 48280.\n",
      "Ep done - 48290.\n",
      "Ep done - 48300.\n",
      "Ep done - 48310.\n",
      "Ep done - 48320.\n",
      "Ep done - 48330.\n",
      "Ep done - 48340.\n",
      "Ep done - 48350.\n",
      "Ep done - 48360.\n",
      "Ep done - 48370.\n",
      "Ep done - 48380.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0         |\n",
      "| time/                   |           |\n",
      "|    fps                  | 337       |\n",
      "|    iterations           | 200       |\n",
      "|    time_elapsed         | 3641      |\n",
      "|    total_timesteps      | 1228800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2224311 |\n",
      "|    clip_fraction        | 0.0686    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00371  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.151     |\n",
      "|    n_updates            | 21220     |\n",
      "|    policy_gradient_loss | -0.00876  |\n",
      "|    value_loss           | 0.304     |\n",
      "---------------------------------------\n",
      "Ep done - 48390.\n",
      "Ep done - 48400.\n",
      "Ep done - 48410.\n",
      "Ep done - 48420.\n",
      "Ep done - 12210.\n",
      "Ep done - 12220.\n",
      "Ep done - 12230.\n",
      "Ep done - 12240.\n",
      "Ep done - 12250.\n",
      "Ep done - 12260.\n",
      "Ep done - 12270.\n",
      "Ep done - 12280.\n",
      "Ep done - 12290.\n",
      "Ep done - 12300.\n",
      "Eval num_timesteps=1230000, episode_reward=-0.06 +/- 1.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.06      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1230000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77841353 |\n",
      "|    clip_fraction        | 0.0529     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00361   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.175      |\n",
      "|    n_updates            | 21230      |\n",
      "|    policy_gradient_loss | 0.00659    |\n",
      "|    value_loss           | 0.295      |\n",
      "----------------------------------------\n",
      "Ep done - 48430.\n",
      "Ep done - 48440.\n",
      "Ep done - 48450.\n",
      "Ep done - 48460.\n",
      "Ep done - 48470.\n",
      "Ep done - 48480.\n",
      "Ep done - 48490.\n",
      "Ep done - 48500.\n",
      "Ep done - 48510.\n",
      "Ep done - 48520.\n",
      "Ep done - 48530.\n",
      "Ep done - 48540.\n",
      "Ep done - 48550.\n",
      "Ep done - 48560.\n",
      "Ep done - 48570.\n",
      "Ep done - 48580.\n",
      "Ep done - 48590.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 3662     |\n",
      "|    total_timesteps | 1234944  |\n",
      "---------------------------------\n",
      "Ep done - 48600.\n",
      "Ep done - 48610.\n",
      "Ep done - 48620.\n",
      "Ep done - 48630.\n",
      "Ep done - 48640.\n",
      "Ep done - 48650.\n",
      "Ep done - 48660.\n",
      "Ep done - 48670.\n",
      "Ep done - 48680.\n",
      "Ep done - 48690.\n",
      "Ep done - 48700.\n",
      "Ep done - 48710.\n",
      "Ep done - 48720.\n",
      "Ep done - 48730.\n",
      "Ep done - 48740.\n",
      "Ep done - 48750.\n",
      "Ep done - 48760.\n",
      "Ep done - 12310.\n",
      "Ep done - 12320.\n",
      "Ep done - 12330.\n",
      "Ep done - 12340.\n",
      "Ep done - 12350.\n",
      "Ep done - 12360.\n",
      "Ep done - 12370.\n",
      "Ep done - 12380.\n",
      "Ep done - 12390.\n",
      "Ep done - 12400.\n",
      "Eval num_timesteps=1240000, episode_reward=0.02 +/- 1.00\n",
      "Episode length: 29.99 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.02      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1240000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7347489 |\n",
      "|    clip_fraction        | 0.0428    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00115  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.142     |\n",
      "|    n_updates            | 21240     |\n",
      "|    policy_gradient_loss | -0.00206  |\n",
      "|    value_loss           | 0.29      |\n",
      "---------------------------------------\n",
      "Ep done - 48770.\n",
      "Ep done - 48780.\n",
      "Ep done - 48790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 3684     |\n",
      "|    total_timesteps | 1241088  |\n",
      "---------------------------------\n",
      "Ep done - 48800.\n",
      "Ep done - 48810.\n",
      "Ep done - 48820.\n",
      "Ep done - 48830.\n",
      "Ep done - 48840.\n",
      "Ep done - 48850.\n",
      "Ep done - 48860.\n",
      "Ep done - 48870.\n",
      "Ep done - 48880.\n",
      "Ep done - 48890.\n",
      "Ep done - 48900.\n",
      "Ep done - 48910.\n",
      "Ep done - 48920.\n",
      "Ep done - 48930.\n",
      "Ep done - 48940.\n",
      "Ep done - 48950.\n",
      "Ep done - 48960.\n",
      "Ep done - 48970.\n",
      "Ep done - 48980.\n",
      "Ep done - 48990.\n",
      "Ep done - 49000.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.02      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 336       |\n",
      "|    iterations           | 203       |\n",
      "|    time_elapsed         | 3701      |\n",
      "|    total_timesteps      | 1247232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2855456 |\n",
      "|    clip_fraction        | 0.0153    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000879 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.127     |\n",
      "|    n_updates            | 21250     |\n",
      "|    policy_gradient_loss | -0.00259  |\n",
      "|    value_loss           | 0.295     |\n",
      "---------------------------------------\n",
      "Ep done - 49010.\n",
      "Ep done - 49020.\n",
      "Ep done - 49030.\n",
      "Ep done - 49040.\n",
      "Ep done - 49050.\n",
      "Ep done - 49060.\n",
      "Ep done - 49070.\n",
      "Ep done - 49080.\n",
      "Ep done - 49090.\n",
      "Ep done - 12410.\n",
      "Ep done - 12420.\n",
      "Ep done - 12430.\n",
      "Ep done - 12440.\n",
      "Ep done - 12450.\n",
      "Ep done - 12460.\n",
      "Ep done - 12470.\n",
      "Ep done - 12480.\n",
      "Ep done - 12490.\n",
      "Ep done - 12500.\n",
      "Eval num_timesteps=1250000, episode_reward=0.31 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 0.31        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1250000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124106295 |\n",
      "|    clip_fraction        | 0.0119      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000333   |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.188       |\n",
      "|    n_updates            | 21260       |\n",
      "|    policy_gradient_loss | 0.00833     |\n",
      "|    value_loss           | 0.299       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.31\n",
      "SELFPLAY: new best model, bumping up generation to 69\n",
      "Ep done - 49100.\n",
      "Ep done - 49110.\n",
      "Ep done - 49120.\n",
      "Ep done - 49130.\n",
      "Ep done - 49140.\n",
      "Ep done - 49150.\n",
      "Ep done - 49160.\n",
      "Ep done - 49170.\n",
      "Ep done - 49180.\n",
      "Ep done - 49190.\n",
      "Ep done - 49200.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.86    |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 3721     |\n",
      "|    total_timesteps | 1253376  |\n",
      "---------------------------------\n",
      "Ep done - 49210.\n",
      "Ep done - 49220.\n",
      "Ep done - 49230.\n",
      "Ep done - 49240.\n",
      "Ep done - 49250.\n",
      "Ep done - 49260.\n",
      "Ep done - 49270.\n",
      "Ep done - 49280.\n",
      "Ep done - 49290.\n",
      "Ep done - 49300.\n",
      "Ep done - 49310.\n",
      "Ep done - 49320.\n",
      "Ep done - 49330.\n",
      "Ep done - 49340.\n",
      "Ep done - 49350.\n",
      "Ep done - 49360.\n",
      "Ep done - 49370.\n",
      "Ep done - 49380.\n",
      "Ep done - 49390.\n",
      "Ep done - 49400.\n",
      "Ep done - 49410.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 337       |\n",
      "|    iterations           | 205       |\n",
      "|    time_elapsed         | 3736      |\n",
      "|    total_timesteps      | 1259520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9095149 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00875  |\n",
      "|    explained_variance   | -1.55e-06 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.127     |\n",
      "|    n_updates            | 21270     |\n",
      "|    policy_gradient_loss | 0.00334   |\n",
      "|    value_loss           | 0.248     |\n",
      "---------------------------------------\n",
      "Ep done - 49420.\n",
      "Ep done - 12510.\n",
      "Ep done - 12520.\n",
      "Ep done - 12530.\n",
      "Ep done - 12540.\n",
      "Ep done - 12550.\n",
      "Ep done - 12560.\n",
      "Ep done - 12570.\n",
      "Ep done - 12580.\n",
      "Ep done - 12590.\n",
      "Ep done - 12600.\n",
      "Eval num_timesteps=1260000, episode_reward=-0.96 +/- 0.28\n",
      "Episode length: 30.11 +/- 0.44\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.96     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1260000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6855698 |\n",
      "|    clip_fraction        | 0.0674    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00262  |\n",
      "|    explained_variance   | -0.537    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00157  |\n",
      "|    n_updates            | 21280     |\n",
      "|    policy_gradient_loss | -0.00975  |\n",
      "|    value_loss           | 0.026     |\n",
      "---------------------------------------\n",
      "Ep done - 49430.\n",
      "Ep done - 49440.\n",
      "Ep done - 49450.\n",
      "Ep done - 49460.\n",
      "Ep done - 49470.\n",
      "Ep done - 49480.\n",
      "Ep done - 49490.\n",
      "Ep done - 49500.\n",
      "Ep done - 49510.\n",
      "Ep done - 49520.\n",
      "Ep done - 49530.\n",
      "Ep done - 49540.\n",
      "Ep done - 49550.\n",
      "Ep done - 49560.\n",
      "Ep done - 49570.\n",
      "Ep done - 49580.\n",
      "Ep done - 49590.\n",
      "Ep done - 49600.\n",
      "Ep done - 49610.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 3757     |\n",
      "|    total_timesteps | 1265664  |\n",
      "---------------------------------\n",
      "Ep done - 49620.\n",
      "Ep done - 49630.\n",
      "Ep done - 49640.\n",
      "Ep done - 49650.\n",
      "Ep done - 49660.\n",
      "Ep done - 49670.\n",
      "Ep done - 49680.\n",
      "Ep done - 49690.\n",
      "Ep done - 49700.\n",
      "Ep done - 49710.\n",
      "Ep done - 49720.\n",
      "Ep done - 49730.\n",
      "Ep done - 49740.\n",
      "Ep done - 49750.\n",
      "Ep done - 49760.\n",
      "Ep done - 12610.\n",
      "Ep done - 12620.\n",
      "Ep done - 12630.\n",
      "Ep done - 12640.\n",
      "Ep done - 12650.\n",
      "Ep done - 12660.\n",
      "Ep done - 12670.\n",
      "Ep done - 12680.\n",
      "Ep done - 12690.\n",
      "Ep done - 12700.\n",
      "Eval num_timesteps=1270000, episode_reward=-0.66 +/- 0.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.66     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1270000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7418445 |\n",
      "|    clip_fraction        | 0.0813    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0033   |\n",
      "|    explained_variance   | -1.67e-06 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0103   |\n",
      "|    n_updates            | 21290     |\n",
      "|    policy_gradient_loss | -0.000489 |\n",
      "|    value_loss           | 0.0309    |\n",
      "---------------------------------------\n",
      "Ep done - 49770.\n",
      "Ep done - 49780.\n",
      "Ep done - 49790.\n",
      "Ep done - 49800.\n",
      "Ep done - 49810.\n",
      "Ep done - 49820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 3777     |\n",
      "|    total_timesteps | 1271808  |\n",
      "---------------------------------\n",
      "Ep done - 49830.\n",
      "Ep done - 49840.\n",
      "Ep done - 49850.\n",
      "Ep done - 49860.\n",
      "Ep done - 49870.\n",
      "Ep done - 49880.\n",
      "Ep done - 49890.\n",
      "Ep done - 49900.\n",
      "Ep done - 49910.\n",
      "Ep done - 49920.\n",
      "Ep done - 49930.\n",
      "Ep done - 49940.\n",
      "Ep done - 49950.\n",
      "Ep done - 49960.\n",
      "Ep done - 49970.\n",
      "Ep done - 49980.\n",
      "Ep done - 49990.\n",
      "Ep done - 50000.\n",
      "Ep done - 50010.\n",
      "Ep done - 50020.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.8      |\n",
      "|    ep_rew_mean          | -0.08     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 337       |\n",
      "|    iterations           | 208       |\n",
      "|    time_elapsed         | 3791      |\n",
      "|    total_timesteps      | 1277952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3817095 |\n",
      "|    clip_fraction        | 0.0584    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0119   |\n",
      "|    explained_variance   | -7.15e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0489    |\n",
      "|    n_updates            | 21300     |\n",
      "|    policy_gradient_loss | -0.00286  |\n",
      "|    value_loss           | 0.126     |\n",
      "---------------------------------------\n",
      "Ep done - 50030.\n",
      "Ep done - 50040.\n",
      "Ep done - 50050.\n",
      "Ep done - 50060.\n",
      "Ep done - 50070.\n",
      "Ep done - 50080.\n",
      "Ep done - 50090.\n",
      "Ep done - 12710.\n",
      "Ep done - 12720.\n",
      "Ep done - 12730.\n",
      "Ep done - 12740.\n",
      "Ep done - 12750.\n",
      "Ep done - 12760.\n",
      "Ep done - 12770.\n",
      "Ep done - 12780.\n",
      "Ep done - 12790.\n",
      "Ep done - 12800.\n",
      "Eval num_timesteps=1280000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.50 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.5      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1280000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2660584 |\n",
      "|    clip_fraction        | 0.019     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00311  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.145     |\n",
      "|    n_updates            | 21310     |\n",
      "|    policy_gradient_loss | 0.00127   |\n",
      "|    value_loss           | 0.316     |\n",
      "---------------------------------------\n",
      "Ep done - 50100.\n",
      "Ep done - 50110.\n",
      "Ep done - 50120.\n",
      "Ep done - 50130.\n",
      "Ep done - 50140.\n",
      "Ep done - 50150.\n",
      "Ep done - 50160.\n",
      "Ep done - 50170.\n",
      "Ep done - 50180.\n",
      "Ep done - 50190.\n",
      "Ep done - 50200.\n",
      "Ep done - 50210.\n",
      "Ep done - 50220.\n",
      "Ep done - 50230.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | -0.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 3811     |\n",
      "|    total_timesteps | 1284096  |\n",
      "---------------------------------\n",
      "Ep done - 50240.\n",
      "Ep done - 50250.\n",
      "Ep done - 50260.\n",
      "Ep done - 50270.\n",
      "Ep done - 50280.\n",
      "Ep done - 50290.\n",
      "Ep done - 50300.\n",
      "Ep done - 50310.\n",
      "Ep done - 50320.\n",
      "Ep done - 50330.\n",
      "Ep done - 50340.\n",
      "Ep done - 50350.\n",
      "Ep done - 50360.\n",
      "Ep done - 50370.\n",
      "Ep done - 50380.\n",
      "Ep done - 50390.\n",
      "Ep done - 50400.\n",
      "Ep done - 50410.\n",
      "Ep done - 50420.\n",
      "Ep done - 50430.\n",
      "Ep done - 12810.\n",
      "Ep done - 12820.\n",
      "Ep done - 12830.\n",
      "Ep done - 12840.\n",
      "Ep done - 12850.\n",
      "Ep done - 12860.\n",
      "Ep done - 12870.\n",
      "Ep done - 12880.\n",
      "Ep done - 12890.\n",
      "Ep done - 12900.\n",
      "Eval num_timesteps=1290000, episode_reward=-0.04 +/- 1.00\n",
      "Episode length: 30.76 +/- 0.91\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.8      |\n",
      "|    mean_reward          | -0.04     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1290000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0153989 |\n",
      "|    clip_fraction        | 0.0734    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00887  |\n",
      "|    explained_variance   | 0.00597   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.075     |\n",
      "|    n_updates            | 21320     |\n",
      "|    policy_gradient_loss | 0.024     |\n",
      "|    value_loss           | 0.0461    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.8     |\n",
      "|    ep_rew_mean     | -0.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 3834     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Ep done - 50440.\n",
      "Ep done - 50450.\n",
      "Ep done - 50460.\n",
      "Ep done - 50470.\n",
      "Ep done - 50480.\n",
      "Ep done - 50490.\n",
      "Ep done - 50500.\n",
      "Ep done - 50510.\n",
      "Ep done - 50520.\n",
      "Ep done - 50530.\n",
      "Ep done - 50540.\n",
      "Ep done - 50550.\n",
      "Ep done - 50560.\n",
      "Ep done - 50570.\n",
      "Ep done - 50580.\n",
      "Ep done - 50590.\n",
      "Ep done - 50600.\n",
      "Ep done - 50610.\n",
      "Ep done - 50620.\n",
      "Ep done - 50630.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 336        |\n",
      "|    iterations           | 211        |\n",
      "|    time_elapsed         | 3850       |\n",
      "|    total_timesteps      | 1296384    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40864912 |\n",
      "|    clip_fraction        | 0.0577     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00514   |\n",
      "|    explained_variance   | 0.0969     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.156      |\n",
      "|    n_updates            | 21330      |\n",
      "|    policy_gradient_loss | 0.00705    |\n",
      "|    value_loss           | 0.321      |\n",
      "----------------------------------------\n",
      "Ep done - 50640.\n",
      "Ep done - 50650.\n",
      "Ep done - 50660.\n",
      "Ep done - 50670.\n",
      "Ep done - 50680.\n",
      "Ep done - 50690.\n",
      "Ep done - 50700.\n",
      "Ep done - 50710.\n",
      "Ep done - 50720.\n",
      "Ep done - 50730.\n",
      "Ep done - 50740.\n",
      "Ep done - 50750.\n",
      "Ep done - 12910.\n",
      "Ep done - 12920.\n",
      "Ep done - 12930.\n",
      "Ep done - 12940.\n",
      "Ep done - 12950.\n",
      "Ep done - 12960.\n",
      "Ep done - 12970.\n",
      "Ep done - 12980.\n",
      "Ep done - 12990.\n",
      "Ep done - 13000.\n",
      "Eval num_timesteps=1300000, episode_reward=-0.10 +/- 0.99\n",
      "Episode length: 30.05 +/- 0.33\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1300000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1380392 |\n",
      "|    clip_fraction        | 0.0301    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000548 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.113     |\n",
      "|    n_updates            | 21340     |\n",
      "|    policy_gradient_loss | -0.0023   |\n",
      "|    value_loss           | 0.23      |\n",
      "---------------------------------------\n",
      "Ep done - 50760.\n",
      "Ep done - 50770.\n",
      "Ep done - 50780.\n",
      "Ep done - 50790.\n",
      "Ep done - 50800.\n",
      "Ep done - 50810.\n",
      "Ep done - 50820.\n",
      "Ep done - 50830.\n",
      "Ep done - 50840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 3869     |\n",
      "|    total_timesteps | 1302528  |\n",
      "---------------------------------\n",
      "Ep done - 50850.\n",
      "Ep done - 50860.\n",
      "Ep done - 50870.\n",
      "Ep done - 50880.\n",
      "Ep done - 50890.\n",
      "Ep done - 50900.\n",
      "Ep done - 50910.\n",
      "Ep done - 50920.\n",
      "Ep done - 50930.\n",
      "Ep done - 50940.\n",
      "Ep done - 50950.\n",
      "Ep done - 50960.\n",
      "Ep done - 50970.\n",
      "Ep done - 50980.\n",
      "Ep done - 50990.\n",
      "Ep done - 51000.\n",
      "Ep done - 51010.\n",
      "Ep done - 51020.\n",
      "Ep done - 51030.\n",
      "Ep done - 51040.\n",
      "Ep done - 51050.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.7       |\n",
      "|    ep_rew_mean          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 336        |\n",
      "|    iterations           | 213        |\n",
      "|    time_elapsed         | 3887       |\n",
      "|    total_timesteps      | 1308672    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68344665 |\n",
      "|    clip_fraction        | 0.0812     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00619   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.159      |\n",
      "|    n_updates            | 21350      |\n",
      "|    policy_gradient_loss | 0.0066     |\n",
      "|    value_loss           | 0.296      |\n",
      "----------------------------------------\n",
      "Ep done - 51060.\n",
      "Ep done - 51070.\n",
      "Ep done - 51080.\n",
      "Ep done - 51090.\n",
      "Ep done - 13010.\n",
      "Ep done - 13020.\n",
      "Ep done - 13030.\n",
      "Ep done - 13040.\n",
      "Ep done - 13050.\n",
      "Ep done - 13060.\n",
      "Ep done - 13070.\n",
      "Ep done - 13080.\n",
      "Ep done - 13090.\n",
      "Ep done - 13100.\n",
      "Eval num_timesteps=1310000, episode_reward=0.54 +/- 0.84\n",
      "Episode length: 30.06 +/- 0.24\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.54      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1310000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9451499 |\n",
      "|    clip_fraction        | 0.0471    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00224  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0686    |\n",
      "|    n_updates            | 21360     |\n",
      "|    policy_gradient_loss | -0.0035   |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.54\n",
      "SELFPLAY: new best model, bumping up generation to 70\n",
      "Ep done - 51100.\n",
      "Ep done - 51110.\n",
      "Ep done - 51120.\n",
      "Ep done - 51130.\n",
      "Ep done - 51140.\n",
      "Ep done - 51150.\n",
      "Ep done - 51160.\n",
      "Ep done - 51170.\n",
      "Ep done - 51180.\n",
      "Ep done - 51190.\n",
      "Ep done - 51200.\n",
      "Ep done - 51210.\n",
      "Ep done - 51220.\n",
      "Ep done - 51230.\n",
      "Ep done - 51240.\n",
      "Ep done - 51250.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 3910     |\n",
      "|    total_timesteps | 1314816  |\n",
      "---------------------------------\n",
      "Ep done - 51260.\n",
      "Ep done - 51270.\n",
      "Ep done - 51280.\n",
      "Ep done - 51290.\n",
      "Ep done - 51300.\n",
      "Ep done - 51310.\n",
      "Ep done - 51320.\n",
      "Ep done - 51330.\n",
      "Ep done - 51340.\n",
      "Ep done - 51350.\n",
      "Ep done - 51360.\n",
      "Ep done - 51370.\n",
      "Ep done - 51380.\n",
      "Ep done - 51390.\n",
      "Ep done - 51400.\n",
      "Ep done - 51410.\n",
      "Ep done - 51420.\n",
      "Ep done - 13110.\n",
      "Ep done - 13120.\n",
      "Ep done - 13130.\n",
      "Ep done - 13140.\n",
      "Ep done - 13150.\n",
      "Ep done - 13160.\n",
      "Ep done - 13170.\n",
      "Ep done - 13180.\n",
      "Ep done - 13190.\n",
      "Ep done - 13200.\n",
      "Eval num_timesteps=1320000, episode_reward=0.82 +/- 0.57\n",
      "Episode length: 30.90 +/- 0.30\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.9      |\n",
      "|    mean_reward          | 0.82      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1320000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.1191185 |\n",
      "|    clip_fraction        | 0.1       |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00764  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0962    |\n",
      "|    n_updates            | 21370     |\n",
      "|    policy_gradient_loss | -0.0136   |\n",
      "|    value_loss           | 0.218     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.82\n",
      "SELFPLAY: new best model, bumping up generation to 71\n",
      "Ep done - 51430.\n",
      "Ep done - 51440.\n",
      "Ep done - 51450.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.6     |\n",
      "|    ep_rew_mean     | 0.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 3930     |\n",
      "|    total_timesteps | 1320960  |\n",
      "---------------------------------\n",
      "Ep done - 51460.\n",
      "Ep done - 51470.\n",
      "Ep done - 51480.\n",
      "Ep done - 51490.\n",
      "Ep done - 51500.\n",
      "Ep done - 51510.\n",
      "Ep done - 51520.\n",
      "Ep done - 51530.\n",
      "Ep done - 51540.\n",
      "Ep done - 51550.\n",
      "Ep done - 51560.\n",
      "Ep done - 51570.\n",
      "Ep done - 51580.\n",
      "Ep done - 51590.\n",
      "Ep done - 51600.\n",
      "Ep done - 51610.\n",
      "Ep done - 51620.\n",
      "Ep done - 51630.\n",
      "Ep done - 51640.\n",
      "Ep done - 51650.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | -0.08    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 336      |\n",
      "|    iterations           | 216      |\n",
      "|    time_elapsed         | 3945     |\n",
      "|    total_timesteps      | 1327104  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.882706 |\n",
      "|    clip_fraction        | 0.104    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.0105  |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0569   |\n",
      "|    n_updates            | 21380    |\n",
      "|    policy_gradient_loss | 0.121    |\n",
      "|    value_loss           | 0.247    |\n",
      "--------------------------------------\n",
      "Ep done - 51660.\n",
      "Ep done - 51670.\n",
      "Ep done - 51680.\n",
      "Ep done - 51690.\n",
      "Ep done - 51700.\n",
      "Ep done - 51710.\n",
      "Ep done - 51720.\n",
      "Ep done - 51730.\n",
      "Ep done - 51740.\n",
      "Ep done - 51750.\n",
      "Ep done - 13210.\n",
      "Ep done - 13220.\n",
      "Ep done - 13230.\n",
      "Ep done - 13240.\n",
      "Ep done - 13250.\n",
      "Ep done - 13260.\n",
      "Ep done - 13270.\n",
      "Ep done - 13280.\n",
      "Ep done - 13290.\n",
      "Ep done - 13300.\n",
      "Eval num_timesteps=1330000, episode_reward=0.48 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.48       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1330000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.91605407 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00826   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.123      |\n",
      "|    n_updates            | 21390      |\n",
      "|    policy_gradient_loss | 0.0187     |\n",
      "|    value_loss           | 0.296      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.48\n",
      "SELFPLAY: new best model, bumping up generation to 72\n",
      "Ep done - 51760.\n",
      "Ep done - 51770.\n",
      "Ep done - 51780.\n",
      "Ep done - 51790.\n",
      "Ep done - 51800.\n",
      "Ep done - 51810.\n",
      "Ep done - 51820.\n",
      "Ep done - 51830.\n",
      "Ep done - 51840.\n",
      "Ep done - 51850.\n",
      "Ep done - 51860.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.95    |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 3967     |\n",
      "|    total_timesteps | 1333248  |\n",
      "---------------------------------\n",
      "Ep done - 51870.\n",
      "Ep done - 51880.\n",
      "Ep done - 51890.\n",
      "Ep done - 51900.\n",
      "Ep done - 51910.\n",
      "Ep done - 51920.\n",
      "Ep done - 51930.\n",
      "Ep done - 51940.\n",
      "Ep done - 51950.\n",
      "Ep done - 51960.\n",
      "Ep done - 51970.\n",
      "Ep done - 51980.\n",
      "Ep done - 51990.\n",
      "Ep done - 52000.\n",
      "Ep done - 52010.\n",
      "Ep done - 52020.\n",
      "Ep done - 52030.\n",
      "Ep done - 52040.\n",
      "Ep done - 52050.\n",
      "Ep done - 52060.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 336        |\n",
      "|    iterations           | 218        |\n",
      "|    time_elapsed         | 3984       |\n",
      "|    total_timesteps      | 1339392    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05543113 |\n",
      "|    clip_fraction        | 0.0207     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00565   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.185      |\n",
      "|    n_updates            | 21400      |\n",
      "|    policy_gradient_loss | 0.000644   |\n",
      "|    value_loss           | 0.281      |\n",
      "----------------------------------------\n",
      "Ep done - 52070.\n",
      "Ep done - 52080.\n",
      "Ep done - 13310.\n",
      "Ep done - 13320.\n",
      "Ep done - 13330.\n",
      "Ep done - 13340.\n",
      "Ep done - 13350.\n",
      "Ep done - 13360.\n",
      "Ep done - 13370.\n",
      "Ep done - 13380.\n",
      "Ep done - 13390.\n",
      "Ep done - 13400.\n",
      "Eval num_timesteps=1340000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1340000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5116194 |\n",
      "|    clip_fraction        | 0.0346    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00177  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0104    |\n",
      "|    n_updates            | 21410     |\n",
      "|    policy_gradient_loss | -0.00867  |\n",
      "|    value_loss           | 0.0526    |\n",
      "---------------------------------------\n",
      "Ep done - 52090.\n",
      "Ep done - 52100.\n",
      "Ep done - 52110.\n",
      "Ep done - 52120.\n",
      "Ep done - 52130.\n",
      "Ep done - 52140.\n",
      "Ep done - 52150.\n",
      "Ep done - 52160.\n",
      "Ep done - 52170.\n",
      "Ep done - 52180.\n",
      "Ep done - 52190.\n",
      "Ep done - 52200.\n",
      "Ep done - 52210.\n",
      "Ep done - 52220.\n",
      "Ep done - 52230.\n",
      "Ep done - 52240.\n",
      "Ep done - 52250.\n",
      "Ep done - 52260.\n",
      "Ep done - 52270.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 4005     |\n",
      "|    total_timesteps | 1345536  |\n",
      "---------------------------------\n",
      "Ep done - 52280.\n",
      "Ep done - 52290.\n",
      "Ep done - 52300.\n",
      "Ep done - 52310.\n",
      "Ep done - 52320.\n",
      "Ep done - 52330.\n",
      "Ep done - 52340.\n",
      "Ep done - 52350.\n",
      "Ep done - 52360.\n",
      "Ep done - 52370.\n",
      "Ep done - 52380.\n",
      "Ep done - 52390.\n",
      "Ep done - 52400.\n",
      "Ep done - 52410.\n",
      "Ep done - 52420.\n",
      "Ep done - 13410.\n",
      "Ep done - 13420.\n",
      "Ep done - 13430.\n",
      "Ep done - 13440.\n",
      "Ep done - 13450.\n",
      "Ep done - 13460.\n",
      "Ep done - 13470.\n",
      "Ep done - 13480.\n",
      "Ep done - 13490.\n",
      "Ep done - 13500.\n",
      "Eval num_timesteps=1350000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1350000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.769429  |\n",
      "|    clip_fraction        | 0.0615    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000451 |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0354   |\n",
      "|    n_updates            | 21420     |\n",
      "|    policy_gradient_loss | -0.0141   |\n",
      "|    value_loss           | 0.0276    |\n",
      "---------------------------------------\n",
      "Ep done - 52430.\n",
      "Ep done - 52440.\n",
      "Ep done - 52450.\n",
      "Ep done - 52460.\n",
      "Ep done - 52470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 4026     |\n",
      "|    total_timesteps | 1351680  |\n",
      "---------------------------------\n",
      "Ep done - 52480.\n",
      "Ep done - 52490.\n",
      "Ep done - 52500.\n",
      "Ep done - 52510.\n",
      "Ep done - 52520.\n",
      "Ep done - 52530.\n",
      "Ep done - 52540.\n",
      "Ep done - 52550.\n",
      "Ep done - 52560.\n",
      "Ep done - 52570.\n",
      "Ep done - 52580.\n",
      "Ep done - 52590.\n",
      "Ep done - 52600.\n",
      "Ep done - 52610.\n",
      "Ep done - 52620.\n",
      "Ep done - 52630.\n",
      "Ep done - 52640.\n",
      "Ep done - 52650.\n",
      "Ep done - 52660.\n",
      "Ep done - 52670.\n",
      "Ep done - 52680.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.62      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 335       |\n",
      "|    iterations           | 221       |\n",
      "|    time_elapsed         | 4043      |\n",
      "|    total_timesteps      | 1357824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9157186 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000245 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00666   |\n",
      "|    n_updates            | 21430     |\n",
      "|    policy_gradient_loss | -0.00487  |\n",
      "|    value_loss           | 0.0203    |\n",
      "---------------------------------------\n",
      "Ep done - 52690.\n",
      "Ep done - 52700.\n",
      "Ep done - 52710.\n",
      "Ep done - 52720.\n",
      "Ep done - 52730.\n",
      "Ep done - 52740.\n",
      "Ep done - 52750.\n",
      "Ep done - 13510.\n",
      "Ep done - 13520.\n",
      "Ep done - 13530.\n",
      "Ep done - 13540.\n",
      "Ep done - 13550.\n",
      "Ep done - 13560.\n",
      "Ep done - 13570.\n",
      "Ep done - 13580.\n",
      "Ep done - 13590.\n",
      "Ep done - 13600.\n",
      "Eval num_timesteps=1360000, episode_reward=0.52 +/- 0.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.52      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1360000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.24e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.158     |\n",
      "|    n_updates            | 21440     |\n",
      "|    policy_gradient_loss | -9.77e-10 |\n",
      "|    value_loss           | 0.305     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.52\n",
      "SELFPLAY: new best model, bumping up generation to 73\n",
      "Ep done - 52760.\n",
      "Ep done - 52770.\n",
      "Ep done - 52780.\n",
      "Ep done - 52790.\n",
      "Ep done - 52800.\n",
      "Ep done - 52810.\n",
      "Ep done - 52820.\n",
      "Ep done - 52830.\n",
      "Ep done - 52840.\n",
      "Ep done - 52850.\n",
      "Ep done - 52860.\n",
      "Ep done - 52870.\n",
      "Ep done - 52880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 4064     |\n",
      "|    total_timesteps | 1363968  |\n",
      "---------------------------------\n",
      "Ep done - 52890.\n",
      "Ep done - 52900.\n",
      "Ep done - 52910.\n",
      "Ep done - 52920.\n",
      "Ep done - 52930.\n",
      "Ep done - 52940.\n",
      "Ep done - 52950.\n",
      "Ep done - 52960.\n",
      "Ep done - 52970.\n",
      "Ep done - 52980.\n",
      "Ep done - 52990.\n",
      "Ep done - 53000.\n",
      "Ep done - 53010.\n",
      "Ep done - 53020.\n",
      "Ep done - 53030.\n",
      "Ep done - 53040.\n",
      "Ep done - 53050.\n",
      "Ep done - 53060.\n",
      "Ep done - 53070.\n",
      "Ep done - 53080.\n",
      "Ep done - 13610.\n",
      "Ep done - 13620.\n",
      "Ep done - 13630.\n",
      "Ep done - 13640.\n",
      "Ep done - 13650.\n",
      "Ep done - 13660.\n",
      "Ep done - 13670.\n",
      "Ep done - 13680.\n",
      "Ep done - 13690.\n",
      "Ep done - 13700.\n",
      "Eval num_timesteps=1370000, episode_reward=0.86 +/- 0.51\n",
      "Episode length: 30.03 +/- 0.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1370000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0796337 |\n",
      "|    clip_fraction        | 0.0378    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000385 |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.048     |\n",
      "|    n_updates            | 21450     |\n",
      "|    policy_gradient_loss | -0.00349  |\n",
      "|    value_loss           | 0.0906    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.86\n",
      "SELFPLAY: new best model, bumping up generation to 74\n",
      "Ep done - 53090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 4087     |\n",
      "|    total_timesteps | 1370112  |\n",
      "---------------------------------\n",
      "Ep done - 53100.\n",
      "Ep done - 53110.\n",
      "Ep done - 53120.\n",
      "Ep done - 53130.\n",
      "Ep done - 53140.\n",
      "Ep done - 53150.\n",
      "Ep done - 53160.\n",
      "Ep done - 53170.\n",
      "Ep done - 53180.\n",
      "Ep done - 53190.\n",
      "Ep done - 53200.\n",
      "Ep done - 53210.\n",
      "Ep done - 53220.\n",
      "Ep done - 53230.\n",
      "Ep done - 53240.\n",
      "Ep done - 53250.\n",
      "Ep done - 53260.\n",
      "Ep done - 53270.\n",
      "Ep done - 53280.\n",
      "Ep done - 53290.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | -0.94    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 335      |\n",
      "|    iterations           | 224      |\n",
      "|    time_elapsed         | 4103     |\n",
      "|    total_timesteps      | 1376256  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.397333 |\n",
      "|    clip_fraction        | 0.139    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00344 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0292   |\n",
      "|    n_updates            | 21460    |\n",
      "|    policy_gradient_loss | 0.0386   |\n",
      "|    value_loss           | 0.101    |\n",
      "--------------------------------------\n",
      "Ep done - 53300.\n",
      "Ep done - 53310.\n",
      "Ep done - 53320.\n",
      "Ep done - 53330.\n",
      "Ep done - 53340.\n",
      "Ep done - 53350.\n",
      "Ep done - 53360.\n",
      "Ep done - 53370.\n",
      "Ep done - 53380.\n",
      "Ep done - 53390.\n",
      "Ep done - 53400.\n",
      "Ep done - 53410.\n",
      "Ep done - 53420.\n",
      "Ep done - 13710.\n",
      "Ep done - 13720.\n",
      "Ep done - 13730.\n",
      "Ep done - 13740.\n",
      "Ep done - 13750.\n",
      "Ep done - 13760.\n",
      "Ep done - 13770.\n",
      "Ep done - 13780.\n",
      "Ep done - 13790.\n",
      "Ep done - 13800.\n",
      "Eval num_timesteps=1380000, episode_reward=-0.96 +/- 0.28\n",
      "Episode length: 30.01 +/- 0.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.96      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1380000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52562106 |\n",
      "|    clip_fraction        | 0.059      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00277   |\n",
      "|    explained_variance   | 2.38e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0176     |\n",
      "|    n_updates            | 21470      |\n",
      "|    policy_gradient_loss | 0.00465    |\n",
      "|    value_loss           | 0.131      |\n",
      "----------------------------------------\n",
      "Ep done - 53430.\n",
      "Ep done - 53440.\n",
      "Ep done - 53450.\n",
      "Ep done - 53460.\n",
      "Ep done - 53470.\n",
      "Ep done - 53480.\n",
      "Ep done - 53490.\n",
      "Ep done - 53500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 4124     |\n",
      "|    total_timesteps | 1382400  |\n",
      "---------------------------------\n",
      "Ep done - 53510.\n",
      "Ep done - 53520.\n",
      "Ep done - 53530.\n",
      "Ep done - 53540.\n",
      "Ep done - 53550.\n",
      "Ep done - 53560.\n",
      "Ep done - 53570.\n",
      "Ep done - 53580.\n",
      "Ep done - 53590.\n",
      "Ep done - 53600.\n",
      "Ep done - 53610.\n",
      "Ep done - 53620.\n",
      "Ep done - 53630.\n",
      "Ep done - 53640.\n",
      "Ep done - 53650.\n",
      "Ep done - 53660.\n",
      "Ep done - 53670.\n",
      "Ep done - 53680.\n",
      "Ep done - 53690.\n",
      "Ep done - 53700.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30.1     |\n",
      "|    ep_rew_mean          | -0.78    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 335      |\n",
      "|    iterations           | 226      |\n",
      "|    time_elapsed         | 4141     |\n",
      "|    total_timesteps      | 1388544  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 3.629162 |\n",
      "|    clip_fraction        | 0.199    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00242 |\n",
      "|    explained_variance   | 0.353    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | -0.0302  |\n",
      "|    n_updates            | 21480    |\n",
      "|    policy_gradient_loss | -0.0294  |\n",
      "|    value_loss           | 0.0194   |\n",
      "--------------------------------------\n",
      "Ep done - 53710.\n",
      "Ep done - 53720.\n",
      "Ep done - 53730.\n",
      "Ep done - 53740.\n",
      "Ep done - 53750.\n",
      "Ep done - 13810.\n",
      "Ep done - 13820.\n",
      "Ep done - 13830.\n",
      "Ep done - 13840.\n",
      "Ep done - 13850.\n",
      "Ep done - 13860.\n",
      "Ep done - 13870.\n",
      "Ep done - 13880.\n",
      "Ep done - 13890.\n",
      "Ep done - 13900.\n",
      "Eval num_timesteps=1390000, episode_reward=-0.64 +/- 0.77\n",
      "Episode length: 29.99 +/- 0.10\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.64    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 1390000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.871961 |\n",
      "|    clip_fraction        | 0.171    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00892 |\n",
      "|    explained_variance   | -0.0904  |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0509   |\n",
      "|    n_updates            | 21490    |\n",
      "|    policy_gradient_loss | 0.0152   |\n",
      "|    value_loss           | 0.109    |\n",
      "--------------------------------------\n",
      "Ep done - 53760.\n",
      "Ep done - 53770.\n",
      "Ep done - 53780.\n",
      "Ep done - 53790.\n",
      "Ep done - 53800.\n",
      "Ep done - 53810.\n",
      "Ep done - 53820.\n",
      "Ep done - 53830.\n",
      "Ep done - 53840.\n",
      "Ep done - 53850.\n",
      "Ep done - 53860.\n",
      "Ep done - 53870.\n",
      "Ep done - 53880.\n",
      "Ep done - 53890.\n",
      "Ep done - 53900.\n",
      "Ep done - 53910.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 4163     |\n",
      "|    total_timesteps | 1394688  |\n",
      "---------------------------------\n",
      "Ep done - 53920.\n",
      "Ep done - 53930.\n",
      "Ep done - 53940.\n",
      "Ep done - 53950.\n",
      "Ep done - 53960.\n",
      "Ep done - 53970.\n",
      "Ep done - 53980.\n",
      "Ep done - 53990.\n",
      "Ep done - 54000.\n",
      "Ep done - 54010.\n",
      "Ep done - 54020.\n",
      "Ep done - 54030.\n",
      "Ep done - 54040.\n",
      "Ep done - 54050.\n",
      "Ep done - 54060.\n",
      "Ep done - 54070.\n",
      "Ep done - 54080.\n",
      "Ep done - 13910.\n",
      "Ep done - 13920.\n",
      "Ep done - 13930.\n",
      "Ep done - 13940.\n",
      "Ep done - 13950.\n",
      "Ep done - 13960.\n",
      "Ep done - 13970.\n",
      "Ep done - 13980.\n",
      "Ep done - 13990.\n",
      "Ep done - 14000.\n",
      "Eval num_timesteps=1400000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.99 +/- 0.10\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -1       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 1400000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.140765 |\n",
      "|    clip_fraction        | 0.177    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00901 |\n",
      "|    explained_variance   | -0.0595  |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0676   |\n",
      "|    n_updates            | 21500    |\n",
      "|    policy_gradient_loss | 0.00794  |\n",
      "|    value_loss           | 0.129    |\n",
      "--------------------------------------\n",
      "Ep done - 54090.\n",
      "Ep done - 54100.\n",
      "Ep done - 54110.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 4186     |\n",
      "|    total_timesteps | 1400832  |\n",
      "---------------------------------\n",
      "Ep done - 54120.\n",
      "Ep done - 54130.\n",
      "Ep done - 54140.\n",
      "Ep done - 54150.\n",
      "Ep done - 54160.\n",
      "Ep done - 54170.\n",
      "Ep done - 54180.\n",
      "Ep done - 54190.\n",
      "Ep done - 54200.\n",
      "Ep done - 54210.\n",
      "Ep done - 54220.\n",
      "Ep done - 54230.\n",
      "Ep done - 54240.\n",
      "Ep done - 54250.\n",
      "Ep done - 54260.\n",
      "Ep done - 54270.\n",
      "Ep done - 54280.\n",
      "Ep done - 54290.\n",
      "Ep done - 54300.\n",
      "Ep done - 54310.\n",
      "Ep done - 54320.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | -0.88     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 334       |\n",
      "|    iterations           | 229       |\n",
      "|    time_elapsed         | 4202      |\n",
      "|    total_timesteps      | 1406976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8783995 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00063  |\n",
      "|    explained_variance   | 0.598     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.013    |\n",
      "|    n_updates            | 21510     |\n",
      "|    policy_gradient_loss | -0.00959  |\n",
      "|    value_loss           | 0.00345   |\n",
      "---------------------------------------\n",
      "Ep done - 54330.\n",
      "Ep done - 54340.\n",
      "Ep done - 54350.\n",
      "Ep done - 54360.\n",
      "Ep done - 54370.\n",
      "Ep done - 54380.\n",
      "Ep done - 54390.\n",
      "Ep done - 54400.\n",
      "Ep done - 54410.\n",
      "Ep done - 54420.\n",
      "Ep done - 14010.\n",
      "Ep done - 14020.\n",
      "Ep done - 14030.\n",
      "Ep done - 14040.\n",
      "Ep done - 14050.\n",
      "Ep done - 14060.\n",
      "Ep done - 14070.\n",
      "Ep done - 14080.\n",
      "Ep done - 14090.\n",
      "Ep done - 14100.\n",
      "Eval num_timesteps=1410000, episode_reward=-0.72 +/- 0.58\n",
      "Episode length: 29.97 +/- 0.30\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.72     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1410000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5699863 |\n",
      "|    clip_fraction        | 0.0677    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00411  |\n",
      "|    explained_variance   | -0.0145   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0121    |\n",
      "|    n_updates            | 21520     |\n",
      "|    policy_gradient_loss | 0.00316   |\n",
      "|    value_loss           | 0.0579    |\n",
      "---------------------------------------\n",
      "Ep done - 54430.\n",
      "Ep done - 54440.\n",
      "Ep done - 54450.\n",
      "Ep done - 54460.\n",
      "Ep done - 54470.\n",
      "Ep done - 54480.\n",
      "Ep done - 54490.\n",
      "Ep done - 54500.\n",
      "Ep done - 54510.\n",
      "Ep done - 54520.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 4222     |\n",
      "|    total_timesteps | 1413120  |\n",
      "---------------------------------\n",
      "Ep done - 54530.\n",
      "Ep done - 54540.\n",
      "Ep done - 54550.\n",
      "Ep done - 54560.\n",
      "Ep done - 54570.\n",
      "Ep done - 54580.\n",
      "Ep done - 54590.\n",
      "Ep done - 54600.\n",
      "Ep done - 54610.\n",
      "Ep done - 54620.\n",
      "Ep done - 54630.\n",
      "Ep done - 54640.\n",
      "Ep done - 54650.\n",
      "Ep done - 54660.\n",
      "Ep done - 54670.\n",
      "Ep done - 54680.\n",
      "Ep done - 54690.\n",
      "Ep done - 54700.\n",
      "Ep done - 54710.\n",
      "Ep done - 54720.\n",
      "Ep done - 54730.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | -0.23      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 334        |\n",
      "|    iterations           | 231        |\n",
      "|    time_elapsed         | 4238       |\n",
      "|    total_timesteps      | 1419264    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93057173 |\n",
      "|    clip_fraction        | 0.0556     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00358   |\n",
      "|    explained_variance   | -0.0773    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0542     |\n",
      "|    n_updates            | 21530      |\n",
      "|    policy_gradient_loss | 0.00773    |\n",
      "|    value_loss           | 0.134      |\n",
      "----------------------------------------\n",
      "Ep done - 54740.\n",
      "Ep done - 54750.\n",
      "Ep done - 14110.\n",
      "Ep done - 14120.\n",
      "Ep done - 14130.\n",
      "Ep done - 14140.\n",
      "Ep done - 14150.\n",
      "Ep done - 14160.\n",
      "Ep done - 14170.\n",
      "Ep done - 14180.\n",
      "Ep done - 14190.\n",
      "Ep done - 14200.\n",
      "Eval num_timesteps=1420000, episode_reward=-0.20 +/- 0.82\n",
      "Episode length: 29.82 +/- 0.38\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.8      |\n",
      "|    mean_reward          | -0.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1420000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1169498 |\n",
      "|    clip_fraction        | 0.0176    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00258  |\n",
      "|    explained_variance   | -0.0162   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.126     |\n",
      "|    n_updates            | 21540     |\n",
      "|    policy_gradient_loss | -0.000813 |\n",
      "|    value_loss           | 0.201     |\n",
      "---------------------------------------\n",
      "Ep done - 54760.\n",
      "Ep done - 54770.\n",
      "Ep done - 54780.\n",
      "Ep done - 54790.\n",
      "Ep done - 54800.\n",
      "Ep done - 54810.\n",
      "Ep done - 54820.\n",
      "Ep done - 54830.\n",
      "Ep done - 54840.\n",
      "Ep done - 54850.\n",
      "Ep done - 54860.\n",
      "Ep done - 54870.\n",
      "Ep done - 54880.\n",
      "Ep done - 54890.\n",
      "Ep done - 54900.\n",
      "Ep done - 54910.\n",
      "Ep done - 54920.\n",
      "Ep done - 54930.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | -0.21    |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 4261     |\n",
      "|    total_timesteps | 1425408  |\n",
      "---------------------------------\n",
      "Ep done - 54940.\n",
      "Ep done - 54950.\n",
      "Ep done - 54960.\n",
      "Ep done - 54970.\n",
      "Ep done - 54980.\n",
      "Ep done - 54990.\n",
      "Ep done - 55000.\n",
      "Ep done - 55010.\n",
      "Ep done - 55020.\n",
      "Ep done - 55030.\n",
      "Ep done - 55040.\n",
      "Ep done - 55050.\n",
      "Ep done - 55060.\n",
      "Ep done - 55070.\n",
      "Ep done - 55080.\n",
      "Ep done - 55090.\n",
      "Ep done - 14210.\n",
      "Ep done - 14220.\n",
      "Ep done - 14230.\n",
      "Ep done - 14240.\n",
      "Ep done - 14250.\n",
      "Ep done - 14260.\n",
      "Ep done - 14270.\n",
      "Ep done - 14280.\n",
      "Ep done - 14290.\n",
      "Ep done - 14300.\n",
      "Eval num_timesteps=1430000, episode_reward=-0.28 +/- 0.95\n",
      "Episode length: 29.82 +/- 0.41\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.8      |\n",
      "|    mean_reward          | -0.28     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1430000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4585384 |\n",
      "|    clip_fraction        | 0.04      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0027   |\n",
      "|    explained_variance   | 0.0693    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0845    |\n",
      "|    n_updates            | 21550     |\n",
      "|    policy_gradient_loss | -0.00272  |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "Ep done - 55100.\n",
      "Ep done - 55110.\n",
      "Ep done - 55120.\n",
      "Ep done - 55130.\n",
      "Ep done - 55140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | -0.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 4280     |\n",
      "|    total_timesteps | 1431552  |\n",
      "---------------------------------\n",
      "Ep done - 55150.\n",
      "Ep done - 55160.\n",
      "Ep done - 55170.\n",
      "Ep done - 55180.\n",
      "Ep done - 55190.\n",
      "Ep done - 55200.\n",
      "Ep done - 55210.\n",
      "Ep done - 55220.\n",
      "Ep done - 55230.\n",
      "Ep done - 55240.\n",
      "Ep done - 55250.\n",
      "Ep done - 55260.\n",
      "Ep done - 55270.\n",
      "Ep done - 55280.\n",
      "Ep done - 55290.\n",
      "Ep done - 55300.\n",
      "Ep done - 55310.\n",
      "Ep done - 55320.\n",
      "Ep done - 55330.\n",
      "Ep done - 55340.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 29.9     |\n",
      "|    ep_rew_mean          | -0.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 334      |\n",
      "|    iterations           | 234      |\n",
      "|    time_elapsed         | 4294     |\n",
      "|    total_timesteps      | 1437696  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.006669 |\n",
      "|    clip_fraction        | 0.0571   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00175 |\n",
      "|    explained_variance   | -0.0788  |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0818   |\n",
      "|    n_updates            | 21560    |\n",
      "|    policy_gradient_loss | -0.00677 |\n",
      "|    value_loss           | 0.206    |\n",
      "--------------------------------------\n",
      "Ep done - 55350.\n",
      "Ep done - 55360.\n",
      "Ep done - 55370.\n",
      "Ep done - 55380.\n",
      "Ep done - 55390.\n",
      "Ep done - 55400.\n",
      "Ep done - 55410.\n",
      "Ep done - 55420.\n",
      "Ep done - 14310.\n",
      "Ep done - 14320.\n",
      "Ep done - 14330.\n",
      "Ep done - 14340.\n",
      "Ep done - 14350.\n",
      "Ep done - 14360.\n",
      "Ep done - 14370.\n",
      "Ep done - 14380.\n",
      "Ep done - 14390.\n",
      "Ep done - 14400.\n",
      "Eval num_timesteps=1440000, episode_reward=-0.04 +/- 1.00\n",
      "Episode length: 29.95 +/- 0.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.9        |\n",
      "|    mean_reward          | -0.04       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028516596 |\n",
      "|    clip_fraction        | 0.0101      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.00133    |\n",
      "|    explained_variance   | 0.00883     |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.153       |\n",
      "|    n_updates            | 21570       |\n",
      "|    policy_gradient_loss | -0.00155    |\n",
      "|    value_loss           | 0.27        |\n",
      "-----------------------------------------\n",
      "Ep done - 55430.\n",
      "Ep done - 55440.\n",
      "Ep done - 55450.\n",
      "Ep done - 55460.\n",
      "Ep done - 55470.\n",
      "Ep done - 55480.\n",
      "Ep done - 55490.\n",
      "Ep done - 55500.\n",
      "Ep done - 55510.\n",
      "Ep done - 55520.\n",
      "Ep done - 55530.\n",
      "Ep done - 55540.\n",
      "Ep done - 55550.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 0.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 4312     |\n",
      "|    total_timesteps | 1443840  |\n",
      "---------------------------------\n",
      "Ep done - 55560.\n",
      "Ep done - 55570.\n",
      "Ep done - 55580.\n",
      "Ep done - 55590.\n",
      "Ep done - 55600.\n",
      "Ep done - 55610.\n",
      "Ep done - 55620.\n",
      "Ep done - 55630.\n",
      "Ep done - 55640.\n",
      "Ep done - 55650.\n",
      "Ep done - 55660.\n",
      "Ep done - 55670.\n",
      "Ep done - 55680.\n",
      "Ep done - 55690.\n",
      "Ep done - 55700.\n",
      "Ep done - 55710.\n",
      "Ep done - 55720.\n",
      "Ep done - 55730.\n",
      "Ep done - 55740.\n",
      "Ep done - 55750.\n",
      "Ep done - 55760.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | 0.16       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 335        |\n",
      "|    iterations           | 236        |\n",
      "|    time_elapsed         | 4326       |\n",
      "|    total_timesteps      | 1449984    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00710723 |\n",
      "|    clip_fraction        | 0.0012     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00062   |\n",
      "|    explained_variance   | 0.0563     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.159      |\n",
      "|    n_updates            | 21580      |\n",
      "|    policy_gradient_loss | -0.000377  |\n",
      "|    value_loss           | 0.301      |\n",
      "----------------------------------------\n",
      "Ep done - 14410.\n",
      "Ep done - 14420.\n",
      "Ep done - 14430.\n",
      "Ep done - 14440.\n",
      "Ep done - 14450.\n",
      "Ep done - 14460.\n",
      "Ep done - 14470.\n",
      "Ep done - 14480.\n",
      "Ep done - 14490.\n",
      "Ep done - 14500.\n",
      "Eval num_timesteps=1450000, episode_reward=-0.24 +/- 0.97\n",
      "Episode length: 29.94 +/- 0.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.9       |\n",
      "|    mean_reward          | -0.24      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1450000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.28066695 |\n",
      "|    clip_fraction        | 0.0232     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00102   |\n",
      "|    explained_variance   | 0.00075    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.119      |\n",
      "|    n_updates            | 21590      |\n",
      "|    policy_gradient_loss | -0.00316   |\n",
      "|    value_loss           | 0.294      |\n",
      "----------------------------------------\n",
      "Ep done - 55770.\n",
      "Ep done - 55780.\n",
      "Ep done - 55790.\n",
      "Ep done - 55800.\n",
      "Ep done - 55810.\n",
      "Ep done - 55820.\n",
      "Ep done - 55830.\n",
      "Ep done - 55840.\n",
      "Ep done - 55850.\n",
      "Ep done - 55860.\n",
      "Ep done - 55870.\n",
      "Ep done - 55880.\n",
      "Ep done - 55890.\n",
      "Ep done - 55900.\n",
      "Ep done - 55910.\n",
      "Ep done - 55920.\n",
      "Ep done - 55930.\n",
      "Ep done - 55940.\n",
      "Ep done - 55950.\n",
      "Ep done - 55960.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 4344     |\n",
      "|    total_timesteps | 1456128  |\n",
      "---------------------------------\n",
      "Ep done - 55970.\n",
      "Ep done - 55980.\n",
      "Ep done - 55990.\n",
      "Ep done - 56000.\n",
      "Ep done - 56010.\n",
      "Ep done - 56020.\n",
      "Ep done - 56030.\n",
      "Ep done - 56040.\n",
      "Ep done - 56050.\n",
      "Ep done - 56060.\n",
      "Ep done - 56070.\n",
      "Ep done - 56080.\n",
      "Ep done - 56090.\n",
      "Ep done - 14510.\n",
      "Ep done - 14520.\n",
      "Ep done - 14530.\n",
      "Ep done - 14540.\n",
      "Ep done - 14550.\n",
      "Ep done - 14560.\n",
      "Ep done - 14570.\n",
      "Ep done - 14580.\n",
      "Ep done - 14590.\n",
      "Ep done - 14600.\n",
      "Eval num_timesteps=1460000, episode_reward=-0.10 +/- 0.99\n",
      "Episode length: 29.90 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.9        |\n",
      "|    mean_reward          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066148974 |\n",
      "|    clip_fraction        | 0.0063      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.00151    |\n",
      "|    explained_variance   | -0.00801    |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 21600       |\n",
      "|    policy_gradient_loss | -0.000599   |\n",
      "|    value_loss           | 0.297       |\n",
      "-----------------------------------------\n",
      "Ep done - 56100.\n",
      "Ep done - 56110.\n",
      "Ep done - 56120.\n",
      "Ep done - 56130.\n",
      "Ep done - 56140.\n",
      "Ep done - 56150.\n",
      "Ep done - 56160.\n",
      "Ep done - 56170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 0.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 4361     |\n",
      "|    total_timesteps | 1462272  |\n",
      "---------------------------------\n",
      "Ep done - 56180.\n",
      "Ep done - 56190.\n",
      "Ep done - 56200.\n",
      "Ep done - 56210.\n",
      "Ep done - 56220.\n",
      "Ep done - 56230.\n",
      "Ep done - 56240.\n",
      "Ep done - 56250.\n",
      "Ep done - 56260.\n",
      "Ep done - 56270.\n",
      "Ep done - 56280.\n",
      "Ep done - 56290.\n",
      "Ep done - 56300.\n",
      "Ep done - 56310.\n",
      "Ep done - 56320.\n",
      "Ep done - 56330.\n",
      "Ep done - 56340.\n",
      "Ep done - 56350.\n",
      "Ep done - 56360.\n",
      "Ep done - 56370.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | 0.24       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 335        |\n",
      "|    iterations           | 239        |\n",
      "|    time_elapsed         | 4375       |\n",
      "|    total_timesteps      | 1468416    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49237815 |\n",
      "|    clip_fraction        | 0.0238     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000463  |\n",
      "|    explained_variance   | 0.00115    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.149      |\n",
      "|    n_updates            | 21610      |\n",
      "|    policy_gradient_loss | -0.003     |\n",
      "|    value_loss           | 0.294      |\n",
      "----------------------------------------\n",
      "Ep done - 56380.\n",
      "Ep done - 56390.\n",
      "Ep done - 56400.\n",
      "Ep done - 56410.\n",
      "Ep done - 56420.\n",
      "Ep done - 56430.\n",
      "Ep done - 14610.\n",
      "Ep done - 14620.\n",
      "Ep done - 14630.\n",
      "Ep done - 14640.\n",
      "Ep done - 14650.\n",
      "Ep done - 14660.\n",
      "Ep done - 14670.\n",
      "Ep done - 14680.\n",
      "Ep done - 14690.\n",
      "Ep done - 14700.\n",
      "Eval num_timesteps=1470000, episode_reward=-0.08 +/- 1.00\n",
      "Episode length: 29.65 +/- 0.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.6       |\n",
      "|    mean_reward          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1470000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09553093 |\n",
      "|    clip_fraction        | 0.0108     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000288  |\n",
      "|    explained_variance   | 0.0121     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.165      |\n",
      "|    n_updates            | 21620      |\n",
      "|    policy_gradient_loss | 0.00311    |\n",
      "|    value_loss           | 0.296      |\n",
      "----------------------------------------\n",
      "Ep done - 56440.\n",
      "Ep done - 56450.\n",
      "Ep done - 56460.\n",
      "Ep done - 56470.\n",
      "Ep done - 56480.\n",
      "Ep done - 56490.\n",
      "Ep done - 56500.\n",
      "Ep done - 56510.\n",
      "Ep done - 56520.\n",
      "Ep done - 56530.\n",
      "Ep done - 56540.\n",
      "Ep done - 56550.\n",
      "Ep done - 56560.\n",
      "Ep done - 56570.\n",
      "Ep done - 56580.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.6     |\n",
      "|    ep_rew_mean     | -0.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 4393     |\n",
      "|    total_timesteps | 1474560  |\n",
      "---------------------------------\n",
      "Ep done - 56590.\n",
      "Ep done - 56600.\n",
      "Ep done - 56610.\n",
      "Ep done - 56620.\n",
      "Ep done - 56630.\n",
      "Ep done - 56640.\n",
      "Ep done - 56650.\n",
      "Ep done - 56660.\n",
      "Ep done - 56670.\n",
      "Ep done - 56680.\n",
      "Ep done - 56690.\n",
      "Ep done - 56700.\n",
      "Ep done - 56710.\n",
      "Ep done - 56720.\n",
      "Ep done - 56730.\n",
      "Ep done - 56740.\n",
      "Ep done - 56750.\n",
      "Ep done - 56760.\n",
      "Ep done - 14710.\n",
      "Ep done - 14720.\n",
      "Ep done - 14730.\n",
      "Ep done - 14740.\n",
      "Ep done - 14750.\n",
      "Ep done - 14760.\n",
      "Ep done - 14770.\n",
      "Ep done - 14780.\n",
      "Ep done - 14790.\n",
      "Ep done - 14800.\n",
      "Eval num_timesteps=1480000, episode_reward=0.04 +/- 1.00\n",
      "Episode length: 30.08 +/- 0.61\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.04      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1480000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6841319 |\n",
      "|    clip_fraction        | 0.0317    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000549 |\n",
      "|    explained_variance   | -0.00242  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.201     |\n",
      "|    n_updates            | 21630     |\n",
      "|    policy_gradient_loss | 0.0036    |\n",
      "|    value_loss           | 0.294     |\n",
      "---------------------------------------\n",
      "Ep done - 56770.\n",
      "Ep done - 56780.\n",
      "Ep done - 56790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 4411     |\n",
      "|    total_timesteps | 1480704  |\n",
      "---------------------------------\n",
      "Ep done - 56800.\n",
      "Ep done - 56810.\n",
      "Ep done - 56820.\n",
      "Ep done - 56830.\n",
      "Ep done - 56840.\n",
      "Ep done - 56850.\n",
      "Ep done - 56860.\n",
      "Ep done - 56870.\n",
      "Ep done - 56880.\n",
      "Ep done - 56890.\n",
      "Ep done - 56900.\n",
      "Ep done - 56910.\n",
      "Ep done - 56920.\n",
      "Ep done - 56930.\n",
      "Ep done - 56940.\n",
      "Ep done - 56950.\n",
      "Ep done - 56960.\n",
      "Ep done - 56970.\n",
      "Ep done - 56980.\n",
      "Ep done - 56990.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.3      |\n",
      "|    ep_rew_mean          | 0.12      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 336       |\n",
      "|    iterations           | 242       |\n",
      "|    time_elapsed         | 4424      |\n",
      "|    total_timesteps      | 1486848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.164334  |\n",
      "|    clip_fraction        | 0.00202   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000203 |\n",
      "|    explained_variance   | -0.0118   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.244     |\n",
      "|    n_updates            | 21640     |\n",
      "|    policy_gradient_loss | 4.01e-05  |\n",
      "|    value_loss           | 0.291     |\n",
      "---------------------------------------\n",
      "Ep done - 57000.\n",
      "Ep done - 57010.\n",
      "Ep done - 57020.\n",
      "Ep done - 57030.\n",
      "Ep done - 57040.\n",
      "Ep done - 57050.\n",
      "Ep done - 57060.\n",
      "Ep done - 57070.\n",
      "Ep done - 57080.\n",
      "Ep done - 57090.\n",
      "Ep done - 14810.\n",
      "Ep done - 14820.\n",
      "Ep done - 14830.\n",
      "Ep done - 14840.\n",
      "Ep done - 14850.\n",
      "Ep done - 14860.\n",
      "Ep done - 14870.\n",
      "Ep done - 14880.\n",
      "Ep done - 14890.\n",
      "Ep done - 14900.\n",
      "Eval num_timesteps=1490000, episode_reward=0.36 +/- 0.88\n",
      "Episode length: 30.51 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.5       |\n",
      "|    mean_reward          | 0.36       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1490000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06360037 |\n",
      "|    clip_fraction        | 0.00301    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000248  |\n",
      "|    explained_variance   | -0.00764   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.159      |\n",
      "|    n_updates            | 21650      |\n",
      "|    policy_gradient_loss | -0.00057   |\n",
      "|    value_loss           | 0.273      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.36\n",
      "SELFPLAY: new best model, bumping up generation to 75\n",
      "Ep done - 57100.\n",
      "Ep done - 57110.\n",
      "Ep done - 57120.\n",
      "Ep done - 57130.\n",
      "Ep done - 57140.\n",
      "Ep done - 57150.\n",
      "Ep done - 57160.\n",
      "Ep done - 57170.\n",
      "Ep done - 57180.\n",
      "Ep done - 57190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 336      |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 4443     |\n",
      "|    total_timesteps | 1492992  |\n",
      "---------------------------------\n",
      "Ep done - 57200.\n",
      "Ep done - 57210.\n",
      "Ep done - 57220.\n",
      "Ep done - 57230.\n",
      "Ep done - 57240.\n",
      "Ep done - 57250.\n",
      "Ep done - 57260.\n",
      "Ep done - 57270.\n",
      "Ep done - 57280.\n",
      "Ep done - 57290.\n",
      "Ep done - 57300.\n",
      "Ep done - 57310.\n",
      "Ep done - 57320.\n",
      "Ep done - 57330.\n",
      "Ep done - 57340.\n",
      "Ep done - 57350.\n",
      "Ep done - 57360.\n",
      "Ep done - 57370.\n",
      "Ep done - 57380.\n",
      "Ep done - 57390.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 31       |\n",
      "|    ep_rew_mean          | 1        |\n",
      "| time/                   |          |\n",
      "|    fps                  | 335      |\n",
      "|    iterations           | 244      |\n",
      "|    time_elapsed         | 4463     |\n",
      "|    total_timesteps      | 1499136  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.023868 |\n",
      "|    clip_fraction        | 0.0833   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00232 |\n",
      "|    explained_variance   | -0.0553  |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.206    |\n",
      "|    n_updates            | 21660    |\n",
      "|    policy_gradient_loss | -0.00739 |\n",
      "|    value_loss           | 0.257    |\n",
      "--------------------------------------\n",
      "Ep done - 57400.\n",
      "Ep done - 57410.\n",
      "Ep done - 57420.\n",
      "Ep done - 14910.\n",
      "Ep done - 14920.\n",
      "Ep done - 14930.\n",
      "Ep done - 14940.\n",
      "Ep done - 14950.\n",
      "Ep done - 14960.\n",
      "Ep done - 14970.\n",
      "Ep done - 14980.\n",
      "Ep done - 14990.\n",
      "Ep done - 15000.\n",
      "Eval num_timesteps=1500000, episode_reward=-0.52 +/- 0.85\n",
      "Episode length: 29.84 +/- 0.66\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.8      |\n",
      "|    mean_reward          | -0.52     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1500000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5522827 |\n",
      "|    clip_fraction        | 0.0225    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000125 |\n",
      "|    explained_variance   | -0.288    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00642   |\n",
      "|    n_updates            | 21670     |\n",
      "|    policy_gradient_loss | -0.000329 |\n",
      "|    value_loss           | 0.00952   |\n",
      "---------------------------------------\n",
      "Ep done - 57430.\n",
      "Ep done - 57440.\n",
      "Ep done - 57450.\n",
      "Ep done - 57460.\n",
      "Ep done - 57470.\n",
      "Ep done - 57480.\n",
      "Ep done - 57490.\n",
      "Ep done - 57500.\n",
      "Ep done - 57510.\n",
      "Ep done - 57520.\n",
      "Ep done - 57530.\n",
      "Ep done - 57540.\n",
      "Ep done - 57550.\n",
      "Ep done - 57560.\n",
      "Ep done - 57570.\n",
      "Ep done - 57580.\n",
      "Ep done - 57590.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.41    |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 4486     |\n",
      "|    total_timesteps | 1505280  |\n",
      "---------------------------------\n",
      "Ep done - 57600.\n",
      "Ep done - 57610.\n",
      "Ep done - 57620.\n",
      "Ep done - 57630.\n",
      "Ep done - 57640.\n",
      "Ep done - 57650.\n",
      "Ep done - 57660.\n",
      "Ep done - 57670.\n",
      "Ep done - 57680.\n",
      "Ep done - 57690.\n",
      "Ep done - 57700.\n",
      "Ep done - 57710.\n",
      "Ep done - 57720.\n",
      "Ep done - 57730.\n",
      "Ep done - 57740.\n",
      "Ep done - 57750.\n",
      "Ep done - 15010.\n",
      "Ep done - 15020.\n",
      "Ep done - 15030.\n",
      "Ep done - 15040.\n",
      "Ep done - 15050.\n",
      "Ep done - 15060.\n",
      "Ep done - 15070.\n",
      "Ep done - 15080.\n",
      "Ep done - 15090.\n",
      "Ep done - 15100.\n",
      "Eval num_timesteps=1510000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.30 +/- 0.46\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.3     |\n",
      "|    mean_reward          | 0.98     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 1510000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.361148 |\n",
      "|    clip_fraction        | 0.0783   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00142 |\n",
      "|    explained_variance   | -0.304   |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.144    |\n",
      "|    n_updates            | 21680    |\n",
      "|    policy_gradient_loss | 0.00763  |\n",
      "|    value_loss           | 0.286    |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 76\n",
      "Ep done - 57760.\n",
      "Ep done - 57770.\n",
      "Ep done - 57780.\n",
      "Ep done - 57790.\n",
      "Ep done - 57800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 335      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 4508     |\n",
      "|    total_timesteps | 1511424  |\n",
      "---------------------------------\n",
      "Ep done - 57810.\n",
      "Ep done - 57820.\n",
      "Ep done - 57830.\n",
      "Ep done - 57840.\n",
      "Ep done - 57850.\n",
      "Ep done - 57860.\n",
      "Ep done - 57870.\n",
      "Ep done - 57880.\n",
      "Ep done - 57890.\n",
      "Ep done - 57900.\n",
      "Ep done - 57910.\n",
      "Ep done - 57920.\n",
      "Ep done - 57930.\n",
      "Ep done - 57940.\n",
      "Ep done - 57950.\n",
      "Ep done - 57960.\n",
      "Ep done - 57970.\n",
      "Ep done - 57980.\n",
      "Ep done - 57990.\n",
      "Ep done - 58000.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 335       |\n",
      "|    iterations           | 247       |\n",
      "|    time_elapsed         | 4529      |\n",
      "|    total_timesteps      | 1517568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.045586  |\n",
      "|    clip_fraction        | 0.0292    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000322 |\n",
      "|    explained_variance   | 1.49e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0482    |\n",
      "|    n_updates            | 21690     |\n",
      "|    policy_gradient_loss | -0.000316 |\n",
      "|    value_loss           | 0.0712    |\n",
      "---------------------------------------\n",
      "Ep done - 58010.\n",
      "Ep done - 58020.\n",
      "Ep done - 58030.\n",
      "Ep done - 58040.\n",
      "Ep done - 58050.\n",
      "Ep done - 58060.\n",
      "Ep done - 58070.\n",
      "Ep done - 58080.\n",
      "Ep done - 15110.\n",
      "Ep done - 15120.\n",
      "Ep done - 15130.\n",
      "Ep done - 15140.\n",
      "Ep done - 15150.\n",
      "Ep done - 15160.\n",
      "Ep done - 15170.\n",
      "Ep done - 15180.\n",
      "Ep done - 15190.\n",
      "Ep done - 15200.\n",
      "Eval num_timesteps=1520000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1520000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.7610023 |\n",
      "|    clip_fraction        | 0.126     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000447 |\n",
      "|    explained_variance   | -1.58     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00607  |\n",
      "|    n_updates            | 21700     |\n",
      "|    policy_gradient_loss | -0.0298   |\n",
      "|    value_loss           | 0.0866    |\n",
      "---------------------------------------\n",
      "Ep done - 58090.\n",
      "Ep done - 58100.\n",
      "Ep done - 58110.\n",
      "Ep done - 58120.\n",
      "Ep done - 58130.\n",
      "Ep done - 58140.\n",
      "Ep done - 58150.\n",
      "Ep done - 58160.\n",
      "Ep done - 58170.\n",
      "Ep done - 58180.\n",
      "Ep done - 58190.\n",
      "Ep done - 58200.\n",
      "Ep done - 58210.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 333      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 4564     |\n",
      "|    total_timesteps | 1523712  |\n",
      "---------------------------------\n",
      "Ep done - 58220.\n",
      "Ep done - 58230.\n",
      "Ep done - 58240.\n",
      "Ep done - 58250.\n",
      "Ep done - 58260.\n",
      "Ep done - 58270.\n",
      "Ep done - 58280.\n",
      "Ep done - 58290.\n",
      "Ep done - 58300.\n",
      "Ep done - 58310.\n",
      "Ep done - 58320.\n",
      "Ep done - 58330.\n",
      "Ep done - 58340.\n",
      "Ep done - 58350.\n",
      "Ep done - 58360.\n",
      "Ep done - 58370.\n",
      "Ep done - 58380.\n",
      "Ep done - 58390.\n",
      "Ep done - 58400.\n",
      "Ep done - 58410.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 1         |\n",
      "| time/                   |           |\n",
      "|    fps                  | 329       |\n",
      "|    iterations           | 249       |\n",
      "|    time_elapsed         | 4640      |\n",
      "|    total_timesteps      | 1529856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8374345 |\n",
      "|    clip_fraction        | 0.0582    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000228 |\n",
      "|    explained_variance   | -1.87e-05 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00825   |\n",
      "|    n_updates            | 21710     |\n",
      "|    policy_gradient_loss | -0.0076   |\n",
      "|    value_loss           | 0.0389    |\n",
      "---------------------------------------\n",
      "Ep done - 58420.\n",
      "Ep done - 15210.\n",
      "Ep done - 15220.\n",
      "Ep done - 15230.\n",
      "Ep done - 15240.\n",
      "Ep done - 15250.\n",
      "Ep done - 15260.\n",
      "Ep done - 15270.\n",
      "Ep done - 15280.\n",
      "Ep done - 15290.\n",
      "Ep done - 15300.\n",
      "Eval num_timesteps=1530000, episode_reward=0.14 +/- 0.99\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.14      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1530000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 6.3537765 |\n",
      "|    clip_fraction        | 0.135     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00102  |\n",
      "|    explained_variance   | 5.78e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0133   |\n",
      "|    n_updates            | 21720     |\n",
      "|    policy_gradient_loss | -0.0123   |\n",
      "|    value_loss           | 0.0542    |\n",
      "---------------------------------------\n",
      "Ep done - 58430.\n",
      "Ep done - 58440.\n",
      "Ep done - 58450.\n",
      "Ep done - 58460.\n",
      "Ep done - 58470.\n",
      "Ep done - 58480.\n",
      "Ep done - 58490.\n",
      "Ep done - 58500.\n",
      "Ep done - 58510.\n",
      "Ep done - 58520.\n",
      "Ep done - 58530.\n",
      "Ep done - 58540.\n",
      "Ep done - 58550.\n",
      "Ep done - 58560.\n",
      "Ep done - 58570.\n",
      "Ep done - 58580.\n",
      "Ep done - 58590.\n",
      "Ep done - 58600.\n",
      "Ep done - 58610.\n",
      "Ep done - 58620.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 324      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 4730     |\n",
      "|    total_timesteps | 1536000  |\n",
      "---------------------------------\n",
      "Ep done - 58630.\n",
      "Ep done - 58640.\n",
      "Ep done - 58650.\n",
      "Ep done - 58660.\n",
      "Ep done - 58670.\n",
      "Ep done - 58680.\n",
      "Ep done - 58690.\n",
      "Ep done - 58700.\n",
      "Ep done - 58710.\n",
      "Ep done - 58720.\n",
      "Ep done - 58730.\n",
      "Ep done - 58740.\n",
      "Ep done - 58750.\n",
      "Ep done - 15310.\n",
      "Ep done - 15320.\n",
      "Ep done - 15330.\n",
      "Ep done - 15340.\n",
      "Ep done - 15350.\n",
      "Ep done - 15360.\n",
      "Ep done - 15370.\n",
      "Ep done - 15380.\n",
      "Ep done - 15390.\n",
      "Ep done - 15400.\n",
      "Eval num_timesteps=1540000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1540000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5251762 |\n",
      "|    clip_fraction        | 0.0493    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000191 |\n",
      "|    explained_variance   | -0.356    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.149     |\n",
      "|    n_updates            | 21730     |\n",
      "|    policy_gradient_loss | -0.00711  |\n",
      "|    value_loss           | 0.302     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 77\n",
      "Ep done - 58760.\n",
      "Ep done - 58770.\n",
      "Ep done - 58780.\n",
      "Ep done - 58790.\n",
      "Ep done - 58800.\n",
      "Ep done - 58810.\n",
      "Ep done - 58820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 319      |\n",
      "|    iterations      | 251      |\n",
      "|    time_elapsed    | 4827     |\n",
      "|    total_timesteps | 1542144  |\n",
      "---------------------------------\n",
      "Ep done - 58830.\n",
      "Ep done - 58840.\n",
      "Ep done - 58850.\n",
      "Ep done - 58860.\n",
      "Ep done - 58870.\n",
      "Ep done - 58880.\n",
      "Ep done - 58890.\n",
      "Ep done - 58900.\n",
      "Ep done - 58910.\n",
      "Ep done - 58920.\n",
      "Ep done - 58930.\n",
      "Ep done - 58940.\n",
      "Ep done - 58950.\n",
      "Ep done - 58960.\n",
      "Ep done - 58970.\n",
      "Ep done - 58980.\n",
      "Ep done - 58990.\n",
      "Ep done - 59000.\n",
      "Ep done - 59010.\n",
      "Ep done - 59020.\n",
      "Ep done - 59030.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 316        |\n",
      "|    iterations           | 252        |\n",
      "|    time_elapsed         | 4890       |\n",
      "|    total_timesteps      | 1548288    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34613773 |\n",
      "|    clip_fraction        | 0.0197     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000166  |\n",
      "|    explained_variance   | 0.356      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00709    |\n",
      "|    n_updates            | 21740      |\n",
      "|    policy_gradient_loss | -0.000394  |\n",
      "|    value_loss           | 0.0112     |\n",
      "----------------------------------------\n",
      "Ep done - 59040.\n",
      "Ep done - 59050.\n",
      "Ep done - 59060.\n",
      "Ep done - 59070.\n",
      "Ep done - 59080.\n",
      "Ep done - 15410.\n",
      "Ep done - 15420.\n",
      "Ep done - 15430.\n",
      "Ep done - 15440.\n",
      "Ep done - 15450.\n",
      "Ep done - 15460.\n",
      "Ep done - 15470.\n",
      "Ep done - 15480.\n",
      "Ep done - 15490.\n",
      "Ep done - 15500.\n",
      "Eval num_timesteps=1550000, episode_reward=-0.24 +/- 0.97\n",
      "Episode length: 30.36 +/- 0.48\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.4      |\n",
      "|    mean_reward          | -0.24     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1550000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6315037 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00438  |\n",
      "|    explained_variance   | -1.36     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00256  |\n",
      "|    n_updates            | 21750     |\n",
      "|    policy_gradient_loss | -0.00656  |\n",
      "|    value_loss           | 0.0282    |\n",
      "---------------------------------------\n",
      "Ep done - 59090.\n",
      "Ep done - 59100.\n",
      "Ep done - 59110.\n",
      "Ep done - 59120.\n",
      "Ep done - 59130.\n",
      "Ep done - 59140.\n",
      "Ep done - 59150.\n",
      "Ep done - 59160.\n",
      "Ep done - 59170.\n",
      "Ep done - 59180.\n",
      "Ep done - 59190.\n",
      "Ep done - 59200.\n",
      "Ep done - 59210.\n",
      "Ep done - 59220.\n",
      "Ep done - 59230.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | -0.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 313      |\n",
      "|    iterations      | 253      |\n",
      "|    time_elapsed    | 4957     |\n",
      "|    total_timesteps | 1554432  |\n",
      "---------------------------------\n",
      "Ep done - 59240.\n",
      "Ep done - 59250.\n",
      "Ep done - 59260.\n",
      "Ep done - 59270.\n",
      "Ep done - 59280.\n",
      "Ep done - 59290.\n",
      "Ep done - 59300.\n",
      "Ep done - 59310.\n",
      "Ep done - 59320.\n",
      "Ep done - 59330.\n",
      "Ep done - 59340.\n",
      "Ep done - 59350.\n",
      "Ep done - 59360.\n",
      "Ep done - 59370.\n",
      "Ep done - 59380.\n",
      "Ep done - 59390.\n",
      "Ep done - 59400.\n",
      "Ep done - 59410.\n",
      "Ep done - 15510.\n",
      "Ep done - 15520.\n",
      "Ep done - 15530.\n",
      "Ep done - 15540.\n",
      "Ep done - 15550.\n",
      "Ep done - 15560.\n",
      "Ep done - 15570.\n",
      "Ep done - 15580.\n",
      "Ep done - 15590.\n",
      "Ep done - 15600.\n",
      "Eval num_timesteps=1560000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 30.70 +/- 0.64\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.7      |\n",
      "|    mean_reward          | 0.6       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1560000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5921642 |\n",
      "|    clip_fraction        | 0.0649    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000997 |\n",
      "|    explained_variance   | -0.25     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.173     |\n",
      "|    n_updates            | 21760     |\n",
      "|    policy_gradient_loss | -0.0042   |\n",
      "|    value_loss           | 0.277     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.6\n",
      "SELFPLAY: new best model, bumping up generation to 78\n",
      "Ep done - 59420.\n",
      "Ep done - 59430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.5     |\n",
      "|    ep_rew_mean     | 0.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 310      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 5031     |\n",
      "|    total_timesteps | 1560576  |\n",
      "---------------------------------\n",
      "Ep done - 59440.\n",
      "Ep done - 59450.\n",
      "Ep done - 59460.\n",
      "Ep done - 59470.\n",
      "Ep done - 59480.\n",
      "Ep done - 59490.\n",
      "Ep done - 59500.\n",
      "Ep done - 59510.\n",
      "Ep done - 59520.\n",
      "Ep done - 59530.\n",
      "Ep done - 59540.\n",
      "Ep done - 59550.\n",
      "Ep done - 59560.\n",
      "Ep done - 59570.\n",
      "Ep done - 59580.\n",
      "Ep done - 59590.\n",
      "Ep done - 59600.\n",
      "Ep done - 59610.\n",
      "Ep done - 59620.\n",
      "Ep done - 59630.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.94      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 306       |\n",
      "|    iterations           | 255       |\n",
      "|    time_elapsed         | 5109      |\n",
      "|    total_timesteps      | 1566720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5838487 |\n",
      "|    clip_fraction        | 0.0224    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00165  |\n",
      "|    explained_variance   | -0.0377   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0824    |\n",
      "|    n_updates            | 21770     |\n",
      "|    policy_gradient_loss | -0.00622  |\n",
      "|    value_loss           | 0.207     |\n",
      "---------------------------------------\n",
      "Ep done - 59640.\n",
      "Ep done - 59650.\n",
      "Ep done - 59660.\n",
      "Ep done - 59670.\n",
      "Ep done - 59680.\n",
      "Ep done - 59690.\n",
      "Ep done - 59700.\n",
      "Ep done - 59710.\n",
      "Ep done - 59720.\n",
      "Ep done - 59730.\n",
      "Ep done - 59740.\n",
      "Ep done - 15610.\n",
      "Ep done - 15620.\n",
      "Ep done - 15630.\n",
      "Ep done - 15640.\n",
      "Ep done - 15650.\n",
      "Ep done - 15660.\n",
      "Ep done - 15670.\n",
      "Ep done - 15680.\n",
      "Ep done - 15690.\n",
      "Ep done - 15700.\n",
      "Eval num_timesteps=1570000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1570000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18855011 |\n",
      "|    clip_fraction        | 0.00871    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000235  |\n",
      "|    explained_variance   | 0.116      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0355     |\n",
      "|    n_updates            | 21780      |\n",
      "|    policy_gradient_loss | -0.00135   |\n",
      "|    value_loss           | 0.0952     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 79\n",
      "Ep done - 59750.\n",
      "Ep done - 59760.\n",
      "Ep done - 59770.\n",
      "Ep done - 59780.\n",
      "Ep done - 59790.\n",
      "Ep done - 59800.\n",
      "Ep done - 59810.\n",
      "Ep done - 59820.\n",
      "Ep done - 59830.\n",
      "Ep done - 59840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 303      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 5189     |\n",
      "|    total_timesteps | 1572864  |\n",
      "---------------------------------\n",
      "Ep done - 59850.\n",
      "Ep done - 59860.\n",
      "Ep done - 59870.\n",
      "Ep done - 59880.\n",
      "Ep done - 59890.\n",
      "Ep done - 59900.\n",
      "Ep done - 59910.\n",
      "Ep done - 59920.\n",
      "Ep done - 59930.\n",
      "Ep done - 59940.\n",
      "Ep done - 59950.\n",
      "Ep done - 59960.\n",
      "Ep done - 59970.\n",
      "Ep done - 59980.\n",
      "Ep done - 59990.\n",
      "Ep done - 60000.\n",
      "Ep done - 60010.\n",
      "Ep done - 60020.\n",
      "Ep done - 60030.\n",
      "Ep done - 60040.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | 0.48       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 300        |\n",
      "|    iterations           | 257        |\n",
      "|    time_elapsed         | 5259       |\n",
      "|    total_timesteps      | 1579008    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38085583 |\n",
      "|    clip_fraction        | 0.0194     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000419  |\n",
      "|    explained_variance   | -0.169     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.142      |\n",
      "|    n_updates            | 21790      |\n",
      "|    policy_gradient_loss | 0.0102     |\n",
      "|    value_loss           | 0.255      |\n",
      "----------------------------------------\n",
      "Ep done - 60050.\n",
      "Ep done - 60060.\n",
      "Ep done - 60070.\n",
      "Ep done - 15710.\n",
      "Ep done - 15720.\n",
      "Ep done - 15730.\n",
      "Ep done - 15740.\n",
      "Ep done - 15750.\n",
      "Ep done - 15760.\n",
      "Ep done - 15770.\n",
      "Ep done - 15780.\n",
      "Ep done - 15790.\n",
      "Ep done - 15800.\n",
      "Eval num_timesteps=1580000, episode_reward=0.46 +/- 0.89\n",
      "Episode length: 30.01 +/- 1.14\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.46      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1580000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3123506 |\n",
      "|    clip_fraction        | 0.0155    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000145 |\n",
      "|    explained_variance   | 0.07      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0766    |\n",
      "|    n_updates            | 21800     |\n",
      "|    policy_gradient_loss | -0.0059   |\n",
      "|    value_loss           | 0.224     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.46\n",
      "SELFPLAY: new best model, bumping up generation to 80\n",
      "Ep done - 60080.\n",
      "Ep done - 60090.\n",
      "Ep done - 60100.\n",
      "Ep done - 60110.\n",
      "Ep done - 60120.\n",
      "Ep done - 60130.\n",
      "Ep done - 60140.\n",
      "Ep done - 60150.\n",
      "Ep done - 60160.\n",
      "Ep done - 60170.\n",
      "Ep done - 60180.\n",
      "Ep done - 60190.\n",
      "Ep done - 60200.\n",
      "Ep done - 60210.\n",
      "Ep done - 60220.\n",
      "Ep done - 60230.\n",
      "Ep done - 60240.\n",
      "Ep done - 60250.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.6     |\n",
      "|    ep_rew_mean     | 0.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 297      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 5323     |\n",
      "|    total_timesteps | 1585152  |\n",
      "---------------------------------\n",
      "Ep done - 60260.\n",
      "Ep done - 60270.\n",
      "Ep done - 60280.\n",
      "Ep done - 60290.\n",
      "Ep done - 60300.\n",
      "Ep done - 60310.\n",
      "Ep done - 60320.\n",
      "Ep done - 60330.\n",
      "Ep done - 60340.\n",
      "Ep done - 60350.\n",
      "Ep done - 60360.\n",
      "Ep done - 60370.\n",
      "Ep done - 60380.\n",
      "Ep done - 60390.\n",
      "Ep done - 60400.\n",
      "Ep done - 60410.\n",
      "Ep done - 15810.\n",
      "Ep done - 15820.\n",
      "Ep done - 15830.\n",
      "Ep done - 15840.\n",
      "Ep done - 15850.\n",
      "Ep done - 15860.\n",
      "Ep done - 15870.\n",
      "Ep done - 15880.\n",
      "Ep done - 15890.\n",
      "Ep done - 15900.\n",
      "Eval num_timesteps=1590000, episode_reward=0.52 +/- 0.85\n",
      "Episode length: 30.09 +/- 0.29\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1590000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05813394 |\n",
      "|    clip_fraction        | 0.00863    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000105  |\n",
      "|    explained_variance   | 0.0115     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.078      |\n",
      "|    n_updates            | 21810      |\n",
      "|    policy_gradient_loss | -0.00371   |\n",
      "|    value_loss           | 0.23       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.52\n",
      "SELFPLAY: new best model, bumping up generation to 81\n",
      "Ep done - 60420.\n",
      "Ep done - 60430.\n",
      "Ep done - 60440.\n",
      "Ep done - 60450.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 295      |\n",
      "|    iterations      | 259      |\n",
      "|    time_elapsed    | 5388     |\n",
      "|    total_timesteps | 1591296  |\n",
      "---------------------------------\n",
      "Ep done - 60460.\n",
      "Ep done - 60470.\n",
      "Ep done - 60480.\n",
      "Ep done - 60490.\n",
      "Ep done - 60500.\n",
      "Ep done - 60510.\n",
      "Ep done - 60520.\n",
      "Ep done - 60530.\n",
      "Ep done - 60540.\n",
      "Ep done - 60550.\n",
      "Ep done - 60560.\n",
      "Ep done - 60570.\n",
      "Ep done - 60580.\n",
      "Ep done - 60590.\n",
      "Ep done - 60600.\n",
      "Ep done - 60610.\n",
      "Ep done - 60620.\n",
      "Ep done - 60630.\n",
      "Ep done - 60640.\n",
      "Ep done - 60650.\n",
      "Ep done - 60660.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.5      |\n",
      "|    ep_rew_mean          | 0.2       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 292       |\n",
      "|    iterations           | 260       |\n",
      "|    time_elapsed         | 5452      |\n",
      "|    total_timesteps      | 1597440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7290892 |\n",
      "|    clip_fraction        | 0.0205    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000363 |\n",
      "|    explained_variance   | -0.0614   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.147     |\n",
      "|    n_updates            | 21820     |\n",
      "|    policy_gradient_loss | 0.568     |\n",
      "|    value_loss           | 0.211     |\n",
      "---------------------------------------\n",
      "Ep done - 60670.\n",
      "Ep done - 60680.\n",
      "Ep done - 60690.\n",
      "Ep done - 60700.\n",
      "Ep done - 60710.\n",
      "Ep done - 60720.\n",
      "Ep done - 60730.\n",
      "Ep done - 60740.\n",
      "Ep done - 60750.\n",
      "Ep done - 15910.\n",
      "Ep done - 15920.\n",
      "Ep done - 15930.\n",
      "Ep done - 15940.\n",
      "Ep done - 15950.\n",
      "Ep done - 15960.\n",
      "Ep done - 15970.\n",
      "Ep done - 15980.\n",
      "Ep done - 15990.\n",
      "Ep done - 16000.\n",
      "Eval num_timesteps=1600000, episode_reward=0.66 +/- 0.75\n",
      "Episode length: 29.81 +/- 0.86\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.8       |\n",
      "|    mean_reward          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1600000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35295117 |\n",
      "|    clip_fraction        | 0.0115     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.88e-05  |\n",
      "|    explained_variance   | 0.101      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.125      |\n",
      "|    n_updates            | 21830      |\n",
      "|    policy_gradient_loss | -0.00407   |\n",
      "|    value_loss           | 0.252      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.66\n",
      "SELFPLAY: new best model, bumping up generation to 82\n",
      "Ep done - 60760.\n",
      "Ep done - 60770.\n",
      "Ep done - 60780.\n",
      "Ep done - 60790.\n",
      "Ep done - 60800.\n",
      "Ep done - 60810.\n",
      "Ep done - 60820.\n",
      "Ep done - 60830.\n",
      "Ep done - 60840.\n",
      "Ep done - 60850.\n",
      "Ep done - 60860.\n",
      "Ep done - 60870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 290      |\n",
      "|    iterations      | 261      |\n",
      "|    time_elapsed    | 5521     |\n",
      "|    total_timesteps | 1603584  |\n",
      "---------------------------------\n",
      "Ep done - 60880.\n",
      "Ep done - 60890.\n",
      "Ep done - 60900.\n",
      "Ep done - 60910.\n",
      "Ep done - 60920.\n",
      "Ep done - 60930.\n",
      "Ep done - 60940.\n",
      "Ep done - 60950.\n",
      "Ep done - 60960.\n",
      "Ep done - 60970.\n",
      "Ep done - 60980.\n",
      "Ep done - 60990.\n",
      "Ep done - 61000.\n",
      "Ep done - 61010.\n",
      "Ep done - 61020.\n",
      "Ep done - 61030.\n",
      "Ep done - 61040.\n",
      "Ep done - 61050.\n",
      "Ep done - 61060.\n",
      "Ep done - 61070.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 288        |\n",
      "|    iterations           | 262        |\n",
      "|    time_elapsed         | 5582       |\n",
      "|    total_timesteps      | 1609728    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15559797 |\n",
      "|    clip_fraction        | 0.00371    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.3e-05   |\n",
      "|    explained_variance   | 0.0173     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0794     |\n",
      "|    n_updates            | 21840      |\n",
      "|    policy_gradient_loss | 0.00851    |\n",
      "|    value_loss           | 0.198      |\n",
      "----------------------------------------\n",
      "Ep done - 61080.\n",
      "Ep done - 16010.\n",
      "Ep done - 16020.\n",
      "Ep done - 16030.\n",
      "Ep done - 16040.\n",
      "Ep done - 16050.\n",
      "Ep done - 16060.\n",
      "Ep done - 16070.\n",
      "Ep done - 16080.\n",
      "Ep done - 16090.\n",
      "Ep done - 16100.\n",
      "Eval num_timesteps=1610000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.11 +/- 0.31\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.56      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1610000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.94e-06 |\n",
      "|    explained_variance   | 0.14      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0982    |\n",
      "|    n_updates            | 21850     |\n",
      "|    policy_gradient_loss | -1.61e-09 |\n",
      "|    value_loss           | 0.203     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 83\n",
      "Ep done - 61090.\n",
      "Ep done - 61100.\n",
      "Ep done - 61110.\n",
      "Ep done - 61120.\n",
      "Ep done - 61130.\n",
      "Ep done - 61140.\n",
      "Ep done - 61150.\n",
      "Ep done - 61160.\n",
      "Ep done - 61170.\n",
      "Ep done - 61180.\n",
      "Ep done - 61190.\n",
      "Ep done - 61200.\n",
      "Ep done - 61210.\n",
      "Ep done - 61220.\n",
      "Ep done - 61230.\n",
      "Ep done - 61240.\n",
      "Ep done - 61250.\n",
      "Ep done - 61260.\n",
      "Ep done - 61270.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 285      |\n",
      "|    iterations      | 263      |\n",
      "|    time_elapsed    | 5652     |\n",
      "|    total_timesteps | 1615872  |\n",
      "---------------------------------\n",
      "Ep done - 61280.\n",
      "Ep done - 61290.\n",
      "Ep done - 61300.\n",
      "Ep done - 61310.\n",
      "Ep done - 61320.\n",
      "Ep done - 61330.\n",
      "Ep done - 61340.\n",
      "Ep done - 61350.\n",
      "Ep done - 61360.\n",
      "Ep done - 61370.\n",
      "Ep done - 61380.\n",
      "Ep done - 61390.\n",
      "Ep done - 61400.\n",
      "Ep done - 61410.\n",
      "Ep done - 16110.\n",
      "Ep done - 16120.\n",
      "Ep done - 16130.\n",
      "Ep done - 16140.\n",
      "Ep done - 16150.\n",
      "Ep done - 16160.\n",
      "Ep done - 16170.\n",
      "Ep done - 16180.\n",
      "Ep done - 16190.\n",
      "Ep done - 16200.\n",
      "Eval num_timesteps=1620000, episode_reward=0.24 +/- 0.84\n",
      "Episode length: 30.08 +/- 0.27\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.24      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1620000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0.000244  |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000252 |\n",
      "|    explained_variance   | -0.209    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.112     |\n",
      "|    n_updates            | 21860     |\n",
      "|    policy_gradient_loss | -2.94e-06 |\n",
      "|    value_loss           | 0.158     |\n",
      "---------------------------------------\n",
      "Ep done - 61420.\n",
      "Ep done - 61430.\n",
      "Ep done - 61440.\n",
      "Ep done - 61450.\n",
      "Ep done - 61460.\n",
      "Ep done - 61470.\n",
      "Ep done - 61480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 5725     |\n",
      "|    total_timesteps | 1622016  |\n",
      "---------------------------------\n",
      "Ep done - 61490.\n",
      "Ep done - 61500.\n",
      "Ep done - 61510.\n",
      "Ep done - 61520.\n",
      "Ep done - 61530.\n",
      "Ep done - 61540.\n",
      "Ep done - 61550.\n",
      "Ep done - 61560.\n",
      "Ep done - 61570.\n",
      "Ep done - 61580.\n",
      "Ep done - 61590.\n",
      "Ep done - 61600.\n",
      "Ep done - 61610.\n",
      "Ep done - 61620.\n",
      "Ep done - 61630.\n",
      "Ep done - 61640.\n",
      "Ep done - 61650.\n",
      "Ep done - 61660.\n",
      "Ep done - 61670.\n",
      "Ep done - 61680.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.47      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 265       |\n",
      "|    time_elapsed         | 5790      |\n",
      "|    total_timesteps      | 1628160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.65e-06 |\n",
      "|    explained_variance   | 0.177     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0491    |\n",
      "|    n_updates            | 21870     |\n",
      "|    policy_gradient_loss | -6.34e-10 |\n",
      "|    value_loss           | 0.175     |\n",
      "---------------------------------------\n",
      "Ep done - 61690.\n",
      "Ep done - 61700.\n",
      "Ep done - 61710.\n",
      "Ep done - 61720.\n",
      "Ep done - 61730.\n",
      "Ep done - 61740.\n",
      "Ep done - 16210.\n",
      "Ep done - 16220.\n",
      "Ep done - 16230.\n",
      "Ep done - 16240.\n",
      "Ep done - 16250.\n",
      "Ep done - 16260.\n",
      "Ep done - 16270.\n",
      "Ep done - 16280.\n",
      "Ep done - 16290.\n",
      "Ep done - 16300.\n",
      "Eval num_timesteps=1630000, episode_reward=0.73 +/- 0.44\n",
      "Episode length: 30.05 +/- 0.22\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.73      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1630000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1158159 |\n",
      "|    clip_fraction        | 0.0049    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000295 |\n",
      "|    explained_variance   | 0.175     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0866    |\n",
      "|    n_updates            | 21880     |\n",
      "|    policy_gradient_loss | -0.00038  |\n",
      "|    value_loss           | 0.169     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.73\n",
      "SELFPLAY: new best model, bumping up generation to 84\n",
      "Ep done - 61750.\n",
      "Ep done - 61760.\n",
      "Ep done - 61770.\n",
      "Ep done - 61780.\n",
      "Ep done - 61790.\n",
      "Ep done - 61800.\n",
      "Ep done - 61810.\n",
      "Ep done - 61820.\n",
      "Ep done - 61830.\n",
      "Ep done - 61840.\n",
      "Ep done - 61850.\n",
      "Ep done - 61860.\n",
      "Ep done - 61870.\n",
      "Ep done - 61880.\n",
      "Ep done - 61890.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.93     |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 5858     |\n",
      "|    total_timesteps | 1634304  |\n",
      "---------------------------------\n",
      "Ep done - 61900.\n",
      "Ep done - 61910.\n",
      "Ep done - 61920.\n",
      "Ep done - 61930.\n",
      "Ep done - 61940.\n",
      "Ep done - 61950.\n",
      "Ep done - 61960.\n",
      "Ep done - 61970.\n",
      "Ep done - 61980.\n",
      "Ep done - 61990.\n",
      "Ep done - 62000.\n",
      "Ep done - 62010.\n",
      "Ep done - 62020.\n",
      "Ep done - 62030.\n",
      "Ep done - 62040.\n",
      "Ep done - 62050.\n",
      "Ep done - 62060.\n",
      "Ep done - 62070.\n",
      "Ep done - 62080.\n",
      "Ep done - 16310.\n",
      "Ep done - 16320.\n",
      "Ep done - 16330.\n",
      "Ep done - 16340.\n",
      "Ep done - 16350.\n",
      "Ep done - 16360.\n",
      "Ep done - 16370.\n",
      "Ep done - 16380.\n",
      "Ep done - 16390.\n",
      "Ep done - 16400.\n",
      "Eval num_timesteps=1640000, episode_reward=0.96 +/- 0.20\n",
      "Episode length: 30.05 +/- 0.22\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.96      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1640000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0.000651  |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000331 |\n",
      "|    explained_variance   | -4.77e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0256    |\n",
      "|    n_updates            | 21890     |\n",
      "|    policy_gradient_loss | -4.24e-05 |\n",
      "|    value_loss           | 0.0512    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.96\n",
      "SELFPLAY: new best model, bumping up generation to 85\n",
      "Ep done - 62090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.96     |\n",
      "| time/              |          |\n",
      "|    fps             | 275      |\n",
      "|    iterations      | 267      |\n",
      "|    time_elapsed    | 5950     |\n",
      "|    total_timesteps | 1640448  |\n",
      "---------------------------------\n",
      "Ep done - 62100.\n",
      "Ep done - 62110.\n",
      "Ep done - 62120.\n",
      "Ep done - 62130.\n",
      "Ep done - 62140.\n",
      "Ep done - 62150.\n",
      "Ep done - 62160.\n",
      "Ep done - 62170.\n",
      "Ep done - 62180.\n",
      "Ep done - 62190.\n",
      "Ep done - 62200.\n",
      "Ep done - 62210.\n",
      "Ep done - 62220.\n",
      "Ep done - 62230.\n",
      "Ep done - 62240.\n",
      "Ep done - 62250.\n",
      "Ep done - 62260.\n",
      "Ep done - 62270.\n",
      "Ep done - 62280.\n",
      "Ep done - 62290.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.96      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 273       |\n",
      "|    iterations           | 268       |\n",
      "|    time_elapsed         | 6030      |\n",
      "|    total_timesteps      | 1646592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.21e-06 |\n",
      "|    explained_variance   | -5.96e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00898   |\n",
      "|    n_updates            | 21900     |\n",
      "|    policy_gradient_loss | 2.14e-09  |\n",
      "|    value_loss           | 0.0287    |\n",
      "---------------------------------------\n",
      "Ep done - 62300.\n",
      "Ep done - 62310.\n",
      "Ep done - 62320.\n",
      "Ep done - 62330.\n",
      "Ep done - 62340.\n",
      "Ep done - 62350.\n",
      "Ep done - 62360.\n",
      "Ep done - 62370.\n",
      "Ep done - 62380.\n",
      "Ep done - 62390.\n",
      "Ep done - 62400.\n",
      "Ep done - 62410.\n",
      "Ep done - 16410.\n",
      "Ep done - 16420.\n",
      "Ep done - 16430.\n",
      "Ep done - 16440.\n",
      "Ep done - 16450.\n",
      "Ep done - 16460.\n",
      "Ep done - 16470.\n",
      "Ep done - 16480.\n",
      "Ep done - 16490.\n",
      "Ep done - 16500.\n",
      "Eval num_timesteps=1650000, episode_reward=0.95 +/- 0.22\n",
      "Episode length: 30.03 +/- 0.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.95      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1650000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.08e-06 |\n",
      "|    explained_variance   | -9.54e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0159    |\n",
      "|    n_updates            | 21910     |\n",
      "|    policy_gradient_loss | -1.51e-09 |\n",
      "|    value_loss           | 0.0212    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.95\n",
      "SELFPLAY: new best model, bumping up generation to 86\n",
      "Ep done - 62420.\n",
      "Ep done - 62430.\n",
      "Ep done - 62440.\n",
      "Ep done - 62450.\n",
      "Ep done - 62460.\n",
      "Ep done - 62470.\n",
      "Ep done - 62480.\n",
      "Ep done - 62490.\n",
      "Ep done - 62500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.96     |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 269      |\n",
      "|    time_elapsed    | 6122     |\n",
      "|    total_timesteps | 1652736  |\n",
      "---------------------------------\n",
      "Ep done - 62510.\n",
      "Ep done - 62520.\n",
      "Ep done - 62530.\n",
      "Ep done - 62540.\n",
      "Ep done - 62550.\n",
      "Ep done - 62560.\n",
      "Ep done - 62570.\n",
      "Ep done - 62580.\n",
      "Ep done - 62590.\n",
      "Ep done - 62600.\n",
      "Ep done - 62610.\n",
      "Ep done - 62620.\n",
      "Ep done - 62630.\n",
      "Ep done - 62640.\n",
      "Ep done - 62650.\n",
      "Ep done - 62660.\n",
      "Ep done - 62670.\n",
      "Ep done - 62680.\n",
      "Ep done - 62690.\n",
      "Ep done - 62700.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.97      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 269       |\n",
      "|    iterations           | 270       |\n",
      "|    time_elapsed         | 6154      |\n",
      "|    total_timesteps      | 1658880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.48e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00315   |\n",
      "|    n_updates            | 21920     |\n",
      "|    policy_gradient_loss | 8.13e-10  |\n",
      "|    value_loss           | 0.0194    |\n",
      "---------------------------------------\n",
      "Ep done - 62710.\n",
      "Ep done - 62720.\n",
      "Ep done - 62730.\n",
      "Ep done - 62740.\n",
      "Ep done - 16510.\n",
      "Ep done - 16520.\n",
      "Ep done - 16530.\n",
      "Ep done - 16540.\n",
      "Ep done - 16550.\n",
      "Ep done - 16560.\n",
      "Ep done - 16570.\n",
      "Ep done - 16580.\n",
      "Ep done - 16590.\n",
      "Ep done - 16600.\n",
      "Eval num_timesteps=1660000, episode_reward=-0.53 +/- 0.84\n",
      "Episode length: 30.23 +/- 0.42\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | -0.53     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1660000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9229852 |\n",
      "|    clip_fraction        | 0.00978   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000343 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00465  |\n",
      "|    n_updates            | 21930     |\n",
      "|    policy_gradient_loss | -0.000413 |\n",
      "|    value_loss           | 0.0182    |\n",
      "---------------------------------------\n",
      "Ep done - 62750.\n",
      "Ep done - 62760.\n",
      "Ep done - 62770.\n",
      "Ep done - 62780.\n",
      "Ep done - 62790.\n",
      "Ep done - 62800.\n",
      "Ep done - 62810.\n",
      "Ep done - 62820.\n",
      "Ep done - 62830.\n",
      "Ep done - 62840.\n",
      "Ep done - 62850.\n",
      "Ep done - 62860.\n",
      "Ep done - 62870.\n",
      "Ep done - 62880.\n",
      "Ep done - 62890.\n",
      "Ep done - 62900.\n",
      "Ep done - 62910.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 271      |\n",
      "|    time_elapsed    | 6176     |\n",
      "|    total_timesteps | 1665024  |\n",
      "---------------------------------\n",
      "Ep done - 62920.\n",
      "Ep done - 62930.\n",
      "Ep done - 62940.\n",
      "Ep done - 62950.\n",
      "Ep done - 62960.\n",
      "Ep done - 62970.\n",
      "Ep done - 62980.\n",
      "Ep done - 62990.\n",
      "Ep done - 63000.\n",
      "Ep done - 63010.\n",
      "Ep done - 63020.\n",
      "Ep done - 63030.\n",
      "Ep done - 63040.\n",
      "Ep done - 63050.\n",
      "Ep done - 63060.\n",
      "Ep done - 63070.\n",
      "Ep done - 16610.\n",
      "Ep done - 16620.\n",
      "Ep done - 16630.\n",
      "Ep done - 16640.\n",
      "Ep done - 16650.\n",
      "Ep done - 16660.\n",
      "Ep done - 16670.\n",
      "Ep done - 16680.\n",
      "Ep done - 16690.\n",
      "Ep done - 16700.\n",
      "Eval num_timesteps=1670000, episode_reward=-0.11 +/- 0.97\n",
      "Episode length: 30.32 +/- 0.47\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | -0.11     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1670000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1577431 |\n",
      "|    clip_fraction        | 0.0509    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00154  |\n",
      "|    explained_variance   | 0.00433   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.144     |\n",
      "|    n_updates            | 21940     |\n",
      "|    policy_gradient_loss | -0.00936  |\n",
      "|    value_loss           | 0.299     |\n",
      "---------------------------------------\n",
      "Ep done - 63080.\n",
      "Ep done - 63090.\n",
      "Ep done - 63100.\n",
      "Ep done - 63110.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | -0.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 6199     |\n",
      "|    total_timesteps | 1671168  |\n",
      "---------------------------------\n",
      "Ep done - 63120.\n",
      "Ep done - 63130.\n",
      "Ep done - 63140.\n",
      "Ep done - 63150.\n",
      "Ep done - 63160.\n",
      "Ep done - 63170.\n",
      "Ep done - 63180.\n",
      "Ep done - 63190.\n",
      "Ep done - 63200.\n",
      "Ep done - 63210.\n",
      "Ep done - 63220.\n",
      "Ep done - 63230.\n",
      "Ep done - 63240.\n",
      "Ep done - 63250.\n",
      "Ep done - 63260.\n",
      "Ep done - 63270.\n",
      "Ep done - 63280.\n",
      "Ep done - 63290.\n",
      "Ep done - 63300.\n",
      "Ep done - 63310.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | 0.19       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 269        |\n",
      "|    iterations           | 273        |\n",
      "|    time_elapsed         | 6225       |\n",
      "|    total_timesteps      | 1677312    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14486875 |\n",
      "|    clip_fraction        | 0.00879    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000222  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.156      |\n",
      "|    n_updates            | 21950      |\n",
      "|    policy_gradient_loss | -0.00191   |\n",
      "|    value_loss           | 0.272      |\n",
      "----------------------------------------\n",
      "Ep done - 63320.\n",
      "Ep done - 63330.\n",
      "Ep done - 63340.\n",
      "Ep done - 63350.\n",
      "Ep done - 63360.\n",
      "Ep done - 63370.\n",
      "Ep done - 63380.\n",
      "Ep done - 63390.\n",
      "Ep done - 63400.\n",
      "Ep done - 16710.\n",
      "Ep done - 16720.\n",
      "Ep done - 16730.\n",
      "Ep done - 16740.\n",
      "Ep done - 16750.\n",
      "Ep done - 16760.\n",
      "Ep done - 16770.\n",
      "Ep done - 16780.\n",
      "Ep done - 16790.\n",
      "Ep done - 16800.\n",
      "Eval num_timesteps=1680000, episode_reward=0.16 +/- 0.97\n",
      "Episode length: 30.44 +/- 0.50\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.4     |\n",
      "|    mean_reward          | 0.16     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 1680000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0.000358 |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00066 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.153    |\n",
      "|    n_updates            | 21960    |\n",
      "|    policy_gradient_loss | 4.48e-05 |\n",
      "|    value_loss           | 0.287    |\n",
      "--------------------------------------\n",
      "Ep done - 63410.\n",
      "Ep done - 63420.\n",
      "Ep done - 63430.\n",
      "Ep done - 63440.\n",
      "Ep done - 63450.\n",
      "Ep done - 63460.\n",
      "Ep done - 63470.\n",
      "Ep done - 63480.\n",
      "Ep done - 63490.\n",
      "Ep done - 63500.\n",
      "Ep done - 63510.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 268      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 6259     |\n",
      "|    total_timesteps | 1683456  |\n",
      "---------------------------------\n",
      "Ep done - 63520.\n",
      "Ep done - 63530.\n",
      "Ep done - 63540.\n",
      "Ep done - 63550.\n",
      "Ep done - 63560.\n",
      "Ep done - 63570.\n",
      "Ep done - 63580.\n",
      "Ep done - 63590.\n",
      "Ep done - 63600.\n",
      "Ep done - 63610.\n",
      "Ep done - 63620.\n",
      "Ep done - 63630.\n",
      "Ep done - 63640.\n",
      "Ep done - 63650.\n",
      "Ep done - 63660.\n",
      "Ep done - 63670.\n",
      "Ep done - 63680.\n",
      "Ep done - 63690.\n",
      "Ep done - 63700.\n",
      "Ep done - 63710.\n",
      "Ep done - 63720.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30.3          |\n",
      "|    ep_rew_mean          | 0.07          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 269           |\n",
      "|    iterations           | 275           |\n",
      "|    time_elapsed         | 6275          |\n",
      "|    total_timesteps      | 1689600       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010146268 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000328     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.169         |\n",
      "|    n_updates            | 21970         |\n",
      "|    policy_gradient_loss | -1.5e-05      |\n",
      "|    value_loss           | 0.286         |\n",
      "-------------------------------------------\n",
      "Ep done - 63730.\n",
      "Ep done - 16810.\n",
      "Ep done - 16820.\n",
      "Ep done - 16830.\n",
      "Ep done - 16840.\n",
      "Ep done - 16850.\n",
      "Ep done - 16860.\n",
      "Ep done - 16870.\n",
      "Ep done - 16880.\n",
      "Ep done - 16890.\n",
      "Ep done - 16900.\n",
      "Eval num_timesteps=1690000, episode_reward=0.18 +/- 0.97\n",
      "Episode length: 30.38 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.4        |\n",
      "|    mean_reward          | 0.18        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1690000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015471426 |\n",
      "|    clip_fraction        | 0.00166     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000462   |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 21980       |\n",
      "|    policy_gradient_loss | 0.000681    |\n",
      "|    value_loss           | 0.281       |\n",
      "-----------------------------------------\n",
      "Ep done - 63740.\n",
      "Ep done - 63750.\n",
      "Ep done - 63760.\n",
      "Ep done - 63770.\n",
      "Ep done - 63780.\n",
      "Ep done - 63790.\n",
      "Ep done - 63800.\n",
      "Ep done - 63810.\n",
      "Ep done - 63820.\n",
      "Ep done - 63830.\n",
      "Ep done - 63840.\n",
      "Ep done - 63850.\n",
      "Ep done - 63860.\n",
      "Ep done - 63870.\n",
      "Ep done - 63880.\n",
      "Ep done - 63890.\n",
      "Ep done - 63900.\n",
      "Ep done - 63910.\n",
      "Ep done - 63920.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | 0.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 6295     |\n",
      "|    total_timesteps | 1695744  |\n",
      "---------------------------------\n",
      "Ep done - 63930.\n",
      "Ep done - 63940.\n",
      "Ep done - 63950.\n",
      "Ep done - 63960.\n",
      "Ep done - 63970.\n",
      "Ep done - 63980.\n",
      "Ep done - 63990.\n",
      "Ep done - 64000.\n",
      "Ep done - 64010.\n",
      "Ep done - 64020.\n",
      "Ep done - 64030.\n",
      "Ep done - 64040.\n",
      "Ep done - 64050.\n",
      "Ep done - 64060.\n",
      "Ep done - 16910.\n",
      "Ep done - 16920.\n",
      "Ep done - 16930.\n",
      "Ep done - 16940.\n",
      "Ep done - 16950.\n",
      "Ep done - 16960.\n",
      "Ep done - 16970.\n",
      "Ep done - 16980.\n",
      "Ep done - 16990.\n",
      "Ep done - 17000.\n",
      "Eval num_timesteps=1700000, episode_reward=0.09 +/- 0.97\n",
      "Episode length: 30.37 +/- 0.48\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.4      |\n",
      "|    mean_reward          | 0.09      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1700000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.76e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.104     |\n",
      "|    n_updates            | 21990     |\n",
      "|    policy_gradient_loss | -6.5e-08  |\n",
      "|    value_loss           | 0.269     |\n",
      "---------------------------------------\n",
      "Ep done - 64070.\n",
      "Ep done - 64080.\n",
      "Ep done - 64090.\n",
      "Ep done - 64100.\n",
      "Ep done - 64110.\n",
      "Ep done - 64120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | 0.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 268      |\n",
      "|    iterations      | 277      |\n",
      "|    time_elapsed    | 6331     |\n",
      "|    total_timesteps | 1701888  |\n",
      "---------------------------------\n",
      "Ep done - 64130.\n",
      "Ep done - 64140.\n",
      "Ep done - 64150.\n",
      "Ep done - 64160.\n",
      "Ep done - 64170.\n",
      "Ep done - 64180.\n",
      "Ep done - 64190.\n",
      "Ep done - 64200.\n",
      "Ep done - 64210.\n",
      "Ep done - 64220.\n",
      "Ep done - 64230.\n",
      "Ep done - 64240.\n",
      "Ep done - 64250.\n",
      "Ep done - 64260.\n",
      "Ep done - 64270.\n",
      "Ep done - 64280.\n",
      "Ep done - 64290.\n",
      "Ep done - 64300.\n",
      "Ep done - 64310.\n",
      "Ep done - 64320.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30.4          |\n",
      "|    ep_rew_mean          | 0.09          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 269           |\n",
      "|    iterations           | 278           |\n",
      "|    time_elapsed         | 6344          |\n",
      "|    total_timesteps      | 1708032       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051745714 |\n",
      "|    clip_fraction        | 0.000488      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000308     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.164         |\n",
      "|    n_updates            | 22000         |\n",
      "|    policy_gradient_loss | -2.64e-05     |\n",
      "|    value_loss           | 0.277         |\n",
      "-------------------------------------------\n",
      "Ep done - 64330.\n",
      "Ep done - 64340.\n",
      "Ep done - 64350.\n",
      "Ep done - 64360.\n",
      "Ep done - 64370.\n",
      "Ep done - 64380.\n",
      "Ep done - 64390.\n",
      "Ep done - 17010.\n",
      "Ep done - 17020.\n",
      "Ep done - 17030.\n",
      "Ep done - 17040.\n",
      "Ep done - 17050.\n",
      "Ep done - 17060.\n",
      "Ep done - 17070.\n",
      "Ep done - 17080.\n",
      "Ep done - 17090.\n",
      "Ep done - 17100.\n",
      "Eval num_timesteps=1710000, episode_reward=-0.02 +/- 0.96\n",
      "Episode length: 30.31 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.3       |\n",
      "|    mean_reward          | -0.02      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1710000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00799236 |\n",
      "|    clip_fraction        | 0.000635   |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.92e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.165      |\n",
      "|    n_updates            | 22010      |\n",
      "|    policy_gradient_loss | 0.000107   |\n",
      "|    value_loss           | 0.282      |\n",
      "----------------------------------------\n",
      "Ep done - 64400.\n",
      "Ep done - 64410.\n",
      "Ep done - 64420.\n",
      "Ep done - 64430.\n",
      "Ep done - 64440.\n",
      "Ep done - 64450.\n",
      "Ep done - 64460.\n",
      "Ep done - 64470.\n",
      "Ep done - 64480.\n",
      "Ep done - 64490.\n",
      "Ep done - 64500.\n",
      "Ep done - 64510.\n",
      "Ep done - 64520.\n",
      "Ep done - 64530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | -0.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 279      |\n",
      "|    time_elapsed    | 6363     |\n",
      "|    total_timesteps | 1714176  |\n",
      "---------------------------------\n",
      "Ep done - 64540.\n",
      "Ep done - 64550.\n",
      "Ep done - 64560.\n",
      "Ep done - 64570.\n",
      "Ep done - 64580.\n",
      "Ep done - 64590.\n",
      "Ep done - 64600.\n",
      "Ep done - 64610.\n",
      "Ep done - 64620.\n",
      "Ep done - 64630.\n",
      "Ep done - 64640.\n",
      "Ep done - 64650.\n",
      "Ep done - 64660.\n",
      "Ep done - 64670.\n",
      "Ep done - 64680.\n",
      "Ep done - 64690.\n",
      "Ep done - 64700.\n",
      "Ep done - 64710.\n",
      "Ep done - 64720.\n",
      "Ep done - 17110.\n",
      "Ep done - 17120.\n",
      "Ep done - 17130.\n",
      "Ep done - 17140.\n",
      "Ep done - 17150.\n",
      "Ep done - 17160.\n",
      "Ep done - 17170.\n",
      "Ep done - 17180.\n",
      "Ep done - 17190.\n",
      "Ep done - 17200.\n",
      "Eval num_timesteps=1720000, episode_reward=-0.10 +/- 0.99\n",
      "Episode length: 30.41 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.4       |\n",
      "|    mean_reward          | -0.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1720000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21898262 |\n",
      "|    clip_fraction        | 0.00285    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.41e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.126      |\n",
      "|    n_updates            | 22020      |\n",
      "|    policy_gradient_loss | 0.000623   |\n",
      "|    value_loss           | 0.285      |\n",
      "----------------------------------------\n",
      "Ep done - 64730.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | -0.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 6381     |\n",
      "|    total_timesteps | 1720320  |\n",
      "---------------------------------\n",
      "Ep done - 64740.\n",
      "Ep done - 64750.\n",
      "Ep done - 64760.\n",
      "Ep done - 64770.\n",
      "Ep done - 64780.\n",
      "Ep done - 64790.\n",
      "Ep done - 64800.\n",
      "Ep done - 64810.\n",
      "Ep done - 64820.\n",
      "Ep done - 64830.\n",
      "Ep done - 64840.\n",
      "Ep done - 64850.\n",
      "Ep done - 64860.\n",
      "Ep done - 64870.\n",
      "Ep done - 64880.\n",
      "Ep done - 64890.\n",
      "Ep done - 64900.\n",
      "Ep done - 64910.\n",
      "Ep done - 64920.\n",
      "Ep done - 64930.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | -0.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 269        |\n",
      "|    iterations           | 281        |\n",
      "|    time_elapsed         | 6395       |\n",
      "|    total_timesteps      | 1726464    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32301518 |\n",
      "|    clip_fraction        | 0.00757    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000455  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.147      |\n",
      "|    n_updates            | 22030      |\n",
      "|    policy_gradient_loss | -0.00137   |\n",
      "|    value_loss           | 0.284      |\n",
      "----------------------------------------\n",
      "Ep done - 64940.\n",
      "Ep done - 64950.\n",
      "Ep done - 64960.\n",
      "Ep done - 64970.\n",
      "Ep done - 64980.\n",
      "Ep done - 64990.\n",
      "Ep done - 65000.\n",
      "Ep done - 65010.\n",
      "Ep done - 65020.\n",
      "Ep done - 65030.\n",
      "Ep done - 65040.\n",
      "Ep done - 65050.\n",
      "Ep done - 17210.\n",
      "Ep done - 17220.\n",
      "Ep done - 17230.\n",
      "Ep done - 17240.\n",
      "Ep done - 17250.\n",
      "Ep done - 17260.\n",
      "Ep done - 17270.\n",
      "Ep done - 17280.\n",
      "Ep done - 17290.\n",
      "Ep done - 17300.\n",
      "Eval num_timesteps=1730000, episode_reward=-0.16 +/- 0.99\n",
      "Episode length: 30.40 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.4       |\n",
      "|    mean_reward          | -0.16      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1730000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.97685796 |\n",
      "|    clip_fraction        | 0.0249     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.88e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.152      |\n",
      "|    n_updates            | 22040      |\n",
      "|    policy_gradient_loss | 0.00438    |\n",
      "|    value_loss           | 0.271      |\n",
      "----------------------------------------\n",
      "Ep done - 65060.\n",
      "Ep done - 65070.\n",
      "Ep done - 65080.\n",
      "Ep done - 65090.\n",
      "Ep done - 65100.\n",
      "Ep done - 65110.\n",
      "Ep done - 65120.\n",
      "Ep done - 65130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | -0.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 270      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 6413     |\n",
      "|    total_timesteps | 1732608  |\n",
      "---------------------------------\n",
      "Ep done - 65140.\n",
      "Ep done - 65150.\n",
      "Ep done - 65160.\n",
      "Ep done - 65170.\n",
      "Ep done - 65180.\n",
      "Ep done - 65190.\n",
      "Ep done - 65200.\n",
      "Ep done - 65210.\n",
      "Ep done - 65220.\n",
      "Ep done - 65230.\n",
      "Ep done - 65240.\n",
      "Ep done - 65250.\n",
      "Ep done - 65260.\n",
      "Ep done - 65270.\n",
      "Ep done - 65280.\n",
      "Ep done - 65290.\n",
      "Ep done - 65300.\n",
      "Ep done - 65310.\n",
      "Ep done - 65320.\n",
      "Ep done - 65330.\n",
      "Ep done - 65340.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.5      |\n",
      "|    ep_rew_mean          | -0.04     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 270       |\n",
      "|    iterations           | 283       |\n",
      "|    time_elapsed         | 6427      |\n",
      "|    total_timesteps      | 1738752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.42e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.122     |\n",
      "|    n_updates            | 22050     |\n",
      "|    policy_gradient_loss | 4.56e-10  |\n",
      "|    value_loss           | 0.261     |\n",
      "---------------------------------------\n",
      "Ep done - 65350.\n",
      "Ep done - 65360.\n",
      "Ep done - 65370.\n",
      "Ep done - 65380.\n",
      "Ep done - 17310.\n",
      "Ep done - 17320.\n",
      "Ep done - 17330.\n",
      "Ep done - 17340.\n",
      "Ep done - 17350.\n",
      "Ep done - 17360.\n",
      "Ep done - 17370.\n",
      "Ep done - 17380.\n",
      "Ep done - 17390.\n",
      "Ep done - 17400.\n",
      "Eval num_timesteps=1740000, episode_reward=-0.30 +/- 0.95\n",
      "Episode length: 30.35 +/- 0.48\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.4      |\n",
      "|    mean_reward          | -0.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1740000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.27e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.152     |\n",
      "|    n_updates            | 22060     |\n",
      "|    policy_gradient_loss | -3.35e-10 |\n",
      "|    value_loss           | 0.287     |\n",
      "---------------------------------------\n",
      "Ep done - 65390.\n",
      "Ep done - 65400.\n",
      "Ep done - 65410.\n",
      "Ep done - 65420.\n",
      "Ep done - 65430.\n",
      "Ep done - 65440.\n",
      "Ep done - 65450.\n",
      "Ep done - 65460.\n",
      "Ep done - 65470.\n",
      "Ep done - 65480.\n",
      "Ep done - 65490.\n",
      "Ep done - 65500.\n",
      "Ep done - 65510.\n",
      "Ep done - 65520.\n",
      "Ep done - 65530.\n",
      "Ep done - 65540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 270      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 6445     |\n",
      "|    total_timesteps | 1744896  |\n",
      "---------------------------------\n",
      "Ep done - 65550.\n",
      "Ep done - 65560.\n",
      "Ep done - 65570.\n",
      "Ep done - 65580.\n",
      "Ep done - 65590.\n",
      "Ep done - 65600.\n",
      "Ep done - 65610.\n",
      "Ep done - 65620.\n",
      "Ep done - 65630.\n",
      "Ep done - 65640.\n",
      "Ep done - 65650.\n",
      "Ep done - 65660.\n",
      "Ep done - 65670.\n",
      "Ep done - 65680.\n",
      "Ep done - 65690.\n",
      "Ep done - 65700.\n",
      "Ep done - 65710.\n",
      "Ep done - 17410.\n",
      "Ep done - 17420.\n",
      "Ep done - 17430.\n",
      "Ep done - 17440.\n",
      "Ep done - 17450.\n",
      "Ep done - 17460.\n",
      "Ep done - 17470.\n",
      "Ep done - 17480.\n",
      "Ep done - 17490.\n",
      "Ep done - 17500.\n",
      "Eval num_timesteps=1750000, episode_reward=-0.18 +/- 0.98\n",
      "Episode length: 30.39 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.4      |\n",
      "|    mean_reward          | -0.18     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1750000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.37e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.165     |\n",
      "|    n_updates            | 22070     |\n",
      "|    policy_gradient_loss | -5.7e-10  |\n",
      "|    value_loss           | 0.276     |\n",
      "---------------------------------------\n",
      "Ep done - 65720.\n",
      "Ep done - 65730.\n",
      "Ep done - 65740.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.5     |\n",
      "|    ep_rew_mean     | 0.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 270      |\n",
      "|    iterations      | 285      |\n",
      "|    time_elapsed    | 6463     |\n",
      "|    total_timesteps | 1751040  |\n",
      "---------------------------------\n",
      "Ep done - 65750.\n",
      "Ep done - 65760.\n",
      "Ep done - 65770.\n",
      "Ep done - 65780.\n",
      "Ep done - 65790.\n",
      "Ep done - 65800.\n",
      "Ep done - 65810.\n",
      "Ep done - 65820.\n",
      "Ep done - 65830.\n",
      "Ep done - 65840.\n",
      "Ep done - 65850.\n",
      "Ep done - 65860.\n",
      "Ep done - 65870.\n",
      "Ep done - 65880.\n",
      "Ep done - 65890.\n",
      "Ep done - 65900.\n",
      "Ep done - 65910.\n",
      "Ep done - 65920.\n",
      "Ep done - 65930.\n",
      "Ep done - 65940.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.4      |\n",
      "|    ep_rew_mean          | -0.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 271       |\n",
      "|    iterations           | 286       |\n",
      "|    time_elapsed         | 6477      |\n",
      "|    total_timesteps      | 1757184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.46e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.148     |\n",
      "|    n_updates            | 22080     |\n",
      "|    policy_gradient_loss | -4.1e-10  |\n",
      "|    value_loss           | 0.287     |\n",
      "---------------------------------------\n",
      "Ep done - 65950.\n",
      "Ep done - 65960.\n",
      "Ep done - 65970.\n",
      "Ep done - 65980.\n",
      "Ep done - 65990.\n",
      "Ep done - 66000.\n",
      "Ep done - 66010.\n",
      "Ep done - 66020.\n",
      "Ep done - 66030.\n",
      "Ep done - 17510.\n",
      "Ep done - 17520.\n",
      "Ep done - 17530.\n",
      "Ep done - 17540.\n",
      "Ep done - 17550.\n",
      "Ep done - 17560.\n",
      "Ep done - 17570.\n",
      "Ep done - 17580.\n",
      "Ep done - 17590.\n",
      "Ep done - 17600.\n",
      "Eval num_timesteps=1760000, episode_reward=-0.18 +/- 0.98\n",
      "Episode length: 30.41 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.4      |\n",
      "|    mean_reward          | -0.18     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1760000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.91e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.139     |\n",
      "|    n_updates            | 22090     |\n",
      "|    policy_gradient_loss | 1.14e-09  |\n",
      "|    value_loss           | 0.288     |\n",
      "---------------------------------------\n",
      "Ep done - 66040.\n",
      "Ep done - 66050.\n",
      "Ep done - 66060.\n",
      "Ep done - 66070.\n",
      "Ep done - 66080.\n",
      "Ep done - 66090.\n",
      "Ep done - 66100.\n",
      "Ep done - 66110.\n",
      "Ep done - 66120.\n",
      "Ep done - 66130.\n",
      "Ep done - 66140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | -0.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 271      |\n",
      "|    iterations      | 287      |\n",
      "|    time_elapsed    | 6496     |\n",
      "|    total_timesteps | 1763328  |\n",
      "---------------------------------\n",
      "Ep done - 66150.\n",
      "Ep done - 66160.\n",
      "Ep done - 66170.\n",
      "Ep done - 66180.\n",
      "Ep done - 66190.\n",
      "Ep done - 66200.\n",
      "Ep done - 66210.\n",
      "Ep done - 66220.\n",
      "Ep done - 66230.\n",
      "Ep done - 66240.\n",
      "Ep done - 66250.\n",
      "Ep done - 66260.\n",
      "Ep done - 66270.\n",
      "Ep done - 66280.\n",
      "Ep done - 66290.\n",
      "Ep done - 66300.\n",
      "Ep done - 66310.\n",
      "Ep done - 66320.\n",
      "Ep done - 66330.\n",
      "Ep done - 66340.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 31         |\n",
      "|    ep_rew_mean          | 0.96       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 271        |\n",
      "|    iterations           | 288        |\n",
      "|    time_elapsed         | 6510       |\n",
      "|    total_timesteps      | 1769472    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32187435 |\n",
      "|    clip_fraction        | 0.0089     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.12e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.135      |\n",
      "|    n_updates            | 22100      |\n",
      "|    policy_gradient_loss | -0.00164   |\n",
      "|    value_loss           | 0.282      |\n",
      "----------------------------------------\n",
      "Ep done - 66350.\n",
      "Ep done - 66360.\n",
      "Ep done - 17610.\n",
      "Ep done - 17620.\n",
      "Ep done - 17630.\n",
      "Ep done - 17640.\n",
      "Ep done - 17650.\n",
      "Ep done - 17660.\n",
      "Ep done - 17670.\n",
      "Ep done - 17680.\n",
      "Ep done - 17690.\n",
      "Ep done - 17700.\n",
      "Eval num_timesteps=1770000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.98 +/- 0.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 31         |\n",
      "|    mean_reward          | 0.98       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1770000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61860895 |\n",
      "|    clip_fraction        | 0.00741    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.41e-05  |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0412     |\n",
      "|    n_updates            | 22110      |\n",
      "|    policy_gradient_loss | -0.000579  |\n",
      "|    value_loss           | 0.0926     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 87\n",
      "Ep done - 66370.\n",
      "Ep done - 66380.\n",
      "Ep done - 66390.\n",
      "Ep done - 66400.\n",
      "Ep done - 66410.\n",
      "Ep done - 66420.\n",
      "Ep done - 66430.\n",
      "Ep done - 66440.\n",
      "Ep done - 66450.\n",
      "Ep done - 66460.\n",
      "Ep done - 66470.\n",
      "Ep done - 66480.\n",
      "Ep done - 66490.\n",
      "Ep done - 66500.\n",
      "Ep done - 66510.\n",
      "Ep done - 66520.\n",
      "Ep done - 66530.\n",
      "Ep done - 66540.\n",
      "Ep done - 66550.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 271      |\n",
      "|    iterations      | 289      |\n",
      "|    time_elapsed    | 6528     |\n",
      "|    total_timesteps | 1775616  |\n",
      "---------------------------------\n",
      "Ep done - 66560.\n",
      "Ep done - 66570.\n",
      "Ep done - 66580.\n",
      "Ep done - 66590.\n",
      "Ep done - 66600.\n",
      "Ep done - 66610.\n",
      "Ep done - 66620.\n",
      "Ep done - 66630.\n",
      "Ep done - 66640.\n",
      "Ep done - 66650.\n",
      "Ep done - 66660.\n",
      "Ep done - 66670.\n",
      "Ep done - 66680.\n",
      "Ep done - 66690.\n",
      "Ep done - 17710.\n",
      "Ep done - 17720.\n",
      "Ep done - 17730.\n",
      "Ep done - 17740.\n",
      "Ep done - 17750.\n",
      "Ep done - 17760.\n",
      "Ep done - 17770.\n",
      "Ep done - 17780.\n",
      "Ep done - 17790.\n",
      "Ep done - 17800.\n",
      "Eval num_timesteps=1780000, episode_reward=0.40 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.4        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1780000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40065655 |\n",
      "|    clip_fraction        | 0.0731     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00224   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.137      |\n",
      "|    n_updates            | 22120      |\n",
      "|    policy_gradient_loss | -0.00598   |\n",
      "|    value_loss           | 0.302      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.4\n",
      "SELFPLAY: new best model, bumping up generation to 88\n",
      "Ep done - 66700.\n",
      "Ep done - 66710.\n",
      "Ep done - 66720.\n",
      "Ep done - 66730.\n",
      "Ep done - 66740.\n",
      "Ep done - 66750.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.6     |\n",
      "|    ep_rew_mean     | 0.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 272      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 6546     |\n",
      "|    total_timesteps | 1781760  |\n",
      "---------------------------------\n",
      "Ep done - 66760.\n",
      "Ep done - 66770.\n",
      "Ep done - 66780.\n",
      "Ep done - 66790.\n",
      "Ep done - 66800.\n",
      "Ep done - 66810.\n",
      "Ep done - 66820.\n",
      "Ep done - 66830.\n",
      "Ep done - 66840.\n",
      "Ep done - 66850.\n",
      "Ep done - 66860.\n",
      "Ep done - 66870.\n",
      "Ep done - 66880.\n",
      "Ep done - 66890.\n",
      "Ep done - 66900.\n",
      "Ep done - 66910.\n",
      "Ep done - 66920.\n",
      "Ep done - 66930.\n",
      "Ep done - 66940.\n",
      "Ep done - 66950.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 31         |\n",
      "|    ep_rew_mean          | 0.94       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 272        |\n",
      "|    iterations           | 291        |\n",
      "|    time_elapsed         | 6560       |\n",
      "|    total_timesteps      | 1787904    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45768246 |\n",
      "|    clip_fraction        | 0.0323     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00111   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.12       |\n",
      "|    n_updates            | 22130      |\n",
      "|    policy_gradient_loss | -0.0042    |\n",
      "|    value_loss           | 0.213      |\n",
      "----------------------------------------\n",
      "Ep done - 66960.\n",
      "Ep done - 66970.\n",
      "Ep done - 66980.\n",
      "Ep done - 66990.\n",
      "Ep done - 67000.\n",
      "Ep done - 67010.\n",
      "Ep done - 67020.\n",
      "Ep done - 17810.\n",
      "Ep done - 17820.\n",
      "Ep done - 17830.\n",
      "Ep done - 17840.\n",
      "Ep done - 17850.\n",
      "Ep done - 17860.\n",
      "Ep done - 17870.\n",
      "Ep done - 17880.\n",
      "Ep done - 17890.\n",
      "Ep done - 17900.\n",
      "Eval num_timesteps=1790000, episode_reward=0.68 +/- 0.73\n",
      "Episode length: 30.79 +/- 0.41\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.8      |\n",
      "|    mean_reward          | 0.68      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1790000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.842345  |\n",
      "|    clip_fraction        | 0.0914    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0142   |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0486    |\n",
      "|    n_updates            | 22140     |\n",
      "|    policy_gradient_loss | 0.237     |\n",
      "|    value_loss           | 0.0475    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 89\n",
      "Ep done - 67030.\n",
      "Ep done - 67040.\n",
      "Ep done - 67050.\n",
      "Ep done - 67060.\n",
      "Ep done - 67070.\n",
      "Ep done - 67080.\n",
      "Ep done - 67090.\n",
      "Ep done - 67100.\n",
      "Ep done - 67110.\n",
      "Ep done - 67120.\n",
      "Ep done - 67130.\n",
      "Ep done - 67140.\n",
      "Ep done - 67150.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 272      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 6578     |\n",
      "|    total_timesteps | 1794048  |\n",
      "---------------------------------\n",
      "Ep done - 67160.\n",
      "Ep done - 67170.\n",
      "Ep done - 67180.\n",
      "Ep done - 67190.\n",
      "Ep done - 67200.\n",
      "Ep done - 67210.\n",
      "Ep done - 67220.\n",
      "Ep done - 67230.\n",
      "Ep done - 67240.\n",
      "Ep done - 67250.\n",
      "Ep done - 67260.\n",
      "Ep done - 67270.\n",
      "Ep done - 67280.\n",
      "Ep done - 67290.\n",
      "Ep done - 67300.\n",
      "Ep done - 67310.\n",
      "Ep done - 67320.\n",
      "Ep done - 67330.\n",
      "Ep done - 67340.\n",
      "Ep done - 67350.\n",
      "Ep done - 17910.\n",
      "Ep done - 17920.\n",
      "Ep done - 17930.\n",
      "Ep done - 17940.\n",
      "Ep done - 17950.\n",
      "Ep done - 17960.\n",
      "Ep done - 17970.\n",
      "Ep done - 17980.\n",
      "Ep done - 17990.\n",
      "Ep done - 18000.\n",
      "Eval num_timesteps=1800000, episode_reward=0.51 +/- 0.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1800000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86660844 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00318   |\n",
      "|    explained_variance   | 0.00131    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.168      |\n",
      "|    n_updates            | 22150      |\n",
      "|    policy_gradient_loss | 0.00578    |\n",
      "|    value_loss           | 0.297      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.51\n",
      "SELFPLAY: new best model, bumping up generation to 90\n",
      "Ep done - 67360.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 272      |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 6597     |\n",
      "|    total_timesteps | 1800192  |\n",
      "---------------------------------\n",
      "Ep done - 67370.\n",
      "Ep done - 67380.\n",
      "Ep done - 67390.\n",
      "Ep done - 67400.\n",
      "Ep done - 67410.\n",
      "Ep done - 67420.\n",
      "Ep done - 67430.\n",
      "Ep done - 67440.\n",
      "Ep done - 67450.\n",
      "Ep done - 67460.\n",
      "Ep done - 67470.\n",
      "Ep done - 67480.\n",
      "Ep done - 67490.\n",
      "Ep done - 67500.\n",
      "Ep done - 67510.\n",
      "Ep done - 67520.\n",
      "Ep done - 67530.\n",
      "Ep done - 67540.\n",
      "Ep done - 67550.\n",
      "Ep done - 67560.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -0.82       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 294         |\n",
      "|    time_elapsed         | 6611        |\n",
      "|    total_timesteps      | 1806336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012890424 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.00757    |\n",
      "|    explained_variance   | -0.114      |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 22160       |\n",
      "|    policy_gradient_loss | -0.000482   |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 67570.\n",
      "Ep done - 67580.\n",
      "Ep done - 67590.\n",
      "Ep done - 67600.\n",
      "Ep done - 67610.\n",
      "Ep done - 67620.\n",
      "Ep done - 67630.\n",
      "Ep done - 67640.\n",
      "Ep done - 67650.\n",
      "Ep done - 67660.\n",
      "Ep done - 67670.\n",
      "Ep done - 67680.\n",
      "Ep done - 18010.\n",
      "Ep done - 18020.\n",
      "Ep done - 18030.\n",
      "Ep done - 18040.\n",
      "Ep done - 18050.\n",
      "Ep done - 18060.\n",
      "Ep done - 18070.\n",
      "Ep done - 18080.\n",
      "Ep done - 18090.\n",
      "Ep done - 18100.\n",
      "Eval num_timesteps=1810000, episode_reward=-0.55 +/- 0.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.55    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 1810000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.058609 |\n",
      "|    clip_fraction        | 0.0684   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00223 |\n",
      "|    explained_variance   | -0.0634  |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0423   |\n",
      "|    n_updates            | 22170    |\n",
      "|    policy_gradient_loss | 0.00257  |\n",
      "|    value_loss           | 0.146    |\n",
      "--------------------------------------\n",
      "Ep done - 67690.\n",
      "Ep done - 67700.\n",
      "Ep done - 67710.\n",
      "Ep done - 67720.\n",
      "Ep done - 67730.\n",
      "Ep done - 67740.\n",
      "Ep done - 67750.\n",
      "Ep done - 67760.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 273      |\n",
      "|    iterations      | 295      |\n",
      "|    time_elapsed    | 6629     |\n",
      "|    total_timesteps | 1812480  |\n",
      "---------------------------------\n",
      "Ep done - 67770.\n",
      "Ep done - 67780.\n",
      "Ep done - 67790.\n",
      "Ep done - 67800.\n",
      "Ep done - 67810.\n",
      "Ep done - 67820.\n",
      "Ep done - 67830.\n",
      "Ep done - 67840.\n",
      "Ep done - 67850.\n",
      "Ep done - 67860.\n",
      "Ep done - 67870.\n",
      "Ep done - 67880.\n",
      "Ep done - 67890.\n",
      "Ep done - 67900.\n",
      "Ep done - 67910.\n",
      "Ep done - 67920.\n",
      "Ep done - 67930.\n",
      "Ep done - 67940.\n",
      "Ep done - 67950.\n",
      "Ep done - 67960.\n",
      "Ep done - 67970.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -0.33        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 273          |\n",
      "|    iterations           | 296          |\n",
      "|    time_elapsed         | 6648         |\n",
      "|    total_timesteps      | 1818624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.880511e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000209    |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.147        |\n",
      "|    n_updates            | 22180        |\n",
      "|    policy_gradient_loss | 1.72e-07     |\n",
      "|    value_loss           | 0.257        |\n",
      "------------------------------------------\n",
      "Ep done - 67980.\n",
      "Ep done - 67990.\n",
      "Ep done - 68000.\n",
      "Ep done - 68010.\n",
      "Ep done - 68020.\n",
      "Ep done - 18110.\n",
      "Ep done - 18120.\n",
      "Ep done - 18130.\n",
      "Ep done - 18140.\n",
      "Ep done - 18150.\n",
      "Ep done - 18160.\n",
      "Ep done - 18170.\n",
      "Ep done - 18180.\n",
      "Ep done - 18190.\n",
      "Ep done - 18200.\n",
      "Eval num_timesteps=1820000, episode_reward=-0.42 +/- 0.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.42        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1820000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013447902 |\n",
      "|    clip_fraction        | 3.26e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000246    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.112        |\n",
      "|    n_updates            | 22190        |\n",
      "|    policy_gradient_loss | 1.1e-05      |\n",
      "|    value_loss           | 0.235        |\n",
      "------------------------------------------\n",
      "Ep done - 68030.\n",
      "Ep done - 68040.\n",
      "Ep done - 68050.\n",
      "Ep done - 68060.\n",
      "Ep done - 68070.\n",
      "Ep done - 68080.\n",
      "Ep done - 68090.\n",
      "Ep done - 68100.\n",
      "Ep done - 68110.\n",
      "Ep done - 68120.\n",
      "Ep done - 68130.\n",
      "Ep done - 68140.\n",
      "Ep done - 68150.\n",
      "Ep done - 68160.\n",
      "Ep done - 68170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.27    |\n",
      "| time/              |          |\n",
      "|    fps             | 270      |\n",
      "|    iterations      | 297      |\n",
      "|    time_elapsed    | 6733     |\n",
      "|    total_timesteps | 1824768  |\n",
      "---------------------------------\n",
      "Ep done - 68180.\n",
      "Ep done - 68190.\n",
      "Ep done - 68200.\n",
      "Ep done - 68210.\n",
      "Ep done - 68220.\n",
      "Ep done - 68230.\n",
      "Ep done - 68240.\n",
      "Ep done - 68250.\n",
      "Ep done - 68260.\n",
      "Ep done - 68270.\n",
      "Ep done - 68280.\n",
      "Ep done - 68290.\n",
      "Ep done - 68300.\n",
      "Ep done - 68310.\n",
      "Ep done - 68320.\n",
      "Ep done - 68330.\n",
      "Ep done - 68340.\n",
      "Ep done - 68350.\n",
      "Ep done - 18210.\n",
      "Ep done - 18220.\n",
      "Ep done - 18230.\n",
      "Ep done - 18240.\n",
      "Ep done - 18250.\n",
      "Ep done - 18260.\n",
      "Ep done - 18270.\n",
      "Ep done - 18280.\n",
      "Ep done - 18290.\n",
      "Ep done - 18300.\n",
      "Eval num_timesteps=1830000, episode_reward=-0.50 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -0.5          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 1830000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.1376016e-05 |\n",
      "|    clip_fraction        | 0.000374      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000442     |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.152         |\n",
      "|    n_updates            | 22200         |\n",
      "|    policy_gradient_loss | -4.92e-06     |\n",
      "|    value_loss           | 0.263         |\n",
      "-------------------------------------------\n",
      "Ep done - 68360.\n",
      "Ep done - 68370.\n",
      "Ep done - 68380.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 268      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 6814     |\n",
      "|    total_timesteps | 1830912  |\n",
      "---------------------------------\n",
      "Ep done - 68390.\n",
      "Ep done - 68400.\n",
      "Ep done - 68410.\n",
      "Ep done - 68420.\n",
      "Ep done - 68430.\n",
      "Ep done - 68440.\n",
      "Ep done - 68450.\n",
      "Ep done - 68460.\n",
      "Ep done - 68470.\n",
      "Ep done - 68480.\n",
      "Ep done - 68490.\n",
      "Ep done - 68500.\n",
      "Ep done - 68510.\n",
      "Ep done - 68520.\n",
      "Ep done - 68530.\n",
      "Ep done - 68540.\n",
      "Ep done - 68550.\n",
      "Ep done - 68560.\n",
      "Ep done - 68570.\n",
      "Ep done - 68580.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -0.52        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 266          |\n",
      "|    iterations           | 299          |\n",
      "|    time_elapsed         | 6880         |\n",
      "|    total_timesteps      | 1837056      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019677205 |\n",
      "|    clip_fraction        | 0.000423     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000154    |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.151        |\n",
      "|    n_updates            | 22210        |\n",
      "|    policy_gradient_loss | 0.000366     |\n",
      "|    value_loss           | 0.269        |\n",
      "------------------------------------------\n",
      "Ep done - 68590.\n",
      "Ep done - 68600.\n",
      "Ep done - 68610.\n",
      "Ep done - 68620.\n",
      "Ep done - 68630.\n",
      "Ep done - 68640.\n",
      "Ep done - 68650.\n",
      "Ep done - 68660.\n",
      "Ep done - 68670.\n",
      "Ep done - 68680.\n",
      "Ep done - 18310.\n",
      "Ep done - 18320.\n",
      "Ep done - 18330.\n",
      "Ep done - 18340.\n",
      "Ep done - 18350.\n",
      "Ep done - 18360.\n",
      "Ep done - 18370.\n",
      "Ep done - 18380.\n",
      "Ep done - 18390.\n",
      "Ep done - 18400.\n",
      "Eval num_timesteps=1840000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -0.6          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 1840000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6474365e-05 |\n",
      "|    clip_fraction        | 0.00013       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000248     |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.123         |\n",
      "|    n_updates            | 22220         |\n",
      "|    policy_gradient_loss | -1.01e-05     |\n",
      "|    value_loss           | 0.248         |\n",
      "-------------------------------------------\n",
      "Ep done - 68690.\n",
      "Ep done - 68700.\n",
      "Ep done - 68710.\n",
      "Ep done - 68720.\n",
      "Ep done - 68730.\n",
      "Ep done - 68740.\n",
      "Ep done - 68750.\n",
      "Ep done - 68760.\n",
      "Ep done - 68770.\n",
      "Ep done - 68780.\n",
      "Ep done - 68790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 6955     |\n",
      "|    total_timesteps | 1843200  |\n",
      "---------------------------------\n",
      "Ep done - 68800.\n",
      "Ep done - 68810.\n",
      "Ep done - 68820.\n",
      "Ep done - 68830.\n",
      "Ep done - 68840.\n",
      "Ep done - 68850.\n",
      "Ep done - 68860.\n",
      "Ep done - 68870.\n",
      "Ep done - 68880.\n",
      "Ep done - 68890.\n",
      "Ep done - 68900.\n",
      "Ep done - 68910.\n",
      "Ep done - 68920.\n",
      "Ep done - 68930.\n",
      "Ep done - 68940.\n",
      "Ep done - 68950.\n",
      "Ep done - 68960.\n",
      "Ep done - 68970.\n",
      "Ep done - 68980.\n",
      "Ep done - 68990.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -0.44       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 301         |\n",
      "|    time_elapsed         | 7016        |\n",
      "|    total_timesteps      | 1849344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008546318 |\n",
      "|    clip_fraction        | 0.00161     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000172   |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 22230       |\n",
      "|    policy_gradient_loss | 0.000362    |\n",
      "|    value_loss           | 0.249       |\n",
      "-----------------------------------------\n",
      "Ep done - 69000.\n",
      "Ep done - 69010.\n",
      "Ep done - 69020.\n",
      "Ep done - 18410.\n",
      "Ep done - 18420.\n",
      "Ep done - 18430.\n",
      "Ep done - 18440.\n",
      "Ep done - 18450.\n",
      "Ep done - 18460.\n",
      "Ep done - 18470.\n",
      "Ep done - 18480.\n",
      "Ep done - 18490.\n",
      "Ep done - 18500.\n",
      "Eval num_timesteps=1850000, episode_reward=-0.62 +/- 0.78\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.62     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1850000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.83e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0995    |\n",
      "|    n_updates            | 22240     |\n",
      "|    policy_gradient_loss | -1.89e-07 |\n",
      "|    value_loss           | 0.247     |\n",
      "---------------------------------------\n",
      "Ep done - 69030.\n",
      "Ep done - 69040.\n",
      "Ep done - 69050.\n",
      "Ep done - 69060.\n",
      "Ep done - 69070.\n",
      "Ep done - 69080.\n",
      "Ep done - 69090.\n",
      "Ep done - 69100.\n",
      "Ep done - 69110.\n",
      "Ep done - 69120.\n",
      "Ep done - 69130.\n",
      "Ep done - 69140.\n",
      "Ep done - 69150.\n",
      "Ep done - 69160.\n",
      "Ep done - 69170.\n",
      "Ep done - 69180.\n",
      "Ep done - 69190.\n",
      "Ep done - 69200.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 261      |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 7086     |\n",
      "|    total_timesteps | 1855488  |\n",
      "---------------------------------\n",
      "Ep done - 69210.\n",
      "Ep done - 69220.\n",
      "Ep done - 69230.\n",
      "Ep done - 69240.\n",
      "Ep done - 69250.\n",
      "Ep done - 69260.\n",
      "Ep done - 69270.\n",
      "Ep done - 69280.\n",
      "Ep done - 69290.\n",
      "Ep done - 69300.\n",
      "Ep done - 69310.\n",
      "Ep done - 69320.\n",
      "Ep done - 69330.\n",
      "Ep done - 69340.\n",
      "Ep done - 69350.\n",
      "Ep done - 18510.\n",
      "Ep done - 18520.\n",
      "Ep done - 18530.\n",
      "Ep done - 18540.\n",
      "Ep done - 18550.\n",
      "Ep done - 18560.\n",
      "Ep done - 18570.\n",
      "Ep done - 18580.\n",
      "Ep done - 18590.\n",
      "Ep done - 18600.\n",
      "Eval num_timesteps=1860000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1860000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3502345 |\n",
      "|    clip_fraction        | 0.0258    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000126 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.108     |\n",
      "|    n_updates            | 22250     |\n",
      "|    policy_gradient_loss | 0.00578   |\n",
      "|    value_loss           | 0.274     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 91\n",
      "Ep done - 69360.\n",
      "Ep done - 69370.\n",
      "Ep done - 69380.\n",
      "Ep done - 69390.\n",
      "Ep done - 69400.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 261      |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 7107     |\n",
      "|    total_timesteps | 1861632  |\n",
      "---------------------------------\n",
      "Ep done - 69410.\n",
      "Ep done - 69420.\n",
      "Ep done - 69430.\n",
      "Ep done - 69440.\n",
      "Ep done - 69450.\n",
      "Ep done - 69460.\n",
      "Ep done - 69470.\n",
      "Ep done - 69480.\n",
      "Ep done - 69490.\n",
      "Ep done - 69500.\n",
      "Ep done - 69510.\n",
      "Ep done - 69520.\n",
      "Ep done - 69530.\n",
      "Ep done - 69540.\n",
      "Ep done - 69550.\n",
      "Ep done - 69560.\n",
      "Ep done - 69570.\n",
      "Ep done - 69580.\n",
      "Ep done - 69590.\n",
      "Ep done - 69600.\n",
      "Ep done - 69610.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 304       |\n",
      "|    time_elapsed         | 7123      |\n",
      "|    total_timesteps      | 1867776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6945152 |\n",
      "|    clip_fraction        | 0.0521    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000272 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.106     |\n",
      "|    n_updates            | 22260     |\n",
      "|    policy_gradient_loss | -0.00538  |\n",
      "|    value_loss           | 0.208     |\n",
      "---------------------------------------\n",
      "Ep done - 69620.\n",
      "Ep done - 69630.\n",
      "Ep done - 69640.\n",
      "Ep done - 69650.\n",
      "Ep done - 69660.\n",
      "Ep done - 69670.\n",
      "Ep done - 69680.\n",
      "Ep done - 18610.\n",
      "Ep done - 18620.\n",
      "Ep done - 18630.\n",
      "Ep done - 18640.\n",
      "Ep done - 18650.\n",
      "Ep done - 18660.\n",
      "Ep done - 18670.\n",
      "Ep done - 18680.\n",
      "Ep done - 18690.\n",
      "Ep done - 18700.\n",
      "Eval num_timesteps=1870000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1870000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.57523865 |\n",
      "|    clip_fraction        | 0.0312     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.26e-05  |\n",
      "|    explained_variance   | 5.36e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0527     |\n",
      "|    n_updates            | 22270      |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Ep done - 69690.\n",
      "Ep done - 69700.\n",
      "Ep done - 69710.\n",
      "Ep done - 69720.\n",
      "Ep done - 69730.\n",
      "Ep done - 69740.\n",
      "Ep done - 69750.\n",
      "Ep done - 69760.\n",
      "Ep done - 69770.\n",
      "Ep done - 69780.\n",
      "Ep done - 69790.\n",
      "Ep done - 69800.\n",
      "Ep done - 69810.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 305      |\n",
      "|    time_elapsed    | 7147     |\n",
      "|    total_timesteps | 1873920  |\n",
      "---------------------------------\n",
      "Ep done - 69820.\n",
      "Ep done - 69830.\n",
      "Ep done - 69840.\n",
      "Ep done - 69850.\n",
      "Ep done - 69860.\n",
      "Ep done - 69870.\n",
      "Ep done - 69880.\n",
      "Ep done - 69890.\n",
      "Ep done - 69900.\n",
      "Ep done - 69910.\n",
      "Ep done - 69920.\n",
      "Ep done - 69930.\n",
      "Ep done - 69940.\n",
      "Ep done - 69950.\n",
      "Ep done - 69960.\n",
      "Ep done - 69970.\n",
      "Ep done - 69980.\n",
      "Ep done - 69990.\n",
      "Ep done - 70000.\n",
      "Ep done - 70010.\n",
      "Ep done - 70020.\n",
      "Ep done - 18710.\n",
      "Ep done - 18720.\n",
      "Ep done - 18730.\n",
      "Ep done - 18740.\n",
      "Ep done - 18750.\n",
      "Ep done - 18760.\n",
      "Ep done - 18770.\n",
      "Ep done - 18780.\n",
      "Ep done - 18790.\n",
      "Ep done - 18800.\n",
      "Eval num_timesteps=1880000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1880000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.1592255 |\n",
      "|    clip_fraction        | 0.0781    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000165 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0252   |\n",
      "|    n_updates            | 22280     |\n",
      "|    policy_gradient_loss | -0.0226   |\n",
      "|    value_loss           | 0.0548    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 261      |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 7185     |\n",
      "|    total_timesteps | 1880064  |\n",
      "---------------------------------\n",
      "Ep done - 70030.\n",
      "Ep done - 70040.\n",
      "Ep done - 70050.\n",
      "Ep done - 70060.\n",
      "Ep done - 70070.\n",
      "Ep done - 70080.\n",
      "Ep done - 70090.\n",
      "Ep done - 70100.\n",
      "Ep done - 70110.\n",
      "Ep done - 70120.\n",
      "Ep done - 70130.\n",
      "Ep done - 70140.\n",
      "Ep done - 70150.\n",
      "Ep done - 70160.\n",
      "Ep done - 70170.\n",
      "Ep done - 70180.\n",
      "Ep done - 70190.\n",
      "Ep done - 70200.\n",
      "Ep done - 70210.\n",
      "Ep done - 70220.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 261        |\n",
      "|    iterations           | 307        |\n",
      "|    time_elapsed         | 7205       |\n",
      "|    total_timesteps      | 1886208    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84904265 |\n",
      "|    clip_fraction        | 0.0233     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.27e-05  |\n",
      "|    explained_variance   | -0.596     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00295   |\n",
      "|    n_updates            | 22290      |\n",
      "|    policy_gradient_loss | -0.0052    |\n",
      "|    value_loss           | 0.0139     |\n",
      "----------------------------------------\n",
      "Ep done - 70230.\n",
      "Ep done - 70240.\n",
      "Ep done - 70250.\n",
      "Ep done - 70260.\n",
      "Ep done - 70270.\n",
      "Ep done - 70280.\n",
      "Ep done - 70290.\n",
      "Ep done - 70300.\n",
      "Ep done - 70310.\n",
      "Ep done - 70320.\n",
      "Ep done - 70330.\n",
      "Ep done - 70340.\n",
      "Ep done - 70350.\n",
      "Ep done - 18810.\n",
      "Ep done - 18820.\n",
      "Ep done - 18830.\n",
      "Ep done - 18840.\n",
      "Ep done - 18850.\n",
      "Ep done - 18860.\n",
      "Ep done - 18870.\n",
      "Ep done - 18880.\n",
      "Ep done - 18890.\n",
      "Ep done - 18900.\n",
      "Eval num_timesteps=1890000, episode_reward=-0.94 +/- 0.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.94     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1890000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.58e-06 |\n",
      "|    explained_variance   | 1.25e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0107    |\n",
      "|    n_updates            | 22300     |\n",
      "|    policy_gradient_loss | 7.57e-09  |\n",
      "|    value_loss           | 0.0236    |\n",
      "---------------------------------------\n",
      "Ep done - 70360.\n",
      "Ep done - 70370.\n",
      "Ep done - 70380.\n",
      "Ep done - 70390.\n",
      "Ep done - 70400.\n",
      "Ep done - 70410.\n",
      "Ep done - 70420.\n",
      "Ep done - 70430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 261      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 7230     |\n",
      "|    total_timesteps | 1892352  |\n",
      "---------------------------------\n",
      "Ep done - 70440.\n",
      "Ep done - 70450.\n",
      "Ep done - 70460.\n",
      "Ep done - 70470.\n",
      "Ep done - 70480.\n",
      "Ep done - 70490.\n",
      "Ep done - 70500.\n",
      "Ep done - 70510.\n",
      "Ep done - 70520.\n",
      "Ep done - 70530.\n",
      "Ep done - 70540.\n",
      "Ep done - 70550.\n",
      "Ep done - 70560.\n",
      "Ep done - 70570.\n",
      "Ep done - 70580.\n",
      "Ep done - 70590.\n",
      "Ep done - 70600.\n",
      "Ep done - 70610.\n",
      "Ep done - 70620.\n",
      "Ep done - 70630.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -0.96        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 261          |\n",
      "|    iterations           | 309          |\n",
      "|    time_elapsed         | 7249         |\n",
      "|    total_timesteps      | 1898496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.985905e-07 |\n",
      "|    clip_fraction        | 0.000114     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.05e-05    |\n",
      "|    explained_variance   | 3.64e-06     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00799      |\n",
      "|    n_updates            | 22310        |\n",
      "|    policy_gradient_loss | 2.34e-06     |\n",
      "|    value_loss           | 0.014        |\n",
      "------------------------------------------\n",
      "Ep done - 70640.\n",
      "Ep done - 70650.\n",
      "Ep done - 70660.\n",
      "Ep done - 70670.\n",
      "Ep done - 70680.\n",
      "Ep done - 18910.\n",
      "Ep done - 18920.\n",
      "Ep done - 18930.\n",
      "Ep done - 18940.\n",
      "Ep done - 18950.\n",
      "Ep done - 18960.\n",
      "Ep done - 18970.\n",
      "Ep done - 18980.\n",
      "Ep done - 18990.\n",
      "Ep done - 19000.\n",
      "Eval num_timesteps=1900000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1900000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.6e-06  |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00422   |\n",
      "|    n_updates            | 22320     |\n",
      "|    policy_gradient_loss | -8.98e-08 |\n",
      "|    value_loss           | 0.0191    |\n",
      "---------------------------------------\n",
      "Ep done - 70690.\n",
      "Ep done - 70700.\n",
      "Ep done - 70710.\n",
      "Ep done - 70720.\n",
      "Ep done - 70730.\n",
      "Ep done - 70740.\n",
      "Ep done - 70750.\n",
      "Ep done - 70760.\n",
      "Ep done - 70770.\n",
      "Ep done - 70780.\n",
      "Ep done - 70790.\n",
      "Ep done - 70800.\n",
      "Ep done - 70810.\n",
      "Ep done - 70820.\n",
      "Ep done - 70830.\n",
      "Ep done - 70840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 261      |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 7272     |\n",
      "|    total_timesteps | 1904640  |\n",
      "---------------------------------\n",
      "Ep done - 70850.\n",
      "Ep done - 70860.\n",
      "Ep done - 70870.\n",
      "Ep done - 70880.\n",
      "Ep done - 70890.\n",
      "Ep done - 70900.\n",
      "Ep done - 70910.\n",
      "Ep done - 70920.\n",
      "Ep done - 70930.\n",
      "Ep done - 70940.\n",
      "Ep done - 70950.\n",
      "Ep done - 70960.\n",
      "Ep done - 70970.\n",
      "Ep done - 70980.\n",
      "Ep done - 70990.\n",
      "Ep done - 71000.\n",
      "Ep done - 71010.\n",
      "Ep done - 71020.\n",
      "Ep done - 19010.\n",
      "Ep done - 19020.\n",
      "Ep done - 19030.\n",
      "Ep done - 19040.\n",
      "Ep done - 19050.\n",
      "Ep done - 19060.\n",
      "Ep done - 19070.\n",
      "Ep done - 19080.\n",
      "Ep done - 19090.\n",
      "Ep done - 19100.\n",
      "Eval num_timesteps=1910000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1910000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.18e-06 |\n",
      "|    explained_variance   | 7.75e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00336   |\n",
      "|    n_updates            | 22330     |\n",
      "|    policy_gradient_loss | 1.01e-09  |\n",
      "|    value_loss           | 0.00818   |\n",
      "---------------------------------------\n",
      "Ep done - 71030.\n",
      "Ep done - 71040.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 261      |\n",
      "|    iterations      | 311      |\n",
      "|    time_elapsed    | 7294     |\n",
      "|    total_timesteps | 1910784  |\n",
      "---------------------------------\n",
      "Ep done - 71050.\n",
      "Ep done - 71060.\n",
      "Ep done - 71070.\n",
      "Ep done - 71080.\n",
      "Ep done - 71090.\n",
      "Ep done - 71100.\n",
      "Ep done - 71110.\n",
      "Ep done - 71120.\n",
      "Ep done - 71130.\n",
      "Ep done - 71140.\n",
      "Ep done - 71150.\n",
      "Ep done - 71160.\n",
      "Ep done - 71170.\n",
      "Ep done - 71180.\n",
      "Ep done - 71190.\n",
      "Ep done - 71200.\n",
      "Ep done - 71210.\n",
      "Ep done - 71220.\n",
      "Ep done - 71230.\n",
      "Ep done - 71240.\n",
      "Ep done - 71250.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 312       |\n",
      "|    time_elapsed         | 7314      |\n",
      "|    total_timesteps      | 1916928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.46e-06 |\n",
      "|    explained_variance   | 4.77e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00626   |\n",
      "|    n_updates            | 22340     |\n",
      "|    policy_gradient_loss | -1.2e-10  |\n",
      "|    value_loss           | 0.0131    |\n",
      "---------------------------------------\n",
      "Ep done - 71260.\n",
      "Ep done - 71270.\n",
      "Ep done - 71280.\n",
      "Ep done - 71290.\n",
      "Ep done - 71300.\n",
      "Ep done - 71310.\n",
      "Ep done - 71320.\n",
      "Ep done - 71330.\n",
      "Ep done - 71340.\n",
      "Ep done - 71350.\n",
      "Ep done - 19110.\n",
      "Ep done - 19120.\n",
      "Ep done - 19130.\n",
      "Ep done - 19140.\n",
      "Ep done - 19150.\n",
      "Ep done - 19160.\n",
      "Ep done - 19170.\n",
      "Ep done - 19180.\n",
      "Ep done - 19190.\n",
      "Ep done - 19200.\n",
      "Eval num_timesteps=1920000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1920000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.06e-06 |\n",
      "|    explained_variance   | 8.34e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0043    |\n",
      "|    n_updates            | 22350     |\n",
      "|    policy_gradient_loss | -5.47e-10 |\n",
      "|    value_loss           | 0.02      |\n",
      "---------------------------------------\n",
      "Ep done - 71360.\n",
      "Ep done - 71370.\n",
      "Ep done - 71380.\n",
      "Ep done - 71390.\n",
      "Ep done - 71400.\n",
      "Ep done - 71410.\n",
      "Ep done - 71420.\n",
      "Ep done - 71430.\n",
      "Ep done - 71440.\n",
      "Ep done - 71450.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 7336     |\n",
      "|    total_timesteps | 1923072  |\n",
      "---------------------------------\n",
      "Ep done - 71460.\n",
      "Ep done - 71470.\n",
      "Ep done - 71480.\n",
      "Ep done - 71490.\n",
      "Ep done - 71500.\n",
      "Ep done - 71510.\n",
      "Ep done - 71520.\n",
      "Ep done - 71530.\n",
      "Ep done - 71540.\n",
      "Ep done - 71550.\n",
      "Ep done - 71560.\n",
      "Ep done - 71570.\n",
      "Ep done - 71580.\n",
      "Ep done - 71590.\n",
      "Ep done - 71600.\n",
      "Ep done - 71610.\n",
      "Ep done - 71620.\n",
      "Ep done - 71630.\n",
      "Ep done - 71640.\n",
      "Ep done - 71650.\n",
      "Ep done - 71660.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 314       |\n",
      "|    time_elapsed         | 7353      |\n",
      "|    total_timesteps      | 1929216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.39e-06 |\n",
      "|    explained_variance   | 2.45e-05  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00344   |\n",
      "|    n_updates            | 22360     |\n",
      "|    policy_gradient_loss | -6.63e-10 |\n",
      "|    value_loss           | 0.0164    |\n",
      "---------------------------------------\n",
      "Ep done - 71670.\n",
      "Ep done - 71680.\n",
      "Ep done - 19210.\n",
      "Ep done - 19220.\n",
      "Ep done - 19230.\n",
      "Ep done - 19240.\n",
      "Ep done - 19250.\n",
      "Ep done - 19260.\n",
      "Ep done - 19270.\n",
      "Ep done - 19280.\n",
      "Ep done - 19290.\n",
      "Ep done - 19300.\n",
      "Eval num_timesteps=1930000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1930000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 3.26e-05  |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000106 |\n",
      "|    explained_variance   | 2.98e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0105    |\n",
      "|    n_updates            | 22370     |\n",
      "|    policy_gradient_loss | 1.43e-06  |\n",
      "|    value_loss           | 0.0286    |\n",
      "---------------------------------------\n",
      "Ep done - 71690.\n",
      "Ep done - 71700.\n",
      "Ep done - 71710.\n",
      "Ep done - 71720.\n",
      "Ep done - 71730.\n",
      "Ep done - 71740.\n",
      "Ep done - 71750.\n",
      "Ep done - 71760.\n",
      "Ep done - 71770.\n",
      "Ep done - 71780.\n",
      "Ep done - 71790.\n",
      "Ep done - 71800.\n",
      "Ep done - 71810.\n",
      "Ep done - 71820.\n",
      "Ep done - 71830.\n",
      "Ep done - 71840.\n",
      "Ep done - 71850.\n",
      "Ep done - 71860.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 315      |\n",
      "|    time_elapsed    | 7376     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Ep done - 71870.\n",
      "Ep done - 71880.\n",
      "Ep done - 71890.\n",
      "Ep done - 71900.\n",
      "Ep done - 71910.\n",
      "Ep done - 71920.\n",
      "Ep done - 71930.\n",
      "Ep done - 71940.\n",
      "Ep done - 71950.\n",
      "Ep done - 71960.\n",
      "Ep done - 71970.\n",
      "Ep done - 71980.\n",
      "Ep done - 71990.\n",
      "Ep done - 72000.\n",
      "Ep done - 72010.\n",
      "Ep done - 72020.\n",
      "Ep done - 19310.\n",
      "Ep done - 19320.\n",
      "Ep done - 19330.\n",
      "Ep done - 19340.\n",
      "Ep done - 19350.\n",
      "Ep done - 19360.\n",
      "Ep done - 19370.\n",
      "Ep done - 19380.\n",
      "Ep done - 19390.\n",
      "Ep done - 19400.\n",
      "Eval num_timesteps=1940000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1940000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.68e-06 |\n",
      "|    explained_variance   | 1.01e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0046    |\n",
      "|    n_updates            | 22380     |\n",
      "|    policy_gradient_loss | 2.48e-09  |\n",
      "|    value_loss           | 0.0123    |\n",
      "---------------------------------------\n",
      "Ep done - 72030.\n",
      "Ep done - 72040.\n",
      "Ep done - 72050.\n",
      "Ep done - 72060.\n",
      "Ep done - 72070.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 7398     |\n",
      "|    total_timesteps | 1941504  |\n",
      "---------------------------------\n",
      "Ep done - 72080.\n",
      "Ep done - 72090.\n",
      "Ep done - 72100.\n",
      "Ep done - 72110.\n",
      "Ep done - 72120.\n",
      "Ep done - 72130.\n",
      "Ep done - 72140.\n",
      "Ep done - 72150.\n",
      "Ep done - 72160.\n",
      "Ep done - 72170.\n",
      "Ep done - 72180.\n",
      "Ep done - 72190.\n",
      "Ep done - 72200.\n",
      "Ep done - 72210.\n",
      "Ep done - 72220.\n",
      "Ep done - 72230.\n",
      "Ep done - 72240.\n",
      "Ep done - 72250.\n",
      "Ep done - 72260.\n",
      "Ep done - 72270.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 317       |\n",
      "|    time_elapsed         | 7416      |\n",
      "|    total_timesteps      | 1947648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.14e-06 |\n",
      "|    explained_variance   | 1.67e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00297   |\n",
      "|    n_updates            | 22390     |\n",
      "|    policy_gradient_loss | -2.93e-10 |\n",
      "|    value_loss           | 0.0117    |\n",
      "---------------------------------------\n",
      "Ep done - 72280.\n",
      "Ep done - 72290.\n",
      "Ep done - 72300.\n",
      "Ep done - 72310.\n",
      "Ep done - 72320.\n",
      "Ep done - 72330.\n",
      "Ep done - 72340.\n",
      "Ep done - 72350.\n",
      "Ep done - 19410.\n",
      "Ep done - 19420.\n",
      "Ep done - 19430.\n",
      "Ep done - 19440.\n",
      "Ep done - 19450.\n",
      "Ep done - 19460.\n",
      "Ep done - 19470.\n",
      "Ep done - 19480.\n",
      "Ep done - 19490.\n",
      "Ep done - 19500.\n",
      "Eval num_timesteps=1950000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1950000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80235547 |\n",
      "|    clip_fraction        | 0.0291     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.81e-05  |\n",
      "|    explained_variance   | 3.1e-06    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.0164    |\n",
      "|    n_updates            | 22400      |\n",
      "|    policy_gradient_loss | -0.00764   |\n",
      "|    value_loss           | 0.0127     |\n",
      "----------------------------------------\n",
      "Ep done - 72360.\n",
      "Ep done - 72370.\n",
      "Ep done - 72380.\n",
      "Ep done - 72390.\n",
      "Ep done - 72400.\n",
      "Ep done - 72410.\n",
      "Ep done - 72420.\n",
      "Ep done - 72430.\n",
      "Ep done - 72440.\n",
      "Ep done - 72450.\n",
      "Ep done - 72460.\n",
      "Ep done - 72470.\n",
      "Ep done - 72480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 7439     |\n",
      "|    total_timesteps | 1953792  |\n",
      "---------------------------------\n",
      "Ep done - 72490.\n",
      "Ep done - 72500.\n",
      "Ep done - 72510.\n",
      "Ep done - 72520.\n",
      "Ep done - 72530.\n",
      "Ep done - 72540.\n",
      "Ep done - 72550.\n",
      "Ep done - 72560.\n",
      "Ep done - 72570.\n",
      "Ep done - 72580.\n",
      "Ep done - 72590.\n",
      "Ep done - 72600.\n",
      "Ep done - 72610.\n",
      "Ep done - 72620.\n",
      "Ep done - 72630.\n",
      "Ep done - 72640.\n",
      "Ep done - 72650.\n",
      "Ep done - 72660.\n",
      "Ep done - 72670.\n",
      "Ep done - 72680.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 319       |\n",
      "|    time_elapsed         | 7456      |\n",
      "|    total_timesteps      | 1959936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.74e-06 |\n",
      "|    explained_variance   | 8.34e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00314   |\n",
      "|    n_updates            | 22410     |\n",
      "|    policy_gradient_loss | 8.82e-10  |\n",
      "|    value_loss           | 0.0228    |\n",
      "---------------------------------------\n",
      "Ep done - 19510.\n",
      "Ep done - 19520.\n",
      "Ep done - 19530.\n",
      "Ep done - 19540.\n",
      "Ep done - 19550.\n",
      "Ep done - 19560.\n",
      "Ep done - 19570.\n",
      "Ep done - 19580.\n",
      "Ep done - 19590.\n",
      "Ep done - 19600.\n",
      "Eval num_timesteps=1960000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | -1          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1960000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004840803 |\n",
      "|    clip_fraction        | 0.000228    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56e-05   |\n",
      "|    explained_variance   | 5.25e-06    |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.00457     |\n",
      "|    n_updates            | 22420       |\n",
      "|    policy_gradient_loss | -2.47e-05   |\n",
      "|    value_loss           | 0.00761     |\n",
      "-----------------------------------------\n",
      "Ep done - 72690.\n",
      "Ep done - 72700.\n",
      "Ep done - 72710.\n",
      "Ep done - 72720.\n",
      "Ep done - 72730.\n",
      "Ep done - 72740.\n",
      "Ep done - 72750.\n",
      "Ep done - 72760.\n",
      "Ep done - 72770.\n",
      "Ep done - 72780.\n",
      "Ep done - 72790.\n",
      "Ep done - 72800.\n",
      "Ep done - 72810.\n",
      "Ep done - 72820.\n",
      "Ep done - 72830.\n",
      "Ep done - 72840.\n",
      "Ep done - 72850.\n",
      "Ep done - 72860.\n",
      "Ep done - 72870.\n",
      "Ep done - 72880.\n",
      "Ep done - 72890.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 7478     |\n",
      "|    total_timesteps | 1966080  |\n",
      "---------------------------------\n",
      "Ep done - 72900.\n",
      "Ep done - 72910.\n",
      "Ep done - 72920.\n",
      "Ep done - 72930.\n",
      "Ep done - 72940.\n",
      "Ep done - 72950.\n",
      "Ep done - 72960.\n",
      "Ep done - 72970.\n",
      "Ep done - 72980.\n",
      "Ep done - 72990.\n",
      "Ep done - 73000.\n",
      "Ep done - 73010.\n",
      "Ep done - 73020.\n",
      "Ep done - 19610.\n",
      "Ep done - 19620.\n",
      "Ep done - 19630.\n",
      "Ep done - 19640.\n",
      "Ep done - 19650.\n",
      "Ep done - 19660.\n",
      "Ep done - 19670.\n",
      "Ep done - 19680.\n",
      "Ep done - 19690.\n",
      "Ep done - 19700.\n",
      "Eval num_timesteps=1970000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1970000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.36e-06 |\n",
      "|    explained_variance   | 1.59e-05  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0038    |\n",
      "|    n_updates            | 22430     |\n",
      "|    policy_gradient_loss | -5.24e-10 |\n",
      "|    value_loss           | 0.00683   |\n",
      "---------------------------------------\n",
      "Ep done - 73030.\n",
      "Ep done - 73040.\n",
      "Ep done - 73050.\n",
      "Ep done - 73060.\n",
      "Ep done - 73070.\n",
      "Ep done - 73080.\n",
      "Ep done - 73090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 321      |\n",
      "|    time_elapsed    | 7500     |\n",
      "|    total_timesteps | 1972224  |\n",
      "---------------------------------\n",
      "Ep done - 73100.\n",
      "Ep done - 73110.\n",
      "Ep done - 73120.\n",
      "Ep done - 73130.\n",
      "Ep done - 73140.\n",
      "Ep done - 73150.\n",
      "Ep done - 73160.\n",
      "Ep done - 73170.\n",
      "Ep done - 73180.\n",
      "Ep done - 73190.\n",
      "Ep done - 73200.\n",
      "Ep done - 73210.\n",
      "Ep done - 73220.\n",
      "Ep done - 73230.\n",
      "Ep done - 73240.\n",
      "Ep done - 73250.\n",
      "Ep done - 73260.\n",
      "Ep done - 73270.\n",
      "Ep done - 73280.\n",
      "Ep done - 73290.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 322       |\n",
      "|    time_elapsed         | 7518      |\n",
      "|    total_timesteps      | 1978368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.35e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.004     |\n",
      "|    n_updates            | 22440     |\n",
      "|    policy_gradient_loss | 2.97e-09  |\n",
      "|    value_loss           | 0.00595   |\n",
      "---------------------------------------\n",
      "Ep done - 73300.\n",
      "Ep done - 73310.\n",
      "Ep done - 73320.\n",
      "Ep done - 73330.\n",
      "Ep done - 73340.\n",
      "Ep done - 73350.\n",
      "Ep done - 19710.\n",
      "Ep done - 19720.\n",
      "Ep done - 19730.\n",
      "Ep done - 19740.\n",
      "Ep done - 19750.\n",
      "Ep done - 19760.\n",
      "Ep done - 19770.\n",
      "Ep done - 19780.\n",
      "Ep done - 19790.\n",
      "Ep done - 19800.\n",
      "Eval num_timesteps=1980000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1980000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41844985 |\n",
      "|    clip_fraction        | 0.0323     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000103  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00623    |\n",
      "|    n_updates            | 22450      |\n",
      "|    policy_gradient_loss | 0.000584   |\n",
      "|    value_loss           | 0.0236     |\n",
      "----------------------------------------\n",
      "Ep done - 73360.\n",
      "Ep done - 73370.\n",
      "Ep done - 73380.\n",
      "Ep done - 73390.\n",
      "Ep done - 73400.\n",
      "Ep done - 73410.\n",
      "Ep done - 73420.\n",
      "Ep done - 73430.\n",
      "Ep done - 73440.\n",
      "Ep done - 73450.\n",
      "Ep done - 73460.\n",
      "Ep done - 73470.\n",
      "Ep done - 73480.\n",
      "Ep done - 73490.\n",
      "Ep done - 73500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 7540     |\n",
      "|    total_timesteps | 1984512  |\n",
      "---------------------------------\n",
      "Ep done - 73510.\n",
      "Ep done - 73520.\n",
      "Ep done - 73530.\n",
      "Ep done - 73540.\n",
      "Ep done - 73550.\n",
      "Ep done - 73560.\n",
      "Ep done - 73570.\n",
      "Ep done - 73580.\n",
      "Ep done - 73590.\n",
      "Ep done - 73600.\n",
      "Ep done - 73610.\n",
      "Ep done - 73620.\n",
      "Ep done - 73630.\n",
      "Ep done - 73640.\n",
      "Ep done - 73650.\n",
      "Ep done - 73660.\n",
      "Ep done - 73670.\n",
      "Ep done - 73680.\n",
      "Ep done - 19810.\n",
      "Ep done - 19820.\n",
      "Ep done - 19830.\n",
      "Ep done - 19840.\n",
      "Ep done - 19850.\n",
      "Ep done - 19860.\n",
      "Ep done - 19870.\n",
      "Ep done - 19880.\n",
      "Ep done - 19890.\n",
      "Ep done - 19900.\n",
      "Eval num_timesteps=1990000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 31.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 31        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1990000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0897269 |\n",
      "|    clip_fraction        | 0.0353    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.34e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0154   |\n",
      "|    n_updates            | 22460     |\n",
      "|    policy_gradient_loss | -0.0111   |\n",
      "|    value_loss           | 0.00795   |\n",
      "---------------------------------------\n",
      "Ep done - 73690.\n",
      "Ep done - 73700.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 7563     |\n",
      "|    total_timesteps | 1990656  |\n",
      "---------------------------------\n",
      "Ep done - 73710.\n",
      "Ep done - 73720.\n",
      "Ep done - 73730.\n",
      "Ep done - 73740.\n",
      "Ep done - 73750.\n",
      "Ep done - 73760.\n",
      "Ep done - 73770.\n",
      "Ep done - 73780.\n",
      "Ep done - 73790.\n",
      "Ep done - 73800.\n",
      "Ep done - 73810.\n",
      "Ep done - 73820.\n",
      "Ep done - 73830.\n",
      "Ep done - 73840.\n",
      "Ep done - 73850.\n",
      "Ep done - 73860.\n",
      "Ep done - 73870.\n",
      "Ep done - 73880.\n",
      "Ep done - 73890.\n",
      "Ep done - 73900.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | 1        |\n",
      "| time/                   |          |\n",
      "|    fps                  | 263      |\n",
      "|    iterations           | 325      |\n",
      "|    time_elapsed         | 7581     |\n",
      "|    total_timesteps      | 1996800  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.202439 |\n",
      "|    clip_fraction        | 0.0435   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00013 |\n",
      "|    explained_variance   | 5.36e-07 |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.00177  |\n",
      "|    n_updates            | 22470    |\n",
      "|    policy_gradient_loss | -0.00673 |\n",
      "|    value_loss           | 0.0189   |\n",
      "--------------------------------------\n",
      "Ep done - 73910.\n",
      "Ep done - 73920.\n",
      "Ep done - 73930.\n",
      "Ep done - 73940.\n",
      "Ep done - 73950.\n",
      "Ep done - 73960.\n",
      "Ep done - 73970.\n",
      "Ep done - 73980.\n",
      "Ep done - 73990.\n",
      "Ep done - 74000.\n",
      "Ep done - 74010.\n",
      "Ep done - 19910.\n",
      "Ep done - 19920.\n",
      "Ep done - 19930.\n",
      "Ep done - 19940.\n",
      "Ep done - 19950.\n",
      "Ep done - 19960.\n",
      "Ep done - 19970.\n",
      "Ep done - 19980.\n",
      "Ep done - 19990.\n",
      "Ep done - 20000.\n",
      "Eval num_timesteps=2000000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2000000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.65e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0868    |\n",
      "|    n_updates            | 22480     |\n",
      "|    policy_gradient_loss | 5.95e-09  |\n",
      "|    value_loss           | 0.192     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 92\n",
      "Ep done - 74020.\n",
      "Ep done - 74030.\n",
      "Ep done - 74040.\n",
      "Ep done - 74050.\n",
      "Ep done - 74060.\n",
      "Ep done - 74070.\n",
      "Ep done - 74080.\n",
      "Ep done - 74090.\n",
      "Ep done - 74100.\n",
      "Ep done - 74110.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 7601     |\n",
      "|    total_timesteps | 2002944  |\n",
      "---------------------------------\n",
      "Ep done - 74120.\n",
      "Ep done - 74130.\n",
      "Ep done - 74140.\n",
      "Ep done - 74150.\n",
      "Ep done - 74160.\n",
      "Ep done - 74170.\n",
      "Ep done - 74180.\n",
      "Ep done - 74190.\n",
      "Ep done - 74200.\n",
      "Ep done - 74210.\n",
      "Ep done - 74220.\n",
      "Ep done - 74230.\n",
      "Ep done - 74240.\n",
      "Ep done - 74250.\n",
      "Ep done - 74260.\n",
      "Ep done - 74270.\n",
      "Ep done - 74280.\n",
      "Ep done - 74290.\n",
      "Ep done - 74300.\n",
      "Ep done - 74310.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 327       |\n",
      "|    time_elapsed         | 7619      |\n",
      "|    total_timesteps      | 2009088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3533678 |\n",
      "|    clip_fraction        | 0.0178    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000357 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.119     |\n",
      "|    n_updates            | 22490     |\n",
      "|    policy_gradient_loss | 0.0511    |\n",
      "|    value_loss           | 0.264     |\n",
      "---------------------------------------\n",
      "Ep done - 74320.\n",
      "Ep done - 74330.\n",
      "Ep done - 74340.\n",
      "Ep done - 20010.\n",
      "Ep done - 20020.\n",
      "Ep done - 20030.\n",
      "Ep done - 20040.\n",
      "Ep done - 20050.\n",
      "Ep done - 20060.\n",
      "Ep done - 20070.\n",
      "Ep done - 20080.\n",
      "Ep done - 20090.\n",
      "Ep done - 20100.\n",
      "Eval num_timesteps=2010000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2010000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025190164 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.61e-05    |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.142        |\n",
      "|    n_updates            | 22500        |\n",
      "|    policy_gradient_loss | -6.5e-05     |\n",
      "|    value_loss           | 0.27         |\n",
      "------------------------------------------\n",
      "Ep done - 74350.\n",
      "Ep done - 74360.\n",
      "Ep done - 74370.\n",
      "Ep done - 74380.\n",
      "Ep done - 74390.\n",
      "Ep done - 74400.\n",
      "Ep done - 74410.\n",
      "Ep done - 74420.\n",
      "Ep done - 74430.\n",
      "Ep done - 74440.\n",
      "Ep done - 74450.\n",
      "Ep done - 74460.\n",
      "Ep done - 74470.\n",
      "Ep done - 74480.\n",
      "Ep done - 74490.\n",
      "Ep done - 74500.\n",
      "Ep done - 74510.\n",
      "Ep done - 74520.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 7640     |\n",
      "|    total_timesteps | 2015232  |\n",
      "---------------------------------\n",
      "Ep done - 74530.\n",
      "Ep done - 74540.\n",
      "Ep done - 74550.\n",
      "Ep done - 74560.\n",
      "Ep done - 74570.\n",
      "Ep done - 74580.\n",
      "Ep done - 74590.\n",
      "Ep done - 74600.\n",
      "Ep done - 74610.\n",
      "Ep done - 74620.\n",
      "Ep done - 74630.\n",
      "Ep done - 74640.\n",
      "Ep done - 74650.\n",
      "Ep done - 74660.\n",
      "Ep done - 74670.\n",
      "Ep done - 74680.\n",
      "Ep done - 20110.\n",
      "Ep done - 20120.\n",
      "Ep done - 20130.\n",
      "Ep done - 20140.\n",
      "Ep done - 20150.\n",
      "Ep done - 20160.\n",
      "Ep done - 20170.\n",
      "Ep done - 20180.\n",
      "Ep done - 20190.\n",
      "Ep done - 20200.\n",
      "Eval num_timesteps=2020000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -0.4          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2020000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7252903e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -5.03e-05     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.11          |\n",
      "|    n_updates            | 22510         |\n",
      "|    policy_gradient_loss | 4.6e-07       |\n",
      "|    value_loss           | 0.239         |\n",
      "-------------------------------------------\n",
      "Ep done - 74690.\n",
      "Ep done - 74700.\n",
      "Ep done - 74710.\n",
      "Ep done - 74720.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 329      |\n",
      "|    time_elapsed    | 7660     |\n",
      "|    total_timesteps | 2021376  |\n",
      "---------------------------------\n",
      "Ep done - 74730.\n",
      "Ep done - 74740.\n",
      "Ep done - 74750.\n",
      "Ep done - 74760.\n",
      "Ep done - 74770.\n",
      "Ep done - 74780.\n",
      "Ep done - 74790.\n",
      "Ep done - 74800.\n",
      "Ep done - 74810.\n",
      "Ep done - 74820.\n",
      "Ep done - 74830.\n",
      "Ep done - 74840.\n",
      "Ep done - 74850.\n",
      "Ep done - 74860.\n",
      "Ep done - 74870.\n",
      "Ep done - 74880.\n",
      "Ep done - 74890.\n",
      "Ep done - 74900.\n",
      "Ep done - 74910.\n",
      "Ep done - 74920.\n",
      "Ep done - 74930.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.38     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 264       |\n",
      "|    iterations           | 330       |\n",
      "|    time_elapsed         | 7678      |\n",
      "|    total_timesteps      | 2027520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6173102 |\n",
      "|    clip_fraction        | 0.00347   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.4e-05  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.12      |\n",
      "|    n_updates            | 22520     |\n",
      "|    policy_gradient_loss | -0.000966 |\n",
      "|    value_loss           | 0.247     |\n",
      "---------------------------------------\n",
      "Ep done - 74940.\n",
      "Ep done - 74950.\n",
      "Ep done - 74960.\n",
      "Ep done - 74970.\n",
      "Ep done - 74980.\n",
      "Ep done - 74990.\n",
      "Ep done - 75000.\n",
      "Ep done - 75010.\n",
      "Ep done - 20210.\n",
      "Ep done - 20220.\n",
      "Ep done - 20230.\n",
      "Ep done - 20240.\n",
      "Ep done - 20250.\n",
      "Ep done - 20260.\n",
      "Ep done - 20270.\n",
      "Ep done - 20280.\n",
      "Ep done - 20290.\n",
      "Ep done - 20300.\n",
      "Eval num_timesteps=2030000, episode_reward=-0.32 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.32    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 2030000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -5.7e-05 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.111    |\n",
      "|    n_updates            | 22530    |\n",
      "|    policy_gradient_loss | 1.85e-07 |\n",
      "|    value_loss           | 0.245    |\n",
      "--------------------------------------\n",
      "Ep done - 75020.\n",
      "Ep done - 75030.\n",
      "Ep done - 75040.\n",
      "Ep done - 75050.\n",
      "Ep done - 75060.\n",
      "Ep done - 75070.\n",
      "Ep done - 75080.\n",
      "Ep done - 75090.\n",
      "Ep done - 75100.\n",
      "Ep done - 75110.\n",
      "Ep done - 75120.\n",
      "Ep done - 75130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 331      |\n",
      "|    time_elapsed    | 7700     |\n",
      "|    total_timesteps | 2033664  |\n",
      "---------------------------------\n",
      "Ep done - 75140.\n",
      "Ep done - 75150.\n",
      "Ep done - 75160.\n",
      "Ep done - 75170.\n",
      "Ep done - 75180.\n",
      "Ep done - 75190.\n",
      "Ep done - 75200.\n",
      "Ep done - 75210.\n",
      "Ep done - 75220.\n",
      "Ep done - 75230.\n",
      "Ep done - 75240.\n",
      "Ep done - 75250.\n",
      "Ep done - 75260.\n",
      "Ep done - 75270.\n",
      "Ep done - 75280.\n",
      "Ep done - 75290.\n",
      "Ep done - 75300.\n",
      "Ep done - 75310.\n",
      "Ep done - 75320.\n",
      "Ep done - 75330.\n",
      "Ep done - 75340.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 332       |\n",
      "|    time_elapsed         | 7728      |\n",
      "|    total_timesteps      | 2039808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.11e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.143     |\n",
      "|    n_updates            | 22540     |\n",
      "|    policy_gradient_loss | 5.2e-08   |\n",
      "|    value_loss           | 0.24      |\n",
      "---------------------------------------\n",
      "Ep done - 20310.\n",
      "Ep done - 20320.\n",
      "Ep done - 20330.\n",
      "Ep done - 20340.\n",
      "Ep done - 20350.\n",
      "Ep done - 20360.\n",
      "Ep done - 20370.\n",
      "Ep done - 20380.\n",
      "Ep done - 20390.\n",
      "Ep done - 20400.\n",
      "Eval num_timesteps=2040000, episode_reward=-0.38 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.38     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2040000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.81e-05 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.121     |\n",
      "|    n_updates            | 22550     |\n",
      "|    policy_gradient_loss | 2.96e-08  |\n",
      "|    value_loss           | 0.247     |\n",
      "---------------------------------------\n",
      "Ep done - 75350.\n",
      "Ep done - 75360.\n",
      "Ep done - 75370.\n",
      "Ep done - 75380.\n",
      "Ep done - 75390.\n",
      "Ep done - 75400.\n",
      "Ep done - 75410.\n",
      "Ep done - 75420.\n",
      "Ep done - 75430.\n",
      "Ep done - 75440.\n",
      "Ep done - 75450.\n",
      "Ep done - 75460.\n",
      "Ep done - 75470.\n",
      "Ep done - 75480.\n",
      "Ep done - 75490.\n",
      "Ep done - 75500.\n",
      "Ep done - 75510.\n",
      "Ep done - 75520.\n",
      "Ep done - 75530.\n",
      "Ep done - 75540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 333      |\n",
      "|    time_elapsed    | 7752     |\n",
      "|    total_timesteps | 2045952  |\n",
      "---------------------------------\n",
      "Ep done - 75550.\n",
      "Ep done - 75560.\n",
      "Ep done - 75570.\n",
      "Ep done - 75580.\n",
      "Ep done - 75590.\n",
      "Ep done - 75600.\n",
      "Ep done - 75610.\n",
      "Ep done - 75620.\n",
      "Ep done - 75630.\n",
      "Ep done - 75640.\n",
      "Ep done - 75650.\n",
      "Ep done - 75660.\n",
      "Ep done - 75670.\n",
      "Ep done - 75680.\n",
      "Ep done - 20410.\n",
      "Ep done - 20420.\n",
      "Ep done - 20430.\n",
      "Ep done - 20440.\n",
      "Ep done - 20450.\n",
      "Ep done - 20460.\n",
      "Ep done - 20470.\n",
      "Ep done - 20480.\n",
      "Ep done - 20490.\n",
      "Ep done - 20500.\n",
      "Eval num_timesteps=2050000, episode_reward=-0.32 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.32     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2050000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.47e-05 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.127     |\n",
      "|    n_updates            | 22560     |\n",
      "|    policy_gradient_loss | 2.38e-08  |\n",
      "|    value_loss           | 0.242     |\n",
      "---------------------------------------\n",
      "Ep done - 75690.\n",
      "Ep done - 75700.\n",
      "Ep done - 75710.\n",
      "Ep done - 75720.\n",
      "Ep done - 75730.\n",
      "Ep done - 75740.\n",
      "Ep done - 75750.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 7781     |\n",
      "|    total_timesteps | 2052096  |\n",
      "---------------------------------\n",
      "Ep done - 75760.\n",
      "Ep done - 75770.\n",
      "Ep done - 75780.\n",
      "Ep done - 75790.\n",
      "Ep done - 75800.\n",
      "Ep done - 75810.\n",
      "Ep done - 75820.\n",
      "Ep done - 75830.\n",
      "Ep done - 75840.\n",
      "Ep done - 75850.\n",
      "Ep done - 75860.\n",
      "Ep done - 75870.\n",
      "Ep done - 75880.\n",
      "Ep done - 75890.\n",
      "Ep done - 75900.\n",
      "Ep done - 75910.\n",
      "Ep done - 75920.\n",
      "Ep done - 75930.\n",
      "Ep done - 75940.\n",
      "Ep done - 75950.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 335       |\n",
      "|    time_elapsed         | 7802      |\n",
      "|    total_timesteps      | 2058240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.08e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.15      |\n",
      "|    n_updates            | 22570     |\n",
      "|    policy_gradient_loss | 6.88e-08  |\n",
      "|    value_loss           | 0.265     |\n",
      "---------------------------------------\n",
      "Ep done - 75960.\n",
      "Ep done - 75970.\n",
      "Ep done - 75980.\n",
      "Ep done - 75990.\n",
      "Ep done - 76000.\n",
      "Ep done - 76010.\n",
      "Ep done - 20510.\n",
      "Ep done - 20520.\n",
      "Ep done - 20530.\n",
      "Ep done - 20540.\n",
      "Ep done - 20550.\n",
      "Ep done - 20560.\n",
      "Ep done - 20570.\n",
      "Ep done - 20580.\n",
      "Ep done - 20590.\n",
      "Ep done - 20600.\n",
      "Eval num_timesteps=2060000, episode_reward=-0.46 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -0.46         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2060000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9616134e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -5.39e-05     |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.105         |\n",
      "|    n_updates            | 22580         |\n",
      "|    policy_gradient_loss | -1.5e-05      |\n",
      "|    value_loss           | 0.245         |\n",
      "-------------------------------------------\n",
      "Ep done - 76020.\n",
      "Ep done - 76030.\n",
      "Ep done - 76040.\n",
      "Ep done - 76050.\n",
      "Ep done - 76060.\n",
      "Ep done - 76070.\n",
      "Ep done - 76080.\n",
      "Ep done - 76090.\n",
      "Ep done - 76100.\n",
      "Ep done - 76110.\n",
      "Ep done - 76120.\n",
      "Ep done - 76130.\n",
      "Ep done - 76140.\n",
      "Ep done - 76150.\n",
      "Ep done - 76160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 7829     |\n",
      "|    total_timesteps | 2064384  |\n",
      "---------------------------------\n",
      "Ep done - 76170.\n",
      "Ep done - 76180.\n",
      "Ep done - 76190.\n",
      "Ep done - 76200.\n",
      "Ep done - 76210.\n",
      "Ep done - 76220.\n",
      "Ep done - 76230.\n",
      "Ep done - 76240.\n",
      "Ep done - 76250.\n",
      "Ep done - 76260.\n",
      "Ep done - 76270.\n",
      "Ep done - 76280.\n",
      "Ep done - 76290.\n",
      "Ep done - 76300.\n",
      "Ep done - 76310.\n",
      "Ep done - 76320.\n",
      "Ep done - 76330.\n",
      "Ep done - 76340.\n",
      "Ep done - 20610.\n",
      "Ep done - 20620.\n",
      "Ep done - 20630.\n",
      "Ep done - 20640.\n",
      "Ep done - 20650.\n",
      "Ep done - 20660.\n",
      "Ep done - 20670.\n",
      "Ep done - 20680.\n",
      "Ep done - 20690.\n",
      "Ep done - 20700.\n",
      "Eval num_timesteps=2070000, episode_reward=-0.38 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.38        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2070000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021054249 |\n",
      "|    clip_fraction        | 3.26e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.9e-05     |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0905       |\n",
      "|    n_updates            | 22590        |\n",
      "|    policy_gradient_loss | -6.76e-06    |\n",
      "|    value_loss           | 0.23         |\n",
      "------------------------------------------\n",
      "Ep done - 76350.\n",
      "Ep done - 76360.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 337      |\n",
      "|    time_elapsed    | 7856     |\n",
      "|    total_timesteps | 2070528  |\n",
      "---------------------------------\n",
      "Ep done - 76370.\n",
      "Ep done - 76380.\n",
      "Ep done - 76390.\n",
      "Ep done - 76400.\n",
      "Ep done - 76410.\n",
      "Ep done - 76420.\n",
      "Ep done - 76430.\n",
      "Ep done - 76440.\n",
      "Ep done - 76450.\n",
      "Ep done - 76460.\n",
      "Ep done - 76470.\n",
      "Ep done - 76480.\n",
      "Ep done - 76490.\n",
      "Ep done - 76500.\n",
      "Ep done - 76510.\n",
      "Ep done - 76520.\n",
      "Ep done - 76530.\n",
      "Ep done - 76540.\n",
      "Ep done - 76550.\n",
      "Ep done - 76560.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.48     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 338       |\n",
      "|    time_elapsed         | 7876      |\n",
      "|    total_timesteps      | 2076672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.37e-05 |\n",
      "|    explained_variance   | 2.44e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.153     |\n",
      "|    n_updates            | 22600     |\n",
      "|    policy_gradient_loss | 2.51e-08  |\n",
      "|    value_loss           | 0.232     |\n",
      "---------------------------------------\n",
      "Ep done - 76570.\n",
      "Ep done - 76580.\n",
      "Ep done - 76590.\n",
      "Ep done - 76600.\n",
      "Ep done - 76610.\n",
      "Ep done - 76620.\n",
      "Ep done - 76630.\n",
      "Ep done - 76640.\n",
      "Ep done - 76650.\n",
      "Ep done - 76660.\n",
      "Ep done - 76670.\n",
      "Ep done - 76680.\n",
      "Ep done - 20710.\n",
      "Ep done - 20720.\n",
      "Ep done - 20730.\n",
      "Ep done - 20740.\n",
      "Ep done - 20750.\n",
      "Ep done - 20760.\n",
      "Ep done - 20770.\n",
      "Ep done - 20780.\n",
      "Ep done - 20790.\n",
      "Ep done - 20800.\n",
      "Eval num_timesteps=2080000, episode_reward=-0.62 +/- 0.78\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.62     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2080000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.64e-05 |\n",
      "|    explained_variance   | -1.45e-05 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.122     |\n",
      "|    n_updates            | 22610     |\n",
      "|    policy_gradient_loss | 6.93e-09  |\n",
      "|    value_loss           | 0.247     |\n",
      "---------------------------------------\n",
      "Ep done - 76690.\n",
      "Ep done - 76700.\n",
      "Ep done - 76710.\n",
      "Ep done - 76720.\n",
      "Ep done - 76730.\n",
      "Ep done - 76740.\n",
      "Ep done - 76750.\n",
      "Ep done - 76760.\n",
      "Ep done - 76770.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 339      |\n",
      "|    time_elapsed    | 7901     |\n",
      "|    total_timesteps | 2082816  |\n",
      "---------------------------------\n",
      "Ep done - 76780.\n",
      "Ep done - 76790.\n",
      "Ep done - 76800.\n",
      "Ep done - 76810.\n",
      "Ep done - 76820.\n",
      "Ep done - 76830.\n",
      "Ep done - 76840.\n",
      "Ep done - 76850.\n",
      "Ep done - 76860.\n",
      "Ep done - 76870.\n",
      "Ep done - 76880.\n",
      "Ep done - 76890.\n",
      "Ep done - 76900.\n",
      "Ep done - 76910.\n",
      "Ep done - 76920.\n",
      "Ep done - 76930.\n",
      "Ep done - 76940.\n",
      "Ep done - 76950.\n",
      "Ep done - 76960.\n",
      "Ep done - 76970.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 340       |\n",
      "|    time_elapsed         | 7924      |\n",
      "|    total_timesteps      | 2088960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.34e-05 |\n",
      "|    explained_variance   | 1.13e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0809    |\n",
      "|    n_updates            | 22620     |\n",
      "|    policy_gradient_loss | -1.88e-10 |\n",
      "|    value_loss           | 0.242     |\n",
      "---------------------------------------\n",
      "Ep done - 76980.\n",
      "Ep done - 76990.\n",
      "Ep done - 77000.\n",
      "Ep done - 77010.\n",
      "Ep done - 20810.\n",
      "Ep done - 20820.\n",
      "Ep done - 20830.\n",
      "Ep done - 20840.\n",
      "Ep done - 20850.\n",
      "Ep done - 20860.\n",
      "Ep done - 20870.\n",
      "Ep done - 20880.\n",
      "Ep done - 20890.\n",
      "Ep done - 20900.\n",
      "Eval num_timesteps=2090000, episode_reward=-0.42 +/- 0.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2090000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.36e-05 |\n",
      "|    explained_variance   | 2.38e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.135     |\n",
      "|    n_updates            | 22630     |\n",
      "|    policy_gradient_loss | 8.29e-10  |\n",
      "|    value_loss           | 0.233     |\n",
      "---------------------------------------\n",
      "Ep done - 77020.\n",
      "Ep done - 77030.\n",
      "Ep done - 77040.\n",
      "Ep done - 77050.\n",
      "Ep done - 77060.\n",
      "Ep done - 77070.\n",
      "Ep done - 77080.\n",
      "Ep done - 77090.\n",
      "Ep done - 77100.\n",
      "Ep done - 77110.\n",
      "Ep done - 77120.\n",
      "Ep done - 77130.\n",
      "Ep done - 77140.\n",
      "Ep done - 77150.\n",
      "Ep done - 77160.\n",
      "Ep done - 77170.\n",
      "Ep done - 77180.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 341      |\n",
      "|    time_elapsed    | 7948     |\n",
      "|    total_timesteps | 2095104  |\n",
      "---------------------------------\n",
      "Ep done - 77190.\n",
      "Ep done - 77200.\n",
      "Ep done - 77210.\n",
      "Ep done - 77220.\n",
      "Ep done - 77230.\n",
      "Ep done - 77240.\n",
      "Ep done - 77250.\n",
      "Ep done - 77260.\n",
      "Ep done - 77270.\n",
      "Ep done - 77280.\n",
      "Ep done - 77290.\n",
      "Ep done - 77300.\n",
      "Ep done - 77310.\n",
      "Ep done - 77320.\n",
      "Ep done - 77330.\n",
      "Ep done - 77340.\n",
      "Ep done - 20910.\n",
      "Ep done - 20920.\n",
      "Ep done - 20930.\n",
      "Ep done - 20940.\n",
      "Ep done - 20950.\n",
      "Ep done - 20960.\n",
      "Ep done - 20970.\n",
      "Ep done - 20980.\n",
      "Ep done - 20990.\n",
      "Ep done - 21000.\n",
      "Eval num_timesteps=2100000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2100000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.22e-05 |\n",
      "|    explained_variance   | -1.97e-05 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0944    |\n",
      "|    n_updates            | 22640     |\n",
      "|    policy_gradient_loss | 9.27e-10  |\n",
      "|    value_loss           | 0.272     |\n",
      "---------------------------------------\n",
      "Ep done - 77350.\n",
      "Ep done - 77360.\n",
      "Ep done - 77370.\n",
      "Ep done - 77380.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 7973     |\n",
      "|    total_timesteps | 2101248  |\n",
      "---------------------------------\n",
      "Ep done - 77390.\n",
      "Ep done - 77400.\n",
      "Ep done - 77410.\n",
      "Ep done - 77420.\n",
      "Ep done - 77430.\n",
      "Ep done - 77440.\n",
      "Ep done - 77450.\n",
      "Ep done - 77460.\n",
      "Ep done - 77470.\n",
      "Ep done - 77480.\n",
      "Ep done - 77490.\n",
      "Ep done - 77500.\n",
      "Ep done - 77510.\n",
      "Ep done - 77520.\n",
      "Ep done - 77530.\n",
      "Ep done - 77540.\n",
      "Ep done - 77550.\n",
      "Ep done - 77560.\n",
      "Ep done - 77570.\n",
      "Ep done - 77580.\n",
      "Ep done - 77590.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.34     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 343       |\n",
      "|    time_elapsed         | 7995      |\n",
      "|    total_timesteps      | 2107392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.14e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.141     |\n",
      "|    n_updates            | 22650     |\n",
      "|    policy_gradient_loss | 4.82e-10  |\n",
      "|    value_loss           | 0.267     |\n",
      "---------------------------------------\n",
      "Ep done - 77600.\n",
      "Ep done - 77610.\n",
      "Ep done - 77620.\n",
      "Ep done - 77630.\n",
      "Ep done - 77640.\n",
      "Ep done - 77650.\n",
      "Ep done - 77660.\n",
      "Ep done - 77670.\n",
      "Ep done - 77680.\n",
      "Ep done - 21010.\n",
      "Ep done - 21020.\n",
      "Ep done - 21030.\n",
      "Ep done - 21040.\n",
      "Ep done - 21050.\n",
      "Ep done - 21060.\n",
      "Ep done - 21070.\n",
      "Ep done - 21080.\n",
      "Ep done - 21090.\n",
      "Ep done - 21100.\n",
      "Eval num_timesteps=2110000, episode_reward=-0.34 +/- 0.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.34     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2110000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.24e-05 |\n",
      "|    explained_variance   | 1.07e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.101     |\n",
      "|    n_updates            | 22660     |\n",
      "|    policy_gradient_loss | -8.67e-10 |\n",
      "|    value_loss           | 0.253     |\n",
      "---------------------------------------\n",
      "Ep done - 77690.\n",
      "Ep done - 77700.\n",
      "Ep done - 77710.\n",
      "Ep done - 77720.\n",
      "Ep done - 77730.\n",
      "Ep done - 77740.\n",
      "Ep done - 77750.\n",
      "Ep done - 77760.\n",
      "Ep done - 77770.\n",
      "Ep done - 77780.\n",
      "Ep done - 77790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 8021     |\n",
      "|    total_timesteps | 2113536  |\n",
      "---------------------------------\n",
      "Ep done - 77800.\n",
      "Ep done - 77810.\n",
      "Ep done - 77820.\n",
      "Ep done - 77830.\n",
      "Ep done - 77840.\n",
      "Ep done - 77850.\n",
      "Ep done - 77860.\n",
      "Ep done - 77870.\n",
      "Ep done - 77880.\n",
      "Ep done - 77890.\n",
      "Ep done - 77900.\n",
      "Ep done - 77910.\n",
      "Ep done - 77920.\n",
      "Ep done - 77930.\n",
      "Ep done - 77940.\n",
      "Ep done - 77950.\n",
      "Ep done - 77960.\n",
      "Ep done - 77970.\n",
      "Ep done - 77980.\n",
      "Ep done - 77990.\n",
      "Ep done - 78000.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 345         |\n",
      "|    time_elapsed         | 8044        |\n",
      "|    total_timesteps      | 2119680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004363263 |\n",
      "|    clip_fraction        | 0.0036      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.00063    |\n",
      "|    explained_variance   | 1.01e-06    |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.141       |\n",
      "|    n_updates            | 22670       |\n",
      "|    policy_gradient_loss | 0.000939    |\n",
      "|    value_loss           | 0.254       |\n",
      "-----------------------------------------\n",
      "Ep done - 78010.\n",
      "Ep done - 21110.\n",
      "Ep done - 21120.\n",
      "Ep done - 21130.\n",
      "Ep done - 21140.\n",
      "Ep done - 21150.\n",
      "Ep done - 21160.\n",
      "Ep done - 21170.\n",
      "Ep done - 21180.\n",
      "Ep done - 21190.\n",
      "Ep done - 21200.\n",
      "Eval num_timesteps=2120000, episode_reward=-0.38 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.38     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2120000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.91e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.125     |\n",
      "|    n_updates            | 22680     |\n",
      "|    policy_gradient_loss | 2.84e-06  |\n",
      "|    value_loss           | 0.262     |\n",
      "---------------------------------------\n",
      "Ep done - 78020.\n",
      "Ep done - 78030.\n",
      "Ep done - 78040.\n",
      "Ep done - 78050.\n",
      "Ep done - 78060.\n",
      "Ep done - 78070.\n",
      "Ep done - 78080.\n",
      "Ep done - 78090.\n",
      "Ep done - 78100.\n",
      "Ep done - 78110.\n",
      "Ep done - 78120.\n",
      "Ep done - 78130.\n",
      "Ep done - 78140.\n",
      "Ep done - 78150.\n",
      "Ep done - 78160.\n",
      "Ep done - 78170.\n",
      "Ep done - 78180.\n",
      "Ep done - 78190.\n",
      "Ep done - 78200.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 8069     |\n",
      "|    total_timesteps | 2125824  |\n",
      "---------------------------------\n",
      "Ep done - 78210.\n",
      "Ep done - 78220.\n",
      "Ep done - 78230.\n",
      "Ep done - 78240.\n",
      "Ep done - 78250.\n",
      "Ep done - 78260.\n",
      "Ep done - 78270.\n",
      "Ep done - 78280.\n",
      "Ep done - 78290.\n",
      "Ep done - 78300.\n",
      "Ep done - 78310.\n",
      "Ep done - 78320.\n",
      "Ep done - 78330.\n",
      "Ep done - 78340.\n",
      "Ep done - 21210.\n",
      "Ep done - 21220.\n",
      "Ep done - 21230.\n",
      "Ep done - 21240.\n",
      "Ep done - 21250.\n",
      "Ep done - 21260.\n",
      "Ep done - 21270.\n",
      "Ep done - 21280.\n",
      "Ep done - 21290.\n",
      "Ep done - 21300.\n",
      "Eval num_timesteps=2130000, episode_reward=-0.26 +/- 0.97\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.26     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2130000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.25e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.147     |\n",
      "|    n_updates            | 22690     |\n",
      "|    policy_gradient_loss | -5.01e-08 |\n",
      "|    value_loss           | 0.26      |\n",
      "---------------------------------------\n",
      "Ep done - 78350.\n",
      "Ep done - 78360.\n",
      "Ep done - 78370.\n",
      "Ep done - 78380.\n",
      "Ep done - 78390.\n",
      "Ep done - 78400.\n",
      "Ep done - 78410.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 347      |\n",
      "|    time_elapsed    | 8096     |\n",
      "|    total_timesteps | 2131968  |\n",
      "---------------------------------\n",
      "Ep done - 78420.\n",
      "Ep done - 78430.\n",
      "Ep done - 78440.\n",
      "Ep done - 78450.\n",
      "Ep done - 78460.\n",
      "Ep done - 78470.\n",
      "Ep done - 78480.\n",
      "Ep done - 78490.\n",
      "Ep done - 78500.\n",
      "Ep done - 78510.\n",
      "Ep done - 78520.\n",
      "Ep done - 78530.\n",
      "Ep done - 78540.\n",
      "Ep done - 78550.\n",
      "Ep done - 78560.\n",
      "Ep done - 78570.\n",
      "Ep done - 78580.\n",
      "Ep done - 78590.\n",
      "Ep done - 78600.\n",
      "Ep done - 78610.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -0.48         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 263           |\n",
      "|    iterations           | 348           |\n",
      "|    time_elapsed         | 8121          |\n",
      "|    total_timesteps      | 2138112       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3735373e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -5.94e-05     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.115         |\n",
      "|    n_updates            | 22700         |\n",
      "|    policy_gradient_loss | 3.51e-07      |\n",
      "|    value_loss           | 0.256         |\n",
      "-------------------------------------------\n",
      "Ep done - 78620.\n",
      "Ep done - 78630.\n",
      "Ep done - 78640.\n",
      "Ep done - 78650.\n",
      "Ep done - 78660.\n",
      "Ep done - 78670.\n",
      "Ep done - 78680.\n",
      "Ep done - 21310.\n",
      "Ep done - 21320.\n",
      "Ep done - 21330.\n",
      "Ep done - 21340.\n",
      "Ep done - 21350.\n",
      "Ep done - 21360.\n",
      "Ep done - 21370.\n",
      "Ep done - 21380.\n",
      "Ep done - 21390.\n",
      "Ep done - 21400.\n",
      "Eval num_timesteps=2140000, episode_reward=-0.34 +/- 0.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.34     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2140000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.46e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0989    |\n",
      "|    n_updates            | 22710     |\n",
      "|    policy_gradient_loss | 4.55e-09  |\n",
      "|    value_loss           | 0.225     |\n",
      "---------------------------------------\n",
      "Ep done - 78690.\n",
      "Ep done - 78700.\n",
      "Ep done - 78710.\n",
      "Ep done - 78720.\n",
      "Ep done - 78730.\n",
      "Ep done - 78740.\n",
      "Ep done - 78750.\n",
      "Ep done - 78760.\n",
      "Ep done - 78770.\n",
      "Ep done - 78780.\n",
      "Ep done - 78790.\n",
      "Ep done - 78800.\n",
      "Ep done - 78810.\n",
      "Ep done - 78820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 349      |\n",
      "|    time_elapsed    | 8148     |\n",
      "|    total_timesteps | 2144256  |\n",
      "---------------------------------\n",
      "Ep done - 78830.\n",
      "Ep done - 78840.\n",
      "Ep done - 78850.\n",
      "Ep done - 78860.\n",
      "Ep done - 78870.\n",
      "Ep done - 78880.\n",
      "Ep done - 78890.\n",
      "Ep done - 78900.\n",
      "Ep done - 78910.\n",
      "Ep done - 78920.\n",
      "Ep done - 78930.\n",
      "Ep done - 78940.\n",
      "Ep done - 78950.\n",
      "Ep done - 78960.\n",
      "Ep done - 78970.\n",
      "Ep done - 78980.\n",
      "Ep done - 78990.\n",
      "Ep done - 79000.\n",
      "Ep done - 79010.\n",
      "Ep done - 21410.\n",
      "Ep done - 21420.\n",
      "Ep done - 21430.\n",
      "Ep done - 21440.\n",
      "Ep done - 21450.\n",
      "Ep done - 21460.\n",
      "Ep done - 21470.\n",
      "Ep done - 21480.\n",
      "Ep done - 21490.\n",
      "Ep done - 21500.\n",
      "Eval num_timesteps=2150000, episode_reward=-0.38 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.38     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2150000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.33e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.127     |\n",
      "|    n_updates            | 22720     |\n",
      "|    policy_gradient_loss | 5.66e-09  |\n",
      "|    value_loss           | 0.268     |\n",
      "---------------------------------------\n",
      "Ep done - 79020.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 8176     |\n",
      "|    total_timesteps | 2150400  |\n",
      "---------------------------------\n",
      "Ep done - 79030.\n",
      "Ep done - 79040.\n",
      "Ep done - 79050.\n",
      "Ep done - 79060.\n",
      "Ep done - 79070.\n",
      "Ep done - 79080.\n",
      "Ep done - 79090.\n",
      "Ep done - 79100.\n",
      "Ep done - 79110.\n",
      "Ep done - 79120.\n",
      "Ep done - 79130.\n",
      "Ep done - 79140.\n",
      "Ep done - 79150.\n",
      "Ep done - 79160.\n",
      "Ep done - 79170.\n",
      "Ep done - 79180.\n",
      "Ep done - 79190.\n",
      "Ep done - 79200.\n",
      "Ep done - 79210.\n",
      "Ep done - 79220.\n",
      "Ep done - 79230.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.48     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 351       |\n",
      "|    time_elapsed         | 8201      |\n",
      "|    total_timesteps      | 2156544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.19e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.116     |\n",
      "|    n_updates            | 22730     |\n",
      "|    policy_gradient_loss | -1.51e-09 |\n",
      "|    value_loss           | 0.241     |\n",
      "---------------------------------------\n",
      "Ep done - 79240.\n",
      "Ep done - 79250.\n",
      "Ep done - 79260.\n",
      "Ep done - 79270.\n",
      "Ep done - 79280.\n",
      "Ep done - 79290.\n",
      "Ep done - 79300.\n",
      "Ep done - 79310.\n",
      "Ep done - 79320.\n",
      "Ep done - 79330.\n",
      "Ep done - 79340.\n",
      "Ep done - 21510.\n",
      "Ep done - 21520.\n",
      "Ep done - 21530.\n",
      "Ep done - 21540.\n",
      "Ep done - 21550.\n",
      "Ep done - 21560.\n",
      "Ep done - 21570.\n",
      "Ep done - 21580.\n",
      "Ep done - 21590.\n",
      "Ep done - 21600.\n",
      "Eval num_timesteps=2160000, episode_reward=-0.44 +/- 0.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2160000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.04e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.108     |\n",
      "|    n_updates            | 22740     |\n",
      "|    policy_gradient_loss | 1.92e-10  |\n",
      "|    value_loss           | 0.254     |\n",
      "---------------------------------------\n",
      "Ep done - 79350.\n",
      "Ep done - 79360.\n",
      "Ep done - 79370.\n",
      "Ep done - 79380.\n",
      "Ep done - 79390.\n",
      "Ep done - 79400.\n",
      "Ep done - 79410.\n",
      "Ep done - 79420.\n",
      "Ep done - 79430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 8228     |\n",
      "|    total_timesteps | 2162688  |\n",
      "---------------------------------\n",
      "Ep done - 79440.\n",
      "Ep done - 79450.\n",
      "Ep done - 79460.\n",
      "Ep done - 79470.\n",
      "Ep done - 79480.\n",
      "Ep done - 79490.\n",
      "Ep done - 79500.\n",
      "Ep done - 79510.\n",
      "Ep done - 79520.\n",
      "Ep done - 79530.\n",
      "Ep done - 79540.\n",
      "Ep done - 79550.\n",
      "Ep done - 79560.\n",
      "Ep done - 79570.\n",
      "Ep done - 79580.\n",
      "Ep done - 79590.\n",
      "Ep done - 79600.\n",
      "Ep done - 79610.\n",
      "Ep done - 79620.\n",
      "Ep done - 79630.\n",
      "Ep done - 79640.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.56     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 353       |\n",
      "|    time_elapsed         | 8249      |\n",
      "|    total_timesteps      | 2168832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.92e-05 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.133     |\n",
      "|    n_updates            | 22750     |\n",
      "|    policy_gradient_loss | -6.43e-09 |\n",
      "|    value_loss           | 0.258     |\n",
      "---------------------------------------\n",
      "Ep done - 79650.\n",
      "Ep done - 79660.\n",
      "Ep done - 79670.\n",
      "Ep done - 79680.\n",
      "Ep done - 21610.\n",
      "Ep done - 21620.\n",
      "Ep done - 21630.\n",
      "Ep done - 21640.\n",
      "Ep done - 21650.\n",
      "Ep done - 21660.\n",
      "Ep done - 21670.\n",
      "Ep done - 21680.\n",
      "Ep done - 21690.\n",
      "Ep done - 21700.\n",
      "Eval num_timesteps=2170000, episode_reward=-0.32 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.32     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2170000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.91e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0791    |\n",
      "|    n_updates            | 22760     |\n",
      "|    policy_gradient_loss | 4.71e-10  |\n",
      "|    value_loss           | 0.232     |\n",
      "---------------------------------------\n",
      "Ep done - 79690.\n",
      "Ep done - 79700.\n",
      "Ep done - 79710.\n",
      "Ep done - 79720.\n",
      "Ep done - 79730.\n",
      "Ep done - 79740.\n",
      "Ep done - 79750.\n",
      "Ep done - 79760.\n",
      "Ep done - 79770.\n",
      "Ep done - 79780.\n",
      "Ep done - 79790.\n",
      "Ep done - 79800.\n",
      "Ep done - 79810.\n",
      "Ep done - 79820.\n",
      "Ep done - 79830.\n",
      "Ep done - 79840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 8283     |\n",
      "|    total_timesteps | 2174976  |\n",
      "---------------------------------\n",
      "Ep done - 79850.\n",
      "Ep done - 79860.\n",
      "Ep done - 79870.\n",
      "Ep done - 79880.\n",
      "Ep done - 79890.\n",
      "Ep done - 79900.\n",
      "Ep done - 79910.\n",
      "Ep done - 79920.\n",
      "Ep done - 79930.\n",
      "Ep done - 79940.\n",
      "Ep done - 79950.\n",
      "Ep done - 79960.\n",
      "Ep done - 79970.\n",
      "Ep done - 79980.\n",
      "Ep done - 79990.\n",
      "Ep done - 80000.\n",
      "Ep done - 80010.\n",
      "Ep done - 21710.\n",
      "Ep done - 21720.\n",
      "Ep done - 21730.\n",
      "Ep done - 21740.\n",
      "Ep done - 21750.\n",
      "Ep done - 21760.\n",
      "Ep done - 21770.\n",
      "Ep done - 21780.\n",
      "Ep done - 21790.\n",
      "Ep done - 21800.\n",
      "Eval num_timesteps=2180000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2180000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.65e-05 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.163     |\n",
      "|    n_updates            | 22770     |\n",
      "|    policy_gradient_loss | 2.62e-09  |\n",
      "|    value_loss           | 0.234     |\n",
      "---------------------------------------\n",
      "Ep done - 80020.\n",
      "Ep done - 80030.\n",
      "Ep done - 80040.\n",
      "Ep done - 80050.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 355      |\n",
      "|    time_elapsed    | 8310     |\n",
      "|    total_timesteps | 2181120  |\n",
      "---------------------------------\n",
      "Ep done - 80060.\n",
      "Ep done - 80070.\n",
      "Ep done - 80080.\n",
      "Ep done - 80090.\n",
      "Ep done - 80100.\n",
      "Ep done - 80110.\n",
      "Ep done - 80120.\n",
      "Ep done - 80130.\n",
      "Ep done - 80140.\n",
      "Ep done - 80150.\n",
      "Ep done - 80160.\n",
      "Ep done - 80170.\n",
      "Ep done - 80180.\n",
      "Ep done - 80190.\n",
      "Ep done - 80200.\n",
      "Ep done - 80210.\n",
      "Ep done - 80220.\n",
      "Ep done - 80230.\n",
      "Ep done - 80240.\n",
      "Ep done - 80250.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 356       |\n",
      "|    time_elapsed         | 8325      |\n",
      "|    total_timesteps      | 2187264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.52e-05 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.109     |\n",
      "|    n_updates            | 22780     |\n",
      "|    policy_gradient_loss | -5.79e-10 |\n",
      "|    value_loss           | 0.23      |\n",
      "---------------------------------------\n",
      "Ep done - 80260.\n",
      "Ep done - 80270.\n",
      "Ep done - 80280.\n",
      "Ep done - 80290.\n",
      "Ep done - 80300.\n",
      "Ep done - 80310.\n",
      "Ep done - 80320.\n",
      "Ep done - 80330.\n",
      "Ep done - 80340.\n",
      "Ep done - 21810.\n",
      "Ep done - 21820.\n",
      "Ep done - 21830.\n",
      "Ep done - 21840.\n",
      "Ep done - 21850.\n",
      "Ep done - 21860.\n",
      "Ep done - 21870.\n",
      "Ep done - 21880.\n",
      "Ep done - 21890.\n",
      "Ep done - 21900.\n",
      "Eval num_timesteps=2190000, episode_reward=-0.50 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.5     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 2190000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.4e-05 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.139    |\n",
      "|    n_updates            | 22790    |\n",
      "|    policy_gradient_loss | 4.1e-10  |\n",
      "|    value_loss           | 0.262    |\n",
      "--------------------------------------\n",
      "Ep done - 80350.\n",
      "Ep done - 80360.\n",
      "Ep done - 80370.\n",
      "Ep done - 80380.\n",
      "Ep done - 80390.\n",
      "Ep done - 80400.\n",
      "Ep done - 80410.\n",
      "Ep done - 80420.\n",
      "Ep done - 80430.\n",
      "Ep done - 80440.\n",
      "Ep done - 80450.\n",
      "Ep done - 80460.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 357      |\n",
      "|    time_elapsed    | 8344     |\n",
      "|    total_timesteps | 2193408  |\n",
      "---------------------------------\n",
      "Ep done - 80470.\n",
      "Ep done - 80480.\n",
      "Ep done - 80490.\n",
      "Ep done - 80500.\n",
      "Ep done - 80510.\n",
      "Ep done - 80520.\n",
      "Ep done - 80530.\n",
      "Ep done - 80540.\n",
      "Ep done - 80550.\n",
      "Ep done - 80560.\n",
      "Ep done - 80570.\n",
      "Ep done - 80580.\n",
      "Ep done - 80590.\n",
      "Ep done - 80600.\n",
      "Ep done - 80610.\n",
      "Ep done - 80620.\n",
      "Ep done - 80630.\n",
      "Ep done - 80640.\n",
      "Ep done - 80650.\n",
      "Ep done - 80660.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.58     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 358       |\n",
      "|    time_elapsed         | 8359      |\n",
      "|    total_timesteps      | 2199552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.33e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.146     |\n",
      "|    n_updates            | 22800     |\n",
      "|    policy_gradient_loss | 2.29e-10  |\n",
      "|    value_loss           | 0.238     |\n",
      "---------------------------------------\n",
      "Ep done - 80670.\n",
      "Ep done - 80680.\n",
      "Ep done - 21910.\n",
      "Ep done - 21920.\n",
      "Ep done - 21930.\n",
      "Ep done - 21940.\n",
      "Ep done - 21950.\n",
      "Ep done - 21960.\n",
      "Ep done - 21970.\n",
      "Ep done - 21980.\n",
      "Ep done - 21990.\n",
      "Ep done - 22000.\n",
      "Eval num_timesteps=2200000, episode_reward=-0.36 +/- 0.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -0.36         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2200000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.04e-05     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.137         |\n",
      "|    n_updates            | 22810         |\n",
      "|    policy_gradient_loss | -1.32e-06     |\n",
      "|    value_loss           | 0.24          |\n",
      "-------------------------------------------\n",
      "Ep done - 80690.\n",
      "Ep done - 80700.\n",
      "Ep done - 80710.\n",
      "Ep done - 80720.\n",
      "Ep done - 80730.\n",
      "Ep done - 80740.\n",
      "Ep done - 80750.\n",
      "Ep done - 80760.\n",
      "Ep done - 80770.\n",
      "Ep done - 80780.\n",
      "Ep done - 80790.\n",
      "Ep done - 80800.\n",
      "Ep done - 80810.\n",
      "Ep done - 80820.\n",
      "Ep done - 80830.\n",
      "Ep done - 80840.\n",
      "Ep done - 80850.\n",
      "Ep done - 80860.\n",
      "Ep done - 80870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 359      |\n",
      "|    time_elapsed    | 8380     |\n",
      "|    total_timesteps | 2205696  |\n",
      "---------------------------------\n",
      "Ep done - 80880.\n",
      "Ep done - 80890.\n",
      "Ep done - 80900.\n",
      "Ep done - 80910.\n",
      "Ep done - 80920.\n",
      "Ep done - 80930.\n",
      "Ep done - 80940.\n",
      "Ep done - 80950.\n",
      "Ep done - 80960.\n",
      "Ep done - 80970.\n",
      "Ep done - 80980.\n",
      "Ep done - 80990.\n",
      "Ep done - 81000.\n",
      "Ep done - 81010.\n",
      "Ep done - 22010.\n",
      "Ep done - 22020.\n",
      "Ep done - 22030.\n",
      "Ep done - 22040.\n",
      "Ep done - 22050.\n",
      "Ep done - 22060.\n",
      "Ep done - 22070.\n",
      "Ep done - 22080.\n",
      "Ep done - 22090.\n",
      "Ep done - 22100.\n",
      "Eval num_timesteps=2210000, episode_reward=-0.50 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2210000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.77e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.122     |\n",
      "|    n_updates            | 22820     |\n",
      "|    policy_gradient_loss | 3.5e-10   |\n",
      "|    value_loss           | 0.249     |\n",
      "---------------------------------------\n",
      "Ep done - 81020.\n",
      "Ep done - 81030.\n",
      "Ep done - 81040.\n",
      "Ep done - 81050.\n",
      "Ep done - 81060.\n",
      "Ep done - 81070.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 8400     |\n",
      "|    total_timesteps | 2211840  |\n",
      "---------------------------------\n",
      "Ep done - 81080.\n",
      "Ep done - 81090.\n",
      "Ep done - 81100.\n",
      "Ep done - 81110.\n",
      "Ep done - 81120.\n",
      "Ep done - 81130.\n",
      "Ep done - 81140.\n",
      "Ep done - 81150.\n",
      "Ep done - 81160.\n",
      "Ep done - 81170.\n",
      "Ep done - 81180.\n",
      "Ep done - 81190.\n",
      "Ep done - 81200.\n",
      "Ep done - 81210.\n",
      "Ep done - 81220.\n",
      "Ep done - 81230.\n",
      "Ep done - 81240.\n",
      "Ep done - 81250.\n",
      "Ep done - 81260.\n",
      "Ep done - 81270.\n",
      "Ep done - 81280.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.28     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 361       |\n",
      "|    time_elapsed         | 8414      |\n",
      "|    total_timesteps      | 2217984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.65e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.128     |\n",
      "|    n_updates            | 22830     |\n",
      "|    policy_gradient_loss | -1.01e-09 |\n",
      "|    value_loss           | 0.254     |\n",
      "---------------------------------------\n",
      "Ep done - 81290.\n",
      "Ep done - 81300.\n",
      "Ep done - 81310.\n",
      "Ep done - 81320.\n",
      "Ep done - 81330.\n",
      "Ep done - 81340.\n",
      "Ep done - 22110.\n",
      "Ep done - 22120.\n",
      "Ep done - 22130.\n",
      "Ep done - 22140.\n",
      "Ep done - 22150.\n",
      "Ep done - 22160.\n",
      "Ep done - 22170.\n",
      "Ep done - 22180.\n",
      "Ep done - 22190.\n",
      "Ep done - 22200.\n",
      "Eval num_timesteps=2220000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2220000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.17e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.129     |\n",
      "|    n_updates            | 22840     |\n",
      "|    policy_gradient_loss | -1.39e-09 |\n",
      "|    value_loss           | 0.268     |\n",
      "---------------------------------------\n",
      "Ep done - 81350.\n",
      "Ep done - 81360.\n",
      "Ep done - 81370.\n",
      "Ep done - 81380.\n",
      "Ep done - 81390.\n",
      "Ep done - 81400.\n",
      "Ep done - 81410.\n",
      "Ep done - 81420.\n",
      "Ep done - 81430.\n",
      "Ep done - 81440.\n",
      "Ep done - 81450.\n",
      "Ep done - 81460.\n",
      "Ep done - 81470.\n",
      "Ep done - 81480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 8433     |\n",
      "|    total_timesteps | 2224128  |\n",
      "---------------------------------\n",
      "Ep done - 81490.\n",
      "Ep done - 81500.\n",
      "Ep done - 81510.\n",
      "Ep done - 81520.\n",
      "Ep done - 81530.\n",
      "Ep done - 81540.\n",
      "Ep done - 81550.\n",
      "Ep done - 81560.\n",
      "Ep done - 81570.\n",
      "Ep done - 81580.\n",
      "Ep done - 81590.\n",
      "Ep done - 81600.\n",
      "Ep done - 81610.\n",
      "Ep done - 81620.\n",
      "Ep done - 81630.\n",
      "Ep done - 81640.\n",
      "Ep done - 81650.\n",
      "Ep done - 81660.\n",
      "Ep done - 81670.\n",
      "Ep done - 81680.\n",
      "Ep done - 22210.\n",
      "Ep done - 22220.\n",
      "Ep done - 22230.\n",
      "Ep done - 22240.\n",
      "Ep done - 22250.\n",
      "Ep done - 22260.\n",
      "Ep done - 22270.\n",
      "Ep done - 22280.\n",
      "Ep done - 22290.\n",
      "Ep done - 22300.\n",
      "Eval num_timesteps=2230000, episode_reward=-0.34 +/- 0.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.34     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2230000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.41e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.121     |\n",
      "|    n_updates            | 22850     |\n",
      "|    policy_gradient_loss | -6.84e-10 |\n",
      "|    value_loss           | 0.246     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 363      |\n",
      "|    time_elapsed    | 8457     |\n",
      "|    total_timesteps | 2230272  |\n",
      "---------------------------------\n",
      "Ep done - 81690.\n",
      "Ep done - 81700.\n",
      "Ep done - 81710.\n",
      "Ep done - 81720.\n",
      "Ep done - 81730.\n",
      "Ep done - 81740.\n",
      "Ep done - 81750.\n",
      "Ep done - 81760.\n",
      "Ep done - 81770.\n",
      "Ep done - 81780.\n",
      "Ep done - 81790.\n",
      "Ep done - 81800.\n",
      "Ep done - 81810.\n",
      "Ep done - 81820.\n",
      "Ep done - 81830.\n",
      "Ep done - 81840.\n",
      "Ep done - 81850.\n",
      "Ep done - 81860.\n",
      "Ep done - 81870.\n",
      "Ep done - 81880.\n",
      "Ep done - 81890.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.54     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 364       |\n",
      "|    time_elapsed         | 8477      |\n",
      "|    total_timesteps      | 2236416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.92e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.13      |\n",
      "|    n_updates            | 22860     |\n",
      "|    policy_gradient_loss | 9.88e-10  |\n",
      "|    value_loss           | 0.244     |\n",
      "---------------------------------------\n",
      "Ep done - 81900.\n",
      "Ep done - 81910.\n",
      "Ep done - 81920.\n",
      "Ep done - 81930.\n",
      "Ep done - 81940.\n",
      "Ep done - 81950.\n",
      "Ep done - 81960.\n",
      "Ep done - 81970.\n",
      "Ep done - 81980.\n",
      "Ep done - 81990.\n",
      "Ep done - 82000.\n",
      "Ep done - 82010.\n",
      "Ep done - 22310.\n",
      "Ep done - 22320.\n",
      "Ep done - 22330.\n",
      "Ep done - 22340.\n",
      "Ep done - 22350.\n",
      "Ep done - 22360.\n",
      "Ep done - 22370.\n",
      "Ep done - 22380.\n",
      "Ep done - 22390.\n",
      "Ep done - 22400.\n",
      "Eval num_timesteps=2240000, episode_reward=-0.46 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2240000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.61e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.104     |\n",
      "|    n_updates            | 22870     |\n",
      "|    policy_gradient_loss | -9.98e-11 |\n",
      "|    value_loss           | 0.231     |\n",
      "---------------------------------------\n",
      "Ep done - 82020.\n",
      "Ep done - 82030.\n",
      "Ep done - 82040.\n",
      "Ep done - 82050.\n",
      "Ep done - 82060.\n",
      "Ep done - 82070.\n",
      "Ep done - 82080.\n",
      "Ep done - 82090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 365      |\n",
      "|    time_elapsed    | 8498     |\n",
      "|    total_timesteps | 2242560  |\n",
      "---------------------------------\n",
      "Ep done - 82100.\n",
      "Ep done - 82110.\n",
      "Ep done - 82120.\n",
      "Ep done - 82130.\n",
      "Ep done - 82140.\n",
      "Ep done - 82150.\n",
      "Ep done - 82160.\n",
      "Ep done - 82170.\n",
      "Ep done - 82180.\n",
      "Ep done - 82190.\n",
      "Ep done - 82200.\n",
      "Ep done - 82210.\n",
      "Ep done - 82220.\n",
      "Ep done - 82230.\n",
      "Ep done - 82240.\n",
      "Ep done - 82250.\n",
      "Ep done - 82260.\n",
      "Ep done - 82270.\n",
      "Ep done - 82280.\n",
      "Ep done - 82290.\n",
      "Ep done - 82300.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.28     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 264       |\n",
      "|    iterations           | 366       |\n",
      "|    time_elapsed         | 8515      |\n",
      "|    total_timesteps      | 2248704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.86e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0929    |\n",
      "|    n_updates            | 22880     |\n",
      "|    policy_gradient_loss | 1.78e-09  |\n",
      "|    value_loss           | 0.234     |\n",
      "---------------------------------------\n",
      "Ep done - 82310.\n",
      "Ep done - 82320.\n",
      "Ep done - 82330.\n",
      "Ep done - 82340.\n",
      "Ep done - 22410.\n",
      "Ep done - 22420.\n",
      "Ep done - 22430.\n",
      "Ep done - 22440.\n",
      "Ep done - 22450.\n",
      "Ep done - 22460.\n",
      "Ep done - 22470.\n",
      "Ep done - 22480.\n",
      "Ep done - 22490.\n",
      "Ep done - 22500.\n",
      "Eval num_timesteps=2250000, episode_reward=-0.42 +/- 0.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.42    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 2250000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -8.3e-06 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.131    |\n",
      "|    n_updates            | 22890    |\n",
      "|    policy_gradient_loss | 3.99e-10 |\n",
      "|    value_loss           | 0.257    |\n",
      "--------------------------------------\n",
      "Ep done - 82350.\n",
      "Ep done - 82360.\n",
      "Ep done - 82370.\n",
      "Ep done - 82380.\n",
      "Ep done - 82390.\n",
      "Ep done - 82400.\n",
      "Ep done - 82410.\n",
      "Ep done - 82420.\n",
      "Ep done - 82430.\n",
      "Ep done - 82440.\n",
      "Ep done - 82450.\n",
      "Ep done - 82460.\n",
      "Ep done - 82470.\n",
      "Ep done - 82480.\n",
      "Ep done - 82490.\n",
      "Ep done - 82500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 367      |\n",
      "|    time_elapsed    | 8539     |\n",
      "|    total_timesteps | 2254848  |\n",
      "---------------------------------\n",
      "Ep done - 82510.\n",
      "Ep done - 82520.\n",
      "Ep done - 82530.\n",
      "Ep done - 82540.\n",
      "Ep done - 82550.\n",
      "Ep done - 82560.\n",
      "Ep done - 82570.\n",
      "Ep done - 82580.\n",
      "Ep done - 82590.\n",
      "Ep done - 82600.\n",
      "Ep done - 82610.\n",
      "Ep done - 82620.\n",
      "Ep done - 82630.\n",
      "Ep done - 82640.\n",
      "Ep done - 82650.\n",
      "Ep done - 82660.\n",
      "Ep done - 82670.\n",
      "Ep done - 82680.\n",
      "Ep done - 22510.\n",
      "Ep done - 22520.\n",
      "Ep done - 22530.\n",
      "Ep done - 22540.\n",
      "Ep done - 22550.\n",
      "Ep done - 22560.\n",
      "Ep done - 22570.\n",
      "Ep done - 22580.\n",
      "Ep done - 22590.\n",
      "Ep done - 22600.\n",
      "Eval num_timesteps=2260000, episode_reward=-0.42 +/- 0.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2260000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.49e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.159     |\n",
      "|    n_updates            | 22900     |\n",
      "|    policy_gradient_loss | -6.16e-09 |\n",
      "|    value_loss           | 0.273     |\n",
      "---------------------------------------\n",
      "Ep done - 82690.\n",
      "Ep done - 82700.\n",
      "Ep done - 82710.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 8560     |\n",
      "|    total_timesteps | 2260992  |\n",
      "---------------------------------\n",
      "Ep done - 82720.\n",
      "Ep done - 82730.\n",
      "Ep done - 82740.\n",
      "Ep done - 82750.\n",
      "Ep done - 82760.\n",
      "Ep done - 82770.\n",
      "Ep done - 82780.\n",
      "Ep done - 82790.\n",
      "Ep done - 82800.\n",
      "Ep done - 82810.\n",
      "Ep done - 82820.\n",
      "Ep done - 82830.\n",
      "Ep done - 82840.\n",
      "Ep done - 82850.\n",
      "Ep done - 82860.\n",
      "Ep done - 82870.\n",
      "Ep done - 82880.\n",
      "Ep done - 82890.\n",
      "Ep done - 82900.\n",
      "Ep done - 82910.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 264       |\n",
      "|    iterations           | 369       |\n",
      "|    time_elapsed         | 8582      |\n",
      "|    total_timesteps      | 2267136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.36e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.138     |\n",
      "|    n_updates            | 22910     |\n",
      "|    policy_gradient_loss | -3.48e-10 |\n",
      "|    value_loss           | 0.266     |\n",
      "---------------------------------------\n",
      "Ep done - 82920.\n",
      "Ep done - 82930.\n",
      "Ep done - 82940.\n",
      "Ep done - 82950.\n",
      "Ep done - 82960.\n",
      "Ep done - 82970.\n",
      "Ep done - 82980.\n",
      "Ep done - 82990.\n",
      "Ep done - 83000.\n",
      "Ep done - 83010.\n",
      "Ep done - 22610.\n",
      "Ep done - 22620.\n",
      "Ep done - 22630.\n",
      "Ep done - 22640.\n",
      "Ep done - 22650.\n",
      "Ep done - 22660.\n",
      "Ep done - 22670.\n",
      "Ep done - 22680.\n",
      "Ep done - 22690.\n",
      "Ep done - 22700.\n",
      "Eval num_timesteps=2270000, episode_reward=-0.22 +/- 0.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.22     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2270000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.66e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.112     |\n",
      "|    n_updates            | 22920     |\n",
      "|    policy_gradient_loss | -4.86e-10 |\n",
      "|    value_loss           | 0.241     |\n",
      "---------------------------------------\n",
      "Ep done - 83020.\n",
      "Ep done - 83030.\n",
      "Ep done - 83040.\n",
      "Ep done - 83050.\n",
      "Ep done - 83060.\n",
      "Ep done - 83070.\n",
      "Ep done - 83080.\n",
      "Ep done - 83090.\n",
      "Ep done - 83100.\n",
      "Ep done - 83110.\n",
      "Ep done - 83120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 8605     |\n",
      "|    total_timesteps | 2273280  |\n",
      "---------------------------------\n",
      "Ep done - 83130.\n",
      "Ep done - 83140.\n",
      "Ep done - 83150.\n",
      "Ep done - 83160.\n",
      "Ep done - 83170.\n",
      "Ep done - 83180.\n",
      "Ep done - 83190.\n",
      "Ep done - 83200.\n",
      "Ep done - 83210.\n",
      "Ep done - 83220.\n",
      "Ep done - 83230.\n",
      "Ep done - 83240.\n",
      "Ep done - 83250.\n",
      "Ep done - 83260.\n",
      "Ep done - 83270.\n",
      "Ep done - 83280.\n",
      "Ep done - 83290.\n",
      "Ep done - 83300.\n",
      "Ep done - 83310.\n",
      "Ep done - 83320.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.32     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 264       |\n",
      "|    iterations           | 371       |\n",
      "|    time_elapsed         | 8622      |\n",
      "|    total_timesteps      | 2279424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.11e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.14      |\n",
      "|    n_updates            | 22930     |\n",
      "|    policy_gradient_loss | 1.83e-10  |\n",
      "|    value_loss           | 0.243     |\n",
      "---------------------------------------\n",
      "Ep done - 83330.\n",
      "Ep done - 83340.\n",
      "Ep done - 22710.\n",
      "Ep done - 22720.\n",
      "Ep done - 22730.\n",
      "Ep done - 22740.\n",
      "Ep done - 22750.\n",
      "Ep done - 22760.\n",
      "Ep done - 22770.\n",
      "Ep done - 22780.\n",
      "Ep done - 22790.\n",
      "Ep done - 22800.\n",
      "Eval num_timesteps=2280000, episode_reward=-0.46 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2280000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.01e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.146     |\n",
      "|    n_updates            | 22940     |\n",
      "|    policy_gradient_loss | 3.91e-10  |\n",
      "|    value_loss           | 0.259     |\n",
      "---------------------------------------\n",
      "Ep done - 83350.\n",
      "Ep done - 83360.\n",
      "Ep done - 83370.\n",
      "Ep done - 83380.\n",
      "Ep done - 83390.\n",
      "Ep done - 83400.\n",
      "Ep done - 83410.\n",
      "Ep done - 83420.\n",
      "Ep done - 83430.\n",
      "Ep done - 83440.\n",
      "Ep done - 83450.\n",
      "Ep done - 83460.\n",
      "Ep done - 83470.\n",
      "Ep done - 83480.\n",
      "Ep done - 83490.\n",
      "Ep done - 83500.\n",
      "Ep done - 83510.\n",
      "Ep done - 83520.\n",
      "Ep done - 83530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 8649     |\n",
      "|    total_timesteps | 2285568  |\n",
      "---------------------------------\n",
      "Ep done - 83540.\n",
      "Ep done - 83550.\n",
      "Ep done - 83560.\n",
      "Ep done - 83570.\n",
      "Ep done - 83580.\n",
      "Ep done - 83590.\n",
      "Ep done - 83600.\n",
      "Ep done - 83610.\n",
      "Ep done - 83620.\n",
      "Ep done - 83630.\n",
      "Ep done - 83640.\n",
      "Ep done - 83650.\n",
      "Ep done - 83660.\n",
      "Ep done - 83670.\n",
      "Ep done - 83680.\n",
      "Ep done - 22810.\n",
      "Ep done - 22820.\n",
      "Ep done - 22830.\n",
      "Ep done - 22840.\n",
      "Ep done - 22850.\n",
      "Ep done - 22860.\n",
      "Ep done - 22870.\n",
      "Ep done - 22880.\n",
      "Ep done - 22890.\n",
      "Ep done - 22900.\n",
      "Eval num_timesteps=2290000, episode_reward=-0.36 +/- 0.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.36     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2290000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.89e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0858    |\n",
      "|    n_updates            | 22950     |\n",
      "|    policy_gradient_loss | -1.81e-10 |\n",
      "|    value_loss           | 0.235     |\n",
      "---------------------------------------\n",
      "Ep done - 83690.\n",
      "Ep done - 83700.\n",
      "Ep done - 83710.\n",
      "Ep done - 83720.\n",
      "Ep done - 83730.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 373      |\n",
      "|    time_elapsed    | 8681     |\n",
      "|    total_timesteps | 2291712  |\n",
      "---------------------------------\n",
      "Ep done - 83740.\n",
      "Ep done - 83750.\n",
      "Ep done - 83760.\n",
      "Ep done - 83770.\n",
      "Ep done - 83780.\n",
      "Ep done - 83790.\n",
      "Ep done - 83800.\n",
      "Ep done - 83810.\n",
      "Ep done - 83820.\n",
      "Ep done - 83830.\n",
      "Ep done - 83840.\n",
      "Ep done - 83850.\n",
      "Ep done - 83860.\n",
      "Ep done - 83870.\n",
      "Ep done - 83880.\n",
      "Ep done - 83890.\n",
      "Ep done - 83900.\n",
      "Ep done - 83910.\n",
      "Ep done - 83920.\n",
      "Ep done - 83930.\n",
      "Ep done - 83940.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.34     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 374       |\n",
      "|    time_elapsed         | 8718      |\n",
      "|    total_timesteps      | 2297856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.91e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.134     |\n",
      "|    n_updates            | 22960     |\n",
      "|    policy_gradient_loss | 5.37e-10  |\n",
      "|    value_loss           | 0.236     |\n",
      "---------------------------------------\n",
      "Ep done - 83950.\n",
      "Ep done - 83960.\n",
      "Ep done - 83970.\n",
      "Ep done - 83980.\n",
      "Ep done - 83990.\n",
      "Ep done - 84000.\n",
      "Ep done - 84010.\n",
      "Ep done - 22910.\n",
      "Ep done - 22920.\n",
      "Ep done - 22930.\n",
      "Ep done - 22940.\n",
      "Ep done - 22950.\n",
      "Ep done - 22960.\n",
      "Ep done - 22970.\n",
      "Ep done - 22980.\n",
      "Ep done - 22990.\n",
      "Ep done - 23000.\n",
      "Eval num_timesteps=2300000, episode_reward=-0.48 +/- 0.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.48     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2300000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.91e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.134     |\n",
      "|    n_updates            | 22970     |\n",
      "|    policy_gradient_loss | -4.56e-11 |\n",
      "|    value_loss           | 0.254     |\n",
      "---------------------------------------\n",
      "Ep done - 84020.\n",
      "Ep done - 84030.\n",
      "Ep done - 84040.\n",
      "Ep done - 84050.\n",
      "Ep done - 84060.\n",
      "Ep done - 84070.\n",
      "Ep done - 84080.\n",
      "Ep done - 84090.\n",
      "Ep done - 84100.\n",
      "Ep done - 84110.\n",
      "Ep done - 84120.\n",
      "Ep done - 84130.\n",
      "Ep done - 84140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 375      |\n",
      "|    time_elapsed    | 8749     |\n",
      "|    total_timesteps | 2304000  |\n",
      "---------------------------------\n",
      "Ep done - 84150.\n",
      "Ep done - 84160.\n",
      "Ep done - 84170.\n",
      "Ep done - 84180.\n",
      "Ep done - 84190.\n",
      "Ep done - 84200.\n",
      "Ep done - 84210.\n",
      "Ep done - 84220.\n",
      "Ep done - 84230.\n",
      "Ep done - 84240.\n",
      "Ep done - 84250.\n",
      "Ep done - 84260.\n",
      "Ep done - 84270.\n",
      "Ep done - 84280.\n",
      "Ep done - 84290.\n",
      "Ep done - 84300.\n",
      "Ep done - 84310.\n",
      "Ep done - 84320.\n",
      "Ep done - 84330.\n",
      "Ep done - 84340.\n",
      "Ep done - 23010.\n",
      "Ep done - 23020.\n",
      "Ep done - 23030.\n",
      "Ep done - 23040.\n",
      "Ep done - 23050.\n",
      "Ep done - 23060.\n",
      "Ep done - 23070.\n",
      "Ep done - 23080.\n",
      "Ep done - 23090.\n",
      "Ep done - 23100.\n",
      "Eval num_timesteps=2310000, episode_reward=-0.28 +/- 0.96\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.28     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2310000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.79e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.117     |\n",
      "|    n_updates            | 22980     |\n",
      "|    policy_gradient_loss | 1.49e-09  |\n",
      "|    value_loss           | 0.241     |\n",
      "---------------------------------------\n",
      "Ep done - 84350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 8778     |\n",
      "|    total_timesteps | 2310144  |\n",
      "---------------------------------\n",
      "Ep done - 84360.\n",
      "Ep done - 84370.\n",
      "Ep done - 84380.\n",
      "Ep done - 84390.\n",
      "Ep done - 84400.\n",
      "Ep done - 84410.\n",
      "Ep done - 84420.\n",
      "Ep done - 84430.\n",
      "Ep done - 84440.\n",
      "Ep done - 84450.\n",
      "Ep done - 84460.\n",
      "Ep done - 84470.\n",
      "Ep done - 84480.\n",
      "Ep done - 84490.\n",
      "Ep done - 84500.\n",
      "Ep done - 84510.\n",
      "Ep done - 84520.\n",
      "Ep done - 84530.\n",
      "Ep done - 84540.\n",
      "Ep done - 84550.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 377       |\n",
      "|    time_elapsed         | 8799      |\n",
      "|    total_timesteps      | 2316288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.65e-06 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.145     |\n",
      "|    n_updates            | 22990     |\n",
      "|    policy_gradient_loss | 9.12e-10  |\n",
      "|    value_loss           | 0.258     |\n",
      "---------------------------------------\n",
      "Ep done - 84560.\n",
      "Ep done - 84570.\n",
      "Ep done - 84580.\n",
      "Ep done - 84590.\n",
      "Ep done - 84600.\n",
      "Ep done - 84610.\n",
      "Ep done - 84620.\n",
      "Ep done - 84630.\n",
      "Ep done - 84640.\n",
      "Ep done - 84650.\n",
      "Ep done - 84660.\n",
      "Ep done - 84670.\n",
      "Ep done - 84680.\n",
      "Ep done - 23110.\n",
      "Ep done - 23120.\n",
      "Ep done - 23130.\n",
      "Ep done - 23140.\n",
      "Ep done - 23150.\n",
      "Ep done - 23160.\n",
      "Ep done - 23170.\n",
      "Ep done - 23180.\n",
      "Ep done - 23190.\n",
      "Ep done - 23200.\n",
      "Eval num_timesteps=2320000, episode_reward=-0.44 +/- 0.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2320000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.68e-06 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.076     |\n",
      "|    n_updates            | 23000     |\n",
      "|    policy_gradient_loss | -1.37e-10 |\n",
      "|    value_loss           | 0.252     |\n",
      "---------------------------------------\n",
      "Ep done - 84690.\n",
      "Ep done - 84700.\n",
      "Ep done - 84710.\n",
      "Ep done - 84720.\n",
      "Ep done - 84730.\n",
      "Ep done - 84740.\n",
      "Ep done - 84750.\n",
      "Ep done - 84760.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 8828     |\n",
      "|    total_timesteps | 2322432  |\n",
      "---------------------------------\n",
      "Ep done - 84770.\n",
      "Ep done - 84780.\n",
      "Ep done - 84790.\n",
      "Ep done - 84800.\n",
      "Ep done - 84810.\n",
      "Ep done - 84820.\n",
      "Ep done - 84830.\n",
      "Ep done - 84840.\n",
      "Ep done - 84850.\n",
      "Ep done - 84860.\n",
      "Ep done - 84870.\n",
      "Ep done - 84880.\n",
      "Ep done - 84890.\n",
      "Ep done - 84900.\n",
      "Ep done - 84910.\n",
      "Ep done - 84920.\n",
      "Ep done - 84930.\n",
      "Ep done - 84940.\n",
      "Ep done - 84950.\n",
      "Ep done - 84960.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 379       |\n",
      "|    time_elapsed         | 8854      |\n",
      "|    total_timesteps      | 2328576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.52e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.117     |\n",
      "|    n_updates            | 23010     |\n",
      "|    policy_gradient_loss | 4.18e-10  |\n",
      "|    value_loss           | 0.247     |\n",
      "---------------------------------------\n",
      "Ep done - 84970.\n",
      "Ep done - 84980.\n",
      "Ep done - 84990.\n",
      "Ep done - 85000.\n",
      "Ep done - 85010.\n",
      "Ep done - 23210.\n",
      "Ep done - 23220.\n",
      "Ep done - 23230.\n",
      "Ep done - 23240.\n",
      "Ep done - 23250.\n",
      "Ep done - 23260.\n",
      "Ep done - 23270.\n",
      "Ep done - 23280.\n",
      "Ep done - 23290.\n",
      "Ep done - 23300.\n",
      "Eval num_timesteps=2330000, episode_reward=-0.56 +/- 0.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.56     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2330000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.47e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.128     |\n",
      "|    n_updates            | 23020     |\n",
      "|    policy_gradient_loss | 1.6e-10   |\n",
      "|    value_loss           | 0.242     |\n",
      "---------------------------------------\n",
      "Ep done - 85020.\n",
      "Ep done - 85030.\n",
      "Ep done - 85040.\n",
      "Ep done - 85050.\n",
      "Ep done - 85060.\n",
      "Ep done - 85070.\n",
      "Ep done - 85080.\n",
      "Ep done - 85090.\n",
      "Ep done - 85100.\n",
      "Ep done - 85110.\n",
      "Ep done - 85120.\n",
      "Ep done - 85130.\n",
      "Ep done - 85140.\n",
      "Ep done - 85150.\n",
      "Ep done - 85160.\n",
      "Ep done - 85170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 8880     |\n",
      "|    total_timesteps | 2334720  |\n",
      "---------------------------------\n",
      "Ep done - 85180.\n",
      "Ep done - 85190.\n",
      "Ep done - 85200.\n",
      "Ep done - 85210.\n",
      "Ep done - 85220.\n",
      "Ep done - 85230.\n",
      "Ep done - 85240.\n",
      "Ep done - 85250.\n",
      "Ep done - 85260.\n",
      "Ep done - 85270.\n",
      "Ep done - 85280.\n",
      "Ep done - 85290.\n",
      "Ep done - 85300.\n",
      "Ep done - 85310.\n",
      "Ep done - 85320.\n",
      "Ep done - 85330.\n",
      "Ep done - 85340.\n",
      "Ep done - 23310.\n",
      "Ep done - 23320.\n",
      "Ep done - 23330.\n",
      "Ep done - 23340.\n",
      "Ep done - 23350.\n",
      "Ep done - 23360.\n",
      "Ep done - 23370.\n",
      "Ep done - 23380.\n",
      "Ep done - 23390.\n",
      "Ep done - 23400.\n",
      "Eval num_timesteps=2340000, episode_reward=-0.38 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.38     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2340000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.4e-06  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.143     |\n",
      "|    n_updates            | 23030     |\n",
      "|    policy_gradient_loss | -1.32e-09 |\n",
      "|    value_loss           | 0.264     |\n",
      "---------------------------------------\n",
      "Ep done - 85350.\n",
      "Ep done - 85360.\n",
      "Ep done - 85370.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 381      |\n",
      "|    time_elapsed    | 8905     |\n",
      "|    total_timesteps | 2340864  |\n",
      "---------------------------------\n",
      "Ep done - 85380.\n",
      "Ep done - 85390.\n",
      "Ep done - 85400.\n",
      "Ep done - 85410.\n",
      "Ep done - 85420.\n",
      "Ep done - 85430.\n",
      "Ep done - 85440.\n",
      "Ep done - 85450.\n",
      "Ep done - 85460.\n",
      "Ep done - 85470.\n",
      "Ep done - 85480.\n",
      "Ep done - 85490.\n",
      "Ep done - 85500.\n",
      "Ep done - 85510.\n",
      "Ep done - 85520.\n",
      "Ep done - 85530.\n",
      "Ep done - 85540.\n",
      "Ep done - 85550.\n",
      "Ep done - 85560.\n",
      "Ep done - 85570.\n",
      "Ep done - 85580.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 382       |\n",
      "|    time_elapsed         | 8926      |\n",
      "|    total_timesteps      | 2347008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.39e-06 |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.186     |\n",
      "|    n_updates            | 23040     |\n",
      "|    policy_gradient_loss | 1.31e-09  |\n",
      "|    value_loss           | 0.235     |\n",
      "---------------------------------------\n",
      "Ep done - 85590.\n",
      "Ep done - 85600.\n",
      "Ep done - 85610.\n",
      "Ep done - 85620.\n",
      "Ep done - 85630.\n",
      "Ep done - 85640.\n",
      "Ep done - 85650.\n",
      "Ep done - 85660.\n",
      "Ep done - 85670.\n",
      "Ep done - 85680.\n",
      "Ep done - 23410.\n",
      "Ep done - 23420.\n",
      "Ep done - 23430.\n",
      "Ep done - 23440.\n",
      "Ep done - 23450.\n",
      "Ep done - 23460.\n",
      "Ep done - 23470.\n",
      "Ep done - 23480.\n",
      "Ep done - 23490.\n",
      "Ep done - 23500.\n",
      "Eval num_timesteps=2350000, episode_reward=-0.50 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2350000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.29e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.305     |\n",
      "|    n_updates            | 23050     |\n",
      "|    policy_gradient_loss | -6.95e-10 |\n",
      "|    value_loss           | 0.224     |\n",
      "---------------------------------------\n",
      "Ep done - 85690.\n",
      "Ep done - 85700.\n",
      "Ep done - 85710.\n",
      "Ep done - 85720.\n",
      "Ep done - 85730.\n",
      "Ep done - 85740.\n",
      "Ep done - 85750.\n",
      "Ep done - 85760.\n",
      "Ep done - 85770.\n",
      "Ep done - 85780.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 383      |\n",
      "|    time_elapsed    | 8961     |\n",
      "|    total_timesteps | 2353152  |\n",
      "---------------------------------\n",
      "Ep done - 85790.\n",
      "Ep done - 85800.\n",
      "Ep done - 85810.\n",
      "Ep done - 85820.\n",
      "Ep done - 85830.\n",
      "Ep done - 85840.\n",
      "Ep done - 85850.\n",
      "Ep done - 85860.\n",
      "Ep done - 85870.\n",
      "Ep done - 85880.\n",
      "Ep done - 85890.\n",
      "Ep done - 85900.\n",
      "Ep done - 85910.\n",
      "Ep done - 85920.\n",
      "Ep done - 85930.\n",
      "Ep done - 85940.\n",
      "Ep done - 85950.\n",
      "Ep done - 85960.\n",
      "Ep done - 85970.\n",
      "Ep done - 85980.\n",
      "Ep done - 85990.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 384       |\n",
      "|    time_elapsed         | 8987      |\n",
      "|    total_timesteps      | 2359296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.23e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.159     |\n",
      "|    n_updates            | 23060     |\n",
      "|    policy_gradient_loss | 4.21e-10  |\n",
      "|    value_loss           | 0.278     |\n",
      "---------------------------------------\n",
      "Ep done - 86000.\n",
      "Ep done - 86010.\n",
      "Ep done - 23510.\n",
      "Ep done - 23520.\n",
      "Ep done - 23530.\n",
      "Ep done - 23540.\n",
      "Ep done - 23550.\n",
      "Ep done - 23560.\n",
      "Ep done - 23570.\n",
      "Ep done - 23580.\n",
      "Ep done - 23590.\n",
      "Ep done - 23600.\n",
      "Eval num_timesteps=2360000, episode_reward=-0.54 +/- 0.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.54     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2360000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.21e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.127     |\n",
      "|    n_updates            | 23070     |\n",
      "|    policy_gradient_loss | 2.08e-10  |\n",
      "|    value_loss           | 0.238     |\n",
      "---------------------------------------\n",
      "Ep done - 86020.\n",
      "Ep done - 86030.\n",
      "Ep done - 86040.\n",
      "Ep done - 86050.\n",
      "Ep done - 86060.\n",
      "Ep done - 86070.\n",
      "Ep done - 86080.\n",
      "Ep done - 86090.\n",
      "Ep done - 86100.\n",
      "Ep done - 86110.\n",
      "Ep done - 86120.\n",
      "Ep done - 86130.\n",
      "Ep done - 86140.\n",
      "Ep done - 86150.\n",
      "Ep done - 86160.\n",
      "Ep done - 86170.\n",
      "Ep done - 86180.\n",
      "Ep done - 86190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 385      |\n",
      "|    time_elapsed    | 9017     |\n",
      "|    total_timesteps | 2365440  |\n",
      "---------------------------------\n",
      "Ep done - 86200.\n",
      "Ep done - 86210.\n",
      "Ep done - 86220.\n",
      "Ep done - 86230.\n",
      "Ep done - 86240.\n",
      "Ep done - 86250.\n",
      "Ep done - 86260.\n",
      "Ep done - 86270.\n",
      "Ep done - 86280.\n",
      "Ep done - 86290.\n",
      "Ep done - 86300.\n",
      "Ep done - 86310.\n",
      "Ep done - 86320.\n",
      "Ep done - 86330.\n",
      "Ep done - 86340.\n",
      "Ep done - 23610.\n",
      "Ep done - 23620.\n",
      "Ep done - 23630.\n",
      "Ep done - 23640.\n",
      "Ep done - 23650.\n",
      "Ep done - 23660.\n",
      "Ep done - 23670.\n",
      "Ep done - 23680.\n",
      "Ep done - 23690.\n",
      "Ep done - 23700.\n",
      "Eval num_timesteps=2370000, episode_reward=-0.54 +/- 0.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.54     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2370000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.06e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0895    |\n",
      "|    n_updates            | 23080     |\n",
      "|    policy_gradient_loss | -3.1e-10  |\n",
      "|    value_loss           | 0.232     |\n",
      "---------------------------------------\n",
      "Ep done - 86350.\n",
      "Ep done - 86360.\n",
      "Ep done - 86370.\n",
      "Ep done - 86380.\n",
      "Ep done - 86390.\n",
      "Ep done - 86400.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 9045     |\n",
      "|    total_timesteps | 2371584  |\n",
      "---------------------------------\n",
      "Ep done - 86410.\n",
      "Ep done - 86420.\n",
      "Ep done - 86430.\n",
      "Ep done - 86440.\n",
      "Ep done - 86450.\n",
      "Ep done - 86460.\n",
      "Ep done - 86470.\n",
      "Ep done - 86480.\n",
      "Ep done - 86490.\n",
      "Ep done - 86500.\n",
      "Ep done - 86510.\n",
      "Ep done - 86520.\n",
      "Ep done - 86530.\n",
      "Ep done - 86540.\n",
      "Ep done - 86550.\n",
      "Ep done - 86560.\n",
      "Ep done - 86570.\n",
      "Ep done - 86580.\n",
      "Ep done - 86590.\n",
      "Ep done - 86600.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 387       |\n",
      "|    time_elapsed         | 9067      |\n",
      "|    total_timesteps      | 2377728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.03e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.124     |\n",
      "|    n_updates            | 23090     |\n",
      "|    policy_gradient_loss | -1.27e-09 |\n",
      "|    value_loss           | 0.242     |\n",
      "---------------------------------------\n",
      "Ep done - 86610.\n",
      "Ep done - 86620.\n",
      "Ep done - 86630.\n",
      "Ep done - 86640.\n",
      "Ep done - 86650.\n",
      "Ep done - 86660.\n",
      "Ep done - 86670.\n",
      "Ep done - 86680.\n",
      "Ep done - 23710.\n",
      "Ep done - 23720.\n",
      "Ep done - 23730.\n",
      "Ep done - 23740.\n",
      "Ep done - 23750.\n",
      "Ep done - 23760.\n",
      "Ep done - 23770.\n",
      "Ep done - 23780.\n",
      "Ep done - 23790.\n",
      "Ep done - 23800.\n",
      "Eval num_timesteps=2380000, episode_reward=-0.42 +/- 0.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2380000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0873    |\n",
      "|    n_updates            | 23100     |\n",
      "|    policy_gradient_loss | -1.14e-09 |\n",
      "|    value_loss           | 0.248     |\n",
      "---------------------------------------\n",
      "Ep done - 86690.\n",
      "Ep done - 86700.\n",
      "Ep done - 86710.\n",
      "Ep done - 86720.\n",
      "Ep done - 86730.\n",
      "Ep done - 86740.\n",
      "Ep done - 86750.\n",
      "Ep done - 86760.\n",
      "Ep done - 86770.\n",
      "Ep done - 86780.\n",
      "Ep done - 86790.\n",
      "Ep done - 86800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 388      |\n",
      "|    time_elapsed    | 9092     |\n",
      "|    total_timesteps | 2383872  |\n",
      "---------------------------------\n",
      "Ep done - 86810.\n",
      "Ep done - 86820.\n",
      "Ep done - 86830.\n",
      "Ep done - 86840.\n",
      "Ep done - 86850.\n",
      "Ep done - 86860.\n",
      "Ep done - 86870.\n",
      "Ep done - 86880.\n",
      "Ep done - 86890.\n",
      "Ep done - 86900.\n",
      "Ep done - 86910.\n",
      "Ep done - 86920.\n",
      "Ep done - 86930.\n",
      "Ep done - 86940.\n",
      "Ep done - 86950.\n",
      "Ep done - 86960.\n",
      "Ep done - 86970.\n",
      "Ep done - 86980.\n",
      "Ep done - 86990.\n",
      "Ep done - 87000.\n",
      "Ep done - 87010.\n",
      "Ep done - 23810.\n",
      "Ep done - 23820.\n",
      "Ep done - 23830.\n",
      "Ep done - 23840.\n",
      "Ep done - 23850.\n",
      "Ep done - 23860.\n",
      "Ep done - 23870.\n",
      "Ep done - 23880.\n",
      "Ep done - 23890.\n",
      "Ep done - 23900.\n",
      "Eval num_timesteps=2390000, episode_reward=-0.52 +/- 0.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.52     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2390000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.91e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.138     |\n",
      "|    n_updates            | 23110     |\n",
      "|    policy_gradient_loss | -3.03e-10 |\n",
      "|    value_loss           | 0.275     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 389      |\n",
      "|    time_elapsed    | 9113     |\n",
      "|    total_timesteps | 2390016  |\n",
      "---------------------------------\n",
      "Ep done - 87020.\n",
      "Ep done - 87030.\n",
      "Ep done - 87040.\n",
      "Ep done - 87050.\n",
      "Ep done - 87060.\n",
      "Ep done - 87070.\n",
      "Ep done - 87080.\n",
      "Ep done - 87090.\n",
      "Ep done - 87100.\n",
      "Ep done - 87110.\n",
      "Ep done - 87120.\n",
      "Ep done - 87130.\n",
      "Ep done - 87140.\n",
      "Ep done - 87150.\n",
      "Ep done - 87160.\n",
      "Ep done - 87170.\n",
      "Ep done - 87180.\n",
      "Ep done - 87190.\n",
      "Ep done - 87200.\n",
      "Ep done - 87210.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 390       |\n",
      "|    time_elapsed         | 9130      |\n",
      "|    total_timesteps      | 2396160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.26e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.121     |\n",
      "|    n_updates            | 23120     |\n",
      "|    policy_gradient_loss | -1.65e-08 |\n",
      "|    value_loss           | 0.248     |\n",
      "---------------------------------------\n",
      "Ep done - 87220.\n",
      "Ep done - 87230.\n",
      "Ep done - 87240.\n",
      "Ep done - 87250.\n",
      "Ep done - 87260.\n",
      "Ep done - 87270.\n",
      "Ep done - 87280.\n",
      "Ep done - 87290.\n",
      "Ep done - 87300.\n",
      "Ep done - 87310.\n",
      "Ep done - 87320.\n",
      "Ep done - 87330.\n",
      "Ep done - 87340.\n",
      "Ep done - 23910.\n",
      "Ep done - 23920.\n",
      "Ep done - 23930.\n",
      "Ep done - 23940.\n",
      "Ep done - 23950.\n",
      "Ep done - 23960.\n",
      "Ep done - 23970.\n",
      "Ep done - 23980.\n",
      "Ep done - 23990.\n",
      "Ep done - 24000.\n",
      "Eval num_timesteps=2400000, episode_reward=-0.30 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2400000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.11e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.143     |\n",
      "|    n_updates            | 23130     |\n",
      "|    policy_gradient_loss | 5.34e-10  |\n",
      "|    value_loss           | 0.241     |\n",
      "---------------------------------------\n",
      "Ep done - 87350.\n",
      "Ep done - 87360.\n",
      "Ep done - 87370.\n",
      "Ep done - 87380.\n",
      "Ep done - 87390.\n",
      "Ep done - 87400.\n",
      "Ep done - 87410.\n",
      "Ep done - 87420.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 391      |\n",
      "|    time_elapsed    | 9150     |\n",
      "|    total_timesteps | 2402304  |\n",
      "---------------------------------\n",
      "Ep done - 87430.\n",
      "Ep done - 87440.\n",
      "Ep done - 87450.\n",
      "Ep done - 87460.\n",
      "Ep done - 87470.\n",
      "Ep done - 87480.\n",
      "Ep done - 87490.\n",
      "Ep done - 87500.\n",
      "Ep done - 87510.\n",
      "Ep done - 87520.\n",
      "Ep done - 87530.\n",
      "Ep done - 87540.\n",
      "Ep done - 87550.\n",
      "Ep done - 87560.\n",
      "Ep done - 87570.\n",
      "Ep done - 87580.\n",
      "Ep done - 87590.\n",
      "Ep done - 87600.\n",
      "Ep done - 87610.\n",
      "Ep done - 87620.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.28     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 392       |\n",
      "|    time_elapsed         | 9167      |\n",
      "|    total_timesteps      | 2408448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.08e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.117     |\n",
      "|    n_updates            | 23140     |\n",
      "|    policy_gradient_loss | -3.5e-10  |\n",
      "|    value_loss           | 0.232     |\n",
      "---------------------------------------\n",
      "Ep done - 87630.\n",
      "Ep done - 87640.\n",
      "Ep done - 87650.\n",
      "Ep done - 87660.\n",
      "Ep done - 87670.\n",
      "Ep done - 87680.\n",
      "Ep done - 24010.\n",
      "Ep done - 24020.\n",
      "Ep done - 24030.\n",
      "Ep done - 24040.\n",
      "Ep done - 24050.\n",
      "Ep done - 24060.\n",
      "Ep done - 24070.\n",
      "Ep done - 24080.\n",
      "Ep done - 24090.\n",
      "Ep done - 24100.\n",
      "Eval num_timesteps=2410000, episode_reward=-0.46 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2410000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.1e-06  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.145     |\n",
      "|    n_updates            | 23150     |\n",
      "|    policy_gradient_loss | -4.57e-10 |\n",
      "|    value_loss           | 0.248     |\n",
      "---------------------------------------\n",
      "Ep done - 87690.\n",
      "Ep done - 87700.\n",
      "Ep done - 87710.\n",
      "Ep done - 87720.\n",
      "Ep done - 87730.\n",
      "Ep done - 87740.\n",
      "Ep done - 87750.\n",
      "Ep done - 87760.\n",
      "Ep done - 87770.\n",
      "Ep done - 87780.\n",
      "Ep done - 87790.\n",
      "Ep done - 87800.\n",
      "Ep done - 87810.\n",
      "Ep done - 87820.\n",
      "Ep done - 87830.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 393      |\n",
      "|    time_elapsed    | 9191     |\n",
      "|    total_timesteps | 2414592  |\n",
      "---------------------------------\n",
      "Ep done - 87840.\n",
      "Ep done - 87850.\n",
      "Ep done - 87860.\n",
      "Ep done - 87870.\n",
      "Ep done - 87880.\n",
      "Ep done - 87890.\n",
      "Ep done - 87900.\n",
      "Ep done - 87910.\n",
      "Ep done - 87920.\n",
      "Ep done - 87930.\n",
      "Ep done - 87940.\n",
      "Ep done - 87950.\n",
      "Ep done - 87960.\n",
      "Ep done - 87970.\n",
      "Ep done - 87980.\n",
      "Ep done - 87990.\n",
      "Ep done - 88000.\n",
      "Ep done - 88010.\n",
      "Ep done - 24110.\n",
      "Ep done - 24120.\n",
      "Ep done - 24130.\n",
      "Ep done - 24140.\n",
      "Ep done - 24150.\n",
      "Ep done - 24160.\n",
      "Ep done - 24170.\n",
      "Ep done - 24180.\n",
      "Ep done - 24190.\n",
      "Ep done - 24200.\n",
      "Eval num_timesteps=2420000, episode_reward=-0.66 +/- 0.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.66     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2420000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.07e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.11      |\n",
      "|    n_updates            | 23160     |\n",
      "|    policy_gradient_loss | -2.62e-10 |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "Ep done - 88020.\n",
      "Ep done - 88030.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 394      |\n",
      "|    time_elapsed    | 9212     |\n",
      "|    total_timesteps | 2420736  |\n",
      "---------------------------------\n",
      "Ep done - 88040.\n",
      "Ep done - 88050.\n",
      "Ep done - 88060.\n",
      "Ep done - 88070.\n",
      "Ep done - 88080.\n",
      "Ep done - 88090.\n",
      "Ep done - 88100.\n",
      "Ep done - 88110.\n",
      "Ep done - 88120.\n",
      "Ep done - 88130.\n",
      "Ep done - 88140.\n",
      "Ep done - 88150.\n",
      "Ep done - 88160.\n",
      "Ep done - 88170.\n",
      "Ep done - 88180.\n",
      "Ep done - 88190.\n",
      "Ep done - 88200.\n",
      "Ep done - 88210.\n",
      "Ep done - 88220.\n",
      "Ep done - 88230.\n",
      "Ep done - 88240.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.52     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 262       |\n",
      "|    iterations           | 395       |\n",
      "|    time_elapsed         | 9231      |\n",
      "|    total_timesteps      | 2426880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.06e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0964    |\n",
      "|    n_updates            | 23170     |\n",
      "|    policy_gradient_loss | -1.06e-09 |\n",
      "|    value_loss           | 0.227     |\n",
      "---------------------------------------\n",
      "Ep done - 88250.\n",
      "Ep done - 88260.\n",
      "Ep done - 88270.\n",
      "Ep done - 88280.\n",
      "Ep done - 88290.\n",
      "Ep done - 88300.\n",
      "Ep done - 88310.\n",
      "Ep done - 88320.\n",
      "Ep done - 88330.\n",
      "Ep done - 88340.\n",
      "Ep done - 24210.\n",
      "Ep done - 24220.\n",
      "Ep done - 24230.\n",
      "Ep done - 24240.\n",
      "Ep done - 24250.\n",
      "Ep done - 24260.\n",
      "Ep done - 24270.\n",
      "Ep done - 24280.\n",
      "Ep done - 24290.\n",
      "Ep done - 24300.\n",
      "Eval num_timesteps=2430000, episode_reward=-0.46 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2430000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.24e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.076     |\n",
      "|    n_updates            | 23180     |\n",
      "|    policy_gradient_loss | -7.93e-11 |\n",
      "|    value_loss           | 0.218     |\n",
      "---------------------------------------\n",
      "Ep done - 88350.\n",
      "Ep done - 88360.\n",
      "Ep done - 88370.\n",
      "Ep done - 88380.\n",
      "Ep done - 88390.\n",
      "Ep done - 88400.\n",
      "Ep done - 88410.\n",
      "Ep done - 88420.\n",
      "Ep done - 88430.\n",
      "Ep done - 88440.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 9254     |\n",
      "|    total_timesteps | 2433024  |\n",
      "---------------------------------\n",
      "Ep done - 88450.\n",
      "Ep done - 88460.\n",
      "Ep done - 88470.\n",
      "Ep done - 88480.\n",
      "Ep done - 88490.\n",
      "Ep done - 88500.\n",
      "Ep done - 88510.\n",
      "Ep done - 88520.\n",
      "Ep done - 88530.\n",
      "Ep done - 88540.\n",
      "Ep done - 88550.\n",
      "Ep done - 88560.\n",
      "Ep done - 88570.\n",
      "Ep done - 88580.\n",
      "Ep done - 88590.\n",
      "Ep done - 88600.\n",
      "Ep done - 88610.\n",
      "Ep done - 88620.\n",
      "Ep done - 88630.\n",
      "Ep done - 88640.\n",
      "Ep done - 88650.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 397       |\n",
      "|    time_elapsed         | 9273      |\n",
      "|    total_timesteps      | 2439168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.39e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.139     |\n",
      "|    n_updates            | 23190     |\n",
      "|    policy_gradient_loss | -1.02e-09 |\n",
      "|    value_loss           | 0.238     |\n",
      "---------------------------------------\n",
      "Ep done - 88660.\n",
      "Ep done - 88670.\n",
      "Ep done - 88680.\n",
      "Ep done - 24310.\n",
      "Ep done - 24320.\n",
      "Ep done - 24330.\n",
      "Ep done - 24340.\n",
      "Ep done - 24350.\n",
      "Ep done - 24360.\n",
      "Ep done - 24370.\n",
      "Ep done - 24380.\n",
      "Ep done - 24390.\n",
      "Ep done - 24400.\n",
      "Eval num_timesteps=2440000, episode_reward=-0.34 +/- 0.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.34     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2440000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.59e-06 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.1       |\n",
      "|    n_updates            | 23200     |\n",
      "|    policy_gradient_loss | -3.17e-10 |\n",
      "|    value_loss           | 0.225     |\n",
      "---------------------------------------\n",
      "Ep done - 88690.\n",
      "Ep done - 88700.\n",
      "Ep done - 88710.\n",
      "Ep done - 88720.\n",
      "Ep done - 88730.\n",
      "Ep done - 88740.\n",
      "Ep done - 88750.\n",
      "Ep done - 88760.\n",
      "Ep done - 88770.\n",
      "Ep done - 88780.\n",
      "Ep done - 88790.\n",
      "Ep done - 88800.\n",
      "Ep done - 88810.\n",
      "Ep done - 88820.\n",
      "Ep done - 88830.\n",
      "Ep done - 88840.\n",
      "Ep done - 88850.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 398      |\n",
      "|    time_elapsed    | 9296     |\n",
      "|    total_timesteps | 2445312  |\n",
      "---------------------------------\n",
      "Ep done - 88860.\n",
      "Ep done - 88870.\n",
      "Ep done - 88880.\n",
      "Ep done - 88890.\n",
      "Ep done - 88900.\n",
      "Ep done - 88910.\n",
      "Ep done - 88920.\n",
      "Ep done - 88930.\n",
      "Ep done - 88940.\n",
      "Ep done - 88950.\n",
      "Ep done - 88960.\n",
      "Ep done - 88970.\n",
      "Ep done - 88980.\n",
      "Ep done - 88990.\n",
      "Ep done - 89000.\n",
      "Ep done - 89010.\n",
      "Ep done - 24410.\n",
      "Ep done - 24420.\n",
      "Ep done - 24430.\n",
      "Ep done - 24440.\n",
      "Ep done - 24450.\n",
      "Ep done - 24460.\n",
      "Ep done - 24470.\n",
      "Ep done - 24480.\n",
      "Ep done - 24490.\n",
      "Ep done - 24500.\n",
      "Eval num_timesteps=2450000, episode_reward=-0.48 +/- 0.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.48        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2450000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.501389e-05 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000119    |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.112        |\n",
      "|    n_updates            | 23210        |\n",
      "|    policy_gradient_loss | -2.56e-05    |\n",
      "|    value_loss           | 0.243        |\n",
      "------------------------------------------\n",
      "Ep done - 89020.\n",
      "Ep done - 89030.\n",
      "Ep done - 89040.\n",
      "Ep done - 89050.\n",
      "Ep done - 89060.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 399      |\n",
      "|    time_elapsed    | 9323     |\n",
      "|    total_timesteps | 2451456  |\n",
      "---------------------------------\n",
      "Ep done - 89070.\n",
      "Ep done - 89080.\n",
      "Ep done - 89090.\n",
      "Ep done - 89100.\n",
      "Ep done - 89110.\n",
      "Ep done - 89120.\n",
      "Ep done - 89130.\n",
      "Ep done - 89140.\n",
      "Ep done - 89150.\n",
      "Ep done - 89160.\n",
      "Ep done - 89170.\n",
      "Ep done - 89180.\n",
      "Ep done - 89190.\n",
      "Ep done - 89200.\n",
      "Ep done - 89210.\n",
      "Ep done - 89220.\n",
      "Ep done - 89230.\n",
      "Ep done - 89240.\n",
      "Ep done - 89250.\n",
      "Ep done - 89260.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -0.56         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 263           |\n",
      "|    iterations           | 400           |\n",
      "|    time_elapsed         | 9344          |\n",
      "|    total_timesteps      | 2457600       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5650175e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.91e-05     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.127         |\n",
      "|    n_updates            | 23220         |\n",
      "|    policy_gradient_loss | -3.76e-06     |\n",
      "|    value_loss           | 0.261         |\n",
      "-------------------------------------------\n",
      "Ep done - 89270.\n",
      "Ep done - 89280.\n",
      "Ep done - 89290.\n",
      "Ep done - 89300.\n",
      "Ep done - 89310.\n",
      "Ep done - 89320.\n",
      "Ep done - 89330.\n",
      "Ep done - 89340.\n",
      "Ep done - 24510.\n",
      "Ep done - 24520.\n",
      "Ep done - 24530.\n",
      "Ep done - 24540.\n",
      "Ep done - 24550.\n",
      "Ep done - 24560.\n",
      "Ep done - 24570.\n",
      "Ep done - 24580.\n",
      "Ep done - 24590.\n",
      "Ep done - 24600.\n",
      "Eval num_timesteps=2460000, episode_reward=-0.62 +/- 0.78\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.62    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 2460000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.4e-05 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.116    |\n",
      "|    n_updates            | 23230    |\n",
      "|    policy_gradient_loss | 7.06e-08 |\n",
      "|    value_loss           | 0.238    |\n",
      "--------------------------------------\n",
      "Ep done - 89350.\n",
      "Ep done - 89360.\n",
      "Ep done - 89370.\n",
      "Ep done - 89380.\n",
      "Ep done - 89390.\n",
      "Ep done - 89400.\n",
      "Ep done - 89410.\n",
      "Ep done - 89420.\n",
      "Ep done - 89430.\n",
      "Ep done - 89440.\n",
      "Ep done - 89450.\n",
      "Ep done - 89460.\n",
      "Ep done - 89470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 401      |\n",
      "|    time_elapsed    | 9371     |\n",
      "|    total_timesteps | 2463744  |\n",
      "---------------------------------\n",
      "Ep done - 89480.\n",
      "Ep done - 89490.\n",
      "Ep done - 89500.\n",
      "Ep done - 89510.\n",
      "Ep done - 89520.\n",
      "Ep done - 89530.\n",
      "Ep done - 89540.\n",
      "Ep done - 89550.\n",
      "Ep done - 89560.\n",
      "Ep done - 89570.\n",
      "Ep done - 89580.\n",
      "Ep done - 89590.\n",
      "Ep done - 89600.\n",
      "Ep done - 89610.\n",
      "Ep done - 89620.\n",
      "Ep done - 89630.\n",
      "Ep done - 89640.\n",
      "Ep done - 89650.\n",
      "Ep done - 89660.\n",
      "Ep done - 89670.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 402       |\n",
      "|    time_elapsed         | 9389      |\n",
      "|    total_timesteps      | 2469888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.22e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.101     |\n",
      "|    n_updates            | 23240     |\n",
      "|    policy_gradient_loss | 3.72e-10  |\n",
      "|    value_loss           | 0.251     |\n",
      "---------------------------------------\n",
      "Ep done - 89680.\n",
      "Ep done - 24610.\n",
      "Ep done - 24620.\n",
      "Ep done - 24630.\n",
      "Ep done - 24640.\n",
      "Ep done - 24650.\n",
      "Ep done - 24660.\n",
      "Ep done - 24670.\n",
      "Ep done - 24680.\n",
      "Ep done - 24690.\n",
      "Ep done - 24700.\n",
      "Eval num_timesteps=2470000, episode_reward=-0.44 +/- 0.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2470000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.06e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.104     |\n",
      "|    n_updates            | 23250     |\n",
      "|    policy_gradient_loss | -1.32e-09 |\n",
      "|    value_loss           | 0.25      |\n",
      "---------------------------------------\n",
      "Ep done - 89690.\n",
      "Ep done - 89700.\n",
      "Ep done - 89710.\n",
      "Ep done - 89720.\n",
      "Ep done - 89730.\n",
      "Ep done - 89740.\n",
      "Ep done - 89750.\n",
      "Ep done - 89760.\n",
      "Ep done - 89770.\n",
      "Ep done - 89780.\n",
      "Ep done - 89790.\n",
      "Ep done - 89800.\n",
      "Ep done - 89810.\n",
      "Ep done - 89820.\n",
      "Ep done - 89830.\n",
      "Ep done - 89840.\n",
      "Ep done - 89850.\n",
      "Ep done - 89860.\n",
      "Ep done - 89870.\n",
      "Ep done - 89880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 403      |\n",
      "|    time_elapsed    | 9412     |\n",
      "|    total_timesteps | 2476032  |\n",
      "---------------------------------\n",
      "Ep done - 89890.\n",
      "Ep done - 89900.\n",
      "Ep done - 89910.\n",
      "Ep done - 89920.\n",
      "Ep done - 89930.\n",
      "Ep done - 89940.\n",
      "Ep done - 89950.\n",
      "Ep done - 89960.\n",
      "Ep done - 89970.\n",
      "Ep done - 89980.\n",
      "Ep done - 89990.\n",
      "Ep done - 90000.\n",
      "Ep done - 90010.\n",
      "Ep done - 24710.\n",
      "Ep done - 24720.\n",
      "Ep done - 24730.\n",
      "Ep done - 24740.\n",
      "Ep done - 24750.\n",
      "Ep done - 24760.\n",
      "Ep done - 24770.\n",
      "Ep done - 24780.\n",
      "Ep done - 24790.\n",
      "Ep done - 24800.\n",
      "Eval num_timesteps=2480000, episode_reward=-0.52 +/- 0.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.52     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2480000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.62e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.162     |\n",
      "|    n_updates            | 23260     |\n",
      "|    policy_gradient_loss | -4.01e-08 |\n",
      "|    value_loss           | 0.225     |\n",
      "---------------------------------------\n",
      "Ep done - 90020.\n",
      "Ep done - 90030.\n",
      "Ep done - 90040.\n",
      "Ep done - 90050.\n",
      "Ep done - 90060.\n",
      "Ep done - 90070.\n",
      "Ep done - 90080.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 404      |\n",
      "|    time_elapsed    | 9432     |\n",
      "|    total_timesteps | 2482176  |\n",
      "---------------------------------\n",
      "Ep done - 90090.\n",
      "Ep done - 90100.\n",
      "Ep done - 90110.\n",
      "Ep done - 90120.\n",
      "Ep done - 90130.\n",
      "Ep done - 90140.\n",
      "Ep done - 90150.\n",
      "Ep done - 90160.\n",
      "Ep done - 90170.\n",
      "Ep done - 90180.\n",
      "Ep done - 90190.\n",
      "Ep done - 90200.\n",
      "Ep done - 90210.\n",
      "Ep done - 90220.\n",
      "Ep done - 90230.\n",
      "Ep done - 90240.\n",
      "Ep done - 90250.\n",
      "Ep done - 90260.\n",
      "Ep done - 90270.\n",
      "Ep done - 90280.\n",
      "Ep done - 90290.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.62     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 405       |\n",
      "|    time_elapsed         | 9448      |\n",
      "|    total_timesteps      | 2488320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.61e-05 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.137     |\n",
      "|    n_updates            | 23270     |\n",
      "|    policy_gradient_loss | 3e-07     |\n",
      "|    value_loss           | 0.259     |\n",
      "---------------------------------------\n",
      "Ep done - 90300.\n",
      "Ep done - 90310.\n",
      "Ep done - 90320.\n",
      "Ep done - 90330.\n",
      "Ep done - 90340.\n",
      "Ep done - 24810.\n",
      "Ep done - 24820.\n",
      "Ep done - 24830.\n",
      "Ep done - 24840.\n",
      "Ep done - 24850.\n",
      "Ep done - 24860.\n",
      "Ep done - 24870.\n",
      "Ep done - 24880.\n",
      "Ep done - 24890.\n",
      "Ep done - 24900.\n",
      "Eval num_timesteps=2490000, episode_reward=-0.46 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2490000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.92e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.113     |\n",
      "|    n_updates            | 23280     |\n",
      "|    policy_gradient_loss | -1.58e-09 |\n",
      "|    value_loss           | 0.202     |\n",
      "---------------------------------------\n",
      "Ep done - 90350.\n",
      "Ep done - 90360.\n",
      "Ep done - 90370.\n",
      "Ep done - 90380.\n",
      "Ep done - 90390.\n",
      "Ep done - 90400.\n",
      "Ep done - 90410.\n",
      "Ep done - 90420.\n",
      "Ep done - 90430.\n",
      "Ep done - 90440.\n",
      "Ep done - 90450.\n",
      "Ep done - 90460.\n",
      "Ep done - 90470.\n",
      "Ep done - 90480.\n",
      "Ep done - 90490.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 9467     |\n",
      "|    total_timesteps | 2494464  |\n",
      "---------------------------------\n",
      "Ep done - 90500.\n",
      "Ep done - 90510.\n",
      "Ep done - 90520.\n",
      "Ep done - 90530.\n",
      "Ep done - 90540.\n",
      "Ep done - 90550.\n",
      "Ep done - 90560.\n",
      "Ep done - 90570.\n",
      "Ep done - 90580.\n",
      "Ep done - 90590.\n",
      "Ep done - 90600.\n",
      "Ep done - 90610.\n",
      "Ep done - 90620.\n",
      "Ep done - 90630.\n",
      "Ep done - 90640.\n",
      "Ep done - 90650.\n",
      "Ep done - 90660.\n",
      "Ep done - 90670.\n",
      "Ep done - 90680.\n",
      "Ep done - 24910.\n",
      "Ep done - 24920.\n",
      "Ep done - 24930.\n",
      "Ep done - 24940.\n",
      "Ep done - 24950.\n",
      "Ep done - 24960.\n",
      "Ep done - 24970.\n",
      "Ep done - 24980.\n",
      "Ep done - 24990.\n",
      "Ep done - 25000.\n",
      "Eval num_timesteps=2500000, episode_reward=-0.42 +/- 0.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2500000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.05e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.111     |\n",
      "|    n_updates            | 23290     |\n",
      "|    policy_gradient_loss | 1.7e-09   |\n",
      "|    value_loss           | 0.237     |\n",
      "---------------------------------------\n",
      "Ep done - 90690.\n",
      "Ep done - 90700.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 407      |\n",
      "|    time_elapsed    | 9486     |\n",
      "|    total_timesteps | 2500608  |\n",
      "---------------------------------\n",
      "Ep done - 90710.\n",
      "Ep done - 90720.\n",
      "Ep done - 90730.\n",
      "Ep done - 90740.\n",
      "Ep done - 90750.\n",
      "Ep done - 90760.\n",
      "Ep done - 90770.\n",
      "Ep done - 90780.\n",
      "Ep done - 90790.\n",
      "Ep done - 90800.\n",
      "Ep done - 90810.\n",
      "Ep done - 90820.\n",
      "Ep done - 90830.\n",
      "Ep done - 90840.\n",
      "Ep done - 90850.\n",
      "Ep done - 90860.\n",
      "Ep done - 90870.\n",
      "Ep done - 90880.\n",
      "Ep done - 90890.\n",
      "Ep done - 90900.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.36     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 263       |\n",
      "|    iterations           | 408       |\n",
      "|    time_elapsed         | 9501      |\n",
      "|    total_timesteps      | 2506752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5e-06    |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.112     |\n",
      "|    n_updates            | 23300     |\n",
      "|    policy_gradient_loss | -2.38e-10 |\n",
      "|    value_loss           | 0.26      |\n",
      "---------------------------------------\n",
      "Ep done - 90910.\n",
      "Ep done - 90920.\n",
      "Ep done - 90930.\n",
      "Ep done - 90940.\n",
      "Ep done - 90950.\n",
      "Ep done - 90960.\n",
      "Ep done - 90970.\n",
      "Ep done - 90980.\n",
      "Ep done - 90990.\n",
      "Ep done - 91000.\n",
      "Ep done - 91010.\n",
      "Ep done - 25010.\n",
      "Ep done - 25020.\n",
      "Ep done - 25030.\n",
      "Ep done - 25040.\n",
      "Ep done - 25050.\n",
      "Ep done - 25060.\n",
      "Ep done - 25070.\n",
      "Ep done - 25080.\n",
      "Ep done - 25090.\n",
      "Ep done - 25100.\n",
      "Eval num_timesteps=2510000, episode_reward=-0.64 +/- 0.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.64     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2510000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.9e-06  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.138     |\n",
      "|    n_updates            | 23310     |\n",
      "|    policy_gradient_loss | -1.77e-09 |\n",
      "|    value_loss           | 0.241     |\n",
      "---------------------------------------\n",
      "Ep done - 91020.\n",
      "Ep done - 91030.\n",
      "Ep done - 91040.\n",
      "Ep done - 91050.\n",
      "Ep done - 91060.\n",
      "Ep done - 91070.\n",
      "Ep done - 91080.\n",
      "Ep done - 91090.\n",
      "Ep done - 91100.\n",
      "Ep done - 91110.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 409      |\n",
      "|    time_elapsed    | 9520     |\n",
      "|    total_timesteps | 2512896  |\n",
      "---------------------------------\n",
      "Ep done - 91120.\n",
      "Ep done - 91130.\n",
      "Ep done - 91140.\n",
      "Ep done - 91150.\n",
      "Ep done - 91160.\n",
      "Ep done - 91170.\n",
      "Ep done - 91180.\n",
      "Ep done - 91190.\n",
      "Ep done - 91200.\n",
      "Ep done - 91210.\n",
      "Ep done - 91220.\n",
      "Ep done - 91230.\n",
      "Ep done - 91240.\n",
      "Ep done - 91250.\n",
      "Ep done - 91260.\n",
      "Ep done - 91270.\n",
      "Ep done - 91280.\n",
      "Ep done - 91290.\n",
      "Ep done - 91300.\n",
      "Ep done - 91310.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 264       |\n",
      "|    iterations           | 410       |\n",
      "|    time_elapsed         | 9536      |\n",
      "|    total_timesteps      | 2519040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.83e-06 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0907    |\n",
      "|    n_updates            | 23320     |\n",
      "|    policy_gradient_loss | 8.34e-11  |\n",
      "|    value_loss           | 0.235     |\n",
      "---------------------------------------\n",
      "Ep done - 91320.\n",
      "Ep done - 91330.\n",
      "Ep done - 91340.\n",
      "Ep done - 25110.\n",
      "Ep done - 25120.\n",
      "Ep done - 25130.\n",
      "Ep done - 25140.\n",
      "Ep done - 25150.\n",
      "Ep done - 25160.\n",
      "Ep done - 25170.\n",
      "Ep done - 25180.\n",
      "Ep done - 25190.\n",
      "Ep done - 25200.\n",
      "Eval num_timesteps=2520000, episode_reward=-0.48 +/- 0.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.48     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2520000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.71e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0871    |\n",
      "|    n_updates            | 23330     |\n",
      "|    policy_gradient_loss | 4.42e-10  |\n",
      "|    value_loss           | 0.206     |\n",
      "---------------------------------------\n",
      "Ep done - 91350.\n",
      "Ep done - 91360.\n",
      "Ep done - 91370.\n",
      "Ep done - 91380.\n",
      "Ep done - 91390.\n",
      "Ep done - 91400.\n",
      "Ep done - 91410.\n",
      "Ep done - 91420.\n",
      "Ep done - 91430.\n",
      "Ep done - 91440.\n",
      "Ep done - 91450.\n",
      "Ep done - 91460.\n",
      "Ep done - 91470.\n",
      "Ep done - 91480.\n",
      "Ep done - 91490.\n",
      "Ep done - 91500.\n",
      "Ep done - 91510.\n",
      "Ep done - 91520.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 411      |\n",
      "|    time_elapsed    | 9555     |\n",
      "|    total_timesteps | 2525184  |\n",
      "---------------------------------\n",
      "Ep done - 91530.\n",
      "Ep done - 91540.\n",
      "Ep done - 91550.\n",
      "Ep done - 91560.\n",
      "Ep done - 91570.\n",
      "Ep done - 91580.\n",
      "Ep done - 91590.\n",
      "Ep done - 91600.\n",
      "Ep done - 91610.\n",
      "Ep done - 91620.\n",
      "Ep done - 91630.\n",
      "Ep done - 91640.\n",
      "Ep done - 91650.\n",
      "Ep done - 91660.\n",
      "Ep done - 91670.\n",
      "Ep done - 91680.\n",
      "Ep done - 25210.\n",
      "Ep done - 25220.\n",
      "Ep done - 25230.\n",
      "Ep done - 25240.\n",
      "Ep done - 25250.\n",
      "Ep done - 25260.\n",
      "Ep done - 25270.\n",
      "Ep done - 25280.\n",
      "Ep done - 25290.\n",
      "Ep done - 25300.\n",
      "Eval num_timesteps=2530000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2530000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.68e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.156     |\n",
      "|    n_updates            | 23340     |\n",
      "|    policy_gradient_loss | 3.44e-10  |\n",
      "|    value_loss           | 0.217     |\n",
      "---------------------------------------\n",
      "Ep done - 91690.\n",
      "Ep done - 91700.\n",
      "Ep done - 91710.\n",
      "Ep done - 91720.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 412      |\n",
      "|    time_elapsed    | 9574     |\n",
      "|    total_timesteps | 2531328  |\n",
      "---------------------------------\n",
      "Ep done - 91730.\n",
      "Ep done - 91740.\n",
      "Ep done - 91750.\n",
      "Ep done - 91760.\n",
      "Ep done - 91770.\n",
      "Ep done - 91780.\n",
      "Ep done - 91790.\n",
      "Ep done - 91800.\n",
      "Ep done - 91810.\n",
      "Ep done - 91820.\n",
      "Ep done - 91830.\n",
      "Ep done - 91840.\n",
      "Ep done - 91850.\n",
      "Ep done - 91860.\n",
      "Ep done - 91870.\n",
      "Ep done - 91880.\n",
      "Ep done - 91890.\n",
      "Ep done - 91900.\n",
      "Ep done - 91910.\n",
      "Ep done - 91920.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -0.42        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 264          |\n",
      "|    iterations           | 413          |\n",
      "|    time_elapsed         | 9589         |\n",
      "|    total_timesteps      | 2537472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008707503 |\n",
      "|    clip_fraction        | 0.000277     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.99e-05    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0828       |\n",
      "|    n_updates            | 23350        |\n",
      "|    policy_gradient_loss | -3.95e-05    |\n",
      "|    value_loss           | 0.22         |\n",
      "------------------------------------------\n",
      "Ep done - 91930.\n",
      "Ep done - 91940.\n",
      "Ep done - 91950.\n",
      "Ep done - 91960.\n",
      "Ep done - 91970.\n",
      "Ep done - 91980.\n",
      "Ep done - 91990.\n",
      "Ep done - 92000.\n",
      "Ep done - 92010.\n",
      "Ep done - 25310.\n",
      "Ep done - 25320.\n",
      "Ep done - 25330.\n",
      "Ep done - 25340.\n",
      "Ep done - 25350.\n",
      "Ep done - 25360.\n",
      "Ep done - 25370.\n",
      "Ep done - 25380.\n",
      "Ep done - 25390.\n",
      "Ep done - 25400.\n",
      "Eval num_timesteps=2540000, episode_reward=-0.58 +/- 0.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.58     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2540000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.39e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.124     |\n",
      "|    n_updates            | 23360     |\n",
      "|    policy_gradient_loss | -4.23e-10 |\n",
      "|    value_loss           | 0.244     |\n",
      "---------------------------------------\n",
      "Ep done - 92020.\n",
      "Ep done - 92030.\n",
      "Ep done - 92040.\n",
      "Ep done - 92050.\n",
      "Ep done - 92060.\n",
      "Ep done - 92070.\n",
      "Ep done - 92080.\n",
      "Ep done - 92090.\n",
      "Ep done - 92100.\n",
      "Ep done - 92110.\n",
      "Ep done - 92120.\n",
      "Ep done - 92130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 9609     |\n",
      "|    total_timesteps | 2543616  |\n",
      "---------------------------------\n",
      "Ep done - 92140.\n",
      "Ep done - 92150.\n",
      "Ep done - 92160.\n",
      "Ep done - 92170.\n",
      "Ep done - 92180.\n",
      "Ep done - 92190.\n",
      "Ep done - 92200.\n",
      "Ep done - 92210.\n",
      "Ep done - 92220.\n",
      "Ep done - 92230.\n",
      "Ep done - 92240.\n",
      "Ep done - 92250.\n",
      "Ep done - 92260.\n",
      "Ep done - 92270.\n",
      "Ep done - 92280.\n",
      "Ep done - 92290.\n",
      "Ep done - 92300.\n",
      "Ep done - 92310.\n",
      "Ep done - 92320.\n",
      "Ep done - 92330.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 264       |\n",
      "|    iterations           | 415       |\n",
      "|    time_elapsed         | 9624      |\n",
      "|    total_timesteps      | 2549760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.31e-06 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.139     |\n",
      "|    n_updates            | 23370     |\n",
      "|    policy_gradient_loss | -8.35e-10 |\n",
      "|    value_loss           | 0.25      |\n",
      "---------------------------------------\n",
      "Ep done - 92340.\n",
      "Ep done - 25410.\n",
      "Ep done - 25420.\n",
      "Ep done - 25430.\n",
      "Ep done - 25440.\n",
      "Ep done - 25450.\n",
      "Ep done - 25460.\n",
      "Ep done - 25470.\n",
      "Ep done - 25480.\n",
      "Ep done - 25490.\n",
      "Ep done - 25500.\n",
      "Eval num_timesteps=2550000, episode_reward=-0.52 +/- 0.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.52     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2550000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.23e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0952    |\n",
      "|    n_updates            | 23380     |\n",
      "|    policy_gradient_loss | 1.43e-10  |\n",
      "|    value_loss           | 0.233     |\n",
      "---------------------------------------\n",
      "Ep done - 92350.\n",
      "Ep done - 92360.\n",
      "Ep done - 92370.\n",
      "Ep done - 92380.\n",
      "Ep done - 92390.\n",
      "Ep done - 92400.\n",
      "Ep done - 92410.\n",
      "Ep done - 92420.\n",
      "Ep done - 92430.\n",
      "Ep done - 92440.\n",
      "Ep done - 92450.\n",
      "Ep done - 92460.\n",
      "Ep done - 92470.\n",
      "Ep done - 92480.\n",
      "Ep done - 92490.\n",
      "Ep done - 92500.\n",
      "Ep done - 92510.\n",
      "Ep done - 92520.\n",
      "Ep done - 92530.\n",
      "Ep done - 92540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 265      |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 9643     |\n",
      "|    total_timesteps | 2555904  |\n",
      "---------------------------------\n",
      "Ep done - 92550.\n",
      "Ep done - 92560.\n",
      "Ep done - 92570.\n",
      "Ep done - 92580.\n",
      "Ep done - 92590.\n",
      "Ep done - 92600.\n",
      "Ep done - 92610.\n",
      "Ep done - 92620.\n",
      "Ep done - 92630.\n",
      "Ep done - 92640.\n",
      "Ep done - 92650.\n",
      "Ep done - 92660.\n",
      "Ep done - 92670.\n",
      "Ep done - 92680.\n",
      "Ep done - 25510.\n",
      "Ep done - 25520.\n",
      "Ep done - 25530.\n",
      "Ep done - 25540.\n",
      "Ep done - 25550.\n",
      "Ep done - 25560.\n",
      "Ep done - 25570.\n",
      "Ep done - 25580.\n",
      "Ep done - 25590.\n",
      "Ep done - 25600.\n",
      "Eval num_timesteps=2560000, episode_reward=-0.58 +/- 0.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.58     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2560000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.27e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.157     |\n",
      "|    n_updates            | 23390     |\n",
      "|    policy_gradient_loss | 6.59e-10  |\n",
      "|    value_loss           | 0.279     |\n",
      "---------------------------------------\n",
      "Ep done - 92690.\n",
      "Ep done - 92700.\n",
      "Ep done - 92710.\n",
      "Ep done - 92720.\n",
      "Ep done - 92730.\n",
      "Ep done - 92740.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 265      |\n",
      "|    iterations      | 417      |\n",
      "|    time_elapsed    | 9662     |\n",
      "|    total_timesteps | 2562048  |\n",
      "---------------------------------\n",
      "Ep done - 92750.\n",
      "Ep done - 92760.\n",
      "Ep done - 92770.\n",
      "Ep done - 92780.\n",
      "Ep done - 92790.\n",
      "Ep done - 92800.\n",
      "Ep done - 92810.\n",
      "Ep done - 92820.\n",
      "Ep done - 92830.\n",
      "Ep done - 92840.\n",
      "Ep done - 92850.\n",
      "Ep done - 92860.\n",
      "Ep done - 92870.\n",
      "Ep done - 92880.\n",
      "Ep done - 92890.\n",
      "Ep done - 92900.\n",
      "Ep done - 92910.\n",
      "Ep done - 92920.\n",
      "Ep done - 92930.\n",
      "Ep done - 92940.\n",
      "Ep done - 92950.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -0.52        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 265          |\n",
      "|    iterations           | 418          |\n",
      "|    time_elapsed         | 9677         |\n",
      "|    total_timesteps      | 2568192      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053147017 |\n",
      "|    clip_fraction        | 0.000472     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.47e-05    |\n",
      "|    explained_variance   | 9.02e-05     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.128        |\n",
      "|    n_updates            | 23400        |\n",
      "|    policy_gradient_loss | -1.42e-05    |\n",
      "|    value_loss           | 0.253        |\n",
      "------------------------------------------\n",
      "Ep done - 92960.\n",
      "Ep done - 92970.\n",
      "Ep done - 92980.\n",
      "Ep done - 92990.\n",
      "Ep done - 93000.\n",
      "Ep done - 93010.\n",
      "Ep done - 25610.\n",
      "Ep done - 25620.\n",
      "Ep done - 25630.\n",
      "Ep done - 25640.\n",
      "Ep done - 25650.\n",
      "Ep done - 25660.\n",
      "Ep done - 25670.\n",
      "Ep done - 25680.\n",
      "Ep done - 25690.\n",
      "Ep done - 25700.\n",
      "Eval num_timesteps=2570000, episode_reward=-0.44 +/- 0.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.44        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2570000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017224267 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000101    |\n",
      "|    explained_variance   | 3.93e-06     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.138        |\n",
      "|    n_updates            | 23410        |\n",
      "|    policy_gradient_loss | -1.8e-05     |\n",
      "|    value_loss           | 0.214        |\n",
      "------------------------------------------\n",
      "Ep done - 93020.\n",
      "Ep done - 93030.\n",
      "Ep done - 93040.\n",
      "Ep done - 93050.\n",
      "Ep done - 93060.\n",
      "Ep done - 93070.\n",
      "Ep done - 93080.\n",
      "Ep done - 93090.\n",
      "Ep done - 93100.\n",
      "Ep done - 93110.\n",
      "Ep done - 93120.\n",
      "Ep done - 93130.\n",
      "Ep done - 93140.\n",
      "Ep done - 93150.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 265      |\n",
      "|    iterations      | 419      |\n",
      "|    time_elapsed    | 9696     |\n",
      "|    total_timesteps | 2574336  |\n",
      "---------------------------------\n",
      "Ep done - 93160.\n",
      "Ep done - 93170.\n",
      "Ep done - 93180.\n",
      "Ep done - 93190.\n",
      "Ep done - 93200.\n",
      "Ep done - 93210.\n",
      "Ep done - 93220.\n",
      "Ep done - 93230.\n",
      "Ep done - 93240.\n",
      "Ep done - 93250.\n",
      "Ep done - 93260.\n",
      "Ep done - 93270.\n",
      "Ep done - 93280.\n",
      "Ep done - 93290.\n",
      "Ep done - 93300.\n",
      "Ep done - 93310.\n",
      "Ep done - 93320.\n",
      "Ep done - 93330.\n",
      "Ep done - 93340.\n",
      "Ep done - 25710.\n",
      "Ep done - 25720.\n",
      "Ep done - 25730.\n",
      "Ep done - 25740.\n",
      "Ep done - 25750.\n",
      "Ep done - 25760.\n",
      "Ep done - 25770.\n",
      "Ep done - 25780.\n",
      "Ep done - 25790.\n",
      "Ep done - 25800.\n",
      "Eval num_timesteps=2580000, episode_reward=-0.48 +/- 0.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.48        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2580000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011872659 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.97e-05    |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0836       |\n",
      "|    n_updates            | 23420        |\n",
      "|    policy_gradient_loss | -3.21e-05    |\n",
      "|    value_loss           | 0.221        |\n",
      "------------------------------------------\n",
      "Ep done - 93350.\n",
      "Ep done - 93360.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 265      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 9715     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Ep done - 93370.\n",
      "Ep done - 93380.\n",
      "Ep done - 93390.\n",
      "Ep done - 93400.\n",
      "Ep done - 93410.\n",
      "Ep done - 93420.\n",
      "Ep done - 93430.\n",
      "Ep done - 93440.\n",
      "Ep done - 93450.\n",
      "Ep done - 93460.\n",
      "Ep done - 93470.\n",
      "Ep done - 93480.\n",
      "Ep done - 93490.\n",
      "Ep done - 93500.\n",
      "Ep done - 93510.\n",
      "Ep done - 93520.\n",
      "Ep done - 93530.\n",
      "Ep done - 93540.\n",
      "Ep done - 93550.\n",
      "Ep done - 93560.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -0.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 265          |\n",
      "|    iterations           | 421          |\n",
      "|    time_elapsed         | 9730         |\n",
      "|    total_timesteps      | 2586624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.721785e-06 |\n",
      "|    clip_fraction        | 0.000179     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00021     |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.127        |\n",
      "|    n_updates            | 23430        |\n",
      "|    policy_gradient_loss | 4.68e-05     |\n",
      "|    value_loss           | 0.248        |\n",
      "------------------------------------------\n",
      "Ep done - 93570.\n",
      "Ep done - 93580.\n",
      "Ep done - 93590.\n",
      "Ep done - 93600.\n",
      "Ep done - 93610.\n",
      "Ep done - 93620.\n",
      "Ep done - 93630.\n",
      "Ep done - 93640.\n",
      "Ep done - 93650.\n",
      "Ep done - 93660.\n",
      "Ep done - 93670.\n",
      "Ep done - 93680.\n",
      "Ep done - 25810.\n",
      "Ep done - 25820.\n",
      "Ep done - 25830.\n",
      "Ep done - 25840.\n",
      "Ep done - 25850.\n",
      "Ep done - 25860.\n",
      "Ep done - 25870.\n",
      "Ep done - 25880.\n",
      "Ep done - 25890.\n",
      "Ep done - 25900.\n",
      "Eval num_timesteps=2590000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.6     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 2590000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0.000374 |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00047 |\n",
      "|    explained_variance   | 0.117    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.145    |\n",
      "|    n_updates            | 23440    |\n",
      "|    policy_gradient_loss | 3.46e-05 |\n",
      "|    value_loss           | 0.26     |\n",
      "--------------------------------------\n",
      "Ep done - 93690.\n",
      "Ep done - 93700.\n",
      "Ep done - 93710.\n",
      "Ep done - 93720.\n",
      "Ep done - 93730.\n",
      "Ep done - 93740.\n",
      "Ep done - 93750.\n",
      "Ep done - 93760.\n",
      "Ep done - 93770.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 265      |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 9749     |\n",
      "|    total_timesteps | 2592768  |\n",
      "---------------------------------\n",
      "Ep done - 93780.\n",
      "Ep done - 93790.\n",
      "Ep done - 93800.\n",
      "Ep done - 93810.\n",
      "Ep done - 93820.\n",
      "Ep done - 93830.\n",
      "Ep done - 93840.\n",
      "Ep done - 93850.\n",
      "Ep done - 93860.\n",
      "Ep done - 93870.\n",
      "Ep done - 93880.\n",
      "Ep done - 93890.\n",
      "Ep done - 93900.\n",
      "Ep done - 93910.\n",
      "Ep done - 93920.\n",
      "Ep done - 93930.\n",
      "Ep done - 93940.\n",
      "Ep done - 93950.\n",
      "Ep done - 93960.\n",
      "Ep done - 93970.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.58     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 266       |\n",
      "|    iterations           | 423       |\n",
      "|    time_elapsed         | 9764      |\n",
      "|    total_timesteps      | 2598912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 1.63e-05  |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.84e-05 |\n",
      "|    explained_variance   | 0.176     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.119     |\n",
      "|    n_updates            | 23450     |\n",
      "|    policy_gradient_loss | 7.05e-06  |\n",
      "|    value_loss           | 0.216     |\n",
      "---------------------------------------\n",
      "Ep done - 93980.\n",
      "Ep done - 93990.\n",
      "Ep done - 94000.\n",
      "Ep done - 94010.\n",
      "Ep done - 25910.\n",
      "Ep done - 25920.\n",
      "Ep done - 25930.\n",
      "Ep done - 25940.\n",
      "Ep done - 25950.\n",
      "Ep done - 25960.\n",
      "Ep done - 25970.\n",
      "Ep done - 25980.\n",
      "Ep done - 25990.\n",
      "Ep done - 26000.\n",
      "Eval num_timesteps=2600000, episode_reward=-0.50 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2600000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.28e-06 |\n",
      "|    explained_variance   | 0.11      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.149     |\n",
      "|    n_updates            | 23460     |\n",
      "|    policy_gradient_loss | 1.59e-09  |\n",
      "|    value_loss           | 0.234     |\n",
      "---------------------------------------\n",
      "Ep done - 94020.\n",
      "Ep done - 94030.\n",
      "Ep done - 94040.\n",
      "Ep done - 94050.\n",
      "Ep done - 94060.\n",
      "Ep done - 94070.\n",
      "Ep done - 94080.\n",
      "Ep done - 94090.\n",
      "Ep done - 94100.\n",
      "Ep done - 94110.\n",
      "Ep done - 94120.\n",
      "Ep done - 94130.\n",
      "Ep done - 94140.\n",
      "Ep done - 94150.\n",
      "Ep done - 94160.\n",
      "Ep done - 94170.\n",
      "Ep done - 94180.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 266      |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 9783     |\n",
      "|    total_timesteps | 2605056  |\n",
      "---------------------------------\n",
      "Ep done - 94190.\n",
      "Ep done - 94200.\n",
      "Ep done - 94210.\n",
      "Ep done - 94220.\n",
      "Ep done - 94230.\n",
      "Ep done - 94240.\n",
      "Ep done - 94250.\n",
      "Ep done - 94260.\n",
      "Ep done - 94270.\n",
      "Ep done - 94280.\n",
      "Ep done - 94290.\n",
      "Ep done - 94300.\n",
      "Ep done - 94310.\n",
      "Ep done - 94320.\n",
      "Ep done - 94330.\n",
      "Ep done - 94340.\n",
      "Ep done - 26010.\n",
      "Ep done - 26020.\n",
      "Ep done - 26030.\n",
      "Ep done - 26040.\n",
      "Ep done - 26050.\n",
      "Ep done - 26060.\n",
      "Ep done - 26070.\n",
      "Ep done - 26080.\n",
      "Ep done - 26090.\n",
      "Ep done - 26100.\n",
      "Eval num_timesteps=2610000, episode_reward=-0.42 +/- 0.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.42        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2610000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.172634e-05 |\n",
      "|    clip_fraction        | 0.000553     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00103     |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.226        |\n",
      "|    n_updates            | 23470        |\n",
      "|    policy_gradient_loss | 2.27e-05     |\n",
      "|    value_loss           | 0.245        |\n",
      "------------------------------------------\n",
      "Ep done - 94350.\n",
      "Ep done - 94360.\n",
      "Ep done - 94370.\n",
      "Ep done - 94380.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 266      |\n",
      "|    iterations      | 425      |\n",
      "|    time_elapsed    | 9802     |\n",
      "|    total_timesteps | 2611200  |\n",
      "---------------------------------\n",
      "Ep done - 94390.\n",
      "Ep done - 94400.\n",
      "Ep done - 94410.\n",
      "Ep done - 94420.\n",
      "Ep done - 94430.\n",
      "Ep done - 94440.\n",
      "Ep done - 94450.\n",
      "Ep done - 94460.\n",
      "Ep done - 94470.\n",
      "Ep done - 94480.\n",
      "Ep done - 94490.\n",
      "Ep done - 94500.\n",
      "Ep done - 94510.\n",
      "Ep done - 94520.\n",
      "Ep done - 94530.\n",
      "Ep done - 94540.\n",
      "Ep done - 94550.\n",
      "Ep done - 94560.\n",
      "Ep done - 94570.\n",
      "Ep done - 94580.\n",
      "Ep done - 94590.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -0.34         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 266           |\n",
      "|    iterations           | 426           |\n",
      "|    time_elapsed         | 9818          |\n",
      "|    total_timesteps      | 2617344       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013746266 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00074      |\n",
      "|    explained_variance   | 0.0996        |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.108         |\n",
      "|    n_updates            | 23480         |\n",
      "|    policy_gradient_loss | 1.8e-05       |\n",
      "|    value_loss           | 0.255         |\n",
      "-------------------------------------------\n",
      "Ep done - 94600.\n",
      "Ep done - 94610.\n",
      "Ep done - 94620.\n",
      "Ep done - 94630.\n",
      "Ep done - 94640.\n",
      "Ep done - 94650.\n",
      "Ep done - 94660.\n",
      "Ep done - 94670.\n",
      "Ep done - 94680.\n",
      "Ep done - 26110.\n",
      "Ep done - 26120.\n",
      "Ep done - 26130.\n",
      "Ep done - 26140.\n",
      "Ep done - 26150.\n",
      "Ep done - 26160.\n",
      "Ep done - 26170.\n",
      "Ep done - 26180.\n",
      "Ep done - 26190.\n",
      "Ep done - 26200.\n",
      "Eval num_timesteps=2620000, episode_reward=-0.32 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.32        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2620000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032938367 |\n",
      "|    clip_fraction        | 0.000374     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000685    |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.116        |\n",
      "|    n_updates            | 23490        |\n",
      "|    policy_gradient_loss | -6.05e-06    |\n",
      "|    value_loss           | 0.237        |\n",
      "------------------------------------------\n",
      "Ep done - 94690.\n",
      "Ep done - 94700.\n",
      "Ep done - 94710.\n",
      "Ep done - 94720.\n",
      "Ep done - 94730.\n",
      "Ep done - 94740.\n",
      "Ep done - 94750.\n",
      "Ep done - 94760.\n",
      "Ep done - 94770.\n",
      "Ep done - 94780.\n",
      "Ep done - 94790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 266      |\n",
      "|    iterations      | 427      |\n",
      "|    time_elapsed    | 9837     |\n",
      "|    total_timesteps | 2623488  |\n",
      "---------------------------------\n",
      "Ep done - 94800.\n",
      "Ep done - 94810.\n",
      "Ep done - 94820.\n",
      "Ep done - 94830.\n",
      "Ep done - 94840.\n",
      "Ep done - 94850.\n",
      "Ep done - 94860.\n",
      "Ep done - 94870.\n",
      "Ep done - 94880.\n",
      "Ep done - 94890.\n",
      "Ep done - 94900.\n",
      "Ep done - 94910.\n",
      "Ep done - 94920.\n",
      "Ep done - 94930.\n",
      "Ep done - 94940.\n",
      "Ep done - 94950.\n",
      "Ep done - 94960.\n",
      "Ep done - 94970.\n",
      "Ep done - 94980.\n",
      "Ep done - 94990.\n",
      "Ep done - 95000.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.48     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 266       |\n",
      "|    iterations           | 428       |\n",
      "|    time_elapsed         | 9852      |\n",
      "|    total_timesteps      | 2629632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0.00693   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000195 |\n",
      "|    explained_variance   | 0.115     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.131     |\n",
      "|    n_updates            | 23500     |\n",
      "|    policy_gradient_loss | 0.00215   |\n",
      "|    value_loss           | 0.233     |\n",
      "---------------------------------------\n",
      "Ep done - 95010.\n",
      "Ep done - 26210.\n",
      "Ep done - 26220.\n",
      "Ep done - 26230.\n",
      "Ep done - 26240.\n",
      "Ep done - 26250.\n",
      "Ep done - 26260.\n",
      "Ep done - 26270.\n",
      "Ep done - 26280.\n",
      "Ep done - 26290.\n",
      "Ep done - 26300.\n",
      "Eval num_timesteps=2630000, episode_reward=-0.24 +/- 0.97\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -0.24         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2630000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014790679 |\n",
      "|    clip_fraction        | 0.000277      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000514     |\n",
      "|    explained_variance   | 0.125         |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.137         |\n",
      "|    n_updates            | 23510         |\n",
      "|    policy_gradient_loss | -2.5e-05      |\n",
      "|    value_loss           | 0.239         |\n",
      "-------------------------------------------\n",
      "Ep done - 95020.\n",
      "Ep done - 95030.\n",
      "Ep done - 95040.\n",
      "Ep done - 95050.\n",
      "Ep done - 95060.\n",
      "Ep done - 95070.\n",
      "Ep done - 95080.\n",
      "Ep done - 95090.\n",
      "Ep done - 95100.\n",
      "Ep done - 95110.\n",
      "Ep done - 95120.\n",
      "Ep done - 95130.\n",
      "Ep done - 95140.\n",
      "Ep done - 95150.\n",
      "Ep done - 95160.\n",
      "Ep done - 95170.\n",
      "Ep done - 95180.\n",
      "Ep done - 95190.\n",
      "Ep done - 95200.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 267      |\n",
      "|    iterations      | 429      |\n",
      "|    time_elapsed    | 9871     |\n",
      "|    total_timesteps | 2635776  |\n",
      "---------------------------------\n",
      "Ep done - 95210.\n",
      "Ep done - 95220.\n",
      "Ep done - 95230.\n",
      "Ep done - 95240.\n",
      "Ep done - 95250.\n",
      "Ep done - 95260.\n",
      "Ep done - 95270.\n",
      "Ep done - 95280.\n",
      "Ep done - 95290.\n",
      "Ep done - 95300.\n",
      "Ep done - 95310.\n",
      "Ep done - 95320.\n",
      "Ep done - 95330.\n",
      "Ep done - 95340.\n",
      "Ep done - 26310.\n",
      "Ep done - 26320.\n",
      "Ep done - 26330.\n",
      "Ep done - 26340.\n",
      "Ep done - 26350.\n",
      "Ep done - 26360.\n",
      "Ep done - 26370.\n",
      "Ep done - 26380.\n",
      "Ep done - 26390.\n",
      "Ep done - 26400.\n",
      "Eval num_timesteps=2640000, episode_reward=-0.36 +/- 0.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.36     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2640000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.26e-06 |\n",
      "|    explained_variance   | 0.107     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.135     |\n",
      "|    n_updates            | 23520     |\n",
      "|    policy_gradient_loss | 7.59e-10  |\n",
      "|    value_loss           | 0.256     |\n",
      "---------------------------------------\n",
      "Ep done - 95350.\n",
      "Ep done - 95360.\n",
      "Ep done - 95370.\n",
      "Ep done - 95380.\n",
      "Ep done - 95390.\n",
      "Ep done - 95400.\n",
      "Ep done - 95410.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 267      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 9891     |\n",
      "|    total_timesteps | 2641920  |\n",
      "---------------------------------\n",
      "Ep done - 95420.\n",
      "Ep done - 95430.\n",
      "Ep done - 95440.\n",
      "Ep done - 95450.\n",
      "Ep done - 95460.\n",
      "Ep done - 95470.\n",
      "Ep done - 95480.\n",
      "Ep done - 95490.\n",
      "Ep done - 95500.\n",
      "Ep done - 95510.\n",
      "Ep done - 95520.\n",
      "Ep done - 95530.\n",
      "Ep done - 95540.\n",
      "Ep done - 95550.\n",
      "Ep done - 95560.\n",
      "Ep done - 95570.\n",
      "Ep done - 95580.\n",
      "Ep done - 95590.\n",
      "Ep done - 95600.\n",
      "Ep done - 95610.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 267       |\n",
      "|    iterations           | 431       |\n",
      "|    time_elapsed         | 9906      |\n",
      "|    total_timesteps      | 2648064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.5e-06  |\n",
      "|    explained_variance   | 0.0785    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0981    |\n",
      "|    n_updates            | 23530     |\n",
      "|    policy_gradient_loss | -2.76e-08 |\n",
      "|    value_loss           | 0.247     |\n",
      "---------------------------------------\n",
      "Ep done - 95620.\n",
      "Ep done - 95630.\n",
      "Ep done - 95640.\n",
      "Ep done - 95650.\n",
      "Ep done - 95660.\n",
      "Ep done - 95670.\n",
      "Ep done - 95680.\n",
      "Ep done - 26410.\n",
      "Ep done - 26420.\n",
      "Ep done - 26430.\n",
      "Ep done - 26440.\n",
      "Ep done - 26450.\n",
      "Ep done - 26460.\n",
      "Ep done - 26470.\n",
      "Ep done - 26480.\n",
      "Ep done - 26490.\n",
      "Ep done - 26500.\n",
      "Eval num_timesteps=2650000, episode_reward=-0.46 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2650000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.48e-06 |\n",
      "|    explained_variance   | 0.0995    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.113     |\n",
      "|    n_updates            | 23540     |\n",
      "|    policy_gradient_loss | -6.42e-09 |\n",
      "|    value_loss           | 0.238     |\n",
      "---------------------------------------\n",
      "Ep done - 95690.\n",
      "Ep done - 95700.\n",
      "Ep done - 95710.\n",
      "Ep done - 95720.\n",
      "Ep done - 95730.\n",
      "Ep done - 95740.\n",
      "Ep done - 95750.\n",
      "Ep done - 95760.\n",
      "Ep done - 95770.\n",
      "Ep done - 95780.\n",
      "Ep done - 95790.\n",
      "Ep done - 95800.\n",
      "Ep done - 95810.\n",
      "Ep done - 95820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 267      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 9925     |\n",
      "|    total_timesteps | 2654208  |\n",
      "---------------------------------\n",
      "Ep done - 95830.\n",
      "Ep done - 95840.\n",
      "Ep done - 95850.\n",
      "Ep done - 95860.\n",
      "Ep done - 95870.\n",
      "Ep done - 95880.\n",
      "Ep done - 95890.\n",
      "Ep done - 95900.\n",
      "Ep done - 95910.\n",
      "Ep done - 95920.\n",
      "Ep done - 95930.\n",
      "Ep done - 95940.\n",
      "Ep done - 95950.\n",
      "Ep done - 95960.\n",
      "Ep done - 95970.\n",
      "Ep done - 95980.\n",
      "Ep done - 95990.\n",
      "Ep done - 96000.\n",
      "Ep done - 96010.\n",
      "Ep done - 26510.\n",
      "Ep done - 26520.\n",
      "Ep done - 26530.\n",
      "Ep done - 26540.\n",
      "Ep done - 26550.\n",
      "Ep done - 26560.\n",
      "Ep done - 26570.\n",
      "Ep done - 26580.\n",
      "Ep done - 26590.\n",
      "Ep done - 26600.\n",
      "Eval num_timesteps=2660000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2660000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.94e-06 |\n",
      "|    explained_variance   | 0.108     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.099     |\n",
      "|    n_updates            | 23550     |\n",
      "|    policy_gradient_loss | 5.7e-10   |\n",
      "|    value_loss           | 0.219     |\n",
      "---------------------------------------\n",
      "Ep done - 96020.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 267      |\n",
      "|    iterations      | 433      |\n",
      "|    time_elapsed    | 9945     |\n",
      "|    total_timesteps | 2660352  |\n",
      "---------------------------------\n",
      "Ep done - 96030.\n",
      "Ep done - 96040.\n",
      "Ep done - 96050.\n",
      "Ep done - 96060.\n",
      "Ep done - 96070.\n",
      "Ep done - 96080.\n",
      "Ep done - 96090.\n",
      "Ep done - 96100.\n",
      "Ep done - 96110.\n",
      "Ep done - 96120.\n",
      "Ep done - 96130.\n",
      "Ep done - 96140.\n",
      "Ep done - 96150.\n",
      "Ep done - 96160.\n",
      "Ep done - 96170.\n",
      "Ep done - 96180.\n",
      "Ep done - 96190.\n",
      "Ep done - 96200.\n",
      "Ep done - 96210.\n",
      "Ep done - 96220.\n",
      "Ep done - 96230.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 267       |\n",
      "|    iterations           | 434       |\n",
      "|    time_elapsed         | 9960      |\n",
      "|    total_timesteps      | 2666496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.05e-06 |\n",
      "|    explained_variance   | 0.124     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0989    |\n",
      "|    n_updates            | 23560     |\n",
      "|    policy_gradient_loss | -9.88e-10 |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "Ep done - 96240.\n",
      "Ep done - 96250.\n",
      "Ep done - 96260.\n",
      "Ep done - 96270.\n",
      "Ep done - 96280.\n",
      "Ep done - 96290.\n",
      "Ep done - 96300.\n",
      "Ep done - 96310.\n",
      "Ep done - 96320.\n",
      "Ep done - 96330.\n",
      "Ep done - 96340.\n",
      "Ep done - 26610.\n",
      "Ep done - 26620.\n",
      "Ep done - 26630.\n",
      "Ep done - 26640.\n",
      "Ep done - 26650.\n",
      "Ep done - 26660.\n",
      "Ep done - 26670.\n",
      "Ep done - 26680.\n",
      "Ep done - 26690.\n",
      "Ep done - 26700.\n",
      "Eval num_timesteps=2670000, episode_reward=-0.36 +/- 0.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.36     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2670000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.33e-06 |\n",
      "|    explained_variance   | 0.114     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0657    |\n",
      "|    n_updates            | 23570     |\n",
      "|    policy_gradient_loss | -8.68e-10 |\n",
      "|    value_loss           | 0.237     |\n",
      "---------------------------------------\n",
      "Ep done - 96350.\n",
      "Ep done - 96360.\n",
      "Ep done - 96370.\n",
      "Ep done - 96380.\n",
      "Ep done - 96390.\n",
      "Ep done - 96400.\n",
      "Ep done - 96410.\n",
      "Ep done - 96420.\n",
      "Ep done - 96430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 267      |\n",
      "|    iterations      | 435      |\n",
      "|    time_elapsed    | 9979     |\n",
      "|    total_timesteps | 2672640  |\n",
      "---------------------------------\n",
      "Ep done - 96440.\n",
      "Ep done - 96450.\n",
      "Ep done - 96460.\n",
      "Ep done - 96470.\n",
      "Ep done - 96480.\n",
      "Ep done - 96490.\n",
      "Ep done - 96500.\n",
      "Ep done - 96510.\n",
      "Ep done - 96520.\n",
      "Ep done - 96530.\n",
      "Ep done - 96540.\n",
      "Ep done - 96550.\n",
      "Ep done - 96560.\n",
      "Ep done - 96570.\n",
      "Ep done - 96580.\n",
      "Ep done - 96590.\n",
      "Ep done - 96600.\n",
      "Ep done - 96610.\n",
      "Ep done - 96620.\n",
      "Ep done - 96630.\n",
      "Ep done - 96640.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 268       |\n",
      "|    iterations           | 436       |\n",
      "|    time_elapsed         | 9994      |\n",
      "|    total_timesteps      | 2678784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.39e-06 |\n",
      "|    explained_variance   | 0.112     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.114     |\n",
      "|    n_updates            | 23580     |\n",
      "|    policy_gradient_loss | -1.65e-08 |\n",
      "|    value_loss           | 0.232     |\n",
      "---------------------------------------\n",
      "Ep done - 96650.\n",
      "Ep done - 96660.\n",
      "Ep done - 96670.\n",
      "Ep done - 96680.\n",
      "Ep done - 26710.\n",
      "Ep done - 26720.\n",
      "Ep done - 26730.\n",
      "Ep done - 26740.\n",
      "Ep done - 26750.\n",
      "Ep done - 26760.\n",
      "Ep done - 26770.\n",
      "Ep done - 26780.\n",
      "Ep done - 26790.\n",
      "Ep done - 26800.\n",
      "Eval num_timesteps=2680000, episode_reward=-0.43 +/- 0.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.43     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2680000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2762537 |\n",
      "|    clip_fraction        | 0.0231    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000179 |\n",
      "|    explained_variance   | 0.123     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0971    |\n",
      "|    n_updates            | 23590     |\n",
      "|    policy_gradient_loss | 0.00696   |\n",
      "|    value_loss           | 0.238     |\n",
      "---------------------------------------\n",
      "Ep done - 96690.\n",
      "Ep done - 96700.\n",
      "Ep done - 96710.\n",
      "Ep done - 96720.\n",
      "Ep done - 96730.\n",
      "Ep done - 96740.\n",
      "Ep done - 96750.\n",
      "Ep done - 96760.\n",
      "Ep done - 96770.\n",
      "Ep done - 96780.\n",
      "Ep done - 96790.\n",
      "Ep done - 96800.\n",
      "Ep done - 96810.\n",
      "Ep done - 96820.\n",
      "Ep done - 96830.\n",
      "Ep done - 96840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 268      |\n",
      "|    iterations      | 437      |\n",
      "|    time_elapsed    | 10013    |\n",
      "|    total_timesteps | 2684928  |\n",
      "---------------------------------\n",
      "Ep done - 96850.\n",
      "Ep done - 96860.\n",
      "Ep done - 96870.\n",
      "Ep done - 96880.\n",
      "Ep done - 96890.\n",
      "Ep done - 96900.\n",
      "Ep done - 96910.\n",
      "Ep done - 96920.\n",
      "Ep done - 96930.\n",
      "Ep done - 96940.\n",
      "Ep done - 96950.\n",
      "Ep done - 96960.\n",
      "Ep done - 96970.\n",
      "Ep done - 96980.\n",
      "Ep done - 96990.\n",
      "Ep done - 97000.\n",
      "Ep done - 97010.\n",
      "Ep done - 26810.\n",
      "Ep done - 26820.\n",
      "Ep done - 26830.\n",
      "Ep done - 26840.\n",
      "Ep done - 26850.\n",
      "Ep done - 26860.\n",
      "Ep done - 26870.\n",
      "Ep done - 26880.\n",
      "Ep done - 26890.\n",
      "Ep done - 26900.\n",
      "Eval num_timesteps=2690000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2690000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000165 |\n",
      "|    explained_variance   | 0.0443    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.108     |\n",
      "|    n_updates            | 23600     |\n",
      "|    policy_gradient_loss | 4.37e-06  |\n",
      "|    value_loss           | 0.209     |\n",
      "---------------------------------------\n",
      "Ep done - 97020.\n",
      "Ep done - 97030.\n",
      "Ep done - 97040.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 268      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 10032    |\n",
      "|    total_timesteps | 2691072  |\n",
      "---------------------------------\n",
      "Ep done - 97050.\n",
      "Ep done - 97060.\n",
      "Ep done - 97070.\n",
      "Ep done - 97080.\n",
      "Ep done - 97090.\n",
      "Ep done - 97100.\n",
      "Ep done - 97110.\n",
      "Ep done - 97120.\n",
      "Ep done - 97130.\n",
      "Ep done - 97140.\n",
      "Ep done - 97150.\n",
      "Ep done - 97160.\n",
      "Ep done - 97170.\n",
      "Ep done - 97180.\n",
      "Ep done - 97190.\n",
      "Ep done - 97200.\n",
      "Ep done - 97210.\n",
      "Ep done - 97220.\n",
      "Ep done - 97230.\n",
      "Ep done - 97240.\n",
      "Ep done - 97250.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 268       |\n",
      "|    iterations           | 439       |\n",
      "|    time_elapsed         | 10047     |\n",
      "|    total_timesteps      | 2697216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0.000114  |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000108 |\n",
      "|    explained_variance   | 0.0457    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.138     |\n",
      "|    n_updates            | 23610     |\n",
      "|    policy_gradient_loss | 2.19e-05  |\n",
      "|    value_loss           | 0.268     |\n",
      "---------------------------------------\n",
      "Ep done - 97260.\n",
      "Ep done - 97270.\n",
      "Ep done - 97280.\n",
      "Ep done - 97290.\n",
      "Ep done - 97300.\n",
      "Ep done - 97310.\n",
      "Ep done - 97320.\n",
      "Ep done - 97330.\n",
      "Ep done - 97340.\n",
      "Ep done - 26910.\n",
      "Ep done - 26920.\n",
      "Ep done - 26930.\n",
      "Ep done - 26940.\n",
      "Ep done - 26950.\n",
      "Ep done - 26960.\n",
      "Ep done - 26970.\n",
      "Ep done - 26980.\n",
      "Ep done - 26990.\n",
      "Ep done - 27000.\n",
      "Eval num_timesteps=2700000, episode_reward=-0.24 +/- 0.97\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -0.24         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2700000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6127556e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -7.22e-05     |\n",
      "|    explained_variance   | 0.0508        |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.124         |\n",
      "|    n_updates            | 23620         |\n",
      "|    policy_gradient_loss | -2.55e-06     |\n",
      "|    value_loss           | 0.236         |\n",
      "-------------------------------------------\n",
      "Ep done - 97350.\n",
      "Ep done - 97360.\n",
      "Ep done - 97370.\n",
      "Ep done - 97380.\n",
      "Ep done - 97390.\n",
      "Ep done - 97400.\n",
      "Ep done - 97410.\n",
      "Ep done - 97420.\n",
      "Ep done - 97430.\n",
      "Ep done - 97440.\n",
      "Ep done - 97450.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 268      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 10066    |\n",
      "|    total_timesteps | 2703360  |\n",
      "---------------------------------\n",
      "Ep done - 97460.\n",
      "Ep done - 97470.\n",
      "Ep done - 97480.\n",
      "Ep done - 97490.\n",
      "Ep done - 97500.\n",
      "Ep done - 97510.\n",
      "Ep done - 97520.\n",
      "Ep done - 97530.\n",
      "Ep done - 97540.\n",
      "Ep done - 97550.\n",
      "Ep done - 97560.\n",
      "Ep done - 97570.\n",
      "Ep done - 97580.\n",
      "Ep done - 97590.\n",
      "Ep done - 97600.\n",
      "Ep done - 97610.\n",
      "Ep done - 97620.\n",
      "Ep done - 97630.\n",
      "Ep done - 97640.\n",
      "Ep done - 97650.\n",
      "Ep done - 97660.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -0.44         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 268           |\n",
      "|    iterations           | 441           |\n",
      "|    time_elapsed         | 10081         |\n",
      "|    total_timesteps      | 2709504       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0069925e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -7.03e-06     |\n",
      "|    explained_variance   | 0.138         |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.083         |\n",
      "|    n_updates            | 23630         |\n",
      "|    policy_gradient_loss | -3.61e-06     |\n",
      "|    value_loss           | 0.227         |\n",
      "-------------------------------------------\n",
      "Ep done - 97670.\n",
      "Ep done - 97680.\n",
      "Ep done - 27010.\n",
      "Ep done - 27020.\n",
      "Ep done - 27030.\n",
      "Ep done - 27040.\n",
      "Ep done - 27050.\n",
      "Ep done - 27060.\n",
      "Ep done - 27070.\n",
      "Ep done - 27080.\n",
      "Ep done - 27090.\n",
      "Ep done - 27100.\n",
      "Eval num_timesteps=2710000, episode_reward=-0.38 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.38     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2710000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.11e-05 |\n",
      "|    explained_variance   | 0.136     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.131     |\n",
      "|    n_updates            | 23640     |\n",
      "|    policy_gradient_loss | 2.29e-07  |\n",
      "|    value_loss           | 0.216     |\n",
      "---------------------------------------\n",
      "Ep done - 97690.\n",
      "Ep done - 97700.\n",
      "Ep done - 97710.\n",
      "Ep done - 97720.\n",
      "Ep done - 97730.\n",
      "Ep done - 97740.\n",
      "Ep done - 97750.\n",
      "Ep done - 97760.\n",
      "Ep done - 97770.\n",
      "Ep done - 97780.\n",
      "Ep done - 97790.\n",
      "Ep done - 97800.\n",
      "Ep done - 97810.\n",
      "Ep done - 97820.\n",
      "Ep done - 97830.\n",
      "Ep done - 97840.\n",
      "Ep done - 97850.\n",
      "Ep done - 97860.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 268      |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 10100    |\n",
      "|    total_timesteps | 2715648  |\n",
      "---------------------------------\n",
      "Ep done - 97870.\n",
      "Ep done - 97880.\n",
      "Ep done - 97890.\n",
      "Ep done - 97900.\n",
      "Ep done - 97910.\n",
      "Ep done - 97920.\n",
      "Ep done - 97930.\n",
      "Ep done - 97940.\n",
      "Ep done - 97950.\n",
      "Ep done - 97960.\n",
      "Ep done - 97970.\n",
      "Ep done - 97980.\n",
      "Ep done - 97990.\n",
      "Ep done - 98000.\n",
      "Ep done - 98010.\n",
      "Ep done - 27110.\n",
      "Ep done - 27120.\n",
      "Ep done - 27130.\n",
      "Ep done - 27140.\n",
      "Ep done - 27150.\n",
      "Ep done - 27160.\n",
      "Ep done - 27170.\n",
      "Ep done - 27180.\n",
      "Ep done - 27190.\n",
      "Ep done - 27200.\n",
      "Eval num_timesteps=2720000, episode_reward=-0.48 +/- 0.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.48        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2720000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057142936 |\n",
      "|    clip_fraction        | 0.00057      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000106    |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0962       |\n",
      "|    n_updates            | 23650        |\n",
      "|    policy_gradient_loss | 0.000539     |\n",
      "|    value_loss           | 0.228        |\n",
      "------------------------------------------\n",
      "Ep done - 98020.\n",
      "Ep done - 98030.\n",
      "Ep done - 98040.\n",
      "Ep done - 98050.\n",
      "Ep done - 98060.\n",
      "Ep done - 98070.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 268      |\n",
      "|    iterations      | 443      |\n",
      "|    time_elapsed    | 10119    |\n",
      "|    total_timesteps | 2721792  |\n",
      "---------------------------------\n",
      "Ep done - 98080.\n",
      "Ep done - 98090.\n",
      "Ep done - 98100.\n",
      "Ep done - 98110.\n",
      "Ep done - 98120.\n",
      "Ep done - 98130.\n",
      "Ep done - 98140.\n",
      "Ep done - 98150.\n",
      "Ep done - 98160.\n",
      "Ep done - 98170.\n",
      "Ep done - 98180.\n",
      "Ep done - 98190.\n",
      "Ep done - 98200.\n",
      "Ep done - 98210.\n",
      "Ep done - 98220.\n",
      "Ep done - 98230.\n",
      "Ep done - 98240.\n",
      "Ep done - 98250.\n",
      "Ep done - 98260.\n",
      "Ep done - 98270.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -0.48        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 269          |\n",
      "|    iterations           | 444          |\n",
      "|    time_elapsed         | 10135        |\n",
      "|    total_timesteps      | 2727936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019195481 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -9.27e-05    |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.154        |\n",
      "|    n_updates            | 23660        |\n",
      "|    policy_gradient_loss | -2.6e-05     |\n",
      "|    value_loss           | 0.287        |\n",
      "------------------------------------------\n",
      "Ep done - 98280.\n",
      "Ep done - 98290.\n",
      "Ep done - 98300.\n",
      "Ep done - 98310.\n",
      "Ep done - 98320.\n",
      "Ep done - 98330.\n",
      "Ep done - 98340.\n",
      "Ep done - 27210.\n",
      "Ep done - 27220.\n",
      "Ep done - 27230.\n",
      "Ep done - 27240.\n",
      "Ep done - 27250.\n",
      "Ep done - 27260.\n",
      "Ep done - 27270.\n",
      "Ep done - 27280.\n",
      "Ep done - 27290.\n",
      "Ep done - 27300.\n",
      "Eval num_timesteps=2730000, episode_reward=-0.30 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2730000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011588124 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.28e-06    |\n",
      "|    explained_variance   | 0.0904       |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.102        |\n",
      "|    n_updates            | 23670        |\n",
      "|    policy_gradient_loss | -1.97e-05    |\n",
      "|    value_loss           | 0.235        |\n",
      "------------------------------------------\n",
      "Ep done - 98350.\n",
      "Ep done - 98360.\n",
      "Ep done - 98370.\n",
      "Ep done - 98380.\n",
      "Ep done - 98390.\n",
      "Ep done - 98400.\n",
      "Ep done - 98410.\n",
      "Ep done - 98420.\n",
      "Ep done - 98430.\n",
      "Ep done - 98440.\n",
      "Ep done - 98450.\n",
      "Ep done - 98460.\n",
      "Ep done - 98470.\n",
      "Ep done - 98480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 445      |\n",
      "|    time_elapsed    | 10153    |\n",
      "|    total_timesteps | 2734080  |\n",
      "---------------------------------\n",
      "Ep done - 98490.\n",
      "Ep done - 98500.\n",
      "Ep done - 98510.\n",
      "Ep done - 98520.\n",
      "Ep done - 98530.\n",
      "Ep done - 98540.\n",
      "Ep done - 98550.\n",
      "Ep done - 98560.\n",
      "Ep done - 98570.\n",
      "Ep done - 98580.\n",
      "Ep done - 98590.\n",
      "Ep done - 98600.\n",
      "Ep done - 98610.\n",
      "Ep done - 98620.\n",
      "Ep done - 98630.\n",
      "Ep done - 98640.\n",
      "Ep done - 98650.\n",
      "Ep done - 98660.\n",
      "Ep done - 98670.\n",
      "Ep done - 98680.\n",
      "Ep done - 27310.\n",
      "Ep done - 27320.\n",
      "Ep done - 27330.\n",
      "Ep done - 27340.\n",
      "Ep done - 27350.\n",
      "Ep done - 27360.\n",
      "Ep done - 27370.\n",
      "Ep done - 27380.\n",
      "Ep done - 27390.\n",
      "Ep done - 27400.\n",
      "Eval num_timesteps=2740000, episode_reward=-0.58 +/- 0.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.58     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2740000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.53e-06 |\n",
      "|    explained_variance   | 0.0797    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.1       |\n",
      "|    n_updates            | 23680     |\n",
      "|    policy_gradient_loss | -1.11e-09 |\n",
      "|    value_loss           | 0.241     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 10172    |\n",
      "|    total_timesteps | 2740224  |\n",
      "---------------------------------\n",
      "Ep done - 98690.\n",
      "Ep done - 98700.\n",
      "Ep done - 98710.\n",
      "Ep done - 98720.\n",
      "Ep done - 98730.\n",
      "Ep done - 98740.\n",
      "Ep done - 98750.\n",
      "Ep done - 98760.\n",
      "Ep done - 98770.\n",
      "Ep done - 98780.\n",
      "Ep done - 98790.\n",
      "Ep done - 98800.\n",
      "Ep done - 98810.\n",
      "Ep done - 98820.\n",
      "Ep done - 98830.\n",
      "Ep done - 98840.\n",
      "Ep done - 98850.\n",
      "Ep done - 98860.\n",
      "Ep done - 98870.\n",
      "Ep done - 98880.\n",
      "Ep done - 98890.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 269       |\n",
      "|    iterations           | 447       |\n",
      "|    time_elapsed         | 10187     |\n",
      "|    total_timesteps      | 2746368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.53e-05 |\n",
      "|    explained_variance   | 0.129     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.13      |\n",
      "|    n_updates            | 23690     |\n",
      "|    policy_gradient_loss | 5.74e-06  |\n",
      "|    value_loss           | 0.222     |\n",
      "---------------------------------------\n",
      "Ep done - 98900.\n",
      "Ep done - 98910.\n",
      "Ep done - 98920.\n",
      "Ep done - 98930.\n",
      "Ep done - 98940.\n",
      "Ep done - 98950.\n",
      "Ep done - 98960.\n",
      "Ep done - 98970.\n",
      "Ep done - 98980.\n",
      "Ep done - 98990.\n",
      "Ep done - 99000.\n",
      "Ep done - 99010.\n",
      "Ep done - 27410.\n",
      "Ep done - 27420.\n",
      "Ep done - 27430.\n",
      "Ep done - 27440.\n",
      "Ep done - 27450.\n",
      "Ep done - 27460.\n",
      "Ep done - 27470.\n",
      "Ep done - 27480.\n",
      "Ep done - 27490.\n",
      "Ep done - 27500.\n",
      "Eval num_timesteps=2750000, episode_reward=-0.56 +/- 0.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.56     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2750000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.19e-06 |\n",
      "|    explained_variance   | 0.0969    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0929    |\n",
      "|    n_updates            | 23700     |\n",
      "|    policy_gradient_loss | 5.76e-10  |\n",
      "|    value_loss           | 0.236     |\n",
      "---------------------------------------\n",
      "Ep done - 99020.\n",
      "Ep done - 99030.\n",
      "Ep done - 99040.\n",
      "Ep done - 99050.\n",
      "Ep done - 99060.\n",
      "Ep done - 99070.\n",
      "Ep done - 99080.\n",
      "Ep done - 99090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 10207    |\n",
      "|    total_timesteps | 2752512  |\n",
      "---------------------------------\n",
      "Ep done - 99100.\n",
      "Ep done - 99110.\n",
      "Ep done - 99120.\n",
      "Ep done - 99130.\n",
      "Ep done - 99140.\n",
      "Ep done - 99150.\n",
      "Ep done - 99160.\n",
      "Ep done - 99170.\n",
      "Ep done - 99180.\n",
      "Ep done - 99190.\n",
      "Ep done - 99200.\n",
      "Ep done - 99210.\n",
      "Ep done - 99220.\n",
      "Ep done - 99230.\n",
      "Ep done - 99240.\n",
      "Ep done - 99250.\n",
      "Ep done - 99260.\n",
      "Ep done - 99270.\n",
      "Ep done - 99280.\n",
      "Ep done - 99290.\n",
      "Ep done - 99300.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 269       |\n",
      "|    iterations           | 449       |\n",
      "|    time_elapsed         | 10222     |\n",
      "|    total_timesteps      | 2758656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.28e-06 |\n",
      "|    explained_variance   | 0.0982    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.11      |\n",
      "|    n_updates            | 23710     |\n",
      "|    policy_gradient_loss | -1.04e-09 |\n",
      "|    value_loss           | 0.224     |\n",
      "---------------------------------------\n",
      "Ep done - 99310.\n",
      "Ep done - 99320.\n",
      "Ep done - 99330.\n",
      "Ep done - 99340.\n",
      "Ep done - 27510.\n",
      "Ep done - 27520.\n",
      "Ep done - 27530.\n",
      "Ep done - 27540.\n",
      "Ep done - 27550.\n",
      "Ep done - 27560.\n",
      "Ep done - 27570.\n",
      "Ep done - 27580.\n",
      "Ep done - 27590.\n",
      "Ep done - 27600.\n",
      "Eval num_timesteps=2760000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2760000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.36e-06 |\n",
      "|    explained_variance   | 0.1       |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.137     |\n",
      "|    n_updates            | 23720     |\n",
      "|    policy_gradient_loss | 4.4e-10   |\n",
      "|    value_loss           | 0.232     |\n",
      "---------------------------------------\n",
      "Ep done - 99350.\n",
      "Ep done - 99360.\n",
      "Ep done - 99370.\n",
      "Ep done - 99380.\n",
      "Ep done - 99390.\n",
      "Ep done - 99400.\n",
      "Ep done - 99410.\n",
      "Ep done - 99420.\n",
      "Ep done - 99430.\n",
      "Ep done - 99440.\n",
      "Ep done - 99450.\n",
      "Ep done - 99460.\n",
      "Ep done - 99470.\n",
      "Ep done - 99480.\n",
      "Ep done - 99490.\n",
      "Ep done - 99500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 269      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 10241    |\n",
      "|    total_timesteps | 2764800  |\n",
      "---------------------------------\n",
      "Ep done - 99510.\n",
      "Ep done - 99520.\n",
      "Ep done - 99530.\n",
      "Ep done - 99540.\n",
      "Ep done - 99550.\n",
      "Ep done - 99560.\n",
      "Ep done - 99570.\n",
      "Ep done - 99580.\n",
      "Ep done - 99590.\n",
      "Ep done - 99600.\n",
      "Ep done - 99610.\n",
      "Ep done - 99620.\n",
      "Ep done - 99630.\n",
      "Ep done - 99640.\n",
      "Ep done - 99650.\n",
      "Ep done - 99660.\n",
      "Ep done - 99670.\n",
      "Ep done - 99680.\n",
      "Ep done - 27610.\n",
      "Ep done - 27620.\n",
      "Ep done - 27630.\n",
      "Ep done - 27640.\n",
      "Ep done - 27650.\n",
      "Ep done - 27660.\n",
      "Ep done - 27670.\n",
      "Ep done - 27680.\n",
      "Ep done - 27690.\n",
      "Ep done - 27700.\n",
      "Eval num_timesteps=2770000, episode_reward=-0.32 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.32     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2770000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.35e-06 |\n",
      "|    explained_variance   | 0.122     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.124     |\n",
      "|    n_updates            | 23730     |\n",
      "|    policy_gradient_loss | -8.87e-10 |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "Ep done - 99690.\n",
      "Ep done - 99700.\n",
      "Ep done - 99710.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 270      |\n",
      "|    iterations      | 451      |\n",
      "|    time_elapsed    | 10260    |\n",
      "|    total_timesteps | 2770944  |\n",
      "---------------------------------\n",
      "Ep done - 99720.\n",
      "Ep done - 99730.\n",
      "Ep done - 99740.\n",
      "Ep done - 99750.\n",
      "Ep done - 99760.\n",
      "Ep done - 99770.\n",
      "Ep done - 99780.\n",
      "Ep done - 99790.\n",
      "Ep done - 99800.\n",
      "Ep done - 99810.\n",
      "Ep done - 99820.\n",
      "Ep done - 99830.\n",
      "Ep done - 99840.\n",
      "Ep done - 99850.\n",
      "Ep done - 99860.\n",
      "Ep done - 99870.\n",
      "Ep done - 99880.\n",
      "Ep done - 99890.\n",
      "Ep done - 99900.\n",
      "Ep done - 99910.\n",
      "Ep done - 99920.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.5       |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 270        |\n",
      "|    iterations           | 452        |\n",
      "|    time_elapsed         | 10276      |\n",
      "|    total_timesteps      | 2777088    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26619947 |\n",
      "|    clip_fraction        | 0.0146     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000417  |\n",
      "|    explained_variance   | 0.116      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.104      |\n",
      "|    n_updates            | 23740      |\n",
      "|    policy_gradient_loss | 0.00305    |\n",
      "|    value_loss           | 0.219      |\n",
      "----------------------------------------\n",
      "Ep done - 99930.\n",
      "Ep done - 99940.\n",
      "Ep done - 99950.\n",
      "Ep done - 99960.\n",
      "Ep done - 99970.\n",
      "Ep done - 99980.\n",
      "Ep done - 99990.\n",
      "Ep done - 100000.\n",
      "Ep done - 100010.\n",
      "Ep done - 27710.\n",
      "Ep done - 27720.\n",
      "Ep done - 27730.\n",
      "Ep done - 27740.\n",
      "Ep done - 27750.\n",
      "Ep done - 27760.\n",
      "Ep done - 27770.\n",
      "Ep done - 27780.\n",
      "Ep done - 27790.\n",
      "Ep done - 27800.\n",
      "Eval num_timesteps=2780000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2780000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.6503415 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00164  |\n",
      "|    explained_variance   | -0.0306   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0189   |\n",
      "|    n_updates            | 23750     |\n",
      "|    policy_gradient_loss | 0.0459    |\n",
      "|    value_loss           | 0.0122    |\n",
      "---------------------------------------\n",
      "Ep done - 100020.\n",
      "Ep done - 100030.\n",
      "Ep done - 100040.\n",
      "Ep done - 100050.\n",
      "Ep done - 100060.\n",
      "Ep done - 100070.\n",
      "Ep done - 100080.\n",
      "Ep done - 100090.\n",
      "Ep done - 100100.\n",
      "Ep done - 100110.\n",
      "Ep done - 100120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 270      |\n",
      "|    iterations      | 453      |\n",
      "|    time_elapsed    | 10295    |\n",
      "|    total_timesteps | 2783232  |\n",
      "---------------------------------\n",
      "Ep done - 100130.\n",
      "Ep done - 100140.\n",
      "Ep done - 100150.\n",
      "Ep done - 100160.\n",
      "Ep done - 100170.\n",
      "Ep done - 100180.\n",
      "Ep done - 100190.\n",
      "Ep done - 100200.\n",
      "Ep done - 100210.\n",
      "Ep done - 100220.\n",
      "Ep done - 100230.\n",
      "Ep done - 100240.\n",
      "Ep done - 100250.\n",
      "Ep done - 100260.\n",
      "Ep done - 100270.\n",
      "Ep done - 100280.\n",
      "Ep done - 100290.\n",
      "Ep done - 100300.\n",
      "Ep done - 100310.\n",
      "Ep done - 100320.\n",
      "Ep done - 100330.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 270       |\n",
      "|    iterations           | 454       |\n",
      "|    time_elapsed         | 10310     |\n",
      "|    total_timesteps      | 2789376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.1100948 |\n",
      "|    clip_fraction        | 0.108     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000403 |\n",
      "|    explained_variance   | 0.686     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0607   |\n",
      "|    n_updates            | 23760     |\n",
      "|    policy_gradient_loss | -0.0268   |\n",
      "|    value_loss           | 0.00451   |\n",
      "---------------------------------------\n",
      "Ep done - 100340.\n",
      "Ep done - 100350.\n",
      "Ep done - 27810.\n",
      "Ep done - 27820.\n",
      "Ep done - 27830.\n",
      "Ep done - 27840.\n",
      "Ep done - 27850.\n",
      "Ep done - 27860.\n",
      "Ep done - 27870.\n",
      "Ep done - 27880.\n",
      "Ep done - 27890.\n",
      "Ep done - 27900.\n",
      "Eval num_timesteps=2790000, episode_reward=-0.40 +/- 0.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2790000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9092617 |\n",
      "|    clip_fraction        | 0.0652    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000175 |\n",
      "|    explained_variance   | 0.248     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0177   |\n",
      "|    n_updates            | 23770     |\n",
      "|    policy_gradient_loss | -0.0121   |\n",
      "|    value_loss           | 0.0018    |\n",
      "---------------------------------------\n",
      "Ep done - 100360.\n",
      "Ep done - 100370.\n",
      "Ep done - 100380.\n",
      "Ep done - 100390.\n",
      "Ep done - 100400.\n",
      "Ep done - 100410.\n",
      "Ep done - 100420.\n",
      "Ep done - 100430.\n",
      "Ep done - 100440.\n",
      "Ep done - 100450.\n",
      "Ep done - 100460.\n",
      "Ep done - 100470.\n",
      "Ep done - 100480.\n",
      "Ep done - 100490.\n",
      "Ep done - 100500.\n",
      "Ep done - 100510.\n",
      "Ep done - 100520.\n",
      "Ep done - 100530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 270      |\n",
      "|    iterations      | 455      |\n",
      "|    time_elapsed    | 10329    |\n",
      "|    total_timesteps | 2795520  |\n",
      "---------------------------------\n",
      "Ep done - 100540.\n",
      "Ep done - 100550.\n",
      "Ep done - 100560.\n",
      "Ep done - 100570.\n",
      "Ep done - 100580.\n",
      "Ep done - 100590.\n",
      "Ep done - 100600.\n",
      "Ep done - 100610.\n",
      "Ep done - 100620.\n",
      "Ep done - 100630.\n",
      "Ep done - 100640.\n",
      "Ep done - 100650.\n",
      "Ep done - 100660.\n",
      "Ep done - 100670.\n",
      "Ep done - 100680.\n",
      "Ep done - 27910.\n",
      "Ep done - 27920.\n",
      "Ep done - 27930.\n",
      "Ep done - 27940.\n",
      "Ep done - 27950.\n",
      "Ep done - 27960.\n",
      "Ep done - 27970.\n",
      "Ep done - 27980.\n",
      "Ep done - 27990.\n",
      "Ep done - 28000.\n",
      "Eval num_timesteps=2800000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2800000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9383485 |\n",
      "|    clip_fraction        | 0.0805    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000251 |\n",
      "|    explained_variance   | -0.0746   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.108     |\n",
      "|    n_updates            | 23780     |\n",
      "|    policy_gradient_loss | -0.00675  |\n",
      "|    value_loss           | 0.226     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 93\n",
      "Ep done - 100690.\n",
      "Ep done - 100700.\n",
      "Ep done - 100710.\n",
      "Ep done - 100720.\n",
      "Ep done - 100730.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 270      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 10348    |\n",
      "|    total_timesteps | 2801664  |\n",
      "---------------------------------\n",
      "Ep done - 100740.\n",
      "Ep done - 100750.\n",
      "Ep done - 100760.\n",
      "Ep done - 100770.\n",
      "Ep done - 100780.\n",
      "Ep done - 100790.\n",
      "Ep done - 100800.\n",
      "Ep done - 100810.\n",
      "Ep done - 100820.\n",
      "Ep done - 100830.\n",
      "Ep done - 100840.\n",
      "Ep done - 100850.\n",
      "Ep done - 100860.\n",
      "Ep done - 100870.\n",
      "Ep done - 100880.\n",
      "Ep done - 100890.\n",
      "Ep done - 100900.\n",
      "Ep done - 100910.\n",
      "Ep done - 100920.\n",
      "Ep done - 100930.\n",
      "Ep done - 100940.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 270       |\n",
      "|    iterations           | 457       |\n",
      "|    time_elapsed         | 10363     |\n",
      "|    total_timesteps      | 2807808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6825886 |\n",
      "|    clip_fraction        | 0.0525    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00203  |\n",
      "|    explained_variance   | 0.00935   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0278    |\n",
      "|    n_updates            | 23790     |\n",
      "|    policy_gradient_loss | -0.00774  |\n",
      "|    value_loss           | 0.151     |\n",
      "---------------------------------------\n",
      "Ep done - 100950.\n",
      "Ep done - 100960.\n",
      "Ep done - 100970.\n",
      "Ep done - 100980.\n",
      "Ep done - 100990.\n",
      "Ep done - 101000.\n",
      "Ep done - 101010.\n",
      "Ep done - 28010.\n",
      "Ep done - 28020.\n",
      "Ep done - 28030.\n",
      "Ep done - 28040.\n",
      "Ep done - 28050.\n",
      "Ep done - 28060.\n",
      "Ep done - 28070.\n",
      "Ep done - 28080.\n",
      "Ep done - 28090.\n",
      "Ep done - 28100.\n",
      "Eval num_timesteps=2810000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2810000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.9227865 |\n",
      "|    clip_fraction        | 0.214     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00377  |\n",
      "|    explained_variance   | -1.65     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0059   |\n",
      "|    n_updates            | 23800     |\n",
      "|    policy_gradient_loss | 0.00605   |\n",
      "|    value_loss           | 0.0276    |\n",
      "---------------------------------------\n",
      "Ep done - 101020.\n",
      "Ep done - 101030.\n",
      "Ep done - 101040.\n",
      "Ep done - 101050.\n",
      "Ep done - 101060.\n",
      "Ep done - 101070.\n",
      "Ep done - 101080.\n",
      "Ep done - 101090.\n",
      "Ep done - 101100.\n",
      "Ep done - 101110.\n",
      "Ep done - 101120.\n",
      "Ep done - 101130.\n",
      "Ep done - 101140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 271      |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 10382    |\n",
      "|    total_timesteps | 2813952  |\n",
      "---------------------------------\n",
      "Ep done - 101150.\n",
      "Ep done - 101160.\n",
      "Ep done - 101170.\n",
      "Ep done - 101180.\n",
      "Ep done - 101190.\n",
      "Ep done - 101200.\n",
      "Ep done - 101210.\n",
      "Ep done - 101220.\n",
      "Ep done - 101230.\n",
      "Ep done - 101240.\n",
      "Ep done - 101250.\n",
      "Ep done - 101260.\n",
      "Ep done - 101270.\n",
      "Ep done - 101280.\n",
      "Ep done - 101290.\n",
      "Ep done - 101300.\n",
      "Ep done - 101310.\n",
      "Ep done - 101320.\n",
      "Ep done - 101330.\n",
      "Ep done - 101340.\n",
      "Ep done - 101350.\n",
      "Ep done - 28110.\n",
      "Ep done - 28120.\n",
      "Ep done - 28130.\n",
      "Ep done - 28140.\n",
      "Ep done - 28150.\n",
      "Ep done - 28160.\n",
      "Ep done - 28170.\n",
      "Ep done - 28180.\n",
      "Ep done - 28190.\n",
      "Ep done - 28200.\n",
      "Eval num_timesteps=2820000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2820000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.5522885 |\n",
      "|    clip_fraction        | 0.153     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000553 |\n",
      "|    explained_variance   | 0.521     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0151   |\n",
      "|    n_updates            | 23810     |\n",
      "|    policy_gradient_loss | -0.0201   |\n",
      "|    value_loss           | 0.00525   |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 271      |\n",
      "|    iterations      | 459      |\n",
      "|    time_elapsed    | 10401    |\n",
      "|    total_timesteps | 2820096  |\n",
      "---------------------------------\n",
      "Ep done - 101360.\n",
      "Ep done - 101370.\n",
      "Ep done - 101380.\n",
      "Ep done - 101390.\n",
      "Ep done - 101400.\n",
      "Ep done - 101410.\n",
      "Ep done - 101420.\n",
      "Ep done - 101430.\n",
      "Ep done - 101440.\n",
      "Ep done - 101450.\n",
      "Ep done - 101460.\n",
      "Ep done - 101470.\n",
      "Ep done - 101480.\n",
      "Ep done - 101490.\n",
      "Ep done - 101500.\n",
      "Ep done - 101510.\n",
      "Ep done - 101520.\n",
      "Ep done - 101530.\n",
      "Ep done - 101540.\n",
      "Ep done - 101550.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.6       |\n",
      "|    ep_rew_mean          | -0.82      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 271        |\n",
      "|    iterations           | 460        |\n",
      "|    time_elapsed         | 10415      |\n",
      "|    total_timesteps      | 2826240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81781507 |\n",
      "|    clip_fraction        | 0.0437     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000157  |\n",
      "|    explained_variance   | 0.648      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00855   |\n",
      "|    n_updates            | 23820      |\n",
      "|    policy_gradient_loss | -0.000999  |\n",
      "|    value_loss           | 0.00357    |\n",
      "----------------------------------------\n",
      "Ep done - 101560.\n",
      "Ep done - 101570.\n",
      "Ep done - 101580.\n",
      "Ep done - 101590.\n",
      "Ep done - 101600.\n",
      "Ep done - 101610.\n",
      "Ep done - 101620.\n",
      "Ep done - 101630.\n",
      "Ep done - 101640.\n",
      "Ep done - 101650.\n",
      "Ep done - 101660.\n",
      "Ep done - 101670.\n",
      "Ep done - 101680.\n",
      "Ep done - 28210.\n",
      "Ep done - 28220.\n",
      "Ep done - 28230.\n",
      "Ep done - 28240.\n",
      "Ep done - 28250.\n",
      "Ep done - 28260.\n",
      "Ep done - 28270.\n",
      "Ep done - 28280.\n",
      "Ep done - 28290.\n",
      "Ep done - 28300.\n",
      "Eval num_timesteps=2830000, episode_reward=0.18 +/- 0.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.18       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2830000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41238952 |\n",
      "|    clip_fraction        | 0.0472     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00144   |\n",
      "|    explained_variance   | 0.000865   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0817     |\n",
      "|    n_updates            | 23830      |\n",
      "|    policy_gradient_loss | 0.00665    |\n",
      "|    value_loss           | 0.0994     |\n",
      "----------------------------------------\n",
      "Ep done - 101690.\n",
      "Ep done - 101700.\n",
      "Ep done - 101710.\n",
      "Ep done - 101720.\n",
      "Ep done - 101730.\n",
      "Ep done - 101740.\n",
      "Ep done - 101750.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 271      |\n",
      "|    iterations      | 461      |\n",
      "|    time_elapsed    | 10434    |\n",
      "|    total_timesteps | 2832384  |\n",
      "---------------------------------\n",
      "Ep done - 101760.\n",
      "Ep done - 101770.\n",
      "Ep done - 101780.\n",
      "Ep done - 101790.\n",
      "Ep done - 101800.\n",
      "Ep done - 101810.\n",
      "Ep done - 101820.\n",
      "Ep done - 101830.\n",
      "Ep done - 101840.\n",
      "Ep done - 101850.\n",
      "Ep done - 101860.\n",
      "Ep done - 101870.\n",
      "Ep done - 101880.\n",
      "Ep done - 101890.\n",
      "Ep done - 101900.\n",
      "Ep done - 101910.\n",
      "Ep done - 101920.\n",
      "Ep done - 101930.\n",
      "Ep done - 101940.\n",
      "Ep done - 101950.\n",
      "Ep done - 101960.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.1       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 271       |\n",
      "|    iterations           | 462       |\n",
      "|    time_elapsed         | 10449     |\n",
      "|    total_timesteps      | 2838528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7310999 |\n",
      "|    clip_fraction        | 0.0361    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000483 |\n",
      "|    explained_variance   | -0.0796   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0482    |\n",
      "|    n_updates            | 23840     |\n",
      "|    policy_gradient_loss | -0.00595  |\n",
      "|    value_loss           | 0.107     |\n",
      "---------------------------------------\n",
      "Ep done - 101970.\n",
      "Ep done - 101980.\n",
      "Ep done - 101990.\n",
      "Ep done - 102000.\n",
      "Ep done - 102010.\n",
      "Ep done - 28310.\n",
      "Ep done - 28320.\n",
      "Ep done - 28330.\n",
      "Ep done - 28340.\n",
      "Ep done - 28350.\n",
      "Ep done - 28360.\n",
      "Ep done - 28370.\n",
      "Ep done - 28380.\n",
      "Ep done - 28390.\n",
      "Ep done - 28400.\n",
      "Eval num_timesteps=2840000, episode_reward=0.22 +/- 0.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.22       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2840000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49134502 |\n",
      "|    clip_fraction        | 0.0468     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00276   |\n",
      "|    explained_variance   | 0.554      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0467     |\n",
      "|    n_updates            | 23850      |\n",
      "|    policy_gradient_loss | 0.00204    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n",
      "Ep done - 102020.\n",
      "Ep done - 102030.\n",
      "Ep done - 102040.\n",
      "Ep done - 102050.\n",
      "Ep done - 102060.\n",
      "Ep done - 102070.\n",
      "Ep done - 102080.\n",
      "Ep done - 102090.\n",
      "Ep done - 102100.\n",
      "Ep done - 102110.\n",
      "Ep done - 102120.\n",
      "Ep done - 102130.\n",
      "Ep done - 102140.\n",
      "Ep done - 102150.\n",
      "Ep done - 102160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 271      |\n",
      "|    iterations      | 463      |\n",
      "|    time_elapsed    | 10467    |\n",
      "|    total_timesteps | 2844672  |\n",
      "---------------------------------\n",
      "Ep done - 102170.\n",
      "Ep done - 102180.\n",
      "Ep done - 102190.\n",
      "Ep done - 102200.\n",
      "Ep done - 102210.\n",
      "Ep done - 102220.\n",
      "Ep done - 102230.\n",
      "Ep done - 102240.\n",
      "Ep done - 102250.\n",
      "Ep done - 102260.\n",
      "Ep done - 102270.\n",
      "Ep done - 102280.\n",
      "Ep done - 102290.\n",
      "Ep done - 102300.\n",
      "Ep done - 102310.\n",
      "Ep done - 102320.\n",
      "Ep done - 102330.\n",
      "Ep done - 102340.\n",
      "Ep done - 28410.\n",
      "Ep done - 28420.\n",
      "Ep done - 28430.\n",
      "Ep done - 28440.\n",
      "Ep done - 28450.\n",
      "Ep done - 28460.\n",
      "Ep done - 28470.\n",
      "Ep done - 28480.\n",
      "Ep done - 28490.\n",
      "Ep done - 28500.\n",
      "Eval num_timesteps=2850000, episode_reward=-0.78 +/- 0.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.78     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2850000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4976875 |\n",
      "|    clip_fraction        | 0.0438    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00113  |\n",
      "|    explained_variance   | 0.425     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0665    |\n",
      "|    n_updates            | 23860     |\n",
      "|    policy_gradient_loss | 0.0143    |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "Ep done - 102350.\n",
      "Ep done - 102360.\n",
      "Ep done - 102370.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 271      |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 10486    |\n",
      "|    total_timesteps | 2850816  |\n",
      "---------------------------------\n",
      "Ep done - 102380.\n",
      "Ep done - 102390.\n",
      "Ep done - 102400.\n",
      "Ep done - 102410.\n",
      "Ep done - 102420.\n",
      "Ep done - 102430.\n",
      "Ep done - 102440.\n",
      "Ep done - 102450.\n",
      "Ep done - 102460.\n",
      "Ep done - 102470.\n",
      "Ep done - 102480.\n",
      "Ep done - 102490.\n",
      "Ep done - 102500.\n",
      "Ep done - 102510.\n",
      "Ep done - 102520.\n",
      "Ep done - 102530.\n",
      "Ep done - 102540.\n",
      "Ep done - 102550.\n",
      "Ep done - 102560.\n",
      "Ep done - 102570.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.89      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 272       |\n",
      "|    iterations           | 465       |\n",
      "|    time_elapsed         | 10501     |\n",
      "|    total_timesteps      | 2856960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2187605 |\n",
      "|    clip_fraction        | 0.0427    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00164  |\n",
      "|    explained_variance   | -0.645    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0413    |\n",
      "|    n_updates            | 23870     |\n",
      "|    policy_gradient_loss | -0.00517  |\n",
      "|    value_loss           | 0.14      |\n",
      "---------------------------------------\n",
      "Ep done - 102580.\n",
      "Ep done - 102590.\n",
      "Ep done - 102600.\n",
      "Ep done - 102610.\n",
      "Ep done - 102620.\n",
      "Ep done - 102630.\n",
      "Ep done - 102640.\n",
      "Ep done - 102650.\n",
      "Ep done - 102660.\n",
      "Ep done - 102670.\n",
      "Ep done - 102680.\n",
      "Ep done - 28510.\n",
      "Ep done - 28520.\n",
      "Ep done - 28530.\n",
      "Ep done - 28540.\n",
      "Ep done - 28550.\n",
      "Ep done - 28560.\n",
      "Ep done - 28570.\n",
      "Ep done - 28580.\n",
      "Ep done - 28590.\n",
      "Ep done - 28600.\n",
      "Eval num_timesteps=2860000, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.94       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2860000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08492937 |\n",
      "|    clip_fraction        | 0.0162     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00139   |\n",
      "|    explained_variance   | -1.62      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00558    |\n",
      "|    n_updates            | 23880      |\n",
      "|    policy_gradient_loss | -0.00085   |\n",
      "|    value_loss           | 0.0525     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.94\n",
      "SELFPLAY: new best model, bumping up generation to 94\n",
      "Ep done - 102690.\n",
      "Ep done - 102700.\n",
      "Ep done - 102710.\n",
      "Ep done - 102720.\n",
      "Ep done - 102730.\n",
      "Ep done - 102740.\n",
      "Ep done - 102750.\n",
      "Ep done - 102760.\n",
      "Ep done - 102770.\n",
      "Ep done - 102780.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | -0.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 272      |\n",
      "|    iterations      | 466      |\n",
      "|    time_elapsed    | 10520    |\n",
      "|    total_timesteps | 2863104  |\n",
      "---------------------------------\n",
      "Ep done - 102790.\n",
      "Ep done - 102800.\n",
      "Ep done - 102810.\n",
      "Ep done - 102820.\n",
      "Ep done - 102830.\n",
      "Ep done - 102840.\n",
      "Ep done - 102850.\n",
      "Ep done - 102860.\n",
      "Ep done - 102870.\n",
      "Ep done - 102880.\n",
      "Ep done - 102890.\n",
      "Ep done - 102900.\n",
      "Ep done - 102910.\n",
      "Ep done - 102920.\n",
      "Ep done - 102930.\n",
      "Ep done - 102940.\n",
      "Ep done - 102950.\n",
      "Ep done - 102960.\n",
      "Ep done - 102970.\n",
      "Ep done - 102980.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 272       |\n",
      "|    iterations           | 467       |\n",
      "|    time_elapsed         | 10535     |\n",
      "|    total_timesteps      | 2869248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2214851 |\n",
      "|    clip_fraction        | 0.0941    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00256  |\n",
      "|    explained_variance   | 0.124     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.106     |\n",
      "|    n_updates            | 23890     |\n",
      "|    policy_gradient_loss | 0.00243   |\n",
      "|    value_loss           | 0.222     |\n",
      "---------------------------------------\n",
      "Ep done - 102990.\n",
      "Ep done - 103000.\n",
      "Ep done - 103010.\n",
      "Ep done - 28610.\n",
      "Ep done - 28620.\n",
      "Ep done - 28630.\n",
      "Ep done - 28640.\n",
      "Ep done - 28650.\n",
      "Ep done - 28660.\n",
      "Ep done - 28670.\n",
      "Ep done - 28680.\n",
      "Ep done - 28690.\n",
      "Ep done - 28700.\n",
      "Eval num_timesteps=2870000, episode_reward=-0.68 +/- 0.73\n",
      "Episode length: 30.14 +/- 0.35\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.1     |\n",
      "|    mean_reward          | -0.68    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 2870000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.211096 |\n",
      "|    clip_fraction        | 0.0819   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00223 |\n",
      "|    explained_variance   | 0.179    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | -0.0196  |\n",
      "|    n_updates            | 23900    |\n",
      "|    policy_gradient_loss | -0.012   |\n",
      "|    value_loss           | 0.0687   |\n",
      "--------------------------------------\n",
      "Ep done - 103020.\n",
      "Ep done - 103030.\n",
      "Ep done - 103040.\n",
      "Ep done - 103050.\n",
      "Ep done - 103060.\n",
      "Ep done - 103070.\n",
      "Ep done - 103080.\n",
      "Ep done - 103090.\n",
      "Ep done - 103100.\n",
      "Ep done - 103110.\n",
      "Ep done - 103120.\n",
      "Ep done - 103130.\n",
      "Ep done - 103140.\n",
      "Ep done - 103150.\n",
      "Ep done - 103160.\n",
      "Ep done - 103170.\n",
      "Ep done - 103180.\n",
      "Ep done - 103190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 272      |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 10554    |\n",
      "|    total_timesteps | 2875392  |\n",
      "---------------------------------\n",
      "Ep done - 103200.\n",
      "Ep done - 103210.\n",
      "Ep done - 103220.\n",
      "Ep done - 103230.\n",
      "Ep done - 103240.\n",
      "Ep done - 103250.\n",
      "Ep done - 103260.\n",
      "Ep done - 103270.\n",
      "Ep done - 103280.\n",
      "Ep done - 103290.\n",
      "Ep done - 103300.\n",
      "Ep done - 103310.\n",
      "Ep done - 103320.\n",
      "Ep done - 103330.\n",
      "Ep done - 103340.\n",
      "Ep done - 28710.\n",
      "Ep done - 28720.\n",
      "Ep done - 28730.\n",
      "Ep done - 28740.\n",
      "Ep done - 28750.\n",
      "Ep done - 28760.\n",
      "Ep done - 28770.\n",
      "Ep done - 28780.\n",
      "Ep done - 28790.\n",
      "Ep done - 28800.\n",
      "Eval num_timesteps=2880000, episode_reward=-0.50 +/- 0.87\n",
      "Episode length: 30.25 +/- 0.43\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | -0.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2880000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8641711 |\n",
      "|    clip_fraction        | 0.0829    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000377 |\n",
      "|    explained_variance   | -0.35     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0736    |\n",
      "|    n_updates            | 23910     |\n",
      "|    policy_gradient_loss | -0.0105   |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Ep done - 103350.\n",
      "Ep done - 103360.\n",
      "Ep done - 103370.\n",
      "Ep done - 103380.\n",
      "Ep done - 103390.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 272      |\n",
      "|    iterations      | 469      |\n",
      "|    time_elapsed    | 10573    |\n",
      "|    total_timesteps | 2881536  |\n",
      "---------------------------------\n",
      "Ep done - 103400.\n",
      "Ep done - 103410.\n",
      "Ep done - 103420.\n",
      "Ep done - 103430.\n",
      "Ep done - 103440.\n",
      "Ep done - 103450.\n",
      "Ep done - 103460.\n",
      "Ep done - 103470.\n",
      "Ep done - 103480.\n",
      "Ep done - 103490.\n",
      "Ep done - 103500.\n",
      "Ep done - 103510.\n",
      "Ep done - 103520.\n",
      "Ep done - 103530.\n",
      "Ep done - 103540.\n",
      "Ep done - 103550.\n",
      "Ep done - 103560.\n",
      "Ep done - 103570.\n",
      "Ep done - 103580.\n",
      "Ep done - 103590.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | -0.56     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 272       |\n",
      "|    iterations           | 470       |\n",
      "|    time_elapsed         | 10587     |\n",
      "|    total_timesteps      | 2887680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2710065 |\n",
      "|    clip_fraction        | 0.0571    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000186 |\n",
      "|    explained_variance   | -0.0712   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.045     |\n",
      "|    n_updates            | 23920     |\n",
      "|    policy_gradient_loss | 0.0317    |\n",
      "|    value_loss           | 0.102     |\n",
      "---------------------------------------\n",
      "Ep done - 103600.\n",
      "Ep done - 103610.\n",
      "Ep done - 103620.\n",
      "Ep done - 103630.\n",
      "Ep done - 103640.\n",
      "Ep done - 103650.\n",
      "Ep done - 103660.\n",
      "Ep done - 103670.\n",
      "Ep done - 28810.\n",
      "Ep done - 28820.\n",
      "Ep done - 28830.\n",
      "Ep done - 28840.\n",
      "Ep done - 28850.\n",
      "Ep done - 28860.\n",
      "Ep done - 28870.\n",
      "Ep done - 28880.\n",
      "Ep done - 28890.\n",
      "Ep done - 28900.\n",
      "Eval num_timesteps=2890000, episode_reward=-0.64 +/- 0.77\n",
      "Episode length: 30.98 +/- 0.14\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 31        |\n",
      "|    mean_reward          | -0.64     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2890000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0330409 |\n",
      "|    clip_fraction        | 0.0461    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000328 |\n",
      "|    explained_variance   | 0.457     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0521    |\n",
      "|    n_updates            | 23930     |\n",
      "|    policy_gradient_loss | -0.00906  |\n",
      "|    value_loss           | 0.166     |\n",
      "---------------------------------------\n",
      "Ep done - 103680.\n",
      "Ep done - 103690.\n",
      "Ep done - 103700.\n",
      "Ep done - 103710.\n",
      "Ep done - 103720.\n",
      "Ep done - 103730.\n",
      "Ep done - 103740.\n",
      "Ep done - 103750.\n",
      "Ep done - 103760.\n",
      "Ep done - 103770.\n",
      "Ep done - 103780.\n",
      "Ep done - 103790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31       |\n",
      "|    ep_rew_mean     | -0.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 272      |\n",
      "|    iterations      | 471      |\n",
      "|    time_elapsed    | 10606    |\n",
      "|    total_timesteps | 2893824  |\n",
      "---------------------------------\n",
      "Ep done - 103800.\n",
      "Ep done - 103810.\n",
      "Ep done - 103820.\n",
      "Ep done - 103830.\n",
      "Ep done - 103840.\n",
      "Ep done - 103850.\n",
      "Ep done - 103860.\n",
      "Ep done - 103870.\n",
      "Ep done - 103880.\n",
      "Ep done - 103890.\n",
      "Ep done - 103900.\n",
      "Ep done - 103910.\n",
      "Ep done - 103920.\n",
      "Ep done - 103930.\n",
      "Ep done - 103940.\n",
      "Ep done - 103950.\n",
      "Ep done - 103960.\n",
      "Ep done - 103970.\n",
      "Ep done - 103980.\n",
      "Ep done - 103990.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 31        |\n",
      "|    ep_rew_mean          | -0.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 273       |\n",
      "|    iterations           | 472       |\n",
      "|    time_elapsed         | 10620     |\n",
      "|    total_timesteps      | 2899968   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1143233 |\n",
      "|    clip_fraction        | 0.0493    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000277 |\n",
      "|    explained_variance   | -0.274    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0667    |\n",
      "|    n_updates            | 23940     |\n",
      "|    policy_gradient_loss | -0.000428 |\n",
      "|    value_loss           | 0.12      |\n",
      "---------------------------------------\n",
      "Ep done - 28910.\n",
      "Ep done - 28920.\n",
      "Ep done - 28930.\n",
      "Ep done - 28940.\n",
      "Ep done - 28950.\n",
      "Ep done - 28960.\n",
      "Ep done - 28970.\n",
      "Ep done - 28980.\n",
      "Ep done - 28990.\n",
      "Ep done - 29000.\n",
      "Eval num_timesteps=2900000, episode_reward=-0.66 +/- 0.75\n",
      "Episode length: 31.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 31           |\n",
      "|    mean_reward          | -0.66        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2900000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005300282 |\n",
      "|    clip_fraction        | 0.000114     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.27e-05    |\n",
      "|    explained_variance   | 0.574        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0638       |\n",
      "|    n_updates            | 23950        |\n",
      "|    policy_gradient_loss | -1.27e-05    |\n",
      "|    value_loss           | 0.11         |\n",
      "------------------------------------------\n",
      "Ep done - 104000.\n",
      "Ep done - 104010.\n",
      "Ep done - 104020.\n",
      "Ep done - 104030.\n",
      "Ep done - 104040.\n",
      "Ep done - 104050.\n",
      "Ep done - 104060.\n",
      "Ep done - 104070.\n",
      "Ep done - 104080.\n",
      "Ep done - 104090.\n",
      "Ep done - 104100.\n",
      "Ep done - 104110.\n",
      "Ep done - 104120.\n",
      "Ep done - 104130.\n",
      "Ep done - 104140.\n",
      "Ep done - 104150.\n",
      "Ep done - 104160.\n",
      "Ep done - 104170.\n",
      "Ep done - 104180.\n",
      "Ep done - 104190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31       |\n",
      "|    ep_rew_mean     | -0.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 273      |\n",
      "|    iterations      | 473      |\n",
      "|    time_elapsed    | 10639    |\n",
      "|    total_timesteps | 2906112  |\n",
      "---------------------------------\n",
      "Ep done - 104200.\n",
      "Ep done - 104210.\n",
      "Ep done - 104220.\n",
      "Ep done - 104230.\n",
      "Ep done - 104240.\n",
      "Ep done - 104250.\n",
      "Ep done - 104260.\n",
      "Ep done - 104270.\n",
      "Ep done - 104280.\n",
      "Ep done - 104290.\n",
      "Ep done - 104300.\n",
      "Ep done - 104310.\n",
      "Ep done - 104320.\n",
      "Ep done - 29010.\n",
      "Ep done - 29020.\n",
      "Ep done - 29030.\n",
      "Ep done - 29040.\n",
      "Ep done - 29050.\n",
      "Ep done - 29060.\n",
      "Ep done - 29070.\n",
      "Ep done - 29080.\n",
      "Ep done - 29090.\n",
      "Ep done - 29100.\n",
      "Eval num_timesteps=2910000, episode_reward=-0.72 +/- 0.69\n",
      "Episode length: 30.11 +/- 0.34\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.72     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2910000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.6549702 |\n",
      "|    clip_fraction        | 0.0282    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000782 |\n",
      "|    explained_variance   | 0.603     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0654    |\n",
      "|    n_updates            | 23960     |\n",
      "|    policy_gradient_loss | -0.00093  |\n",
      "|    value_loss           | 0.143     |\n",
      "---------------------------------------\n",
      "Ep done - 104330.\n",
      "Ep done - 104340.\n",
      "Ep done - 104350.\n",
      "Ep done - 104360.\n",
      "Ep done - 104370.\n",
      "Ep done - 104380.\n",
      "Ep done - 104390.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 273      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 10658    |\n",
      "|    total_timesteps | 2912256  |\n",
      "---------------------------------\n",
      "Ep done - 104400.\n",
      "Ep done - 104410.\n",
      "Ep done - 104420.\n",
      "Ep done - 104430.\n",
      "Ep done - 104440.\n",
      "Ep done - 104450.\n",
      "Ep done - 104460.\n",
      "Ep done - 104470.\n",
      "Ep done - 104480.\n",
      "Ep done - 104490.\n",
      "Ep done - 104500.\n",
      "Ep done - 104510.\n",
      "Ep done - 104520.\n",
      "Ep done - 104530.\n",
      "Ep done - 104540.\n",
      "Ep done - 104550.\n",
      "Ep done - 104560.\n",
      "Ep done - 104570.\n",
      "Ep done - 104580.\n",
      "Ep done - 104590.\n",
      "Ep done - 104600.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | -0.94    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 273      |\n",
      "|    iterations           | 475      |\n",
      "|    time_elapsed         | 10673    |\n",
      "|    total_timesteps      | 2918400  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.4662   |\n",
      "|    clip_fraction        | 0.131    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00159 |\n",
      "|    explained_variance   | -2.72    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0316   |\n",
      "|    n_updates            | 23970    |\n",
      "|    policy_gradient_loss | -0.0065  |\n",
      "|    value_loss           | 0.112    |\n",
      "--------------------------------------\n",
      "Ep done - 104610.\n",
      "Ep done - 104620.\n",
      "Ep done - 104630.\n",
      "Ep done - 104640.\n",
      "Ep done - 104650.\n",
      "Ep done - 29110.\n",
      "Ep done - 29120.\n",
      "Ep done - 29130.\n",
      "Ep done - 29140.\n",
      "Ep done - 29150.\n",
      "Ep done - 29160.\n",
      "Ep done - 29170.\n",
      "Ep done - 29180.\n",
      "Ep done - 29190.\n",
      "Ep done - 29200.\n",
      "Eval num_timesteps=2920000, episode_reward=-0.19 +/- 0.39\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.19     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2920000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5163911 |\n",
      "|    clip_fraction        | 0.0138    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000335 |\n",
      "|    explained_variance   | -0.659    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00211   |\n",
      "|    n_updates            | 23980     |\n",
      "|    policy_gradient_loss | -0.00146  |\n",
      "|    value_loss           | 0.0217    |\n",
      "---------------------------------------\n",
      "Ep done - 104660.\n",
      "Ep done - 104670.\n",
      "Ep done - 104680.\n",
      "Ep done - 104690.\n",
      "Ep done - 104700.\n",
      "Ep done - 104710.\n",
      "Ep done - 104720.\n",
      "Ep done - 104730.\n",
      "Ep done - 104740.\n",
      "Ep done - 104750.\n",
      "Ep done - 104760.\n",
      "Ep done - 104770.\n",
      "Ep done - 104780.\n",
      "Ep done - 104790.\n",
      "Ep done - 104800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 273      |\n",
      "|    iterations      | 476      |\n",
      "|    time_elapsed    | 10691    |\n",
      "|    total_timesteps | 2924544  |\n",
      "---------------------------------\n",
      "Ep done - 104810.\n",
      "Ep done - 104820.\n",
      "Ep done - 104830.\n",
      "Ep done - 104840.\n",
      "Ep done - 104850.\n",
      "Ep done - 104860.\n",
      "Ep done - 104870.\n",
      "Ep done - 104880.\n",
      "Ep done - 104890.\n",
      "Ep done - 104900.\n",
      "Ep done - 104910.\n",
      "Ep done - 104920.\n",
      "Ep done - 104930.\n",
      "Ep done - 104940.\n",
      "Ep done - 104950.\n",
      "Ep done - 104960.\n",
      "Ep done - 104970.\n",
      "Ep done - 104980.\n",
      "Ep done - 29210.\n",
      "Ep done - 29220.\n",
      "Ep done - 29230.\n",
      "Ep done - 29240.\n",
      "Ep done - 29250.\n",
      "Ep done - 29260.\n",
      "Ep done - 29270.\n",
      "Ep done - 29280.\n",
      "Ep done - 29290.\n",
      "Ep done - 29300.\n",
      "Eval num_timesteps=2930000, episode_reward=0.50 +/- 0.87\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.5       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2930000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 6.3464847 |\n",
      "|    clip_fraction        | 0.0755    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000511 |\n",
      "|    explained_variance   | -0.929    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0189   |\n",
      "|    n_updates            | 23990     |\n",
      "|    policy_gradient_loss | -0.01     |\n",
      "|    value_loss           | 0.0331    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.5\n",
      "SELFPLAY: new best model, bumping up generation to 95\n",
      "Ep done - 104990.\n",
      "Ep done - 105000.\n",
      "Ep done - 105010.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 273      |\n",
      "|    iterations      | 477      |\n",
      "|    time_elapsed    | 10710    |\n",
      "|    total_timesteps | 2930688  |\n",
      "---------------------------------\n",
      "Ep done - 105020.\n",
      "Ep done - 105030.\n",
      "Ep done - 105040.\n",
      "Ep done - 105050.\n",
      "Ep done - 105060.\n",
      "Ep done - 105070.\n",
      "Ep done - 105080.\n",
      "Ep done - 105090.\n",
      "Ep done - 105100.\n",
      "Ep done - 105110.\n",
      "Ep done - 105120.\n",
      "Ep done - 105130.\n",
      "Ep done - 105140.\n",
      "Ep done - 105150.\n",
      "Ep done - 105160.\n",
      "Ep done - 105170.\n",
      "Ep done - 105180.\n",
      "Ep done - 105190.\n",
      "Ep done - 105200.\n",
      "Ep done - 105210.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.46      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 273        |\n",
      "|    iterations           | 478        |\n",
      "|    time_elapsed         | 10724      |\n",
      "|    total_timesteps      | 2936832    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39035964 |\n",
      "|    clip_fraction        | 0.0363     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00129   |\n",
      "|    explained_variance   | 0.12       |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0976     |\n",
      "|    n_updates            | 24000      |\n",
      "|    policy_gradient_loss | 0.00173    |\n",
      "|    value_loss           | 0.202      |\n",
      "----------------------------------------\n",
      "Ep done - 105220.\n",
      "Ep done - 105230.\n",
      "Ep done - 105240.\n",
      "Ep done - 105250.\n",
      "Ep done - 105260.\n",
      "Ep done - 105270.\n",
      "Ep done - 105280.\n",
      "Ep done - 105290.\n",
      "Ep done - 105300.\n",
      "Ep done - 105310.\n",
      "Ep done - 105320.\n",
      "Ep done - 29310.\n",
      "Ep done - 29320.\n",
      "Ep done - 29330.\n",
      "Ep done - 29340.\n",
      "Ep done - 29350.\n",
      "Ep done - 29360.\n",
      "Ep done - 29370.\n",
      "Ep done - 29380.\n",
      "Ep done - 29390.\n",
      "Ep done - 29400.\n",
      "Eval num_timesteps=2940000, episode_reward=0.68 +/- 0.73\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.68      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2940000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1785036 |\n",
      "|    clip_fraction        | 0.0493    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00128  |\n",
      "|    explained_variance   | 0.119     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0823    |\n",
      "|    n_updates            | 24010     |\n",
      "|    policy_gradient_loss | 0.00615   |\n",
      "|    value_loss           | 0.167     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 96\n",
      "Ep done - 105330.\n",
      "Ep done - 105340.\n",
      "Ep done - 105350.\n",
      "Ep done - 105360.\n",
      "Ep done - 105370.\n",
      "Ep done - 105380.\n",
      "Ep done - 105390.\n",
      "Ep done - 105400.\n",
      "Ep done - 105410.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 273      |\n",
      "|    iterations      | 479      |\n",
      "|    time_elapsed    | 10744    |\n",
      "|    total_timesteps | 2942976  |\n",
      "---------------------------------\n",
      "Ep done - 105420.\n",
      "Ep done - 105430.\n",
      "Ep done - 105440.\n",
      "Ep done - 105450.\n",
      "Ep done - 105460.\n",
      "Ep done - 105470.\n",
      "Ep done - 105480.\n",
      "Ep done - 105490.\n",
      "Ep done - 105500.\n",
      "Ep done - 105510.\n",
      "Ep done - 105520.\n",
      "Ep done - 105530.\n",
      "Ep done - 105540.\n",
      "Ep done - 105550.\n",
      "Ep done - 105560.\n",
      "Ep done - 105570.\n",
      "Ep done - 105580.\n",
      "Ep done - 105590.\n",
      "Ep done - 105600.\n",
      "Ep done - 105610.\n",
      "Ep done - 105620.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.96      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 274       |\n",
      "|    iterations           | 480       |\n",
      "|    time_elapsed         | 10759     |\n",
      "|    total_timesteps      | 2949120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.4223921 |\n",
      "|    clip_fraction        | 0.119     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0026   |\n",
      "|    explained_variance   | -1.02     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0455    |\n",
      "|    n_updates            | 24020     |\n",
      "|    policy_gradient_loss | -0.000437 |\n",
      "|    value_loss           | 0.16      |\n",
      "---------------------------------------\n",
      "Ep done - 105630.\n",
      "Ep done - 105640.\n",
      "Ep done - 105650.\n",
      "Ep done - 29410.\n",
      "Ep done - 29420.\n",
      "Ep done - 29430.\n",
      "Ep done - 29440.\n",
      "Ep done - 29450.\n",
      "Ep done - 29460.\n",
      "Ep done - 29470.\n",
      "Ep done - 29480.\n",
      "Ep done - 29490.\n",
      "Ep done - 29500.\n",
      "Eval num_timesteps=2950000, episode_reward=0.78 +/- 0.63\n",
      "Episode length: 30.06 +/- 0.24\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.78      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2950000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9212509 |\n",
      "|    clip_fraction        | 0.0703    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00112  |\n",
      "|    explained_variance   | -0.476    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000539  |\n",
      "|    n_updates            | 24030     |\n",
      "|    policy_gradient_loss | -0.00182  |\n",
      "|    value_loss           | 0.0359    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.78\n",
      "SELFPLAY: new best model, bumping up generation to 97\n",
      "Ep done - 105660.\n",
      "Ep done - 105670.\n",
      "Ep done - 105680.\n",
      "Ep done - 105690.\n",
      "Ep done - 105700.\n",
      "Ep done - 105710.\n",
      "Ep done - 105720.\n",
      "Ep done - 105730.\n",
      "Ep done - 105740.\n",
      "Ep done - 105750.\n",
      "Ep done - 105760.\n",
      "Ep done - 105770.\n",
      "Ep done - 105780.\n",
      "Ep done - 105790.\n",
      "Ep done - 105800.\n",
      "Ep done - 105810.\n",
      "Ep done - 105820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 274      |\n",
      "|    iterations      | 481      |\n",
      "|    time_elapsed    | 10778    |\n",
      "|    total_timesteps | 2955264  |\n",
      "---------------------------------\n",
      "Ep done - 105830.\n",
      "Ep done - 105840.\n",
      "Ep done - 105850.\n",
      "Ep done - 105860.\n",
      "Ep done - 105870.\n",
      "Ep done - 105880.\n",
      "Ep done - 105890.\n",
      "Ep done - 105900.\n",
      "Ep done - 105910.\n",
      "Ep done - 105920.\n",
      "Ep done - 105930.\n",
      "Ep done - 105940.\n",
      "Ep done - 105950.\n",
      "Ep done - 105960.\n",
      "Ep done - 105970.\n",
      "Ep done - 105980.\n",
      "Ep done - 29510.\n",
      "Ep done - 29520.\n",
      "Ep done - 29530.\n",
      "Ep done - 29540.\n",
      "Ep done - 29550.\n",
      "Ep done - 29560.\n",
      "Ep done - 29570.\n",
      "Ep done - 29580.\n",
      "Ep done - 29590.\n",
      "Ep done - 29600.\n",
      "Eval num_timesteps=2960000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.04 +/- 0.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2960000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3423722 |\n",
      "|    clip_fraction        | 0.0328    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000652 |\n",
      "|    explained_variance   | -0.0519   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0529    |\n",
      "|    n_updates            | 24040     |\n",
      "|    policy_gradient_loss | -0.000801 |\n",
      "|    value_loss           | 0.182     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 98\n",
      "Ep done - 105990.\n",
      "Ep done - 106000.\n",
      "Ep done - 106010.\n",
      "Ep done - 106020.\n",
      "Ep done - 106030.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.91     |\n",
      "| time/              |          |\n",
      "|    fps             | 274      |\n",
      "|    iterations      | 482      |\n",
      "|    time_elapsed    | 10797    |\n",
      "|    total_timesteps | 2961408  |\n",
      "---------------------------------\n",
      "Ep done - 106040.\n",
      "Ep done - 106050.\n",
      "Ep done - 106060.\n",
      "Ep done - 106070.\n",
      "Ep done - 106080.\n",
      "Ep done - 106090.\n",
      "Ep done - 106100.\n",
      "Ep done - 106110.\n",
      "Ep done - 106120.\n",
      "Ep done - 106130.\n",
      "Ep done - 106140.\n",
      "Ep done - 106150.\n",
      "Ep done - 106160.\n",
      "Ep done - 106170.\n",
      "Ep done - 106180.\n",
      "Ep done - 106190.\n",
      "Ep done - 106200.\n",
      "Ep done - 106210.\n",
      "Ep done - 106220.\n",
      "Ep done - 106230.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.9      |\n",
      "|    ep_rew_mean          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 274       |\n",
      "|    iterations           | 483       |\n",
      "|    time_elapsed         | 10811     |\n",
      "|    total_timesteps      | 2967552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 7.2681155 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00152  |\n",
      "|    explained_variance   | -1.66     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0106    |\n",
      "|    n_updates            | 24050     |\n",
      "|    policy_gradient_loss | -0.0198   |\n",
      "|    value_loss           | 0.0452    |\n",
      "---------------------------------------\n",
      "Ep done - 106240.\n",
      "Ep done - 106250.\n",
      "Ep done - 106260.\n",
      "Ep done - 106270.\n",
      "Ep done - 106280.\n",
      "Ep done - 106290.\n",
      "Ep done - 106300.\n",
      "Ep done - 106310.\n",
      "Ep done - 106320.\n",
      "Ep done - 29610.\n",
      "Ep done - 29620.\n",
      "Ep done - 29630.\n",
      "Ep done - 29640.\n",
      "Ep done - 29650.\n",
      "Ep done - 29660.\n",
      "Ep done - 29670.\n",
      "Ep done - 29680.\n",
      "Ep done - 29690.\n",
      "Ep done - 29700.\n",
      "Eval num_timesteps=2970000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2970000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.004937  |\n",
      "|    clip_fraction        | 0.0765    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000843 |\n",
      "|    explained_variance   | -1.27     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0023   |\n",
      "|    n_updates            | 24060     |\n",
      "|    policy_gradient_loss | -0.0095   |\n",
      "|    value_loss           | 0.0448    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 99\n",
      "Ep done - 106330.\n",
      "Ep done - 106340.\n",
      "Ep done - 106350.\n",
      "Ep done - 106360.\n",
      "Ep done - 106370.\n",
      "Ep done - 106380.\n",
      "Ep done - 106390.\n",
      "Ep done - 106400.\n",
      "Ep done - 106410.\n",
      "Ep done - 106420.\n",
      "Ep done - 106430.\n",
      "Ep done - 106440.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 274      |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 10830    |\n",
      "|    total_timesteps | 2973696  |\n",
      "---------------------------------\n",
      "Ep done - 106450.\n",
      "Ep done - 106460.\n",
      "Ep done - 106470.\n",
      "Ep done - 106480.\n",
      "Ep done - 106490.\n",
      "Ep done - 106500.\n",
      "Ep done - 106510.\n",
      "Ep done - 106520.\n",
      "Ep done - 106530.\n",
      "Ep done - 106540.\n",
      "Ep done - 106550.\n",
      "Ep done - 106560.\n",
      "Ep done - 106570.\n",
      "Ep done - 106580.\n",
      "Ep done - 106590.\n",
      "Ep done - 106600.\n",
      "Ep done - 106610.\n",
      "Ep done - 106620.\n",
      "Ep done - 106630.\n",
      "Ep done - 106640.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.36      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 274       |\n",
      "|    iterations           | 485       |\n",
      "|    time_elapsed         | 10845     |\n",
      "|    total_timesteps      | 2979840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.4800942 |\n",
      "|    clip_fraction        | 0.108     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00355  |\n",
      "|    explained_variance   | -2.48     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0359    |\n",
      "|    n_updates            | 24070     |\n",
      "|    policy_gradient_loss | -0.00427  |\n",
      "|    value_loss           | 0.0704    |\n",
      "---------------------------------------\n",
      "Ep done - 106650.\n",
      "Ep done - 29710.\n",
      "Ep done - 29720.\n",
      "Ep done - 29730.\n",
      "Ep done - 29740.\n",
      "Ep done - 29750.\n",
      "Ep done - 29760.\n",
      "Ep done - 29770.\n",
      "Ep done - 29780.\n",
      "Ep done - 29790.\n",
      "Ep done - 29800.\n",
      "Eval num_timesteps=2980000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2980000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44997597 |\n",
      "|    clip_fraction        | 0.0511     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00159   |\n",
      "|    explained_variance   | 7.75e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.152      |\n",
      "|    n_updates            | 24080      |\n",
      "|    policy_gradient_loss | 0.0052     |\n",
      "|    value_loss           | 0.277      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 100\n",
      "Ep done - 106660.\n",
      "Ep done - 106670.\n",
      "Ep done - 106680.\n",
      "Ep done - 106690.\n",
      "Ep done - 106700.\n",
      "Ep done - 106710.\n",
      "Ep done - 106720.\n",
      "Ep done - 106730.\n",
      "Ep done - 106740.\n",
      "Ep done - 106750.\n",
      "Ep done - 106760.\n",
      "Ep done - 106770.\n",
      "Ep done - 106780.\n",
      "Ep done - 106790.\n",
      "Ep done - 106800.\n",
      "Ep done - 106810.\n",
      "Ep done - 106820.\n",
      "Ep done - 106830.\n",
      "Ep done - 106840.\n",
      "Ep done - 106850.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.96     |\n",
      "| time/              |          |\n",
      "|    fps             | 274      |\n",
      "|    iterations      | 486      |\n",
      "|    time_elapsed    | 10865    |\n",
      "|    total_timesteps | 2985984  |\n",
      "---------------------------------\n",
      "Ep done - 106860.\n",
      "Ep done - 106870.\n",
      "Ep done - 106880.\n",
      "Ep done - 106890.\n",
      "Ep done - 106900.\n",
      "Ep done - 106910.\n",
      "Ep done - 106920.\n",
      "Ep done - 106930.\n",
      "Ep done - 106940.\n",
      "Ep done - 106950.\n",
      "Ep done - 106960.\n",
      "Ep done - 106970.\n",
      "Ep done - 106980.\n",
      "Ep done - 29810.\n",
      "Ep done - 29820.\n",
      "Ep done - 29830.\n",
      "Ep done - 29840.\n",
      "Ep done - 29850.\n",
      "Ep done - 29860.\n",
      "Ep done - 29870.\n",
      "Ep done - 29880.\n",
      "Ep done - 29890.\n",
      "Ep done - 29900.\n",
      "Eval num_timesteps=2990000, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.94      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2990000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2265445 |\n",
      "|    clip_fraction        | 0.0343    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000742 |\n",
      "|    explained_variance   | -5.33e-05 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0113    |\n",
      "|    n_updates            | 24090     |\n",
      "|    policy_gradient_loss | -0.00244  |\n",
      "|    value_loss           | 0.0568    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.94\n",
      "SELFPLAY: new best model, bumping up generation to 101\n",
      "Ep done - 106990.\n",
      "Ep done - 107000.\n",
      "Ep done - 107010.\n",
      "Ep done - 107020.\n",
      "Ep done - 107030.\n",
      "Ep done - 107040.\n",
      "Ep done - 107050.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.92     |\n",
      "| time/              |          |\n",
      "|    fps             | 274      |\n",
      "|    iterations      | 487      |\n",
      "|    time_elapsed    | 10883    |\n",
      "|    total_timesteps | 2992128  |\n",
      "---------------------------------\n",
      "Ep done - 107060.\n",
      "Ep done - 107070.\n",
      "Ep done - 107080.\n",
      "Ep done - 107090.\n",
      "Ep done - 107100.\n",
      "Ep done - 107110.\n",
      "Ep done - 107120.\n",
      "Ep done - 107130.\n",
      "Ep done - 107140.\n",
      "Ep done - 107150.\n",
      "Ep done - 107160.\n",
      "Ep done - 107170.\n",
      "Ep done - 107180.\n",
      "Ep done - 107190.\n",
      "Ep done - 107200.\n",
      "Ep done - 107210.\n",
      "Ep done - 107220.\n",
      "Ep done - 107230.\n",
      "Ep done - 107240.\n",
      "Ep done - 107250.\n",
      "Ep done - 107260.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | 0.96     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 275      |\n",
      "|    iterations           | 488      |\n",
      "|    time_elapsed         | 10898    |\n",
      "|    total_timesteps      | 2998272  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 8.208423 |\n",
      "|    clip_fraction        | 0.188    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00149 |\n",
      "|    explained_variance   | -0.00461 |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.00636  |\n",
      "|    n_updates            | 24100    |\n",
      "|    policy_gradient_loss | -0.0221  |\n",
      "|    value_loss           | 0.0572   |\n",
      "--------------------------------------\n",
      "Ep done - 107270.\n",
      "Ep done - 107280.\n",
      "Ep done - 107290.\n",
      "Ep done - 107300.\n",
      "Ep done - 107310.\n",
      "Ep done - 29910.\n",
      "Ep done - 29920.\n",
      "Ep done - 29930.\n",
      "Ep done - 29940.\n",
      "Ep done - 29950.\n",
      "Ep done - 29960.\n",
      "Ep done - 29970.\n",
      "Ep done - 29980.\n",
      "Ep done - 29990.\n",
      "Ep done - 30000.\n",
      "Eval num_timesteps=3000000, episode_reward=-0.88 +/- 0.38\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.88     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3000000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.534196  |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00204  |\n",
      "|    explained_variance   | -2.86e-06 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0419    |\n",
      "|    n_updates            | 24110     |\n",
      "|    policy_gradient_loss | 0.0772    |\n",
      "|    value_loss           | 0.0291    |\n",
      "---------------------------------------\n",
      "Ep done - 107320.\n",
      "Ep done - 107330.\n",
      "Ep done - 107340.\n",
      "Ep done - 107350.\n",
      "Ep done - 107360.\n",
      "Ep done - 107370.\n",
      "Ep done - 107380.\n",
      "Ep done - 107390.\n",
      "Ep done - 107400.\n",
      "Ep done - 107410.\n",
      "Ep done - 107420.\n",
      "Ep done - 107430.\n",
      "Ep done - 107440.\n",
      "Ep done - 107450.\n",
      "Ep done - 107460.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.95    |\n",
      "| time/              |          |\n",
      "|    fps             | 275      |\n",
      "|    iterations      | 489      |\n",
      "|    time_elapsed    | 10917    |\n",
      "|    total_timesteps | 3004416  |\n",
      "---------------------------------\n",
      "Ep done - 107470.\n",
      "Ep done - 107480.\n",
      "Ep done - 107490.\n",
      "Ep done - 107500.\n",
      "Ep done - 107510.\n",
      "Ep done - 107520.\n",
      "Ep done - 107530.\n",
      "Ep done - 107540.\n",
      "Ep done - 107550.\n",
      "Ep done - 107560.\n",
      "Ep done - 107570.\n",
      "Ep done - 107580.\n",
      "Ep done - 107590.\n",
      "Ep done - 107600.\n",
      "Ep done - 107610.\n",
      "Ep done - 107620.\n",
      "Ep done - 107630.\n",
      "Ep done - 107640.\n",
      "Ep done - 107650.\n",
      "Ep done - 30010.\n",
      "Ep done - 30020.\n",
      "Ep done - 30030.\n",
      "Ep done - 30040.\n",
      "Ep done - 30050.\n",
      "Ep done - 30060.\n",
      "Ep done - 30070.\n",
      "Ep done - 30080.\n",
      "Ep done - 30090.\n",
      "Ep done - 30100.\n",
      "Eval num_timesteps=3010000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3010000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.0356338 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00254  |\n",
      "|    explained_variance   | 1.91e-05  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0777    |\n",
      "|    n_updates            | 24120     |\n",
      "|    policy_gradient_loss | 0.134     |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "Ep done - 107660.\n",
      "Ep done - 107670.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 275      |\n",
      "|    iterations      | 490      |\n",
      "|    time_elapsed    | 10936    |\n",
      "|    total_timesteps | 3010560  |\n",
      "---------------------------------\n",
      "Ep done - 107680.\n",
      "Ep done - 107690.\n",
      "Ep done - 107700.\n",
      "Ep done - 107710.\n",
      "Ep done - 107720.\n",
      "Ep done - 107730.\n",
      "Ep done - 107740.\n",
      "Ep done - 107750.\n",
      "Ep done - 107760.\n",
      "Ep done - 107770.\n",
      "Ep done - 107780.\n",
      "Ep done - 107790.\n",
      "Ep done - 107800.\n",
      "Ep done - 107810.\n",
      "Ep done - 107820.\n",
      "Ep done - 107830.\n",
      "Ep done - 107840.\n",
      "Ep done - 107850.\n",
      "Ep done - 107860.\n",
      "Ep done - 107870.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | -1       |\n",
      "| time/                   |          |\n",
      "|    fps                  | 275      |\n",
      "|    iterations           | 491      |\n",
      "|    time_elapsed         | 10950    |\n",
      "|    total_timesteps      | 3016704  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 4.495869 |\n",
      "|    clip_fraction        | 0.118    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00186 |\n",
      "|    explained_variance   | 6.56e-07 |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0163   |\n",
      "|    n_updates            | 24130    |\n",
      "|    policy_gradient_loss | -0.00885 |\n",
      "|    value_loss           | 0.0569   |\n",
      "--------------------------------------\n",
      "Ep done - 107880.\n",
      "Ep done - 107890.\n",
      "Ep done - 107900.\n",
      "Ep done - 107910.\n",
      "Ep done - 107920.\n",
      "Ep done - 107930.\n",
      "Ep done - 107940.\n",
      "Ep done - 107950.\n",
      "Ep done - 107960.\n",
      "Ep done - 107970.\n",
      "Ep done - 107980.\n",
      "Ep done - 30110.\n",
      "Ep done - 30120.\n",
      "Ep done - 30130.\n",
      "Ep done - 30140.\n",
      "Ep done - 30150.\n",
      "Ep done - 30160.\n",
      "Ep done - 30170.\n",
      "Ep done - 30180.\n",
      "Ep done - 30190.\n",
      "Ep done - 30200.\n",
      "Eval num_timesteps=3020000, episode_reward=0.92 +/- 0.39\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.92      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3020000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.5691252 |\n",
      "|    clip_fraction        | 0.0594    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00187  |\n",
      "|    explained_variance   | 1.31e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0288    |\n",
      "|    n_updates            | 24140     |\n",
      "|    policy_gradient_loss | -0.00766  |\n",
      "|    value_loss           | 0.0289    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.92\n",
      "SELFPLAY: new best model, bumping up generation to 102\n",
      "Ep done - 107990.\n",
      "Ep done - 108000.\n",
      "Ep done - 108010.\n",
      "Ep done - 108020.\n",
      "Ep done - 108030.\n",
      "Ep done - 108040.\n",
      "Ep done - 108050.\n",
      "Ep done - 108060.\n",
      "Ep done - 108070.\n",
      "Ep done - 108080.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.12     |\n",
      "| time/              |          |\n",
      "|    fps             | 275      |\n",
      "|    iterations      | 492      |\n",
      "|    time_elapsed    | 10969    |\n",
      "|    total_timesteps | 3022848  |\n",
      "---------------------------------\n",
      "Ep done - 108090.\n",
      "Ep done - 108100.\n",
      "Ep done - 108110.\n",
      "Ep done - 108120.\n",
      "Ep done - 108130.\n",
      "Ep done - 108140.\n",
      "Ep done - 108150.\n",
      "Ep done - 108160.\n",
      "Ep done - 108170.\n",
      "Ep done - 108180.\n",
      "Ep done - 108190.\n",
      "Ep done - 108200.\n",
      "Ep done - 108210.\n",
      "Ep done - 108220.\n",
      "Ep done - 108230.\n",
      "Ep done - 108240.\n",
      "Ep done - 108250.\n",
      "Ep done - 108260.\n",
      "Ep done - 108270.\n",
      "Ep done - 108280.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 493         |\n",
      "|    time_elapsed         | 10984       |\n",
      "|    total_timesteps      | 3028992     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119033515 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.00573    |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.172       |\n",
      "|    n_updates            | 24150       |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    value_loss           | 0.329       |\n",
      "-----------------------------------------\n",
      "Ep done - 108290.\n",
      "Ep done - 108300.\n",
      "Ep done - 108310.\n",
      "Ep done - 30210.\n",
      "Ep done - 30220.\n",
      "Ep done - 30230.\n",
      "Ep done - 30240.\n",
      "Ep done - 30250.\n",
      "Ep done - 30260.\n",
      "Ep done - 30270.\n",
      "Ep done - 30280.\n",
      "Ep done - 30290.\n",
      "Ep done - 30300.\n",
      "Eval num_timesteps=3030000, episode_reward=0.96 +/- 0.28\n",
      "Episode length: 30.53 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.5       |\n",
      "|    mean_reward          | 0.96       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3030000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.28255215 |\n",
      "|    clip_fraction        | 0.0305     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000247  |\n",
      "|    explained_variance   | 2.38e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.164      |\n",
      "|    n_updates            | 24160      |\n",
      "|    policy_gradient_loss | -0.00744   |\n",
      "|    value_loss           | 0.29       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.96\n",
      "SELFPLAY: new best model, bumping up generation to 103\n",
      "Ep done - 108320.\n",
      "Ep done - 108330.\n",
      "Ep done - 108340.\n",
      "Ep done - 108350.\n",
      "Ep done - 108360.\n",
      "Ep done - 108370.\n",
      "Ep done - 108380.\n",
      "Ep done - 108390.\n",
      "Ep done - 108400.\n",
      "Ep done - 108410.\n",
      "Ep done - 108420.\n",
      "Ep done - 108430.\n",
      "Ep done - 108440.\n",
      "Ep done - 108450.\n",
      "Ep done - 108460.\n",
      "Ep done - 108470.\n",
      "Ep done - 108480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.74     |\n",
      "| time/              |          |\n",
      "|    fps             | 275      |\n",
      "|    iterations      | 494      |\n",
      "|    time_elapsed    | 11003    |\n",
      "|    total_timesteps | 3035136  |\n",
      "---------------------------------\n",
      "Ep done - 108490.\n",
      "Ep done - 108500.\n",
      "Ep done - 108510.\n",
      "Ep done - 108520.\n",
      "Ep done - 108530.\n",
      "Ep done - 108540.\n",
      "Ep done - 108550.\n",
      "Ep done - 108560.\n",
      "Ep done - 108570.\n",
      "Ep done - 108580.\n",
      "Ep done - 108590.\n",
      "Ep done - 108600.\n",
      "Ep done - 108610.\n",
      "Ep done - 108620.\n",
      "Ep done - 108630.\n",
      "Ep done - 108640.\n",
      "Ep done - 30310.\n",
      "Ep done - 30320.\n",
      "Ep done - 30330.\n",
      "Ep done - 30340.\n",
      "Ep done - 30350.\n",
      "Ep done - 30360.\n",
      "Ep done - 30370.\n",
      "Ep done - 30380.\n",
      "Ep done - 30390.\n",
      "Ep done - 30400.\n",
      "Eval num_timesteps=3040000, episode_reward=0.50 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.5       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3040000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4356285 |\n",
      "|    clip_fraction        | 0.0276    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000416 |\n",
      "|    explained_variance   | -3.58e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 3.08      |\n",
      "|    n_updates            | 24170     |\n",
      "|    policy_gradient_loss | 0.0286    |\n",
      "|    value_loss           | 0.0842    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.5\n",
      "SELFPLAY: new best model, bumping up generation to 104\n",
      "Ep done - 108650.\n",
      "Ep done - 108660.\n",
      "Ep done - 108670.\n",
      "Ep done - 108680.\n",
      "Ep done - 108690.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 275      |\n",
      "|    iterations      | 495      |\n",
      "|    time_elapsed    | 11022    |\n",
      "|    total_timesteps | 3041280  |\n",
      "---------------------------------\n",
      "Ep done - 108700.\n",
      "Ep done - 108710.\n",
      "Ep done - 108720.\n",
      "Ep done - 108730.\n",
      "Ep done - 108740.\n",
      "Ep done - 108750.\n",
      "Ep done - 108760.\n",
      "Ep done - 108770.\n",
      "Ep done - 108780.\n",
      "Ep done - 108790.\n",
      "Ep done - 108800.\n",
      "Ep done - 108810.\n",
      "Ep done - 108820.\n",
      "Ep done - 108830.\n",
      "Ep done - 108840.\n",
      "Ep done - 108850.\n",
      "Ep done - 108860.\n",
      "Ep done - 108870.\n",
      "Ep done - 108880.\n",
      "Ep done - 108890.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.2        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 496        |\n",
      "|    time_elapsed         | 11036      |\n",
      "|    total_timesteps      | 3047424    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46713996 |\n",
      "|    clip_fraction        | 0.0298     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000775  |\n",
      "|    explained_variance   | 1.79e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.089      |\n",
      "|    n_updates            | 24180      |\n",
      "|    policy_gradient_loss | -0.00673   |\n",
      "|    value_loss           | 0.255      |\n",
      "----------------------------------------\n",
      "Ep done - 108900.\n",
      "Ep done - 108910.\n",
      "Ep done - 108920.\n",
      "Ep done - 108930.\n",
      "Ep done - 108940.\n",
      "Ep done - 108950.\n",
      "Ep done - 108960.\n",
      "Ep done - 108970.\n",
      "Ep done - 108980.\n",
      "Ep done - 30410.\n",
      "Ep done - 30420.\n",
      "Ep done - 30430.\n",
      "Ep done - 30440.\n",
      "Ep done - 30450.\n",
      "Ep done - 30460.\n",
      "Ep done - 30470.\n",
      "Ep done - 30480.\n",
      "Ep done - 30490.\n",
      "Ep done - 30500.\n",
      "Eval num_timesteps=3050000, episode_reward=0.46 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.46       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3050000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41818514 |\n",
      "|    clip_fraction        | 0.0222     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000666  |\n",
      "|    explained_variance   | 4.12e-05   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.1        |\n",
      "|    n_updates            | 24190      |\n",
      "|    policy_gradient_loss | -0.00463   |\n",
      "|    value_loss           | 0.266      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.46\n",
      "SELFPLAY: new best model, bumping up generation to 105\n",
      "Ep done - 108990.\n",
      "Ep done - 109000.\n",
      "Ep done - 109010.\n",
      "Ep done - 109020.\n",
      "Ep done - 109030.\n",
      "Ep done - 109040.\n",
      "Ep done - 109050.\n",
      "Ep done - 109060.\n",
      "Ep done - 109070.\n",
      "Ep done - 109080.\n",
      "Ep done - 109090.\n",
      "Ep done - 109100.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 276      |\n",
      "|    iterations      | 497      |\n",
      "|    time_elapsed    | 11055    |\n",
      "|    total_timesteps | 3053568  |\n",
      "---------------------------------\n",
      "Ep done - 109110.\n",
      "Ep done - 109120.\n",
      "Ep done - 109130.\n",
      "Ep done - 109140.\n",
      "Ep done - 109150.\n",
      "Ep done - 109160.\n",
      "Ep done - 109170.\n",
      "Ep done - 109180.\n",
      "Ep done - 109190.\n",
      "Ep done - 109200.\n",
      "Ep done - 109210.\n",
      "Ep done - 109220.\n",
      "Ep done - 109230.\n",
      "Ep done - 109240.\n",
      "Ep done - 109250.\n",
      "Ep done - 109260.\n",
      "Ep done - 109270.\n",
      "Ep done - 109280.\n",
      "Ep done - 109290.\n",
      "Ep done - 109300.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 498        |\n",
      "|    time_elapsed         | 11069      |\n",
      "|    total_timesteps      | 3059712    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71637106 |\n",
      "|    clip_fraction        | 0.044      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000682  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.121      |\n",
      "|    n_updates            | 24200      |\n",
      "|    policy_gradient_loss | -0.00859   |\n",
      "|    value_loss           | 0.267      |\n",
      "----------------------------------------\n",
      "Ep done - 109310.\n",
      "Ep done - 30510.\n",
      "Ep done - 30520.\n",
      "Ep done - 30530.\n",
      "Ep done - 30540.\n",
      "Ep done - 30550.\n",
      "Ep done - 30560.\n",
      "Ep done - 30570.\n",
      "Ep done - 30580.\n",
      "Ep done - 30590.\n",
      "Ep done - 30600.\n",
      "Eval num_timesteps=3060000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3060000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1284115 |\n",
      "|    clip_fraction        | 0.042     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00298  |\n",
      "|    n_updates            | 24210     |\n",
      "|    policy_gradient_loss | -0.0112   |\n",
      "|    value_loss           | 0.0651    |\n",
      "---------------------------------------\n",
      "Ep done - 109320.\n",
      "Ep done - 109330.\n",
      "Ep done - 109340.\n",
      "Ep done - 109350.\n",
      "Ep done - 109360.\n",
      "Ep done - 109370.\n",
      "Ep done - 109380.\n",
      "Ep done - 109390.\n",
      "Ep done - 109400.\n",
      "Ep done - 109410.\n",
      "Ep done - 109420.\n",
      "Ep done - 109430.\n",
      "Ep done - 109440.\n",
      "Ep done - 109450.\n",
      "Ep done - 109460.\n",
      "Ep done - 109470.\n",
      "Ep done - 109480.\n",
      "Ep done - 109490.\n",
      "Ep done - 109500.\n",
      "Ep done - 109510.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 276      |\n",
      "|    iterations      | 499      |\n",
      "|    time_elapsed    | 11088    |\n",
      "|    total_timesteps | 3065856  |\n",
      "---------------------------------\n",
      "Ep done - 109520.\n",
      "Ep done - 109530.\n",
      "Ep done - 109540.\n",
      "Ep done - 109550.\n",
      "Ep done - 109560.\n",
      "Ep done - 109570.\n",
      "Ep done - 109580.\n",
      "Ep done - 109590.\n",
      "Ep done - 109600.\n",
      "Ep done - 109610.\n",
      "Ep done - 109620.\n",
      "Ep done - 109630.\n",
      "Ep done - 109640.\n",
      "Ep done - 30610.\n",
      "Ep done - 30620.\n",
      "Ep done - 30630.\n",
      "Ep done - 30640.\n",
      "Ep done - 30650.\n",
      "Ep done - 30660.\n",
      "Ep done - 30670.\n",
      "Ep done - 30680.\n",
      "Ep done - 30690.\n",
      "Ep done - 30700.\n",
      "Eval num_timesteps=3070000, episode_reward=-0.99 +/- 0.10\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.99     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3070000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7557174 |\n",
      "|    clip_fraction        | 0.0358    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000142 |\n",
      "|    explained_variance   | -3.58e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0051    |\n",
      "|    n_updates            | 24220     |\n",
      "|    policy_gradient_loss | -0.00775  |\n",
      "|    value_loss           | 0.0294    |\n",
      "---------------------------------------\n",
      "Ep done - 109650.\n",
      "Ep done - 109660.\n",
      "Ep done - 109670.\n",
      "Ep done - 109680.\n",
      "Ep done - 109690.\n",
      "Ep done - 109700.\n",
      "Ep done - 109710.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 276      |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 11107    |\n",
      "|    total_timesteps | 3072000  |\n",
      "---------------------------------\n",
      "Ep done - 109720.\n",
      "Ep done - 109730.\n",
      "Ep done - 109740.\n",
      "Ep done - 109750.\n",
      "Ep done - 109760.\n",
      "Ep done - 109770.\n",
      "Ep done - 109780.\n",
      "Ep done - 109790.\n",
      "Ep done - 109800.\n",
      "Ep done - 109810.\n",
      "Ep done - 109820.\n",
      "Ep done - 109830.\n",
      "Ep done - 109840.\n",
      "Ep done - 109850.\n",
      "Ep done - 109860.\n",
      "Ep done - 109870.\n",
      "Ep done - 109880.\n",
      "Ep done - 109890.\n",
      "Ep done - 109900.\n",
      "Ep done - 109910.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.4      |\n",
      "|    ep_rew_mean          | -0.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 276       |\n",
      "|    iterations           | 501       |\n",
      "|    time_elapsed         | 11121     |\n",
      "|    total_timesteps      | 3078144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.193291  |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000465 |\n",
      "|    explained_variance   | 4.77e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00951  |\n",
      "|    n_updates            | 24230     |\n",
      "|    policy_gradient_loss | -0.00878  |\n",
      "|    value_loss           | 0.0167    |\n",
      "---------------------------------------\n",
      "Ep done - 109920.\n",
      "Ep done - 109930.\n",
      "Ep done - 109940.\n",
      "Ep done - 109950.\n",
      "Ep done - 109960.\n",
      "Ep done - 109970.\n",
      "Ep done - 30710.\n",
      "Ep done - 30720.\n",
      "Ep done - 30730.\n",
      "Ep done - 30740.\n",
      "Ep done - 30750.\n",
      "Ep done - 30760.\n",
      "Ep done - 30770.\n",
      "Ep done - 30780.\n",
      "Ep done - 30790.\n",
      "Ep done - 30800.\n",
      "Eval num_timesteps=3080000, episode_reward=-0.18 +/- 0.98\n",
      "Episode length: 30.41 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.4      |\n",
      "|    mean_reward          | -0.18     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3080000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2496701 |\n",
      "|    clip_fraction        | 0.0232    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000136 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.169     |\n",
      "|    n_updates            | 24240     |\n",
      "|    policy_gradient_loss | 0.0049    |\n",
      "|    value_loss           | 0.283     |\n",
      "---------------------------------------\n",
      "Ep done - 109980.\n",
      "Ep done - 109990.\n",
      "Ep done - 110000.\n",
      "Ep done - 110010.\n",
      "Ep done - 110020.\n",
      "Ep done - 110030.\n",
      "Ep done - 110040.\n",
      "Ep done - 110050.\n",
      "Ep done - 110060.\n",
      "Ep done - 110070.\n",
      "Ep done - 110080.\n",
      "Ep done - 110090.\n",
      "Ep done - 110100.\n",
      "Ep done - 110110.\n",
      "Ep done - 110120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | -0.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 276      |\n",
      "|    iterations      | 502      |\n",
      "|    time_elapsed    | 11141    |\n",
      "|    total_timesteps | 3084288  |\n",
      "---------------------------------\n",
      "Ep done - 110130.\n",
      "Ep done - 110140.\n",
      "Ep done - 110150.\n",
      "Ep done - 110160.\n",
      "Ep done - 110170.\n",
      "Ep done - 110180.\n",
      "Ep done - 110190.\n",
      "Ep done - 110200.\n",
      "Ep done - 110210.\n",
      "Ep done - 110220.\n",
      "Ep done - 110230.\n",
      "Ep done - 110240.\n",
      "Ep done - 110250.\n",
      "Ep done - 110260.\n",
      "Ep done - 110270.\n",
      "Ep done - 110280.\n",
      "Ep done - 110290.\n",
      "Ep done - 110300.\n",
      "Ep done - 30810.\n",
      "Ep done - 30820.\n",
      "Ep done - 30830.\n",
      "Ep done - 30840.\n",
      "Ep done - 30850.\n",
      "Ep done - 30860.\n",
      "Ep done - 30870.\n",
      "Ep done - 30880.\n",
      "Ep done - 30890.\n",
      "Ep done - 30900.\n",
      "Eval num_timesteps=3090000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 31.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 31         |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3090000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.37288332 |\n",
      "|    clip_fraction        | 0.0163     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.51e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.106      |\n",
      "|    n_updates            | 24250      |\n",
      "|    policy_gradient_loss | -0.00399   |\n",
      "|    value_loss           | 0.273      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 106\n",
      "Ep done - 110310.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.9     |\n",
      "|    ep_rew_mean     | 0.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 276      |\n",
      "|    iterations      | 503      |\n",
      "|    time_elapsed    | 11159    |\n",
      "|    total_timesteps | 3090432  |\n",
      "---------------------------------\n",
      "Ep done - 110320.\n",
      "Ep done - 110330.\n",
      "Ep done - 110340.\n",
      "Ep done - 110350.\n",
      "Ep done - 110360.\n",
      "Ep done - 110370.\n",
      "Ep done - 110380.\n",
      "Ep done - 110390.\n",
      "Ep done - 110400.\n",
      "Ep done - 110410.\n",
      "Ep done - 110420.\n",
      "Ep done - 110430.\n",
      "Ep done - 110440.\n",
      "Ep done - 110450.\n",
      "Ep done - 110460.\n",
      "Ep done - 110470.\n",
      "Ep done - 110480.\n",
      "Ep done - 110490.\n",
      "Ep done - 110500.\n",
      "Ep done - 110510.\n",
      "Ep done - 110520.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | 0.16         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 277          |\n",
      "|    iterations           | 504          |\n",
      "|    time_elapsed         | 11174        |\n",
      "|    total_timesteps      | 3096576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056400765 |\n",
      "|    clip_fraction        | 0.00212      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000134    |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0879       |\n",
      "|    n_updates            | 24260        |\n",
      "|    policy_gradient_loss | -7.61e-05    |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "Ep done - 110530.\n",
      "Ep done - 110540.\n",
      "Ep done - 110550.\n",
      "Ep done - 110560.\n",
      "Ep done - 110570.\n",
      "Ep done - 110580.\n",
      "Ep done - 110590.\n",
      "Ep done - 110600.\n",
      "Ep done - 110610.\n",
      "Ep done - 110620.\n",
      "Ep done - 110630.\n",
      "Ep done - 30910.\n",
      "Ep done - 30920.\n",
      "Ep done - 30930.\n",
      "Ep done - 30940.\n",
      "Ep done - 30950.\n",
      "Ep done - 30960.\n",
      "Ep done - 30970.\n",
      "Ep done - 30980.\n",
      "Ep done - 30990.\n",
      "Ep done - 31000.\n",
      "Eval num_timesteps=3100000, episode_reward=0.13 +/- 0.72\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.13       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3100000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50065273 |\n",
      "|    clip_fraction        | 0.0548     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000501  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0711     |\n",
      "|    n_updates            | 24270      |\n",
      "|    policy_gradient_loss | 2.61e-05   |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Ep done - 110640.\n",
      "Ep done - 110650.\n",
      "Ep done - 110660.\n",
      "Ep done - 110670.\n",
      "Ep done - 110680.\n",
      "Ep done - 110690.\n",
      "Ep done - 110700.\n",
      "Ep done - 110710.\n",
      "Ep done - 110720.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 277      |\n",
      "|    iterations      | 505      |\n",
      "|    time_elapsed    | 11193    |\n",
      "|    total_timesteps | 3102720  |\n",
      "---------------------------------\n",
      "Ep done - 110730.\n",
      "Ep done - 110740.\n",
      "Ep done - 110750.\n",
      "Ep done - 110760.\n",
      "Ep done - 110770.\n",
      "Ep done - 110780.\n",
      "Ep done - 110790.\n",
      "Ep done - 110800.\n",
      "Ep done - 110810.\n",
      "Ep done - 110820.\n",
      "Ep done - 110830.\n",
      "Ep done - 110840.\n",
      "Ep done - 110850.\n",
      "Ep done - 110860.\n",
      "Ep done - 110870.\n",
      "Ep done - 110880.\n",
      "Ep done - 110890.\n",
      "Ep done - 110900.\n",
      "Ep done - 110910.\n",
      "Ep done - 110920.\n",
      "Ep done - 110930.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | 0.06          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 277           |\n",
      "|    iterations           | 506           |\n",
      "|    time_elapsed         | 11207         |\n",
      "|    total_timesteps      | 3108864       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0462008e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -4.93e-05     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.0789        |\n",
      "|    n_updates            | 24280         |\n",
      "|    policy_gradient_loss | -1.2e-06      |\n",
      "|    value_loss           | 0.168         |\n",
      "-------------------------------------------\n",
      "Ep done - 110940.\n",
      "Ep done - 110950.\n",
      "Ep done - 110960.\n",
      "Ep done - 110970.\n",
      "Ep done - 31010.\n",
      "Ep done - 31020.\n",
      "Ep done - 31030.\n",
      "Ep done - 31040.\n",
      "Ep done - 31050.\n",
      "Ep done - 31060.\n",
      "Ep done - 31070.\n",
      "Ep done - 31080.\n",
      "Ep done - 31090.\n",
      "Ep done - 31100.\n",
      "Eval num_timesteps=3110000, episode_reward=0.12 +/- 0.70\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.12      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3110000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.46e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0738    |\n",
      "|    n_updates            | 24290     |\n",
      "|    policy_gradient_loss | 1.13e-08  |\n",
      "|    value_loss           | 0.144     |\n",
      "---------------------------------------\n",
      "Ep done - 110980.\n",
      "Ep done - 110990.\n",
      "Ep done - 111000.\n",
      "Ep done - 111010.\n",
      "Ep done - 111020.\n",
      "Ep done - 111030.\n",
      "Ep done - 111040.\n",
      "Ep done - 111050.\n",
      "Ep done - 111060.\n",
      "Ep done - 111070.\n",
      "Ep done - 111080.\n",
      "Ep done - 111090.\n",
      "Ep done - 111100.\n",
      "Ep done - 111110.\n",
      "Ep done - 111120.\n",
      "Ep done - 111130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 277      |\n",
      "|    iterations      | 507      |\n",
      "|    time_elapsed    | 11227    |\n",
      "|    total_timesteps | 3115008  |\n",
      "---------------------------------\n",
      "Ep done - 111140.\n",
      "Ep done - 111150.\n",
      "Ep done - 111160.\n",
      "Ep done - 111170.\n",
      "Ep done - 111180.\n",
      "Ep done - 111190.\n",
      "Ep done - 111200.\n",
      "Ep done - 111210.\n",
      "Ep done - 111220.\n",
      "Ep done - 111230.\n",
      "Ep done - 111240.\n",
      "Ep done - 111250.\n",
      "Ep done - 111260.\n",
      "Ep done - 111270.\n",
      "Ep done - 111280.\n",
      "Ep done - 111290.\n",
      "Ep done - 111300.\n",
      "Ep done - 31110.\n",
      "Ep done - 31120.\n",
      "Ep done - 31130.\n",
      "Ep done - 31140.\n",
      "Ep done - 31150.\n",
      "Ep done - 31160.\n",
      "Ep done - 31170.\n",
      "Ep done - 31180.\n",
      "Ep done - 31190.\n",
      "Ep done - 31200.\n",
      "Eval num_timesteps=3120000, episode_reward=0.12 +/- 0.74\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.12      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3120000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.3e-05  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0669    |\n",
      "|    n_updates            | 24300     |\n",
      "|    policy_gradient_loss | -6.38e-10 |\n",
      "|    value_loss           | 0.139     |\n",
      "---------------------------------------\n",
      "Ep done - 111310.\n",
      "Ep done - 111320.\n",
      "Ep done - 111330.\n",
      "Ep done - 111340.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.12     |\n",
      "| time/              |          |\n",
      "|    fps             | 277      |\n",
      "|    iterations      | 508      |\n",
      "|    time_elapsed    | 11246    |\n",
      "|    total_timesteps | 3121152  |\n",
      "---------------------------------\n",
      "Ep done - 111350.\n",
      "Ep done - 111360.\n",
      "Ep done - 111370.\n",
      "Ep done - 111380.\n",
      "Ep done - 111390.\n",
      "Ep done - 111400.\n",
      "Ep done - 111410.\n",
      "Ep done - 111420.\n",
      "Ep done - 111430.\n",
      "Ep done - 111440.\n",
      "Ep done - 111450.\n",
      "Ep done - 111460.\n",
      "Ep done - 111470.\n",
      "Ep done - 111480.\n",
      "Ep done - 111490.\n",
      "Ep done - 111500.\n",
      "Ep done - 111510.\n",
      "Ep done - 111520.\n",
      "Ep done - 111530.\n",
      "Ep done - 111540.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.08      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 277       |\n",
      "|    iterations           | 509       |\n",
      "|    time_elapsed         | 11260     |\n",
      "|    total_timesteps      | 3127296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.63e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0726    |\n",
      "|    n_updates            | 24310     |\n",
      "|    policy_gradient_loss | 1.29e-08  |\n",
      "|    value_loss           | 0.156     |\n",
      "---------------------------------------\n",
      "Ep done - 111550.\n",
      "Ep done - 111560.\n",
      "Ep done - 111570.\n",
      "Ep done - 111580.\n",
      "Ep done - 111590.\n",
      "Ep done - 111600.\n",
      "Ep done - 111610.\n",
      "Ep done - 111620.\n",
      "Ep done - 111630.\n",
      "Ep done - 31210.\n",
      "Ep done - 31220.\n",
      "Ep done - 31230.\n",
      "Ep done - 31240.\n",
      "Ep done - 31250.\n",
      "Ep done - 31260.\n",
      "Ep done - 31270.\n",
      "Ep done - 31280.\n",
      "Ep done - 31290.\n",
      "Ep done - 31300.\n",
      "Eval num_timesteps=3130000, episode_reward=0.02 +/- 0.66\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.02      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3130000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.54e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0725    |\n",
      "|    n_updates            | 24320     |\n",
      "|    policy_gradient_loss | 2.91e-08  |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "Ep done - 111640.\n",
      "Ep done - 111650.\n",
      "Ep done - 111660.\n",
      "Ep done - 111670.\n",
      "Ep done - 111680.\n",
      "Ep done - 111690.\n",
      "Ep done - 111700.\n",
      "Ep done - 111710.\n",
      "Ep done - 111720.\n",
      "Ep done - 111730.\n",
      "Ep done - 111740.\n",
      "Ep done - 111750.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 277      |\n",
      "|    iterations      | 510      |\n",
      "|    time_elapsed    | 11279    |\n",
      "|    total_timesteps | 3133440  |\n",
      "---------------------------------\n",
      "Ep done - 111760.\n",
      "Ep done - 111770.\n",
      "Ep done - 111780.\n",
      "Ep done - 111790.\n",
      "Ep done - 111800.\n",
      "Ep done - 111810.\n",
      "Ep done - 111820.\n",
      "Ep done - 111830.\n",
      "Ep done - 111840.\n",
      "Ep done - 111850.\n",
      "Ep done - 111860.\n",
      "Ep done - 111870.\n",
      "Ep done - 111880.\n",
      "Ep done - 111890.\n",
      "Ep done - 111900.\n",
      "Ep done - 111910.\n",
      "Ep done - 111920.\n",
      "Ep done - 111930.\n",
      "Ep done - 111940.\n",
      "Ep done - 111950.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.07     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 277       |\n",
      "|    iterations           | 511       |\n",
      "|    time_elapsed         | 11294     |\n",
      "|    total_timesteps      | 3139584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.59e-05 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0609    |\n",
      "|    n_updates            | 24330     |\n",
      "|    policy_gradient_loss | 2.69e-08  |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "Ep done - 111960.\n",
      "Ep done - 111970.\n",
      "Ep done - 31310.\n",
      "Ep done - 31320.\n",
      "Ep done - 31330.\n",
      "Ep done - 31340.\n",
      "Ep done - 31350.\n",
      "Ep done - 31360.\n",
      "Ep done - 31370.\n",
      "Ep done - 31380.\n",
      "Ep done - 31390.\n",
      "Ep done - 31400.\n",
      "Eval num_timesteps=3140000, episode_reward=0.03 +/- 0.70\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.03      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3140000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.12e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0966    |\n",
      "|    n_updates            | 24340     |\n",
      "|    policy_gradient_loss | 8.84e-09  |\n",
      "|    value_loss           | 0.152     |\n",
      "---------------------------------------\n",
      "Ep done - 111980.\n",
      "Ep done - 111990.\n",
      "Ep done - 112000.\n",
      "Ep done - 112010.\n",
      "Ep done - 112020.\n",
      "Ep done - 112030.\n",
      "Ep done - 112040.\n",
      "Ep done - 112050.\n",
      "Ep done - 112060.\n",
      "Ep done - 112070.\n",
      "Ep done - 112080.\n",
      "Ep done - 112090.\n",
      "Ep done - 112100.\n",
      "Ep done - 112110.\n",
      "Ep done - 112120.\n",
      "Ep done - 112130.\n",
      "Ep done - 112140.\n",
      "Ep done - 112150.\n",
      "Ep done - 112160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 512      |\n",
      "|    time_elapsed    | 11313    |\n",
      "|    total_timesteps | 3145728  |\n",
      "---------------------------------\n",
      "Ep done - 112170.\n",
      "Ep done - 112180.\n",
      "Ep done - 112190.\n",
      "Ep done - 112200.\n",
      "Ep done - 112210.\n",
      "Ep done - 112220.\n",
      "Ep done - 112230.\n",
      "Ep done - 112240.\n",
      "Ep done - 112250.\n",
      "Ep done - 112260.\n",
      "Ep done - 112270.\n",
      "Ep done - 112280.\n",
      "Ep done - 112290.\n",
      "Ep done - 112300.\n",
      "Ep done - 31410.\n",
      "Ep done - 31420.\n",
      "Ep done - 31430.\n",
      "Ep done - 31440.\n",
      "Ep done - 31450.\n",
      "Ep done - 31460.\n",
      "Ep done - 31470.\n",
      "Ep done - 31480.\n",
      "Ep done - 31490.\n",
      "Ep done - 31500.\n",
      "Eval num_timesteps=3150000, episode_reward=0.12 +/- 0.70\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.12      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3150000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.46e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0746    |\n",
      "|    n_updates            | 24350     |\n",
      "|    policy_gradient_loss | -1.54e-07 |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "Ep done - 112310.\n",
      "Ep done - 112320.\n",
      "Ep done - 112330.\n",
      "Ep done - 112340.\n",
      "Ep done - 112350.\n",
      "Ep done - 112360.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 513      |\n",
      "|    time_elapsed    | 11332    |\n",
      "|    total_timesteps | 3151872  |\n",
      "---------------------------------\n",
      "Ep done - 112370.\n",
      "Ep done - 112380.\n",
      "Ep done - 112390.\n",
      "Ep done - 112400.\n",
      "Ep done - 112410.\n",
      "Ep done - 112420.\n",
      "Ep done - 112430.\n",
      "Ep done - 112440.\n",
      "Ep done - 112450.\n",
      "Ep done - 112460.\n",
      "Ep done - 112470.\n",
      "Ep done - 112480.\n",
      "Ep done - 112490.\n",
      "Ep done - 112500.\n",
      "Ep done - 112510.\n",
      "Ep done - 112520.\n",
      "Ep done - 112530.\n",
      "Ep done - 112540.\n",
      "Ep done - 112550.\n",
      "Ep done - 112560.\n",
      "Ep done - 112570.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.12      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 278       |\n",
      "|    iterations           | 514       |\n",
      "|    time_elapsed         | 11347     |\n",
      "|    total_timesteps      | 3158016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.92e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0644    |\n",
      "|    n_updates            | 24360     |\n",
      "|    policy_gradient_loss | 7.74e-09  |\n",
      "|    value_loss           | 0.148     |\n",
      "---------------------------------------\n",
      "Ep done - 112580.\n",
      "Ep done - 112590.\n",
      "Ep done - 112600.\n",
      "Ep done - 112610.\n",
      "Ep done - 112620.\n",
      "Ep done - 112630.\n",
      "Ep done - 31510.\n",
      "Ep done - 31520.\n",
      "Ep done - 31530.\n",
      "Ep done - 31540.\n",
      "Ep done - 31550.\n",
      "Ep done - 31560.\n",
      "Ep done - 31570.\n",
      "Ep done - 31580.\n",
      "Ep done - 31590.\n",
      "Ep done - 31600.\n",
      "Eval num_timesteps=3160000, episode_reward=0.07 +/- 0.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.07      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3160000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.7e-05  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.104     |\n",
      "|    n_updates            | 24370     |\n",
      "|    policy_gradient_loss | -3.41e-08 |\n",
      "|    value_loss           | 0.156     |\n",
      "---------------------------------------\n",
      "Ep done - 112640.\n",
      "Ep done - 112650.\n",
      "Ep done - 112660.\n",
      "Ep done - 112670.\n",
      "Ep done - 112680.\n",
      "Ep done - 112690.\n",
      "Ep done - 112700.\n",
      "Ep done - 112710.\n",
      "Ep done - 112720.\n",
      "Ep done - 112730.\n",
      "Ep done - 112740.\n",
      "Ep done - 112750.\n",
      "Ep done - 112760.\n",
      "Ep done - 112770.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 515      |\n",
      "|    time_elapsed    | 11365    |\n",
      "|    total_timesteps | 3164160  |\n",
      "---------------------------------\n",
      "Ep done - 112780.\n",
      "Ep done - 112790.\n",
      "Ep done - 112800.\n",
      "Ep done - 112810.\n",
      "Ep done - 112820.\n",
      "Ep done - 112830.\n",
      "Ep done - 112840.\n",
      "Ep done - 112850.\n",
      "Ep done - 112860.\n",
      "Ep done - 112870.\n",
      "Ep done - 112880.\n",
      "Ep done - 112890.\n",
      "Ep done - 112900.\n",
      "Ep done - 112910.\n",
      "Ep done - 112920.\n",
      "Ep done - 112930.\n",
      "Ep done - 112940.\n",
      "Ep done - 112950.\n",
      "Ep done - 112960.\n",
      "Ep done - 112970.\n",
      "Ep done - 31610.\n",
      "Ep done - 31620.\n",
      "Ep done - 31630.\n",
      "Ep done - 31640.\n",
      "Ep done - 31650.\n",
      "Ep done - 31660.\n",
      "Ep done - 31670.\n",
      "Ep done - 31680.\n",
      "Ep done - 31690.\n",
      "Ep done - 31700.\n",
      "Eval num_timesteps=3170000, episode_reward=-0.36 +/- 0.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.36      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3170000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45124912 |\n",
      "|    clip_fraction        | 0.032      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.27e-05  |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0879     |\n",
      "|    n_updates            | 24380      |\n",
      "|    policy_gradient_loss | 0.00479    |\n",
      "|    value_loss           | 0.131      |\n",
      "----------------------------------------\n",
      "Ep done - 112980.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 516      |\n",
      "|    time_elapsed    | 11384    |\n",
      "|    total_timesteps | 3170304  |\n",
      "---------------------------------\n",
      "Ep done - 112990.\n",
      "Ep done - 113000.\n",
      "Ep done - 113010.\n",
      "Ep done - 113020.\n",
      "Ep done - 113030.\n",
      "Ep done - 113040.\n",
      "Ep done - 113050.\n",
      "Ep done - 113060.\n",
      "Ep done - 113070.\n",
      "Ep done - 113080.\n",
      "Ep done - 113090.\n",
      "Ep done - 113100.\n",
      "Ep done - 113110.\n",
      "Ep done - 113120.\n",
      "Ep done - 113130.\n",
      "Ep done - 113140.\n",
      "Ep done - 113150.\n",
      "Ep done - 113160.\n",
      "Ep done - 113170.\n",
      "Ep done - 113180.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.5       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 278       |\n",
      "|    iterations           | 517       |\n",
      "|    time_elapsed         | 11399     |\n",
      "|    total_timesteps      | 3176448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1312209 |\n",
      "|    clip_fraction        | 0.0397    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00145  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.121     |\n",
      "|    n_updates            | 24390     |\n",
      "|    policy_gradient_loss | -4.85e-05 |\n",
      "|    value_loss           | 0.257     |\n",
      "---------------------------------------\n",
      "Ep done - 113190.\n",
      "Ep done - 113200.\n",
      "Ep done - 113210.\n",
      "Ep done - 113220.\n",
      "Ep done - 113230.\n",
      "Ep done - 113240.\n",
      "Ep done - 113250.\n",
      "Ep done - 113260.\n",
      "Ep done - 113270.\n",
      "Ep done - 113280.\n",
      "Ep done - 113290.\n",
      "Ep done - 113300.\n",
      "Ep done - 31710.\n",
      "Ep done - 31720.\n",
      "Ep done - 31730.\n",
      "Ep done - 31740.\n",
      "Ep done - 31750.\n",
      "Ep done - 31760.\n",
      "Ep done - 31770.\n",
      "Ep done - 31780.\n",
      "Ep done - 31790.\n",
      "Ep done - 31800.\n",
      "Eval num_timesteps=3180000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.56      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3180000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.96e-05 |\n",
      "|    explained_variance   | 1.79e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.143     |\n",
      "|    n_updates            | 24400     |\n",
      "|    policy_gradient_loss | -1.2e-09  |\n",
      "|    value_loss           | 0.24      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 107\n",
      "Ep done - 113310.\n",
      "Ep done - 113320.\n",
      "Ep done - 113330.\n",
      "Ep done - 113340.\n",
      "Ep done - 113350.\n",
      "Ep done - 113360.\n",
      "Ep done - 113370.\n",
      "Ep done - 113380.\n",
      "Ep done - 113390.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.94     |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 518      |\n",
      "|    time_elapsed    | 11418    |\n",
      "|    total_timesteps | 3182592  |\n",
      "---------------------------------\n",
      "Ep done - 113400.\n",
      "Ep done - 113410.\n",
      "Ep done - 113420.\n",
      "Ep done - 113430.\n",
      "Ep done - 113440.\n",
      "Ep done - 113450.\n",
      "Ep done - 113460.\n",
      "Ep done - 113470.\n",
      "Ep done - 113480.\n",
      "Ep done - 113490.\n",
      "Ep done - 113500.\n",
      "Ep done - 113510.\n",
      "Ep done - 113520.\n",
      "Ep done - 113530.\n",
      "Ep done - 113540.\n",
      "Ep done - 113550.\n",
      "Ep done - 113560.\n",
      "Ep done - 113570.\n",
      "Ep done - 113580.\n",
      "Ep done - 113590.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.99       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 278        |\n",
      "|    iterations           | 519        |\n",
      "|    time_elapsed         | 11432      |\n",
      "|    total_timesteps      | 3188736    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33614743 |\n",
      "|    clip_fraction        | 0.0356     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000295  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.079      |\n",
      "|    n_updates            | 24410      |\n",
      "|    policy_gradient_loss | 5.65e-05   |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "Ep done - 113600.\n",
      "Ep done - 113610.\n",
      "Ep done - 113620.\n",
      "Ep done - 113630.\n",
      "Ep done - 31810.\n",
      "Ep done - 31820.\n",
      "Ep done - 31830.\n",
      "Ep done - 31840.\n",
      "Ep done - 31850.\n",
      "Ep done - 31860.\n",
      "Ep done - 31870.\n",
      "Ep done - 31880.\n",
      "Ep done - 31890.\n",
      "Ep done - 31900.\n",
      "Eval num_timesteps=3190000, episode_reward=-0.96 +/- 0.28\n",
      "Episode length: 29.40 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.4      |\n",
      "|    mean_reward          | -0.96     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3190000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 7.815168  |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00067  |\n",
      "|    explained_variance   | 2.98e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.000257 |\n",
      "|    n_updates            | 24420     |\n",
      "|    policy_gradient_loss | -0.0154   |\n",
      "|    value_loss           | 0.0598    |\n",
      "---------------------------------------\n",
      "Ep done - 113640.\n",
      "Ep done - 113650.\n",
      "Ep done - 113660.\n",
      "Ep done - 113670.\n",
      "Ep done - 113680.\n",
      "Ep done - 113690.\n",
      "Ep done - 113700.\n",
      "Ep done - 113710.\n",
      "Ep done - 113720.\n",
      "Ep done - 113730.\n",
      "Ep done - 113740.\n",
      "Ep done - 113750.\n",
      "Ep done - 113760.\n",
      "Ep done - 113770.\n",
      "Ep done - 113780.\n",
      "Ep done - 113790.\n",
      "Ep done - 113800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.3     |\n",
      "|    ep_rew_mean     | -0.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 520      |\n",
      "|    time_elapsed    | 11452    |\n",
      "|    total_timesteps | 3194880  |\n",
      "---------------------------------\n",
      "Ep done - 113810.\n",
      "Ep done - 113820.\n",
      "Ep done - 113830.\n",
      "Ep done - 113840.\n",
      "Ep done - 113850.\n",
      "Ep done - 113860.\n",
      "Ep done - 113870.\n",
      "Ep done - 113880.\n",
      "Ep done - 113890.\n",
      "Ep done - 113900.\n",
      "Ep done - 113910.\n",
      "Ep done - 113920.\n",
      "Ep done - 113930.\n",
      "Ep done - 113940.\n",
      "Ep done - 113950.\n",
      "Ep done - 113960.\n",
      "Ep done - 113970.\n",
      "Ep done - 31910.\n",
      "Ep done - 31920.\n",
      "Ep done - 31930.\n",
      "Ep done - 31940.\n",
      "Ep done - 31950.\n",
      "Ep done - 31960.\n",
      "Ep done - 31970.\n",
      "Ep done - 31980.\n",
      "Ep done - 31990.\n",
      "Ep done - 32000.\n",
      "Eval num_timesteps=3200000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.65      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3200000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1411343 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000867 |\n",
      "|    explained_variance   | -8.34e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0324    |\n",
      "|    n_updates            | 24430     |\n",
      "|    policy_gradient_loss | -0.0148   |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.65\n",
      "SELFPLAY: new best model, bumping up generation to 108\n",
      "Ep done - 113980.\n",
      "Ep done - 113990.\n",
      "Ep done - 114000.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 521      |\n",
      "|    time_elapsed    | 11471    |\n",
      "|    total_timesteps | 3201024  |\n",
      "---------------------------------\n",
      "Ep done - 114010.\n",
      "Ep done - 114020.\n",
      "Ep done - 114030.\n",
      "Ep done - 114040.\n",
      "Ep done - 114050.\n",
      "Ep done - 114060.\n",
      "Ep done - 114070.\n",
      "Ep done - 114080.\n",
      "Ep done - 114090.\n",
      "Ep done - 114100.\n",
      "Ep done - 114110.\n",
      "Ep done - 114120.\n",
      "Ep done - 114130.\n",
      "Ep done - 114140.\n",
      "Ep done - 114150.\n",
      "Ep done - 114160.\n",
      "Ep done - 114170.\n",
      "Ep done - 114180.\n",
      "Ep done - 114190.\n",
      "Ep done - 114200.\n",
      "Ep done - 114210.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.88     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 522       |\n",
      "|    time_elapsed         | 11486     |\n",
      "|    total_timesteps      | 3207168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 5.124962  |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000761 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0647    |\n",
      "|    n_updates            | 24440     |\n",
      "|    policy_gradient_loss | -0.0157   |\n",
      "|    value_loss           | 0.14      |\n",
      "---------------------------------------\n",
      "Ep done - 114220.\n",
      "Ep done - 114230.\n",
      "Ep done - 114240.\n",
      "Ep done - 114250.\n",
      "Ep done - 114260.\n",
      "Ep done - 114270.\n",
      "Ep done - 114280.\n",
      "Ep done - 114290.\n",
      "Ep done - 114300.\n",
      "Ep done - 32010.\n",
      "Ep done - 32020.\n",
      "Ep done - 32030.\n",
      "Ep done - 32040.\n",
      "Ep done - 32050.\n",
      "Ep done - 32060.\n",
      "Ep done - 32070.\n",
      "Ep done - 32080.\n",
      "Ep done - 32090.\n",
      "Ep done - 32100.\n",
      "Eval num_timesteps=3210000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3210000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.904481  |\n",
      "|    clip_fraction        | 0.11      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00175  |\n",
      "|    explained_variance   | -3.58e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0368    |\n",
      "|    n_updates            | 24450     |\n",
      "|    policy_gradient_loss | 0.00202   |\n",
      "|    value_loss           | 0.126     |\n",
      "---------------------------------------\n",
      "Ep done - 114310.\n",
      "Ep done - 114320.\n",
      "Ep done - 114330.\n",
      "Ep done - 114340.\n",
      "Ep done - 114350.\n",
      "Ep done - 114360.\n",
      "Ep done - 114370.\n",
      "Ep done - 114380.\n",
      "Ep done - 114390.\n",
      "Ep done - 114400.\n",
      "Ep done - 114410.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 523      |\n",
      "|    time_elapsed    | 11505    |\n",
      "|    total_timesteps | 3213312  |\n",
      "---------------------------------\n",
      "Ep done - 114420.\n",
      "Ep done - 114430.\n",
      "Ep done - 114440.\n",
      "Ep done - 114450.\n",
      "Ep done - 114460.\n",
      "Ep done - 114470.\n",
      "Ep done - 114480.\n",
      "Ep done - 114490.\n",
      "Ep done - 114500.\n",
      "Ep done - 114510.\n",
      "Ep done - 114520.\n",
      "Ep done - 114530.\n",
      "Ep done - 114540.\n",
      "Ep done - 114550.\n",
      "Ep done - 114560.\n",
      "Ep done - 114570.\n",
      "Ep done - 114580.\n",
      "Ep done - 114590.\n",
      "Ep done - 114600.\n",
      "Ep done - 114610.\n",
      "Ep done - 114620.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 1         |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 524       |\n",
      "|    time_elapsed         | 11520     |\n",
      "|    total_timesteps      | 3219456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7530079 |\n",
      "|    clip_fraction        | 0.0529    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000139 |\n",
      "|    explained_variance   | -4.77e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00783  |\n",
      "|    n_updates            | 24460     |\n",
      "|    policy_gradient_loss | -0.0174   |\n",
      "|    value_loss           | 0.0337    |\n",
      "---------------------------------------\n",
      "Ep done - 114630.\n",
      "Ep done - 114640.\n",
      "Ep done - 32110.\n",
      "Ep done - 32120.\n",
      "Ep done - 32130.\n",
      "Ep done - 32140.\n",
      "Ep done - 32150.\n",
      "Ep done - 32160.\n",
      "Ep done - 32170.\n",
      "Ep done - 32180.\n",
      "Ep done - 32190.\n",
      "Ep done - 32200.\n",
      "Eval num_timesteps=3220000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3220000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7435478 |\n",
      "|    clip_fraction        | 0.019     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00017  |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.053     |\n",
      "|    n_updates            | 24470     |\n",
      "|    policy_gradient_loss | -0.00165  |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 109\n",
      "Ep done - 114650.\n",
      "Ep done - 114660.\n",
      "Ep done - 114670.\n",
      "Ep done - 114680.\n",
      "Ep done - 114690.\n",
      "Ep done - 114700.\n",
      "Ep done - 114710.\n",
      "Ep done - 114720.\n",
      "Ep done - 114730.\n",
      "Ep done - 114740.\n",
      "Ep done - 114750.\n",
      "Ep done - 114760.\n",
      "Ep done - 114770.\n",
      "Ep done - 114780.\n",
      "Ep done - 114790.\n",
      "Ep done - 114800.\n",
      "Ep done - 114810.\n",
      "Ep done - 114820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.8     |\n",
      "|    ep_rew_mean     | -0.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 525      |\n",
      "|    time_elapsed    | 11539    |\n",
      "|    total_timesteps | 3225600  |\n",
      "---------------------------------\n",
      "Ep done - 114830.\n",
      "Ep done - 114840.\n",
      "Ep done - 114850.\n",
      "Ep done - 114860.\n",
      "Ep done - 114870.\n",
      "Ep done - 114880.\n",
      "Ep done - 114890.\n",
      "Ep done - 114900.\n",
      "Ep done - 114910.\n",
      "Ep done - 114920.\n",
      "Ep done - 114930.\n",
      "Ep done - 114940.\n",
      "Ep done - 114950.\n",
      "Ep done - 114960.\n",
      "Ep done - 114970.\n",
      "Ep done - 32210.\n",
      "Ep done - 32220.\n",
      "Ep done - 32230.\n",
      "Ep done - 32240.\n",
      "Ep done - 32250.\n",
      "Ep done - 32260.\n",
      "Ep done - 32270.\n",
      "Ep done - 32280.\n",
      "Ep done - 32290.\n",
      "Ep done - 32300.\n",
      "Eval num_timesteps=3230000, episode_reward=-0.90 +/- 0.44\n",
      "Episode length: 30.03 +/- 0.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3230000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6434422 |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00363  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.103     |\n",
      "|    n_updates            | 24480     |\n",
      "|    policy_gradient_loss | 0.0032    |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "Ep done - 114980.\n",
      "Ep done - 114990.\n",
      "Ep done - 115000.\n",
      "Ep done - 115010.\n",
      "Ep done - 115020.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 526      |\n",
      "|    time_elapsed    | 11557    |\n",
      "|    total_timesteps | 3231744  |\n",
      "---------------------------------\n",
      "Ep done - 115030.\n",
      "Ep done - 115040.\n",
      "Ep done - 115050.\n",
      "Ep done - 115060.\n",
      "Ep done - 115070.\n",
      "Ep done - 115080.\n",
      "Ep done - 115090.\n",
      "Ep done - 115100.\n",
      "Ep done - 115110.\n",
      "Ep done - 115120.\n",
      "Ep done - 115130.\n",
      "Ep done - 115140.\n",
      "Ep done - 115150.\n",
      "Ep done - 115160.\n",
      "Ep done - 115170.\n",
      "Ep done - 115180.\n",
      "Ep done - 115190.\n",
      "Ep done - 115200.\n",
      "Ep done - 115210.\n",
      "Ep done - 115220.\n",
      "Ep done - 115230.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.86     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 527       |\n",
      "|    time_elapsed         | 11572     |\n",
      "|    total_timesteps      | 3237888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.3363817 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00347  |\n",
      "|    explained_variance   | -3.58e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00461  |\n",
      "|    n_updates            | 24490     |\n",
      "|    policy_gradient_loss | -0.00592  |\n",
      "|    value_loss           | 0.0898    |\n",
      "---------------------------------------\n",
      "Ep done - 115240.\n",
      "Ep done - 115250.\n",
      "Ep done - 115260.\n",
      "Ep done - 115270.\n",
      "Ep done - 115280.\n",
      "Ep done - 115290.\n",
      "Ep done - 115300.\n",
      "Ep done - 32310.\n",
      "Ep done - 32320.\n",
      "Ep done - 32330.\n",
      "Ep done - 32340.\n",
      "Ep done - 32350.\n",
      "Ep done - 32360.\n",
      "Ep done - 32370.\n",
      "Ep done - 32380.\n",
      "Ep done - 32390.\n",
      "Ep done - 32400.\n",
      "Eval num_timesteps=3240000, episode_reward=-0.70 +/- 0.71\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3240000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.3802366 |\n",
      "|    clip_fraction        | 0.109     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00622  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0306    |\n",
      "|    n_updates            | 24500     |\n",
      "|    policy_gradient_loss | 0.0173    |\n",
      "|    value_loss           | 0.0757    |\n",
      "---------------------------------------\n",
      "Ep done - 115310.\n",
      "Ep done - 115320.\n",
      "Ep done - 115330.\n",
      "Ep done - 115340.\n",
      "Ep done - 115350.\n",
      "Ep done - 115360.\n",
      "Ep done - 115370.\n",
      "Ep done - 115380.\n",
      "Ep done - 115390.\n",
      "Ep done - 115400.\n",
      "Ep done - 115410.\n",
      "Ep done - 115420.\n",
      "Ep done - 115430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 528      |\n",
      "|    time_elapsed    | 11591    |\n",
      "|    total_timesteps | 3244032  |\n",
      "---------------------------------\n",
      "Ep done - 115440.\n",
      "Ep done - 115450.\n",
      "Ep done - 115460.\n",
      "Ep done - 115470.\n",
      "Ep done - 115480.\n",
      "Ep done - 115490.\n",
      "Ep done - 115500.\n",
      "Ep done - 115510.\n",
      "Ep done - 115520.\n",
      "Ep done - 115530.\n",
      "Ep done - 115540.\n",
      "Ep done - 115550.\n",
      "Ep done - 115560.\n",
      "Ep done - 115570.\n",
      "Ep done - 115580.\n",
      "Ep done - 115590.\n",
      "Ep done - 115600.\n",
      "Ep done - 115610.\n",
      "Ep done - 115620.\n",
      "Ep done - 115630.\n",
      "Ep done - 32410.\n",
      "Ep done - 32420.\n",
      "Ep done - 32430.\n",
      "Ep done - 32440.\n",
      "Ep done - 32450.\n",
      "Ep done - 32460.\n",
      "Ep done - 32470.\n",
      "Ep done - 32480.\n",
      "Ep done - 32490.\n",
      "Ep done - 32500.\n",
      "Eval num_timesteps=3250000, episode_reward=-0.58 +/- 0.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.58      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3250000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10750299 |\n",
      "|    clip_fraction        | 0.00335    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00224   |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0431     |\n",
      "|    n_updates            | 24510      |\n",
      "|    policy_gradient_loss | 0.000796   |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Ep done - 115640.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 529      |\n",
      "|    time_elapsed    | 11610    |\n",
      "|    total_timesteps | 3250176  |\n",
      "---------------------------------\n",
      "Ep done - 115650.\n",
      "Ep done - 115660.\n",
      "Ep done - 115670.\n",
      "Ep done - 115680.\n",
      "Ep done - 115690.\n",
      "Ep done - 115700.\n",
      "Ep done - 115710.\n",
      "Ep done - 115720.\n",
      "Ep done - 115730.\n",
      "Ep done - 115740.\n",
      "Ep done - 115750.\n",
      "Ep done - 115760.\n",
      "Ep done - 115770.\n",
      "Ep done - 115780.\n",
      "Ep done - 115790.\n",
      "Ep done - 115800.\n",
      "Ep done - 115810.\n",
      "Ep done - 115820.\n",
      "Ep done - 115830.\n",
      "Ep done - 115840.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 530        |\n",
      "|    time_elapsed         | 11625      |\n",
      "|    total_timesteps      | 3256320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39577493 |\n",
      "|    clip_fraction        | 0.0219     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00178   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0767     |\n",
      "|    n_updates            | 24520      |\n",
      "|    policy_gradient_loss | -0.000428  |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "Ep done - 115850.\n",
      "Ep done - 115860.\n",
      "Ep done - 115870.\n",
      "Ep done - 115880.\n",
      "Ep done - 115890.\n",
      "Ep done - 115900.\n",
      "Ep done - 115910.\n",
      "Ep done - 115920.\n",
      "Ep done - 115930.\n",
      "Ep done - 115940.\n",
      "Ep done - 115950.\n",
      "Ep done - 115960.\n",
      "Ep done - 115970.\n",
      "Ep done - 32510.\n",
      "Ep done - 32520.\n",
      "Ep done - 32530.\n",
      "Ep done - 32540.\n",
      "Ep done - 32550.\n",
      "Ep done - 32560.\n",
      "Ep done - 32570.\n",
      "Ep done - 32580.\n",
      "Ep done - 32590.\n",
      "Ep done - 32600.\n",
      "Eval num_timesteps=3260000, episode_reward=-0.50 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3260000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014248021 |\n",
      "|    clip_fraction        | 0.00283      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0011      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0754       |\n",
      "|    n_updates            | 24530        |\n",
      "|    policy_gradient_loss | 0.000742     |\n",
      "|    value_loss           | 0.206        |\n",
      "------------------------------------------\n",
      "Ep done - 115980.\n",
      "Ep done - 115990.\n",
      "Ep done - 116000.\n",
      "Ep done - 116010.\n",
      "Ep done - 116020.\n",
      "Ep done - 116030.\n",
      "Ep done - 116040.\n",
      "Ep done - 116050.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 531      |\n",
      "|    time_elapsed    | 11644    |\n",
      "|    total_timesteps | 3262464  |\n",
      "---------------------------------\n",
      "Ep done - 116060.\n",
      "Ep done - 116070.\n",
      "Ep done - 116080.\n",
      "Ep done - 116090.\n",
      "Ep done - 116100.\n",
      "Ep done - 116110.\n",
      "Ep done - 116120.\n",
      "Ep done - 116130.\n",
      "Ep done - 116140.\n",
      "Ep done - 116150.\n",
      "Ep done - 116160.\n",
      "Ep done - 116170.\n",
      "Ep done - 116180.\n",
      "Ep done - 116190.\n",
      "Ep done - 116200.\n",
      "Ep done - 116210.\n",
      "Ep done - 116220.\n",
      "Ep done - 116230.\n",
      "Ep done - 116240.\n",
      "Ep done - 116250.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -0.56         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 280           |\n",
      "|    iterations           | 532           |\n",
      "|    time_elapsed         | 11659         |\n",
      "|    total_timesteps      | 3268608       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1253481e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000154     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.0575        |\n",
      "|    n_updates            | 24540         |\n",
      "|    policy_gradient_loss | -2.65e-07     |\n",
      "|    value_loss           | 0.19          |\n",
      "-------------------------------------------\n",
      "Ep done - 116260.\n",
      "Ep done - 116270.\n",
      "Ep done - 116280.\n",
      "Ep done - 116290.\n",
      "Ep done - 116300.\n",
      "Ep done - 32610.\n",
      "Ep done - 32620.\n",
      "Ep done - 32630.\n",
      "Ep done - 32640.\n",
      "Ep done - 32650.\n",
      "Ep done - 32660.\n",
      "Ep done - 32670.\n",
      "Ep done - 32680.\n",
      "Ep done - 32690.\n",
      "Ep done - 32700.\n",
      "Eval num_timesteps=3270000, episode_reward=-0.52 +/- 0.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.52        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3270000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.717126e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000141    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.105        |\n",
      "|    n_updates            | 24550        |\n",
      "|    policy_gradient_loss | -6.07e-08    |\n",
      "|    value_loss           | 0.172        |\n",
      "------------------------------------------\n",
      "Ep done - 116310.\n",
      "Ep done - 116320.\n",
      "Ep done - 116330.\n",
      "Ep done - 116340.\n",
      "Ep done - 116350.\n",
      "Ep done - 116360.\n",
      "Ep done - 116370.\n",
      "Ep done - 116380.\n",
      "Ep done - 116390.\n",
      "Ep done - 116400.\n",
      "Ep done - 116410.\n",
      "Ep done - 116420.\n",
      "Ep done - 116430.\n",
      "Ep done - 116440.\n",
      "Ep done - 116450.\n",
      "Ep done - 116460.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 533      |\n",
      "|    time_elapsed    | 11678    |\n",
      "|    total_timesteps | 3274752  |\n",
      "---------------------------------\n",
      "Ep done - 116470.\n",
      "Ep done - 116480.\n",
      "Ep done - 116490.\n",
      "Ep done - 116500.\n",
      "Ep done - 116510.\n",
      "Ep done - 116520.\n",
      "Ep done - 116530.\n",
      "Ep done - 116540.\n",
      "Ep done - 116550.\n",
      "Ep done - 116560.\n",
      "Ep done - 116570.\n",
      "Ep done - 116580.\n",
      "Ep done - 116590.\n",
      "Ep done - 116600.\n",
      "Ep done - 116610.\n",
      "Ep done - 116620.\n",
      "Ep done - 116630.\n",
      "Ep done - 32710.\n",
      "Ep done - 32720.\n",
      "Ep done - 32730.\n",
      "Ep done - 32740.\n",
      "Ep done - 32750.\n",
      "Ep done - 32760.\n",
      "Ep done - 32770.\n",
      "Ep done - 32780.\n",
      "Ep done - 32790.\n",
      "Ep done - 32800.\n",
      "Eval num_timesteps=3280000, episode_reward=-0.52 +/- 0.85\n",
      "Episode length: 29.99 +/- 0.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.52        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3280000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020623622 |\n",
      "|    clip_fraction        | 0.00103      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000941    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0871       |\n",
      "|    n_updates            | 24560        |\n",
      "|    policy_gradient_loss | 0.000135     |\n",
      "|    value_loss           | 0.198        |\n",
      "------------------------------------------\n",
      "Ep done - 116640.\n",
      "Ep done - 116650.\n",
      "Ep done - 116660.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 534      |\n",
      "|    time_elapsed    | 11697    |\n",
      "|    total_timesteps | 3280896  |\n",
      "---------------------------------\n",
      "Ep done - 116670.\n",
      "Ep done - 116680.\n",
      "Ep done - 116690.\n",
      "Ep done - 116700.\n",
      "Ep done - 116710.\n",
      "Ep done - 116720.\n",
      "Ep done - 116730.\n",
      "Ep done - 116740.\n",
      "Ep done - 116750.\n",
      "Ep done - 116760.\n",
      "Ep done - 116770.\n",
      "Ep done - 116780.\n",
      "Ep done - 116790.\n",
      "Ep done - 116800.\n",
      "Ep done - 116810.\n",
      "Ep done - 116820.\n",
      "Ep done - 116830.\n",
      "Ep done - 116840.\n",
      "Ep done - 116850.\n",
      "Ep done - 116860.\n",
      "Ep done - 116870.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.42      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 535        |\n",
      "|    time_elapsed         | 11712      |\n",
      "|    total_timesteps      | 3287040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22217627 |\n",
      "|    clip_fraction        | 0.0228     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000335  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0916     |\n",
      "|    n_updates            | 24570      |\n",
      "|    policy_gradient_loss | -0.00383   |\n",
      "|    value_loss           | 0.229      |\n",
      "----------------------------------------\n",
      "Ep done - 116880.\n",
      "Ep done - 116890.\n",
      "Ep done - 116900.\n",
      "Ep done - 116910.\n",
      "Ep done - 116920.\n",
      "Ep done - 116930.\n",
      "Ep done - 116940.\n",
      "Ep done - 116950.\n",
      "Ep done - 116960.\n",
      "Ep done - 116970.\n",
      "Ep done - 32810.\n",
      "Ep done - 32820.\n",
      "Ep done - 32830.\n",
      "Ep done - 32840.\n",
      "Ep done - 32850.\n",
      "Ep done - 32860.\n",
      "Ep done - 32870.\n",
      "Ep done - 32880.\n",
      "Ep done - 32890.\n",
      "Ep done - 32900.\n",
      "Eval num_timesteps=3290000, episode_reward=-0.58 +/- 0.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.58      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3290000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.51288295 |\n",
      "|    clip_fraction        | 0.0212     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000294  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.172      |\n",
      "|    n_updates            | 24580      |\n",
      "|    policy_gradient_loss | -0.00349   |\n",
      "|    value_loss           | 0.231      |\n",
      "----------------------------------------\n",
      "Ep done - 116980.\n",
      "Ep done - 116990.\n",
      "Ep done - 117000.\n",
      "Ep done - 117010.\n",
      "Ep done - 117020.\n",
      "Ep done - 117030.\n",
      "Ep done - 117040.\n",
      "Ep done - 117050.\n",
      "Ep done - 117060.\n",
      "Ep done - 117070.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 536      |\n",
      "|    time_elapsed    | 11730    |\n",
      "|    total_timesteps | 3293184  |\n",
      "---------------------------------\n",
      "Ep done - 117080.\n",
      "Ep done - 117090.\n",
      "Ep done - 117100.\n",
      "Ep done - 117110.\n",
      "Ep done - 117120.\n",
      "Ep done - 117130.\n",
      "Ep done - 117140.\n",
      "Ep done - 117150.\n",
      "Ep done - 117160.\n",
      "Ep done - 117170.\n",
      "Ep done - 117180.\n",
      "Ep done - 117190.\n",
      "Ep done - 117200.\n",
      "Ep done - 117210.\n",
      "Ep done - 117220.\n",
      "Ep done - 117230.\n",
      "Ep done - 117240.\n",
      "Ep done - 117250.\n",
      "Ep done - 117260.\n",
      "Ep done - 117270.\n",
      "Ep done - 117280.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -0.46        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 280          |\n",
      "|    iterations           | 537          |\n",
      "|    time_elapsed         | 11745        |\n",
      "|    total_timesteps      | 3299328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.803669e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000142    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.131        |\n",
      "|    n_updates            | 24590        |\n",
      "|    policy_gradient_loss | -7.51e-07    |\n",
      "|    value_loss           | 0.222        |\n",
      "------------------------------------------\n",
      "Ep done - 117290.\n",
      "Ep done - 117300.\n",
      "Ep done - 32910.\n",
      "Ep done - 32920.\n",
      "Ep done - 32930.\n",
      "Ep done - 32940.\n",
      "Ep done - 32950.\n",
      "Ep done - 32960.\n",
      "Ep done - 32970.\n",
      "Ep done - 32980.\n",
      "Ep done - 32990.\n",
      "Ep done - 33000.\n",
      "Eval num_timesteps=3300000, episode_reward=-0.48 +/- 0.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.48        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3300000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035880988 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000369    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.146        |\n",
      "|    n_updates            | 24600        |\n",
      "|    policy_gradient_loss | 6.76e-05     |\n",
      "|    value_loss           | 0.226        |\n",
      "------------------------------------------\n",
      "Ep done - 117310.\n",
      "Ep done - 117320.\n",
      "Ep done - 117330.\n",
      "Ep done - 117340.\n",
      "Ep done - 117350.\n",
      "Ep done - 117360.\n",
      "Ep done - 117370.\n",
      "Ep done - 117380.\n",
      "Ep done - 117390.\n",
      "Ep done - 117400.\n",
      "Ep done - 117410.\n",
      "Ep done - 117420.\n",
      "Ep done - 117430.\n",
      "Ep done - 117440.\n",
      "Ep done - 117450.\n",
      "Ep done - 117460.\n",
      "Ep done - 117470.\n",
      "Ep done - 117480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 538      |\n",
      "|    time_elapsed    | 11764    |\n",
      "|    total_timesteps | 3305472  |\n",
      "---------------------------------\n",
      "Ep done - 117490.\n",
      "Ep done - 117500.\n",
      "Ep done - 117510.\n",
      "Ep done - 117520.\n",
      "Ep done - 117530.\n",
      "Ep done - 117540.\n",
      "Ep done - 117550.\n",
      "Ep done - 117560.\n",
      "Ep done - 117570.\n",
      "Ep done - 117580.\n",
      "Ep done - 117590.\n",
      "Ep done - 117600.\n",
      "Ep done - 117610.\n",
      "Ep done - 117620.\n",
      "Ep done - 117630.\n",
      "Ep done - 33010.\n",
      "Ep done - 33020.\n",
      "Ep done - 33030.\n",
      "Ep done - 33040.\n",
      "Ep done - 33050.\n",
      "Ep done - 33060.\n",
      "Ep done - 33070.\n",
      "Ep done - 33080.\n",
      "Ep done - 33090.\n",
      "Ep done - 33100.\n",
      "Eval num_timesteps=3310000, episode_reward=-0.52 +/- 0.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.52        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3310000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.173086e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000112    |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0836       |\n",
      "|    n_updates            | 24610        |\n",
      "|    policy_gradient_loss | 7.52e-07     |\n",
      "|    value_loss           | 0.213        |\n",
      "------------------------------------------\n",
      "Ep done - 117640.\n",
      "Ep done - 117650.\n",
      "Ep done - 117660.\n",
      "Ep done - 117670.\n",
      "Ep done - 117680.\n",
      "Ep done - 117690.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 539      |\n",
      "|    time_elapsed    | 11787    |\n",
      "|    total_timesteps | 3311616  |\n",
      "---------------------------------\n",
      "Ep done - 117700.\n",
      "Ep done - 117710.\n",
      "Ep done - 117720.\n",
      "Ep done - 117730.\n",
      "Ep done - 117740.\n",
      "Ep done - 117750.\n",
      "Ep done - 117760.\n",
      "Ep done - 117770.\n",
      "Ep done - 117780.\n",
      "Ep done - 117790.\n",
      "Ep done - 117800.\n",
      "Ep done - 117810.\n",
      "Ep done - 117820.\n",
      "Ep done - 117830.\n",
      "Ep done - 117840.\n",
      "Ep done - 117850.\n",
      "Ep done - 117860.\n",
      "Ep done - 117870.\n",
      "Ep done - 117880.\n",
      "Ep done - 117890.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.66     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 540       |\n",
      "|    time_elapsed         | 11805     |\n",
      "|    total_timesteps      | 3317760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3460054 |\n",
      "|    clip_fraction        | 0.0218    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0003   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.125     |\n",
      "|    n_updates            | 24620     |\n",
      "|    policy_gradient_loss | -0.00337  |\n",
      "|    value_loss           | 0.226     |\n",
      "---------------------------------------\n",
      "Ep done - 117900.\n",
      "Ep done - 117910.\n",
      "Ep done - 117920.\n",
      "Ep done - 117930.\n",
      "Ep done - 117940.\n",
      "Ep done - 117950.\n",
      "Ep done - 117960.\n",
      "Ep done - 117970.\n",
      "Ep done - 33110.\n",
      "Ep done - 33120.\n",
      "Ep done - 33130.\n",
      "Ep done - 33140.\n",
      "Ep done - 33150.\n",
      "Ep done - 33160.\n",
      "Ep done - 33170.\n",
      "Ep done - 33180.\n",
      "Ep done - 33190.\n",
      "Ep done - 33200.\n",
      "Eval num_timesteps=3320000, episode_reward=-0.64 +/- 0.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.64     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3320000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8406752 |\n",
      "|    clip_fraction        | 0.0287    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000338 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0467    |\n",
      "|    n_updates            | 24630     |\n",
      "|    policy_gradient_loss | 0.00652   |\n",
      "|    value_loss           | 0.189     |\n",
      "---------------------------------------\n",
      "Ep done - 117980.\n",
      "Ep done - 117990.\n",
      "Ep done - 118000.\n",
      "Ep done - 118010.\n",
      "Ep done - 118020.\n",
      "Ep done - 118030.\n",
      "Ep done - 118040.\n",
      "Ep done - 118050.\n",
      "Ep done - 118060.\n",
      "Ep done - 118070.\n",
      "Ep done - 118080.\n",
      "Ep done - 118090.\n",
      "Ep done - 118100.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 541      |\n",
      "|    time_elapsed    | 11828    |\n",
      "|    total_timesteps | 3323904  |\n",
      "---------------------------------\n",
      "Ep done - 118110.\n",
      "Ep done - 118120.\n",
      "Ep done - 118130.\n",
      "Ep done - 118140.\n",
      "Ep done - 118150.\n",
      "Ep done - 118160.\n",
      "Ep done - 118170.\n",
      "Ep done - 118180.\n",
      "Ep done - 118190.\n",
      "Ep done - 118200.\n",
      "Ep done - 118210.\n",
      "Ep done - 118220.\n",
      "Ep done - 118230.\n",
      "Ep done - 118240.\n",
      "Ep done - 118250.\n",
      "Ep done - 118260.\n",
      "Ep done - 118270.\n",
      "Ep done - 118280.\n",
      "Ep done - 118290.\n",
      "Ep done - 118300.\n",
      "Ep done - 33210.\n",
      "Ep done - 33220.\n",
      "Ep done - 33230.\n",
      "Ep done - 33240.\n",
      "Ep done - 33250.\n",
      "Ep done - 33260.\n",
      "Ep done - 33270.\n",
      "Ep done - 33280.\n",
      "Ep done - 33290.\n",
      "Ep done - 33300.\n",
      "Eval num_timesteps=3330000, episode_reward=-0.56 +/- 0.83\n",
      "Episode length: 29.87 +/- 0.34\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.9      |\n",
      "|    mean_reward          | -0.56     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3330000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7602215 |\n",
      "|    clip_fraction        | 0.0235    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000183 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0659    |\n",
      "|    n_updates            | 24640     |\n",
      "|    policy_gradient_loss | -0.00425  |\n",
      "|    value_loss           | 0.173     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 542      |\n",
      "|    time_elapsed    | 11855    |\n",
      "|    total_timesteps | 3330048  |\n",
      "---------------------------------\n",
      "Ep done - 118310.\n",
      "Ep done - 118320.\n",
      "Ep done - 118330.\n",
      "Ep done - 118340.\n",
      "Ep done - 118350.\n",
      "Ep done - 118360.\n",
      "Ep done - 118370.\n",
      "Ep done - 118380.\n",
      "Ep done - 118390.\n",
      "Ep done - 118400.\n",
      "Ep done - 118410.\n",
      "Ep done - 118420.\n",
      "Ep done - 118430.\n",
      "Ep done - 118440.\n",
      "Ep done - 118450.\n",
      "Ep done - 118460.\n",
      "Ep done - 118470.\n",
      "Ep done - 118480.\n",
      "Ep done - 118490.\n",
      "Ep done - 118500.\n",
      "Ep done - 118510.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.62      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 543        |\n",
      "|    time_elapsed         | 11872      |\n",
      "|    total_timesteps      | 3336192    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68569326 |\n",
      "|    clip_fraction        | 0.0266     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000501  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.117      |\n",
      "|    n_updates            | 24650      |\n",
      "|    policy_gradient_loss | -0.0041    |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "Ep done - 118520.\n",
      "Ep done - 118530.\n",
      "Ep done - 118540.\n",
      "Ep done - 118550.\n",
      "Ep done - 118560.\n",
      "Ep done - 118570.\n",
      "Ep done - 118580.\n",
      "Ep done - 118590.\n",
      "Ep done - 118600.\n",
      "Ep done - 118610.\n",
      "Ep done - 118620.\n",
      "Ep done - 118630.\n",
      "Ep done - 33310.\n",
      "Ep done - 33320.\n",
      "Ep done - 33330.\n",
      "Ep done - 33340.\n",
      "Ep done - 33350.\n",
      "Ep done - 33360.\n",
      "Ep done - 33370.\n",
      "Ep done - 33380.\n",
      "Ep done - 33390.\n",
      "Ep done - 33400.\n",
      "Eval num_timesteps=3340000, episode_reward=-0.33 +/- 0.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.33      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3340000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08183053 |\n",
      "|    clip_fraction        | 0.00718    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000764  |\n",
      "|    explained_variance   | 5.96e-08   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.124      |\n",
      "|    n_updates            | 24660      |\n",
      "|    policy_gradient_loss | 0.00168    |\n",
      "|    value_loss           | 0.201      |\n",
      "----------------------------------------\n",
      "Ep done - 118640.\n",
      "Ep done - 118650.\n",
      "Ep done - 118660.\n",
      "Ep done - 118670.\n",
      "Ep done - 118680.\n",
      "Ep done - 118690.\n",
      "Ep done - 118700.\n",
      "Ep done - 118710.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 544      |\n",
      "|    time_elapsed    | 11893    |\n",
      "|    total_timesteps | 3342336  |\n",
      "---------------------------------\n",
      "Ep done - 118720.\n",
      "Ep done - 118730.\n",
      "Ep done - 118740.\n",
      "Ep done - 118750.\n",
      "Ep done - 118760.\n",
      "Ep done - 118770.\n",
      "Ep done - 118780.\n",
      "Ep done - 118790.\n",
      "Ep done - 118800.\n",
      "Ep done - 118810.\n",
      "Ep done - 118820.\n",
      "Ep done - 118830.\n",
      "Ep done - 118840.\n",
      "Ep done - 118850.\n",
      "Ep done - 118860.\n",
      "Ep done - 118870.\n",
      "Ep done - 118880.\n",
      "Ep done - 118890.\n",
      "Ep done - 118900.\n",
      "Ep done - 118910.\n",
      "Ep done - 118920.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -0.54       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 545         |\n",
      "|    time_elapsed         | 11909       |\n",
      "|    total_timesteps      | 3348480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020578383 |\n",
      "|    clip_fraction        | 0.000798    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000164   |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 24670       |\n",
      "|    policy_gradient_loss | -0.000179   |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Ep done - 118930.\n",
      "Ep done - 118940.\n",
      "Ep done - 118950.\n",
      "Ep done - 118960.\n",
      "Ep done - 118970.\n",
      "Ep done - 33410.\n",
      "Ep done - 33420.\n",
      "Ep done - 33430.\n",
      "Ep done - 33440.\n",
      "Ep done - 33450.\n",
      "Ep done - 33460.\n",
      "Ep done - 33470.\n",
      "Ep done - 33480.\n",
      "Ep done - 33490.\n",
      "Ep done - 33500.\n",
      "Eval num_timesteps=3350000, episode_reward=-0.64 +/- 0.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.64      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3350000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20232528 |\n",
      "|    clip_fraction        | 0.00876    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000193  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.129      |\n",
      "|    n_updates            | 24680      |\n",
      "|    policy_gradient_loss | 0.000314   |\n",
      "|    value_loss           | 0.216      |\n",
      "----------------------------------------\n",
      "Ep done - 118980.\n",
      "Ep done - 118990.\n",
      "Ep done - 119000.\n",
      "Ep done - 119010.\n",
      "Ep done - 119020.\n",
      "Ep done - 119030.\n",
      "Ep done - 119040.\n",
      "Ep done - 119050.\n",
      "Ep done - 119060.\n",
      "Ep done - 119070.\n",
      "Ep done - 119080.\n",
      "Ep done - 119090.\n",
      "Ep done - 119100.\n",
      "Ep done - 119110.\n",
      "Ep done - 119120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 546      |\n",
      "|    time_elapsed    | 11930    |\n",
      "|    total_timesteps | 3354624  |\n",
      "---------------------------------\n",
      "Ep done - 119130.\n",
      "Ep done - 119140.\n",
      "Ep done - 119150.\n",
      "Ep done - 119160.\n",
      "Ep done - 119170.\n",
      "Ep done - 119180.\n",
      "Ep done - 119190.\n",
      "Ep done - 119200.\n",
      "Ep done - 119210.\n",
      "Ep done - 119220.\n",
      "Ep done - 119230.\n",
      "Ep done - 119240.\n",
      "Ep done - 119250.\n",
      "Ep done - 119260.\n",
      "Ep done - 119270.\n",
      "Ep done - 119280.\n",
      "Ep done - 119290.\n",
      "Ep done - 119300.\n",
      "Ep done - 33510.\n",
      "Ep done - 33520.\n",
      "Ep done - 33530.\n",
      "Ep done - 33540.\n",
      "Ep done - 33550.\n",
      "Ep done - 33560.\n",
      "Ep done - 33570.\n",
      "Ep done - 33580.\n",
      "Ep done - 33590.\n",
      "Ep done - 33600.\n",
      "Eval num_timesteps=3360000, episode_reward=-0.53 +/- 0.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.53      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3360000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05797274 |\n",
      "|    clip_fraction        | 0.00461    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000433  |\n",
      "|    explained_variance   | 1.79e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.148      |\n",
      "|    n_updates            | 24690      |\n",
      "|    policy_gradient_loss | -0.000685  |\n",
      "|    value_loss           | 0.242      |\n",
      "----------------------------------------\n",
      "Ep done - 119310.\n",
      "Ep done - 119320.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 547      |\n",
      "|    time_elapsed    | 11952    |\n",
      "|    total_timesteps | 3360768  |\n",
      "---------------------------------\n",
      "Ep done - 119330.\n",
      "Ep done - 119340.\n",
      "Ep done - 119350.\n",
      "Ep done - 119360.\n",
      "Ep done - 119370.\n",
      "Ep done - 119380.\n",
      "Ep done - 119390.\n",
      "Ep done - 119400.\n",
      "Ep done - 119410.\n",
      "Ep done - 119420.\n",
      "Ep done - 119430.\n",
      "Ep done - 119440.\n",
      "Ep done - 119450.\n",
      "Ep done - 119460.\n",
      "Ep done - 119470.\n",
      "Ep done - 119480.\n",
      "Ep done - 119490.\n",
      "Ep done - 119500.\n",
      "Ep done - 119510.\n",
      "Ep done - 119520.\n",
      "Ep done - 119530.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -0.55        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 548          |\n",
      "|    time_elapsed         | 11968        |\n",
      "|    total_timesteps      | 3366912      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037118515 |\n",
      "|    clip_fraction        | 0.000944     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000902    |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.137        |\n",
      "|    n_updates            | 24700        |\n",
      "|    policy_gradient_loss | 0.000317     |\n",
      "|    value_loss           | 0.202        |\n",
      "------------------------------------------\n",
      "Ep done - 119540.\n",
      "Ep done - 119550.\n",
      "Ep done - 119560.\n",
      "Ep done - 119570.\n",
      "Ep done - 119580.\n",
      "Ep done - 119590.\n",
      "Ep done - 119600.\n",
      "Ep done - 119610.\n",
      "Ep done - 119620.\n",
      "Ep done - 119630.\n",
      "Ep done - 33610.\n",
      "Ep done - 33620.\n",
      "Ep done - 33630.\n",
      "Ep done - 33640.\n",
      "Ep done - 33650.\n",
      "Ep done - 33660.\n",
      "Ep done - 33670.\n",
      "Ep done - 33680.\n",
      "Ep done - 33690.\n",
      "Ep done - 33700.\n",
      "Eval num_timesteps=3370000, episode_reward=-0.56 +/- 0.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -0.56         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 3370000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034413792 |\n",
      "|    clip_fraction        | 0.0012        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000899     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.067         |\n",
      "|    n_updates            | 24710         |\n",
      "|    policy_gradient_loss | 0.000144      |\n",
      "|    value_loss           | 0.196         |\n",
      "-------------------------------------------\n",
      "Ep done - 119640.\n",
      "Ep done - 119650.\n",
      "Ep done - 119660.\n",
      "Ep done - 119670.\n",
      "Ep done - 119680.\n",
      "Ep done - 119690.\n",
      "Ep done - 119700.\n",
      "Ep done - 119710.\n",
      "Ep done - 119720.\n",
      "Ep done - 119730.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 549      |\n",
      "|    time_elapsed    | 11989    |\n",
      "|    total_timesteps | 3373056  |\n",
      "---------------------------------\n",
      "Ep done - 119740.\n",
      "Ep done - 119750.\n",
      "Ep done - 119760.\n",
      "Ep done - 119770.\n",
      "Ep done - 119780.\n",
      "Ep done - 119790.\n",
      "Ep done - 119800.\n",
      "Ep done - 119810.\n",
      "Ep done - 119820.\n",
      "Ep done - 119830.\n",
      "Ep done - 119840.\n",
      "Ep done - 119850.\n",
      "Ep done - 119860.\n",
      "Ep done - 119870.\n",
      "Ep done - 119880.\n",
      "Ep done - 119890.\n",
      "Ep done - 119900.\n",
      "Ep done - 119910.\n",
      "Ep done - 119920.\n",
      "Ep done - 119930.\n",
      "Ep done - 119940.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -0.57       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 550         |\n",
      "|    time_elapsed         | 12006       |\n",
      "|    total_timesteps      | 3379200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003793889 |\n",
      "|    clip_fraction        | 0.000586    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000454   |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 24720       |\n",
      "|    policy_gradient_loss | 0.000304    |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Ep done - 119950.\n",
      "Ep done - 119960.\n",
      "Ep done - 119970.\n",
      "Ep done - 33710.\n",
      "Ep done - 33720.\n",
      "Ep done - 33730.\n",
      "Ep done - 33740.\n",
      "Ep done - 33750.\n",
      "Ep done - 33760.\n",
      "Ep done - 33770.\n",
      "Ep done - 33780.\n",
      "Ep done - 33790.\n",
      "Ep done - 33800.\n",
      "Eval num_timesteps=3380000, episode_reward=-0.54 +/- 0.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | -0.54       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005013209 |\n",
      "|    clip_fraction        | 0.00308     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000969   |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 24730       |\n",
      "|    policy_gradient_loss | 0.000686    |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "Ep done - 119980.\n",
      "Ep done - 119990.\n",
      "Ep done - 120000.\n",
      "Ep done - 120010.\n",
      "Ep done - 120020.\n",
      "Ep done - 120030.\n",
      "Ep done - 120040.\n",
      "Ep done - 120050.\n",
      "Ep done - 120060.\n",
      "Ep done - 120070.\n",
      "Ep done - 120080.\n",
      "Ep done - 120090.\n",
      "Ep done - 120100.\n",
      "Ep done - 120110.\n",
      "Ep done - 120120.\n",
      "Ep done - 120130.\n",
      "Ep done - 120140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 551      |\n",
      "|    time_elapsed    | 12033    |\n",
      "|    total_timesteps | 3385344  |\n",
      "---------------------------------\n",
      "Ep done - 120150.\n",
      "Ep done - 120160.\n",
      "Ep done - 120170.\n",
      "Ep done - 120180.\n",
      "Ep done - 120190.\n",
      "Ep done - 120200.\n",
      "Ep done - 120210.\n",
      "Ep done - 120220.\n",
      "Ep done - 120230.\n",
      "Ep done - 120240.\n",
      "Ep done - 120250.\n",
      "Ep done - 120260.\n",
      "Ep done - 120270.\n",
      "Ep done - 120280.\n",
      "Ep done - 120290.\n",
      "Ep done - 120300.\n",
      "Ep done - 33810.\n",
      "Ep done - 33820.\n",
      "Ep done - 33830.\n",
      "Ep done - 33840.\n",
      "Ep done - 33850.\n",
      "Ep done - 33860.\n",
      "Ep done - 33870.\n",
      "Ep done - 33880.\n",
      "Ep done - 33890.\n",
      "Ep done - 33900.\n",
      "Eval num_timesteps=3390000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3390000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81157225 |\n",
      "|    clip_fraction        | 0.0231     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000219  |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.123      |\n",
      "|    n_updates            | 24740      |\n",
      "|    policy_gradient_loss | 0.00875    |\n",
      "|    value_loss           | 0.211      |\n",
      "----------------------------------------\n",
      "Ep done - 120310.\n",
      "Ep done - 120320.\n",
      "Ep done - 120330.\n",
      "Ep done - 120340.\n",
      "Ep done - 120350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 552      |\n",
      "|    time_elapsed    | 12056    |\n",
      "|    total_timesteps | 3391488  |\n",
      "---------------------------------\n",
      "Ep done - 120360.\n",
      "Ep done - 120370.\n",
      "Ep done - 120380.\n",
      "Ep done - 120390.\n",
      "Ep done - 120400.\n",
      "Ep done - 120410.\n",
      "Ep done - 120420.\n",
      "Ep done - 120430.\n",
      "Ep done - 120440.\n",
      "Ep done - 120450.\n",
      "Ep done - 120460.\n",
      "Ep done - 120470.\n",
      "Ep done - 120480.\n",
      "Ep done - 120490.\n",
      "Ep done - 120500.\n",
      "Ep done - 120510.\n",
      "Ep done - 120520.\n",
      "Ep done - 120530.\n",
      "Ep done - 120540.\n",
      "Ep done - 120550.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 553       |\n",
      "|    time_elapsed         | 12074     |\n",
      "|    total_timesteps      | 3397632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1991962 |\n",
      "|    clip_fraction        | 0.0515    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000235 |\n",
      "|    explained_variance   | 4.17e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00614   |\n",
      "|    n_updates            | 24750     |\n",
      "|    policy_gradient_loss | -0.0109   |\n",
      "|    value_loss           | 0.0313    |\n",
      "---------------------------------------\n",
      "Ep done - 120560.\n",
      "Ep done - 120570.\n",
      "Ep done - 120580.\n",
      "Ep done - 120590.\n",
      "Ep done - 120600.\n",
      "Ep done - 120610.\n",
      "Ep done - 120620.\n",
      "Ep done - 120630.\n",
      "Ep done - 33910.\n",
      "Ep done - 33920.\n",
      "Ep done - 33930.\n",
      "Ep done - 33940.\n",
      "Ep done - 33950.\n",
      "Ep done - 33960.\n",
      "Ep done - 33970.\n",
      "Ep done - 33980.\n",
      "Ep done - 33990.\n",
      "Ep done - 34000.\n",
      "Eval num_timesteps=3400000, episode_reward=-0.74 +/- 0.67\n",
      "Episode length: 29.99 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.74     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3400000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9660238 |\n",
      "|    clip_fraction        | 0.0597    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00022  |\n",
      "|    explained_variance   | 1.79e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00299   |\n",
      "|    n_updates            | 24760     |\n",
      "|    policy_gradient_loss | -0.0146   |\n",
      "|    value_loss           | 0.0241    |\n",
      "---------------------------------------\n",
      "Ep done - 120640.\n",
      "Ep done - 120650.\n",
      "Ep done - 120660.\n",
      "Ep done - 120670.\n",
      "Ep done - 120680.\n",
      "Ep done - 120690.\n",
      "Ep done - 120700.\n",
      "Ep done - 120710.\n",
      "Ep done - 120720.\n",
      "Ep done - 120730.\n",
      "Ep done - 120740.\n",
      "Ep done - 120750.\n",
      "Ep done - 120760.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 554      |\n",
      "|    time_elapsed    | 12113    |\n",
      "|    total_timesteps | 3403776  |\n",
      "---------------------------------\n",
      "Ep done - 120770.\n",
      "Ep done - 120780.\n",
      "Ep done - 120790.\n",
      "Ep done - 120800.\n",
      "Ep done - 120810.\n",
      "Ep done - 120820.\n",
      "Ep done - 120830.\n",
      "Ep done - 120840.\n",
      "Ep done - 120850.\n",
      "Ep done - 120860.\n",
      "Ep done - 120870.\n",
      "Ep done - 120880.\n",
      "Ep done - 120890.\n",
      "Ep done - 120900.\n",
      "Ep done - 120910.\n",
      "Ep done - 120920.\n",
      "Ep done - 120930.\n",
      "Ep done - 120940.\n",
      "Ep done - 120950.\n",
      "Ep done - 120960.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.96       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 555        |\n",
      "|    time_elapsed         | 12132      |\n",
      "|    total_timesteps      | 3409920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35234728 |\n",
      "|    clip_fraction        | 0.02       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000213  |\n",
      "|    explained_variance   | 4.17e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0369     |\n",
      "|    n_updates            | 24770      |\n",
      "|    policy_gradient_loss | -0.000937  |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Ep done - 120970.\n",
      "Ep done - 34010.\n",
      "Ep done - 34020.\n",
      "Ep done - 34030.\n",
      "Ep done - 34040.\n",
      "Ep done - 34050.\n",
      "Ep done - 34060.\n",
      "Ep done - 34070.\n",
      "Ep done - 34080.\n",
      "Ep done - 34090.\n",
      "Ep done - 34100.\n",
      "Eval num_timesteps=3410000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.08 +/- 0.31\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3410000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.7987354 |\n",
      "|    clip_fraction        | 0.0594    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000554 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.109     |\n",
      "|    n_updates            | 24780     |\n",
      "|    policy_gradient_loss | -0.005    |\n",
      "|    value_loss           | 0.19      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 110\n",
      "Ep done - 120980.\n",
      "Ep done - 120990.\n",
      "Ep done - 121000.\n",
      "Ep done - 121010.\n",
      "Ep done - 121020.\n",
      "Ep done - 121030.\n",
      "Ep done - 121040.\n",
      "Ep done - 121050.\n",
      "Ep done - 121060.\n",
      "Ep done - 121070.\n",
      "Ep done - 121080.\n",
      "Ep done - 121090.\n",
      "Ep done - 121100.\n",
      "Ep done - 121110.\n",
      "Ep done - 121120.\n",
      "Ep done - 121130.\n",
      "Ep done - 121140.\n",
      "Ep done - 121150.\n",
      "Ep done - 121160.\n",
      "Ep done - 121170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 556      |\n",
      "|    time_elapsed    | 12153    |\n",
      "|    total_timesteps | 3416064  |\n",
      "---------------------------------\n",
      "Ep done - 121180.\n",
      "Ep done - 121190.\n",
      "Ep done - 121200.\n",
      "Ep done - 121210.\n",
      "Ep done - 121220.\n",
      "Ep done - 121230.\n",
      "Ep done - 121240.\n",
      "Ep done - 121250.\n",
      "Ep done - 121260.\n",
      "Ep done - 121270.\n",
      "Ep done - 121280.\n",
      "Ep done - 121290.\n",
      "Ep done - 121300.\n",
      "Ep done - 34110.\n",
      "Ep done - 34120.\n",
      "Ep done - 34130.\n",
      "Ep done - 34140.\n",
      "Ep done - 34150.\n",
      "Ep done - 34160.\n",
      "Ep done - 34170.\n",
      "Ep done - 34180.\n",
      "Ep done - 34190.\n",
      "Ep done - 34200.\n",
      "Eval num_timesteps=3420000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 30.11 +/- 0.31\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3420000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.1477127 |\n",
      "|    clip_fraction        | 0.0702    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00131  |\n",
      "|    explained_variance   | 2.38e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0258    |\n",
      "|    n_updates            | 24790     |\n",
      "|    policy_gradient_loss | 0.244     |\n",
      "|    value_loss           | 0.0917    |\n",
      "---------------------------------------\n",
      "Ep done - 121310.\n",
      "Ep done - 121320.\n",
      "Ep done - 121330.\n",
      "Ep done - 121340.\n",
      "Ep done - 121350.\n",
      "Ep done - 121360.\n",
      "Ep done - 121370.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 557      |\n",
      "|    time_elapsed    | 12174    |\n",
      "|    total_timesteps | 3422208  |\n",
      "---------------------------------\n",
      "Ep done - 121380.\n",
      "Ep done - 121390.\n",
      "Ep done - 121400.\n",
      "Ep done - 121410.\n",
      "Ep done - 121420.\n",
      "Ep done - 121430.\n",
      "Ep done - 121440.\n",
      "Ep done - 121450.\n",
      "Ep done - 121460.\n",
      "Ep done - 121470.\n",
      "Ep done - 121480.\n",
      "Ep done - 121490.\n",
      "Ep done - 121500.\n",
      "Ep done - 121510.\n",
      "Ep done - 121520.\n",
      "Ep done - 121530.\n",
      "Ep done - 121540.\n",
      "Ep done - 121550.\n",
      "Ep done - 121560.\n",
      "Ep done - 121570.\n",
      "Ep done - 121580.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.08       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 558        |\n",
      "|    time_elapsed         | 12192      |\n",
      "|    total_timesteps      | 3428352    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29257622 |\n",
      "|    clip_fraction        | 0.0398     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000963  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.146      |\n",
      "|    n_updates            | 24800      |\n",
      "|    policy_gradient_loss | 0.00846    |\n",
      "|    value_loss           | 0.3        |\n",
      "----------------------------------------\n",
      "Ep done - 121590.\n",
      "Ep done - 121600.\n",
      "Ep done - 121610.\n",
      "Ep done - 121620.\n",
      "Ep done - 121630.\n",
      "Ep done - 34210.\n",
      "Ep done - 34220.\n",
      "Ep done - 34230.\n",
      "Ep done - 34240.\n",
      "Ep done - 34250.\n",
      "Ep done - 34260.\n",
      "Ep done - 34270.\n",
      "Ep done - 34280.\n",
      "Ep done - 34290.\n",
      "Ep done - 34300.\n",
      "Eval num_timesteps=3430000, episode_reward=-0.69 +/- 0.48\n",
      "Episode length: 29.97 +/- 0.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.69     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3430000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6615119 |\n",
      "|    clip_fraction        | 0.0319    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00088  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.167     |\n",
      "|    n_updates            | 24810     |\n",
      "|    policy_gradient_loss | 0.00474   |\n",
      "|    value_loss           | 0.292     |\n",
      "---------------------------------------\n",
      "Ep done - 121640.\n",
      "Ep done - 121650.\n",
      "Ep done - 121660.\n",
      "Ep done - 121670.\n",
      "Ep done - 121680.\n",
      "Ep done - 121690.\n",
      "Ep done - 121700.\n",
      "Ep done - 121710.\n",
      "Ep done - 121720.\n",
      "Ep done - 121730.\n",
      "Ep done - 121740.\n",
      "Ep done - 121750.\n",
      "Ep done - 121760.\n",
      "Ep done - 121770.\n",
      "Ep done - 121780.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 559      |\n",
      "|    time_elapsed    | 12213    |\n",
      "|    total_timesteps | 3434496  |\n",
      "---------------------------------\n",
      "Ep done - 121790.\n",
      "Ep done - 121800.\n",
      "Ep done - 121810.\n",
      "Ep done - 121820.\n",
      "Ep done - 121830.\n",
      "Ep done - 121840.\n",
      "Ep done - 121850.\n",
      "Ep done - 121860.\n",
      "Ep done - 121870.\n",
      "Ep done - 121880.\n",
      "Ep done - 121890.\n",
      "Ep done - 121900.\n",
      "Ep done - 121910.\n",
      "Ep done - 121920.\n",
      "Ep done - 121930.\n",
      "Ep done - 121940.\n",
      "Ep done - 121950.\n",
      "Ep done - 121960.\n",
      "Ep done - 34310.\n",
      "Ep done - 34320.\n",
      "Ep done - 34330.\n",
      "Ep done - 34340.\n",
      "Ep done - 34350.\n",
      "Ep done - 34360.\n",
      "Ep done - 34370.\n",
      "Ep done - 34380.\n",
      "Ep done - 34390.\n",
      "Ep done - 34400.\n",
      "Eval num_timesteps=3440000, episode_reward=-0.45 +/- 0.70\n",
      "Episode length: 30.02 +/- 0.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.45     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3440000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0582525 |\n",
      "|    clip_fraction        | 0.0603    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000927 |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0291    |\n",
      "|    n_updates            | 24820     |\n",
      "|    policy_gradient_loss | -0.00928  |\n",
      "|    value_loss           | 0.0868    |\n",
      "---------------------------------------\n",
      "Ep done - 121970.\n",
      "Ep done - 121980.\n",
      "Ep done - 121990.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 560      |\n",
      "|    time_elapsed    | 12237    |\n",
      "|    total_timesteps | 3440640  |\n",
      "---------------------------------\n",
      "Ep done - 122000.\n",
      "Ep done - 122010.\n",
      "Ep done - 122020.\n",
      "Ep done - 122030.\n",
      "Ep done - 122040.\n",
      "Ep done - 122050.\n",
      "Ep done - 122060.\n",
      "Ep done - 122070.\n",
      "Ep done - 122080.\n",
      "Ep done - 122090.\n",
      "Ep done - 122100.\n",
      "Ep done - 122110.\n",
      "Ep done - 122120.\n",
      "Ep done - 122130.\n",
      "Ep done - 122140.\n",
      "Ep done - 122150.\n",
      "Ep done - 122160.\n",
      "Ep done - 122170.\n",
      "Ep done - 122180.\n",
      "Ep done - 122190.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | 0.35       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 561        |\n",
      "|    time_elapsed         | 12253      |\n",
      "|    total_timesteps      | 3446784    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82749104 |\n",
      "|    clip_fraction        | 0.0565     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00108   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.117      |\n",
      "|    n_updates            | 24830      |\n",
      "|    policy_gradient_loss | 0.00137    |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "Ep done - 122200.\n",
      "Ep done - 122210.\n",
      "Ep done - 122220.\n",
      "Ep done - 122230.\n",
      "Ep done - 122240.\n",
      "Ep done - 122250.\n",
      "Ep done - 122260.\n",
      "Ep done - 122270.\n",
      "Ep done - 122280.\n",
      "Ep done - 122290.\n",
      "Ep done - 122300.\n",
      "Ep done - 34410.\n",
      "Ep done - 34420.\n",
      "Ep done - 34430.\n",
      "Ep done - 34440.\n",
      "Ep done - 34450.\n",
      "Ep done - 34460.\n",
      "Ep done - 34470.\n",
      "Ep done - 34480.\n",
      "Ep done - 34490.\n",
      "Ep done - 34500.\n",
      "Eval num_timesteps=3450000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.84 +/- 0.37\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.8     |\n",
      "|    mean_reward          | 1        |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 3450000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 4.101068 |\n",
      "|    clip_fraction        | 0.199    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00279 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0396   |\n",
      "|    n_updates            | 24840    |\n",
      "|    policy_gradient_loss | -0.015   |\n",
      "|    value_loss           | 0.147    |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 111\n",
      "Ep done - 122310.\n",
      "Ep done - 122320.\n",
      "Ep done - 122330.\n",
      "Ep done - 122340.\n",
      "Ep done - 122350.\n",
      "Ep done - 122360.\n",
      "Ep done - 122370.\n",
      "Ep done - 122380.\n",
      "Ep done - 122390.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 562      |\n",
      "|    time_elapsed    | 12273    |\n",
      "|    total_timesteps | 3452928  |\n",
      "---------------------------------\n",
      "Ep done - 122400.\n",
      "Ep done - 122410.\n",
      "Ep done - 122420.\n",
      "Ep done - 122430.\n",
      "Ep done - 122440.\n",
      "Ep done - 122450.\n",
      "Ep done - 122460.\n",
      "Ep done - 122470.\n",
      "Ep done - 122480.\n",
      "Ep done - 122490.\n",
      "Ep done - 122500.\n",
      "Ep done - 122510.\n",
      "Ep done - 122520.\n",
      "Ep done - 122530.\n",
      "Ep done - 122540.\n",
      "Ep done - 122550.\n",
      "Ep done - 122560.\n",
      "Ep done - 122570.\n",
      "Ep done - 122580.\n",
      "Ep done - 122590.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.6      |\n",
      "|    ep_rew_mean          | 0.5       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 563       |\n",
      "|    time_elapsed         | 12289     |\n",
      "|    total_timesteps      | 3459072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 12.343289 |\n",
      "|    clip_fraction        | 0.325     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00912  |\n",
      "|    explained_variance   | -5.96e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0151   |\n",
      "|    n_updates            | 24850     |\n",
      "|    policy_gradient_loss | 0.0508    |\n",
      "|    value_loss           | 0.0768    |\n",
      "---------------------------------------\n",
      "Ep done - 122600.\n",
      "Ep done - 122610.\n",
      "Ep done - 122620.\n",
      "Ep done - 34510.\n",
      "Ep done - 34520.\n",
      "Ep done - 34530.\n",
      "Ep done - 34540.\n",
      "Ep done - 34550.\n",
      "Ep done - 34560.\n",
      "Ep done - 34570.\n",
      "Ep done - 34580.\n",
      "Ep done - 34590.\n",
      "Ep done - 34600.\n",
      "Eval num_timesteps=3460000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.68 +/- 0.47\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.7      |\n",
      "|    mean_reward          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3460000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3019532 |\n",
      "|    clip_fraction        | 0.0229    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00185  |\n",
      "|    explained_variance   | 1.02e-05  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0962    |\n",
      "|    n_updates            | 24860     |\n",
      "|    policy_gradient_loss | -0.00304  |\n",
      "|    value_loss           | 0.211     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 112\n",
      "Ep done - 122630.\n",
      "Ep done - 122640.\n",
      "Ep done - 122650.\n",
      "Ep done - 122660.\n",
      "Ep done - 122670.\n",
      "Ep done - 122680.\n",
      "Ep done - 122690.\n",
      "Ep done - 122700.\n",
      "Ep done - 122710.\n",
      "Ep done - 122720.\n",
      "Ep done - 122730.\n",
      "Ep done - 122740.\n",
      "Ep done - 122750.\n",
      "Ep done - 122760.\n",
      "Ep done - 122770.\n",
      "Ep done - 122780.\n",
      "Ep done - 122790.\n",
      "Ep done - 122800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 564      |\n",
      "|    time_elapsed    | 12309    |\n",
      "|    total_timesteps | 3465216  |\n",
      "---------------------------------\n",
      "Ep done - 122810.\n",
      "Ep done - 122820.\n",
      "Ep done - 122830.\n",
      "Ep done - 122840.\n",
      "Ep done - 122850.\n",
      "Ep done - 122860.\n",
      "Ep done - 122870.\n",
      "Ep done - 122880.\n",
      "Ep done - 122890.\n",
      "Ep done - 122900.\n",
      "Ep done - 122910.\n",
      "Ep done - 122920.\n",
      "Ep done - 122930.\n",
      "Ep done - 122940.\n",
      "Ep done - 122950.\n",
      "Ep done - 122960.\n",
      "Ep done - 34610.\n",
      "Ep done - 34620.\n",
      "Ep done - 34630.\n",
      "Ep done - 34640.\n",
      "Ep done - 34650.\n",
      "Ep done - 34660.\n",
      "Ep done - 34670.\n",
      "Ep done - 34680.\n",
      "Ep done - 34690.\n",
      "Ep done - 34700.\n",
      "Eval num_timesteps=3470000, episode_reward=-0.28 +/- 0.96\n",
      "Episode length: 29.99 +/- 0.10\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.28    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 3470000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.231659 |\n",
      "|    clip_fraction        | 0.0948   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00406 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.159    |\n",
      "|    n_updates            | 24870    |\n",
      "|    policy_gradient_loss | 0.0291   |\n",
      "|    value_loss           | 0.298    |\n",
      "--------------------------------------\n",
      "Ep done - 122970.\n",
      "Ep done - 122980.\n",
      "Ep done - 122990.\n",
      "Ep done - 123000.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 565      |\n",
      "|    time_elapsed    | 12330    |\n",
      "|    total_timesteps | 3471360  |\n",
      "---------------------------------\n",
      "Ep done - 123010.\n",
      "Ep done - 123020.\n",
      "Ep done - 123030.\n",
      "Ep done - 123040.\n",
      "Ep done - 123050.\n",
      "Ep done - 123060.\n",
      "Ep done - 123070.\n",
      "Ep done - 123080.\n",
      "Ep done - 123090.\n",
      "Ep done - 123100.\n",
      "Ep done - 123110.\n",
      "Ep done - 123120.\n",
      "Ep done - 123130.\n",
      "Ep done - 123140.\n",
      "Ep done - 123150.\n",
      "Ep done - 123160.\n",
      "Ep done - 123170.\n",
      "Ep done - 123180.\n",
      "Ep done - 123190.\n",
      "Ep done - 123200.\n",
      "Ep done - 123210.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.02      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 566       |\n",
      "|    time_elapsed         | 12345     |\n",
      "|    total_timesteps      | 3477504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0728409 |\n",
      "|    clip_fraction        | 0.027     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000771 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.159     |\n",
      "|    n_updates            | 24880     |\n",
      "|    policy_gradient_loss | 0.0176    |\n",
      "|    value_loss           | 0.298     |\n",
      "---------------------------------------\n",
      "Ep done - 123220.\n",
      "Ep done - 123230.\n",
      "Ep done - 123240.\n",
      "Ep done - 123250.\n",
      "Ep done - 123260.\n",
      "Ep done - 123270.\n",
      "Ep done - 123280.\n",
      "Ep done - 123290.\n",
      "Ep done - 34710.\n",
      "Ep done - 34720.\n",
      "Ep done - 34730.\n",
      "Ep done - 34740.\n",
      "Ep done - 34750.\n",
      "Ep done - 34760.\n",
      "Ep done - 34770.\n",
      "Ep done - 34780.\n",
      "Ep done - 34790.\n",
      "Ep done - 34800.\n",
      "Eval num_timesteps=3480000, episode_reward=-0.22 +/- 0.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.22     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3480000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6909351 |\n",
      "|    clip_fraction        | 0.0277    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000782 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.154     |\n",
      "|    n_updates            | 24890     |\n",
      "|    policy_gradient_loss | 0.000481  |\n",
      "|    value_loss           | 0.296     |\n",
      "---------------------------------------\n",
      "Ep done - 123300.\n",
      "Ep done - 123310.\n",
      "Ep done - 123320.\n",
      "Ep done - 123330.\n",
      "Ep done - 123340.\n",
      "Ep done - 123350.\n",
      "Ep done - 123360.\n",
      "Ep done - 123370.\n",
      "Ep done - 123380.\n",
      "Ep done - 123390.\n",
      "Ep done - 123400.\n",
      "Ep done - 123410.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 567      |\n",
      "|    time_elapsed    | 12365    |\n",
      "|    total_timesteps | 3483648  |\n",
      "---------------------------------\n",
      "Ep done - 123420.\n",
      "Ep done - 123430.\n",
      "Ep done - 123440.\n",
      "Ep done - 123450.\n",
      "Ep done - 123460.\n",
      "Ep done - 123470.\n",
      "Ep done - 123480.\n",
      "Ep done - 123490.\n",
      "Ep done - 123500.\n",
      "Ep done - 123510.\n",
      "Ep done - 123520.\n",
      "Ep done - 123530.\n",
      "Ep done - 123540.\n",
      "Ep done - 123550.\n",
      "Ep done - 123560.\n",
      "Ep done - 123570.\n",
      "Ep done - 123580.\n",
      "Ep done - 123590.\n",
      "Ep done - 123600.\n",
      "Ep done - 123610.\n",
      "Ep done - 123620.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.22      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 568        |\n",
      "|    time_elapsed         | 12381      |\n",
      "|    total_timesteps      | 3489792    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22512591 |\n",
      "|    clip_fraction        | 0.0201     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00101   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.144      |\n",
      "|    n_updates            | 24900      |\n",
      "|    policy_gradient_loss | -0.00292   |\n",
      "|    value_loss           | 0.286      |\n",
      "----------------------------------------\n",
      "Ep done - 34810.\n",
      "Ep done - 34820.\n",
      "Ep done - 34830.\n",
      "Ep done - 34840.\n",
      "Ep done - 34850.\n",
      "Ep done - 34860.\n",
      "Ep done - 34870.\n",
      "Ep done - 34880.\n",
      "Ep done - 34890.\n",
      "Ep done - 34900.\n",
      "Eval num_timesteps=3490000, episode_reward=-0.22 +/- 0.98\n",
      "Episode length: 30.01 +/- 0.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -0.22        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3490000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0136242835 |\n",
      "|    clip_fraction        | 0.0022       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000504    |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.104        |\n",
      "|    n_updates            | 24910        |\n",
      "|    policy_gradient_loss | -0.000274    |\n",
      "|    value_loss           | 0.278        |\n",
      "------------------------------------------\n",
      "Ep done - 123630.\n",
      "Ep done - 123640.\n",
      "Ep done - 123650.\n",
      "Ep done - 123660.\n",
      "Ep done - 123670.\n",
      "Ep done - 123680.\n",
      "Ep done - 123690.\n",
      "Ep done - 123700.\n",
      "Ep done - 123710.\n",
      "Ep done - 123720.\n",
      "Ep done - 123730.\n",
      "Ep done - 123740.\n",
      "Ep done - 123750.\n",
      "Ep done - 123760.\n",
      "Ep done - 123770.\n",
      "Ep done - 123780.\n",
      "Ep done - 123790.\n",
      "Ep done - 123800.\n",
      "Ep done - 123810.\n",
      "Ep done - 123820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 569      |\n",
      "|    time_elapsed    | 12401    |\n",
      "|    total_timesteps | 3495936  |\n",
      "---------------------------------\n",
      "Ep done - 123830.\n",
      "Ep done - 123840.\n",
      "Ep done - 123850.\n",
      "Ep done - 123860.\n",
      "Ep done - 123870.\n",
      "Ep done - 123880.\n",
      "Ep done - 123890.\n",
      "Ep done - 123900.\n",
      "Ep done - 123910.\n",
      "Ep done - 123920.\n",
      "Ep done - 123930.\n",
      "Ep done - 123940.\n",
      "Ep done - 123950.\n",
      "Ep done - 123960.\n",
      "Ep done - 34910.\n",
      "Ep done - 34920.\n",
      "Ep done - 34930.\n",
      "Ep done - 34940.\n",
      "Ep done - 34950.\n",
      "Ep done - 34960.\n",
      "Ep done - 34970.\n",
      "Ep done - 34980.\n",
      "Ep done - 34990.\n",
      "Ep done - 35000.\n",
      "Eval num_timesteps=3500000, episode_reward=-0.10 +/- 0.99\n",
      "Episode length: 29.99 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3500000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2716233 |\n",
      "|    clip_fraction        | 0.0689    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00177  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.115     |\n",
      "|    n_updates            | 24920     |\n",
      "|    policy_gradient_loss | 0.00703   |\n",
      "|    value_loss           | 0.28      |\n",
      "---------------------------------------\n",
      "Ep done - 123970.\n",
      "Ep done - 123980.\n",
      "Ep done - 123990.\n",
      "Ep done - 124000.\n",
      "Ep done - 124010.\n",
      "Ep done - 124020.\n",
      "Ep done - 124030.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 570      |\n",
      "|    time_elapsed    | 12421    |\n",
      "|    total_timesteps | 3502080  |\n",
      "---------------------------------\n",
      "Ep done - 124040.\n",
      "Ep done - 124050.\n",
      "Ep done - 124060.\n",
      "Ep done - 124070.\n",
      "Ep done - 124080.\n",
      "Ep done - 124090.\n",
      "Ep done - 124100.\n",
      "Ep done - 124110.\n",
      "Ep done - 124120.\n",
      "Ep done - 124130.\n",
      "Ep done - 124140.\n",
      "Ep done - 124150.\n",
      "Ep done - 124160.\n",
      "Ep done - 124170.\n",
      "Ep done - 124180.\n",
      "Ep done - 124190.\n",
      "Ep done - 124200.\n",
      "Ep done - 124210.\n",
      "Ep done - 124220.\n",
      "Ep done - 124230.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.41      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 571       |\n",
      "|    time_elapsed         | 12436     |\n",
      "|    total_timesteps      | 3508224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6342016 |\n",
      "|    clip_fraction        | 0.043     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00093  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.129     |\n",
      "|    n_updates            | 24930     |\n",
      "|    policy_gradient_loss | -0.000183 |\n",
      "|    value_loss           | 0.293     |\n",
      "---------------------------------------\n",
      "Ep done - 124240.\n",
      "Ep done - 124250.\n",
      "Ep done - 124260.\n",
      "Ep done - 124270.\n",
      "Ep done - 124280.\n",
      "Ep done - 124290.\n",
      "Ep done - 35010.\n",
      "Ep done - 35020.\n",
      "Ep done - 35030.\n",
      "Ep done - 35040.\n",
      "Ep done - 35050.\n",
      "Ep done - 35060.\n",
      "Ep done - 35070.\n",
      "Ep done - 35080.\n",
      "Ep done - 35090.\n",
      "Ep done - 35100.\n",
      "Eval num_timesteps=3510000, episode_reward=0.04 +/- 1.00\n",
      "Episode length: 30.51 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.5      |\n",
      "|    mean_reward          | 0.04      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3510000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 6.8057466 |\n",
      "|    clip_fraction        | 0.123     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00201  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0684    |\n",
      "|    n_updates            | 24940     |\n",
      "|    policy_gradient_loss | 0.00206   |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "Ep done - 124300.\n",
      "Ep done - 124310.\n",
      "Ep done - 124320.\n",
      "Ep done - 124330.\n",
      "Ep done - 124340.\n",
      "Ep done - 124350.\n",
      "Ep done - 124360.\n",
      "Ep done - 124370.\n",
      "Ep done - 124380.\n",
      "Ep done - 124390.\n",
      "Ep done - 124400.\n",
      "Ep done - 124410.\n",
      "Ep done - 124420.\n",
      "Ep done - 124430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.5     |\n",
      "|    ep_rew_mean     | 0.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 572      |\n",
      "|    time_elapsed    | 12456    |\n",
      "|    total_timesteps | 3514368  |\n",
      "---------------------------------\n",
      "Ep done - 124440.\n",
      "Ep done - 124450.\n",
      "Ep done - 124460.\n",
      "Ep done - 124470.\n",
      "Ep done - 124480.\n",
      "Ep done - 124490.\n",
      "Ep done - 124500.\n",
      "Ep done - 124510.\n",
      "Ep done - 124520.\n",
      "Ep done - 124530.\n",
      "Ep done - 124540.\n",
      "Ep done - 124550.\n",
      "Ep done - 124560.\n",
      "Ep done - 124570.\n",
      "Ep done - 124580.\n",
      "Ep done - 124590.\n",
      "Ep done - 124600.\n",
      "Ep done - 124610.\n",
      "Ep done - 35110.\n",
      "Ep done - 35120.\n",
      "Ep done - 35130.\n",
      "Ep done - 35140.\n",
      "Ep done - 35150.\n",
      "Ep done - 35160.\n",
      "Ep done - 35170.\n",
      "Ep done - 35180.\n",
      "Ep done - 35190.\n",
      "Ep done - 35200.\n",
      "Eval num_timesteps=3520000, episode_reward=0.90 +/- 0.44\n",
      "Episode length: 30.95 +/- 0.22\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.9      |\n",
      "|    mean_reward          | 0.9       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3520000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.2189496 |\n",
      "|    clip_fraction        | 0.135     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00336  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.111     |\n",
      "|    n_updates            | 24950     |\n",
      "|    policy_gradient_loss | 0.0052    |\n",
      "|    value_loss           | 0.293     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.9\n",
      "SELFPLAY: new best model, bumping up generation to 113\n",
      "Ep done - 124620.\n",
      "Ep done - 124630.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.7     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 573      |\n",
      "|    time_elapsed    | 12477    |\n",
      "|    total_timesteps | 3520512  |\n",
      "---------------------------------\n",
      "Ep done - 124640.\n",
      "Ep done - 124650.\n",
      "Ep done - 124660.\n",
      "Ep done - 124670.\n",
      "Ep done - 124680.\n",
      "Ep done - 124690.\n",
      "Ep done - 124700.\n",
      "Ep done - 124710.\n",
      "Ep done - 124720.\n",
      "Ep done - 124730.\n",
      "Ep done - 124740.\n",
      "Ep done - 124750.\n",
      "Ep done - 124760.\n",
      "Ep done - 124770.\n",
      "Ep done - 124780.\n",
      "Ep done - 124790.\n",
      "Ep done - 124800.\n",
      "Ep done - 124810.\n",
      "Ep done - 124820.\n",
      "Ep done - 124830.\n",
      "Ep done - 124840.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.12      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 574       |\n",
      "|    time_elapsed         | 12492     |\n",
      "|    total_timesteps      | 3526656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.7561367 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00216  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0538    |\n",
      "|    n_updates            | 24960     |\n",
      "|    policy_gradient_loss | -0.0164   |\n",
      "|    value_loss           | 0.161     |\n",
      "---------------------------------------\n",
      "Ep done - 124850.\n",
      "Ep done - 124860.\n",
      "Ep done - 124870.\n",
      "Ep done - 124880.\n",
      "Ep done - 124890.\n",
      "Ep done - 124900.\n",
      "Ep done - 124910.\n",
      "Ep done - 124920.\n",
      "Ep done - 124930.\n",
      "Ep done - 124940.\n",
      "Ep done - 124950.\n",
      "Ep done - 35210.\n",
      "Ep done - 35220.\n",
      "Ep done - 35230.\n",
      "Ep done - 35240.\n",
      "Ep done - 35250.\n",
      "Ep done - 35260.\n",
      "Ep done - 35270.\n",
      "Ep done - 35280.\n",
      "Ep done - 35290.\n",
      "Ep done - 35300.\n",
      "Eval num_timesteps=3530000, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.94      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3530000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0520374 |\n",
      "|    clip_fraction        | 0.0998    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00246  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.153     |\n",
      "|    n_updates            | 24970     |\n",
      "|    policy_gradient_loss | -0.00311  |\n",
      "|    value_loss           | 0.284     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.94\n",
      "SELFPLAY: new best model, bumping up generation to 114\n",
      "Ep done - 124960.\n",
      "Ep done - 124970.\n",
      "Ep done - 124980.\n",
      "Ep done - 124990.\n",
      "Ep done - 125000.\n",
      "Ep done - 125010.\n",
      "Ep done - 125020.\n",
      "Ep done - 125030.\n",
      "Ep done - 125040.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 575      |\n",
      "|    time_elapsed    | 12511    |\n",
      "|    total_timesteps | 3532800  |\n",
      "---------------------------------\n",
      "Ep done - 125050.\n",
      "Ep done - 125060.\n",
      "Ep done - 125070.\n",
      "Ep done - 125080.\n",
      "Ep done - 125090.\n",
      "Ep done - 125100.\n",
      "Ep done - 125110.\n",
      "Ep done - 125120.\n",
      "Ep done - 125130.\n",
      "Ep done - 125140.\n",
      "Ep done - 125150.\n",
      "Ep done - 125160.\n",
      "Ep done - 125170.\n",
      "Ep done - 125180.\n",
      "Ep done - 125190.\n",
      "Ep done - 125200.\n",
      "Ep done - 125210.\n",
      "Ep done - 125220.\n",
      "Ep done - 125230.\n",
      "Ep done - 125240.\n",
      "Ep done - 125250.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.36     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 576       |\n",
      "|    time_elapsed         | 12526     |\n",
      "|    total_timesteps      | 3538944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9509111 |\n",
      "|    clip_fraction        | 0.0641    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00447  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.179     |\n",
      "|    n_updates            | 24980     |\n",
      "|    policy_gradient_loss | -0.00285  |\n",
      "|    value_loss           | 0.277     |\n",
      "---------------------------------------\n",
      "Ep done - 125260.\n",
      "Ep done - 125270.\n",
      "Ep done - 125280.\n",
      "Ep done - 35310.\n",
      "Ep done - 35320.\n",
      "Ep done - 35330.\n",
      "Ep done - 35340.\n",
      "Ep done - 35350.\n",
      "Ep done - 35360.\n",
      "Ep done - 35370.\n",
      "Ep done - 35380.\n",
      "Ep done - 35390.\n",
      "Ep done - 35400.\n",
      "Eval num_timesteps=3540000, episode_reward=-0.28 +/- 0.96\n",
      "Episode length: 30.19 +/- 0.39\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | -0.28     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3540000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8846531 |\n",
      "|    clip_fraction        | 0.0393    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00174  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.155     |\n",
      "|    n_updates            | 24990     |\n",
      "|    policy_gradient_loss | 0.00521   |\n",
      "|    value_loss           | 0.28      |\n",
      "---------------------------------------\n",
      "Ep done - 125290.\n",
      "Ep done - 125300.\n",
      "Ep done - 125310.\n",
      "Ep done - 125320.\n",
      "Ep done - 125330.\n",
      "Ep done - 125340.\n",
      "Ep done - 125350.\n",
      "Ep done - 125360.\n",
      "Ep done - 125370.\n",
      "Ep done - 125380.\n",
      "Ep done - 125390.\n",
      "Ep done - 125400.\n",
      "Ep done - 125410.\n",
      "Ep done - 125420.\n",
      "Ep done - 125430.\n",
      "Ep done - 125440.\n",
      "Ep done - 125450.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 577      |\n",
      "|    time_elapsed    | 12546    |\n",
      "|    total_timesteps | 3545088  |\n",
      "---------------------------------\n",
      "Ep done - 125460.\n",
      "Ep done - 125470.\n",
      "Ep done - 125480.\n",
      "Ep done - 125490.\n",
      "Ep done - 125500.\n",
      "Ep done - 125510.\n",
      "Ep done - 125520.\n",
      "Ep done - 125530.\n",
      "Ep done - 125540.\n",
      "Ep done - 125550.\n",
      "Ep done - 125560.\n",
      "Ep done - 125570.\n",
      "Ep done - 125580.\n",
      "Ep done - 125590.\n",
      "Ep done - 125600.\n",
      "Ep done - 125610.\n",
      "Ep done - 35410.\n",
      "Ep done - 35420.\n",
      "Ep done - 35430.\n",
      "Ep done - 35440.\n",
      "Ep done - 35450.\n",
      "Ep done - 35460.\n",
      "Ep done - 35470.\n",
      "Ep done - 35480.\n",
      "Ep done - 35490.\n",
      "Ep done - 35500.\n",
      "Eval num_timesteps=3550000, episode_reward=-0.08 +/- 1.00\n",
      "Episode length: 30.21 +/- 0.41\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | -0.08     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3550000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8004953 |\n",
      "|    clip_fraction        | 0.124     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00316  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.146     |\n",
      "|    n_updates            | 25000     |\n",
      "|    policy_gradient_loss | 0.00434   |\n",
      "|    value_loss           | 0.281     |\n",
      "---------------------------------------\n",
      "Ep done - 125620.\n",
      "Ep done - 125630.\n",
      "Ep done - 125640.\n",
      "Ep done - 125650.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 578      |\n",
      "|    time_elapsed    | 12567    |\n",
      "|    total_timesteps | 3551232  |\n",
      "---------------------------------\n",
      "Ep done - 125660.\n",
      "Ep done - 125670.\n",
      "Ep done - 125680.\n",
      "Ep done - 125690.\n",
      "Ep done - 125700.\n",
      "Ep done - 125710.\n",
      "Ep done - 125720.\n",
      "Ep done - 125730.\n",
      "Ep done - 125740.\n",
      "Ep done - 125750.\n",
      "Ep done - 125760.\n",
      "Ep done - 125770.\n",
      "Ep done - 125780.\n",
      "Ep done - 125790.\n",
      "Ep done - 125800.\n",
      "Ep done - 125810.\n",
      "Ep done - 125820.\n",
      "Ep done - 125830.\n",
      "Ep done - 125840.\n",
      "Ep done - 125850.\n",
      "Ep done - 125860.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.94      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 579       |\n",
      "|    time_elapsed         | 12584     |\n",
      "|    total_timesteps      | 3557376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0151212 |\n",
      "|    clip_fraction        | 0.12      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00725  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.126     |\n",
      "|    n_updates            | 25010     |\n",
      "|    policy_gradient_loss | 0.0198    |\n",
      "|    value_loss           | 0.291     |\n",
      "---------------------------------------\n",
      "Ep done - 125870.\n",
      "Ep done - 125880.\n",
      "Ep done - 125890.\n",
      "Ep done - 125900.\n",
      "Ep done - 125910.\n",
      "Ep done - 125920.\n",
      "Ep done - 125930.\n",
      "Ep done - 125940.\n",
      "Ep done - 35510.\n",
      "Ep done - 35520.\n",
      "Ep done - 35530.\n",
      "Ep done - 35540.\n",
      "Ep done - 35550.\n",
      "Ep done - 35560.\n",
      "Ep done - 35570.\n",
      "Ep done - 35580.\n",
      "Ep done - 35590.\n",
      "Ep done - 35600.\n",
      "Eval num_timesteps=3560000, episode_reward=-0.89 +/- 0.42\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.89     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3560000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.5396192 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00475  |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.044     |\n",
      "|    n_updates            | 25020     |\n",
      "|    policy_gradient_loss | 0.011     |\n",
      "|    value_loss           | 0.121     |\n",
      "---------------------------------------\n",
      "Ep done - 125950.\n",
      "Ep done - 125960.\n",
      "Ep done - 125970.\n",
      "Ep done - 125980.\n",
      "Ep done - 125990.\n",
      "Ep done - 126000.\n",
      "Ep done - 126010.\n",
      "Ep done - 126020.\n",
      "Ep done - 126030.\n",
      "Ep done - 126040.\n",
      "Ep done - 126050.\n",
      "Ep done - 126060.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 580      |\n",
      "|    time_elapsed    | 12605    |\n",
      "|    total_timesteps | 3563520  |\n",
      "---------------------------------\n",
      "Ep done - 126070.\n",
      "Ep done - 126080.\n",
      "Ep done - 126090.\n",
      "Ep done - 126100.\n",
      "Ep done - 126110.\n",
      "Ep done - 126120.\n",
      "Ep done - 126130.\n",
      "Ep done - 126140.\n",
      "Ep done - 126150.\n",
      "Ep done - 126160.\n",
      "Ep done - 126170.\n",
      "Ep done - 126180.\n",
      "Ep done - 126190.\n",
      "Ep done - 126200.\n",
      "Ep done - 126210.\n",
      "Ep done - 126220.\n",
      "Ep done - 126230.\n",
      "Ep done - 126240.\n",
      "Ep done - 126250.\n",
      "Ep done - 126260.\n",
      "Ep done - 126270.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.9      |\n",
      "|    ep_rew_mean          | -0.69     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 581       |\n",
      "|    time_elapsed         | 12621     |\n",
      "|    total_timesteps      | 3569664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6621445 |\n",
      "|    clip_fraction        | 0.0658    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00626  |\n",
      "|    explained_variance   | 0.00132   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0872    |\n",
      "|    n_updates            | 25030     |\n",
      "|    policy_gradient_loss | -0.00267  |\n",
      "|    value_loss           | 0.184     |\n",
      "---------------------------------------\n",
      "Ep done - 126280.\n",
      "Ep done - 35610.\n",
      "Ep done - 35620.\n",
      "Ep done - 35630.\n",
      "Ep done - 35640.\n",
      "Ep done - 35650.\n",
      "Ep done - 35660.\n",
      "Ep done - 35670.\n",
      "Ep done - 35680.\n",
      "Ep done - 35690.\n",
      "Ep done - 35700.\n",
      "Eval num_timesteps=3570000, episode_reward=0.70 +/- 0.71\n",
      "Episode length: 30.04 +/- 0.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.7       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3570000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.6283817 |\n",
      "|    clip_fraction        | 0.123     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00359  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.119     |\n",
      "|    n_updates            | 25040     |\n",
      "|    policy_gradient_loss | -0.0043   |\n",
      "|    value_loss           | 0.15      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 115\n",
      "Ep done - 126290.\n",
      "Ep done - 126300.\n",
      "Ep done - 126310.\n",
      "Ep done - 126320.\n",
      "Ep done - 126330.\n",
      "Ep done - 126340.\n",
      "Ep done - 126350.\n",
      "Ep done - 126360.\n",
      "Ep done - 126370.\n",
      "Ep done - 126380.\n",
      "Ep done - 126390.\n",
      "Ep done - 126400.\n",
      "Ep done - 126410.\n",
      "Ep done - 126420.\n",
      "Ep done - 126430.\n",
      "Ep done - 126440.\n",
      "Ep done - 126450.\n",
      "Ep done - 126460.\n",
      "Ep done - 126470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 582      |\n",
      "|    time_elapsed    | 12642    |\n",
      "|    total_timesteps | 3575808  |\n",
      "---------------------------------\n",
      "Ep done - 126480.\n",
      "Ep done - 126490.\n",
      "Ep done - 126500.\n",
      "Ep done - 126510.\n",
      "Ep done - 126520.\n",
      "Ep done - 126530.\n",
      "Ep done - 126540.\n",
      "Ep done - 126550.\n",
      "Ep done - 126560.\n",
      "Ep done - 126570.\n",
      "Ep done - 126580.\n",
      "Ep done - 126590.\n",
      "Ep done - 126600.\n",
      "Ep done - 126610.\n",
      "Ep done - 35710.\n",
      "Ep done - 35720.\n",
      "Ep done - 35730.\n",
      "Ep done - 35740.\n",
      "Ep done - 35750.\n",
      "Ep done - 35760.\n",
      "Ep done - 35770.\n",
      "Ep done - 35780.\n",
      "Ep done - 35790.\n",
      "Ep done - 35800.\n",
      "Eval num_timesteps=3580000, episode_reward=-0.53 +/- 0.71\n",
      "Episode length: 30.21 +/- 0.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | -0.53      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3580000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.27204457 |\n",
      "|    clip_fraction        | 0.00353    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00124   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0678     |\n",
      "|    n_updates            | 25050      |\n",
      "|    policy_gradient_loss | 0.000143   |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "Ep done - 126620.\n",
      "Ep done - 126630.\n",
      "Ep done - 126640.\n",
      "Ep done - 126650.\n",
      "Ep done - 126660.\n",
      "Ep done - 126670.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.53    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 583      |\n",
      "|    time_elapsed    | 12664    |\n",
      "|    total_timesteps | 3581952  |\n",
      "---------------------------------\n",
      "Ep done - 126680.\n",
      "Ep done - 126690.\n",
      "Ep done - 126700.\n",
      "Ep done - 126710.\n",
      "Ep done - 126720.\n",
      "Ep done - 126730.\n",
      "Ep done - 126740.\n",
      "Ep done - 126750.\n",
      "Ep done - 126760.\n",
      "Ep done - 126770.\n",
      "Ep done - 126780.\n",
      "Ep done - 126790.\n",
      "Ep done - 126800.\n",
      "Ep done - 126810.\n",
      "Ep done - 126820.\n",
      "Ep done - 126830.\n",
      "Ep done - 126840.\n",
      "Ep done - 126850.\n",
      "Ep done - 126860.\n",
      "Ep done - 126870.\n",
      "Ep done - 126880.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 584        |\n",
      "|    time_elapsed         | 12680      |\n",
      "|    total_timesteps      | 3588096    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60137326 |\n",
      "|    clip_fraction        | 0.0177     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000148  |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0386     |\n",
      "|    n_updates            | 25060      |\n",
      "|    policy_gradient_loss | -0.00354   |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Ep done - 126890.\n",
      "Ep done - 126900.\n",
      "Ep done - 126910.\n",
      "Ep done - 126920.\n",
      "Ep done - 126930.\n",
      "Ep done - 126940.\n",
      "Ep done - 35810.\n",
      "Ep done - 35820.\n",
      "Ep done - 35830.\n",
      "Ep done - 35840.\n",
      "Ep done - 35850.\n",
      "Ep done - 35860.\n",
      "Ep done - 35870.\n",
      "Ep done - 35880.\n",
      "Ep done - 35890.\n",
      "Ep done - 35900.\n",
      "Eval num_timesteps=3590000, episode_reward=0.02 +/- 0.35\n",
      "Episode length: 30.23 +/- 0.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3590000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054083128 |\n",
      "|    clip_fraction        | 0.000537    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.00011    |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0836      |\n",
      "|    n_updates            | 25070       |\n",
      "|    policy_gradient_loss | -0.000109   |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Ep done - 126950.\n",
      "Ep done - 126960.\n",
      "Ep done - 126970.\n",
      "Ep done - 126980.\n",
      "Ep done - 126990.\n",
      "Ep done - 127000.\n",
      "Ep done - 127010.\n",
      "Ep done - 127020.\n",
      "Ep done - 127030.\n",
      "Ep done - 127040.\n",
      "Ep done - 127050.\n",
      "Ep done - 127060.\n",
      "Ep done - 127070.\n",
      "Ep done - 127080.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 585      |\n",
      "|    time_elapsed    | 12702    |\n",
      "|    total_timesteps | 3594240  |\n",
      "---------------------------------\n",
      "Ep done - 127090.\n",
      "Ep done - 127100.\n",
      "Ep done - 127110.\n",
      "Ep done - 127120.\n",
      "Ep done - 127130.\n",
      "Ep done - 127140.\n",
      "Ep done - 127150.\n",
      "Ep done - 127160.\n",
      "Ep done - 127170.\n",
      "Ep done - 127180.\n",
      "Ep done - 127190.\n",
      "Ep done - 127200.\n",
      "Ep done - 127210.\n",
      "Ep done - 127220.\n",
      "Ep done - 127230.\n",
      "Ep done - 127240.\n",
      "Ep done - 127250.\n",
      "Ep done - 127260.\n",
      "Ep done - 127270.\n",
      "Ep done - 35910.\n",
      "Ep done - 35920.\n",
      "Ep done - 35930.\n",
      "Ep done - 35940.\n",
      "Ep done - 35950.\n",
      "Ep done - 35960.\n",
      "Ep done - 35970.\n",
      "Ep done - 35980.\n",
      "Ep done - 35990.\n",
      "Ep done - 36000.\n",
      "Eval num_timesteps=3600000, episode_reward=0.64 +/- 0.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.64      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3600000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1754625 |\n",
      "|    clip_fraction        | 0.0618    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000258 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0304    |\n",
      "|    n_updates            | 25080     |\n",
      "|    policy_gradient_loss | -0.00669  |\n",
      "|    value_loss           | 0.0851    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.64\n",
      "SELFPLAY: new best model, bumping up generation to 116\n",
      "Ep done - 127280.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 586      |\n",
      "|    time_elapsed    | 12724    |\n",
      "|    total_timesteps | 3600384  |\n",
      "---------------------------------\n",
      "Ep done - 127290.\n",
      "Ep done - 127300.\n",
      "Ep done - 127310.\n",
      "Ep done - 127320.\n",
      "Ep done - 127330.\n",
      "Ep done - 127340.\n",
      "Ep done - 127350.\n",
      "Ep done - 127360.\n",
      "Ep done - 127370.\n",
      "Ep done - 127380.\n",
      "Ep done - 127390.\n",
      "Ep done - 127400.\n",
      "Ep done - 127410.\n",
      "Ep done - 127420.\n",
      "Ep done - 127430.\n",
      "Ep done - 127440.\n",
      "Ep done - 127450.\n",
      "Ep done - 127460.\n",
      "Ep done - 127470.\n",
      "Ep done - 127480.\n",
      "Ep done - 127490.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.3      |\n",
      "|    ep_rew_mean          | 1         |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 587       |\n",
      "|    time_elapsed         | 12741     |\n",
      "|    total_timesteps      | 3606528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0584345 |\n",
      "|    clip_fraction        | 0.0701    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00147  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0929    |\n",
      "|    n_updates            | 25090     |\n",
      "|    policy_gradient_loss | -0.00309  |\n",
      "|    value_loss           | 0.233     |\n",
      "---------------------------------------\n",
      "Ep done - 127500.\n",
      "Ep done - 127510.\n",
      "Ep done - 127520.\n",
      "Ep done - 127530.\n",
      "Ep done - 127540.\n",
      "Ep done - 127550.\n",
      "Ep done - 127560.\n",
      "Ep done - 127570.\n",
      "Ep done - 127580.\n",
      "Ep done - 127590.\n",
      "Ep done - 127600.\n",
      "Ep done - 36010.\n",
      "Ep done - 36020.\n",
      "Ep done - 36030.\n",
      "Ep done - 36040.\n",
      "Ep done - 36050.\n",
      "Ep done - 36060.\n",
      "Ep done - 36070.\n",
      "Ep done - 36080.\n",
      "Ep done - 36090.\n",
      "Ep done - 36100.\n",
      "Eval num_timesteps=3610000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 30.02 +/- 0.14\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3610000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 6.6845307 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000535 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00802  |\n",
      "|    n_updates            | 25100     |\n",
      "|    policy_gradient_loss | -0.00397  |\n",
      "|    value_loss           | 0.0538    |\n",
      "---------------------------------------\n",
      "Ep done - 127610.\n",
      "Ep done - 127620.\n",
      "Ep done - 127630.\n",
      "Ep done - 127640.\n",
      "Ep done - 127650.\n",
      "Ep done - 127660.\n",
      "Ep done - 127670.\n",
      "Ep done - 127680.\n",
      "Ep done - 127690.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 588      |\n",
      "|    time_elapsed    | 12762    |\n",
      "|    total_timesteps | 3612672  |\n",
      "---------------------------------\n",
      "Ep done - 127700.\n",
      "Ep done - 127710.\n",
      "Ep done - 127720.\n",
      "Ep done - 127730.\n",
      "Ep done - 127740.\n",
      "Ep done - 127750.\n",
      "Ep done - 127760.\n",
      "Ep done - 127770.\n",
      "Ep done - 127780.\n",
      "Ep done - 127790.\n",
      "Ep done - 127800.\n",
      "Ep done - 127810.\n",
      "Ep done - 127820.\n",
      "Ep done - 127830.\n",
      "Ep done - 127840.\n",
      "Ep done - 127850.\n",
      "Ep done - 127860.\n",
      "Ep done - 127870.\n",
      "Ep done - 127880.\n",
      "Ep done - 127890.\n",
      "Ep done - 127900.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.47      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 589       |\n",
      "|    time_elapsed         | 12779     |\n",
      "|    total_timesteps      | 3618816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0599954 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00305  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.122     |\n",
      "|    n_updates            | 25110     |\n",
      "|    policy_gradient_loss | 0.00075   |\n",
      "|    value_loss           | 0.304     |\n",
      "---------------------------------------\n",
      "Ep done - 127910.\n",
      "Ep done - 127920.\n",
      "Ep done - 127930.\n",
      "Ep done - 127940.\n",
      "Ep done - 36110.\n",
      "Ep done - 36120.\n",
      "Ep done - 36130.\n",
      "Ep done - 36140.\n",
      "Ep done - 36150.\n",
      "Ep done - 36160.\n",
      "Ep done - 36170.\n",
      "Ep done - 36180.\n",
      "Ep done - 36190.\n",
      "Ep done - 36200.\n",
      "Eval num_timesteps=3620000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3620000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3780458 |\n",
      "|    clip_fraction        | 0.0451    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00205  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0937    |\n",
      "|    n_updates            | 25120     |\n",
      "|    policy_gradient_loss | 0.00392   |\n",
      "|    value_loss           | 0.245     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 117\n",
      "Ep done - 127950.\n",
      "Ep done - 127960.\n",
      "Ep done - 127970.\n",
      "Ep done - 127980.\n",
      "Ep done - 127990.\n",
      "Ep done - 128000.\n",
      "Ep done - 128010.\n",
      "Ep done - 128020.\n",
      "Ep done - 128030.\n",
      "Ep done - 128040.\n",
      "Ep done - 128050.\n",
      "Ep done - 128060.\n",
      "Ep done - 128070.\n",
      "Ep done - 128080.\n",
      "Ep done - 128090.\n",
      "Ep done - 128100.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 590      |\n",
      "|    time_elapsed    | 12800    |\n",
      "|    total_timesteps | 3624960  |\n",
      "---------------------------------\n",
      "Ep done - 128110.\n",
      "Ep done - 128120.\n",
      "Ep done - 128130.\n",
      "Ep done - 128140.\n",
      "Ep done - 128150.\n",
      "Ep done - 128160.\n",
      "Ep done - 128170.\n",
      "Ep done - 128180.\n",
      "Ep done - 128190.\n",
      "Ep done - 128200.\n",
      "Ep done - 128210.\n",
      "Ep done - 128220.\n",
      "Ep done - 128230.\n",
      "Ep done - 128240.\n",
      "Ep done - 128250.\n",
      "Ep done - 128260.\n",
      "Ep done - 128270.\n",
      "Ep done - 36210.\n",
      "Ep done - 36220.\n",
      "Ep done - 36230.\n",
      "Ep done - 36240.\n",
      "Ep done - 36250.\n",
      "Ep done - 36260.\n",
      "Ep done - 36270.\n",
      "Ep done - 36280.\n",
      "Ep done - 36290.\n",
      "Ep done - 36300.\n",
      "Eval num_timesteps=3630000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.10 +/- 0.30\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3630000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.2190967 |\n",
      "|    clip_fraction        | 0.196     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000666 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00312  |\n",
      "|    n_updates            | 25130     |\n",
      "|    policy_gradient_loss | -0.023    |\n",
      "|    value_loss           | 0.0385    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 118\n",
      "Ep done - 128280.\n",
      "Ep done - 128290.\n",
      "Ep done - 128300.\n",
      "Ep done - 128310.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 591      |\n",
      "|    time_elapsed    | 12822    |\n",
      "|    total_timesteps | 3631104  |\n",
      "---------------------------------\n",
      "Ep done - 128320.\n",
      "Ep done - 128330.\n",
      "Ep done - 128340.\n",
      "Ep done - 128350.\n",
      "Ep done - 128360.\n",
      "Ep done - 128370.\n",
      "Ep done - 128380.\n",
      "Ep done - 128390.\n",
      "Ep done - 128400.\n",
      "Ep done - 128410.\n",
      "Ep done - 128420.\n",
      "Ep done - 128430.\n",
      "Ep done - 128440.\n",
      "Ep done - 128450.\n",
      "Ep done - 128460.\n",
      "Ep done - 128470.\n",
      "Ep done - 128480.\n",
      "Ep done - 128490.\n",
      "Ep done - 128500.\n",
      "Ep done - 128510.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 592       |\n",
      "|    time_elapsed         | 12839     |\n",
      "|    total_timesteps      | 3637248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 8.872962  |\n",
      "|    clip_fraction        | 0.228     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000566 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.18      |\n",
      "|    n_updates            | 25140     |\n",
      "|    policy_gradient_loss | 0.0201    |\n",
      "|    value_loss           | 0.0228    |\n",
      "---------------------------------------\n",
      "Ep done - 128520.\n",
      "Ep done - 128530.\n",
      "Ep done - 128540.\n",
      "Ep done - 128550.\n",
      "Ep done - 128560.\n",
      "Ep done - 128570.\n",
      "Ep done - 128580.\n",
      "Ep done - 128590.\n",
      "Ep done - 128600.\n",
      "Ep done - 36310.\n",
      "Ep done - 36320.\n",
      "Ep done - 36330.\n",
      "Ep done - 36340.\n",
      "Ep done - 36350.\n",
      "Ep done - 36360.\n",
      "Ep done - 36370.\n",
      "Ep done - 36380.\n",
      "Ep done - 36390.\n",
      "Ep done - 36400.\n",
      "Eval num_timesteps=3640000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3640000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0417511 |\n",
      "|    clip_fraction        | 0.109     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00785  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.148     |\n",
      "|    n_updates            | 25150     |\n",
      "|    policy_gradient_loss | 0.00577   |\n",
      "|    value_loss           | 0.311     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 119\n",
      "Ep done - 128610.\n",
      "Ep done - 128620.\n",
      "Ep done - 128630.\n",
      "Ep done - 128640.\n",
      "Ep done - 128650.\n",
      "Ep done - 128660.\n",
      "Ep done - 128670.\n",
      "Ep done - 128680.\n",
      "Ep done - 128690.\n",
      "Ep done - 128700.\n",
      "Ep done - 128710.\n",
      "Ep done - 128720.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | -0.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 593      |\n",
      "|    time_elapsed    | 12860    |\n",
      "|    total_timesteps | 3643392  |\n",
      "---------------------------------\n",
      "Ep done - 128730.\n",
      "Ep done - 128740.\n",
      "Ep done - 128750.\n",
      "Ep done - 128760.\n",
      "Ep done - 128770.\n",
      "Ep done - 128780.\n",
      "Ep done - 128790.\n",
      "Ep done - 128800.\n",
      "Ep done - 128810.\n",
      "Ep done - 128820.\n",
      "Ep done - 128830.\n",
      "Ep done - 128840.\n",
      "Ep done - 128850.\n",
      "Ep done - 128860.\n",
      "Ep done - 128870.\n",
      "Ep done - 128880.\n",
      "Ep done - 128890.\n",
      "Ep done - 128900.\n",
      "Ep done - 128910.\n",
      "Ep done - 128920.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.15      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 594        |\n",
      "|    time_elapsed         | 12877      |\n",
      "|    total_timesteps      | 3649536    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18451925 |\n",
      "|    clip_fraction        | 0.0164     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00165   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.129      |\n",
      "|    n_updates            | 25160      |\n",
      "|    policy_gradient_loss | 0.00139    |\n",
      "|    value_loss           | 0.291      |\n",
      "----------------------------------------\n",
      "Ep done - 128930.\n",
      "Ep done - 128940.\n",
      "Ep done - 36410.\n",
      "Ep done - 36420.\n",
      "Ep done - 36430.\n",
      "Ep done - 36440.\n",
      "Ep done - 36450.\n",
      "Ep done - 36460.\n",
      "Ep done - 36470.\n",
      "Ep done - 36480.\n",
      "Ep done - 36490.\n",
      "Ep done - 36500.\n",
      "Eval num_timesteps=3650000, episode_reward=-0.40 +/- 0.92\n",
      "Episode length: 29.91 +/- 0.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.9       |\n",
      "|    mean_reward          | -0.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3650000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47592792 |\n",
      "|    clip_fraction        | 0.0463     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00142   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.135      |\n",
      "|    n_updates            | 25170      |\n",
      "|    policy_gradient_loss | -0.000665  |\n",
      "|    value_loss           | 0.29       |\n",
      "----------------------------------------\n",
      "Ep done - 128950.\n",
      "Ep done - 128960.\n",
      "Ep done - 128970.\n",
      "Ep done - 128980.\n",
      "Ep done - 128990.\n",
      "Ep done - 129000.\n",
      "Ep done - 129010.\n",
      "Ep done - 129020.\n",
      "Ep done - 129030.\n",
      "Ep done - 129040.\n",
      "Ep done - 129050.\n",
      "Ep done - 129060.\n",
      "Ep done - 129070.\n",
      "Ep done - 129080.\n",
      "Ep done - 129090.\n",
      "Ep done - 129100.\n",
      "Ep done - 129110.\n",
      "Ep done - 129120.\n",
      "Ep done - 129130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 595      |\n",
      "|    time_elapsed    | 12898    |\n",
      "|    total_timesteps | 3655680  |\n",
      "---------------------------------\n",
      "Ep done - 129140.\n",
      "Ep done - 129150.\n",
      "Ep done - 129160.\n",
      "Ep done - 129170.\n",
      "Ep done - 129180.\n",
      "Ep done - 129190.\n",
      "Ep done - 129200.\n",
      "Ep done - 129210.\n",
      "Ep done - 129220.\n",
      "Ep done - 129230.\n",
      "Ep done - 129240.\n",
      "Ep done - 129250.\n",
      "Ep done - 129260.\n",
      "Ep done - 129270.\n",
      "Ep done - 36510.\n",
      "Ep done - 36520.\n",
      "Ep done - 36530.\n",
      "Ep done - 36540.\n",
      "Ep done - 36550.\n",
      "Ep done - 36560.\n",
      "Ep done - 36570.\n",
      "Ep done - 36580.\n",
      "Ep done - 36590.\n",
      "Ep done - 36600.\n",
      "Eval num_timesteps=3660000, episode_reward=0.00 +/- 1.00\n",
      "Episode length: 30.06 +/- 0.24\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3660000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46696988 |\n",
      "|    clip_fraction        | 0.0234     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00169   |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.112      |\n",
      "|    n_updates            | 25180      |\n",
      "|    policy_gradient_loss | -0.00271   |\n",
      "|    value_loss           | 0.286      |\n",
      "----------------------------------------\n",
      "Ep done - 129280.\n",
      "Ep done - 129290.\n",
      "Ep done - 129300.\n",
      "Ep done - 129310.\n",
      "Ep done - 129320.\n",
      "Ep done - 129330.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 596      |\n",
      "|    time_elapsed    | 12919    |\n",
      "|    total_timesteps | 3661824  |\n",
      "---------------------------------\n",
      "Ep done - 129340.\n",
      "Ep done - 129350.\n",
      "Ep done - 129360.\n",
      "Ep done - 129370.\n",
      "Ep done - 129380.\n",
      "Ep done - 129390.\n",
      "Ep done - 129400.\n",
      "Ep done - 129410.\n",
      "Ep done - 129420.\n",
      "Ep done - 129430.\n",
      "Ep done - 129440.\n",
      "Ep done - 129450.\n",
      "Ep done - 129460.\n",
      "Ep done - 129470.\n",
      "Ep done - 129480.\n",
      "Ep done - 129490.\n",
      "Ep done - 129500.\n",
      "Ep done - 129510.\n",
      "Ep done - 129520.\n",
      "Ep done - 129530.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | 0             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 283           |\n",
      "|    iterations           | 597           |\n",
      "|    time_elapsed         | 12936         |\n",
      "|    total_timesteps      | 3667968       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9115548e-08 |\n",
      "|    clip_fraction        | 0.000781      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000692     |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.153         |\n",
      "|    n_updates            | 25190         |\n",
      "|    policy_gradient_loss | -0.000138     |\n",
      "|    value_loss           | 0.277         |\n",
      "-------------------------------------------\n",
      "Ep done - 129540.\n",
      "Ep done - 129550.\n",
      "Ep done - 129560.\n",
      "Ep done - 129570.\n",
      "Ep done - 129580.\n",
      "Ep done - 129590.\n",
      "Ep done - 129600.\n",
      "Ep done - 36610.\n",
      "Ep done - 36620.\n",
      "Ep done - 36630.\n",
      "Ep done - 36640.\n",
      "Ep done - 36650.\n",
      "Ep done - 36660.\n",
      "Ep done - 36670.\n",
      "Ep done - 36680.\n",
      "Ep done - 36690.\n",
      "Ep done - 36700.\n",
      "Eval num_timesteps=3670000, episode_reward=-0.21 +/- 0.97\n",
      "Episode length: 29.93 +/- 0.32\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.9      |\n",
      "|    mean_reward          | -0.21     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3670000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2275877 |\n",
      "|    clip_fraction        | 0.0201    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000285 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.184     |\n",
      "|    n_updates            | 25200     |\n",
      "|    policy_gradient_loss | -0.00391  |\n",
      "|    value_loss           | 0.293     |\n",
      "---------------------------------------\n",
      "Ep done - 129610.\n",
      "Ep done - 129620.\n",
      "Ep done - 129630.\n",
      "Ep done - 129640.\n",
      "Ep done - 129650.\n",
      "Ep done - 129660.\n",
      "Ep done - 129670.\n",
      "Ep done - 129680.\n",
      "Ep done - 129690.\n",
      "Ep done - 129700.\n",
      "Ep done - 129710.\n",
      "Ep done - 129720.\n",
      "Ep done - 129730.\n",
      "Ep done - 129740.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 598      |\n",
      "|    time_elapsed    | 12957    |\n",
      "|    total_timesteps | 3674112  |\n",
      "---------------------------------\n",
      "Ep done - 129750.\n",
      "Ep done - 129760.\n",
      "Ep done - 129770.\n",
      "Ep done - 129780.\n",
      "Ep done - 129790.\n",
      "Ep done - 129800.\n",
      "Ep done - 129810.\n",
      "Ep done - 129820.\n",
      "Ep done - 129830.\n",
      "Ep done - 129840.\n",
      "Ep done - 129850.\n",
      "Ep done - 129860.\n",
      "Ep done - 129870.\n",
      "Ep done - 129880.\n",
      "Ep done - 129890.\n",
      "Ep done - 129900.\n",
      "Ep done - 129910.\n",
      "Ep done - 129920.\n",
      "Ep done - 129930.\n",
      "Ep done - 129940.\n",
      "Ep done - 36710.\n",
      "Ep done - 36720.\n",
      "Ep done - 36730.\n",
      "Ep done - 36740.\n",
      "Ep done - 36750.\n",
      "Ep done - 36760.\n",
      "Ep done - 36770.\n",
      "Ep done - 36780.\n",
      "Ep done - 36790.\n",
      "Ep done - 36800.\n",
      "Eval num_timesteps=3680000, episode_reward=-0.70 +/- 0.71\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3680000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3637396 |\n",
      "|    clip_fraction        | 0.0471    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0015   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.15      |\n",
      "|    n_updates            | 25210     |\n",
      "|    policy_gradient_loss | -0.00881  |\n",
      "|    value_loss           | 0.275     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 599      |\n",
      "|    time_elapsed    | 12978    |\n",
      "|    total_timesteps | 3680256  |\n",
      "---------------------------------\n",
      "Ep done - 129950.\n",
      "Ep done - 129960.\n",
      "Ep done - 129970.\n",
      "Ep done - 129980.\n",
      "Ep done - 129990.\n",
      "Ep done - 130000.\n",
      "Ep done - 130010.\n",
      "Ep done - 130020.\n",
      "Ep done - 130030.\n",
      "Ep done - 130040.\n",
      "Ep done - 130050.\n",
      "Ep done - 130060.\n",
      "Ep done - 130070.\n",
      "Ep done - 130080.\n",
      "Ep done - 130090.\n",
      "Ep done - 130100.\n",
      "Ep done - 130110.\n",
      "Ep done - 130120.\n",
      "Ep done - 130130.\n",
      "Ep done - 130140.\n",
      "Ep done - 130150.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.32     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 600       |\n",
      "|    time_elapsed         | 12996     |\n",
      "|    total_timesteps      | 3686400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6304029 |\n",
      "|    clip_fraction        | 0.0792    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00319  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0482    |\n",
      "|    n_updates            | 25220     |\n",
      "|    policy_gradient_loss | -0.0158   |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "Ep done - 130160.\n",
      "Ep done - 130170.\n",
      "Ep done - 130180.\n",
      "Ep done - 130190.\n",
      "Ep done - 130200.\n",
      "Ep done - 130210.\n",
      "Ep done - 130220.\n",
      "Ep done - 130230.\n",
      "Ep done - 130240.\n",
      "Ep done - 130250.\n",
      "Ep done - 130260.\n",
      "Ep done - 130270.\n",
      "Ep done - 36810.\n",
      "Ep done - 36820.\n",
      "Ep done - 36830.\n",
      "Ep done - 36840.\n",
      "Ep done - 36850.\n",
      "Ep done - 36860.\n",
      "Ep done - 36870.\n",
      "Ep done - 36880.\n",
      "Ep done - 36890.\n",
      "Ep done - 36900.\n",
      "Eval num_timesteps=3690000, episode_reward=-0.42 +/- 0.91\n",
      "Episode length: 29.89 +/- 0.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.9       |\n",
      "|    mean_reward          | -0.42      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3690000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44243038 |\n",
      "|    clip_fraction        | 0.0203     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000607  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.115      |\n",
      "|    n_updates            | 25230      |\n",
      "|    policy_gradient_loss | 0.00265    |\n",
      "|    value_loss           | 0.279      |\n",
      "----------------------------------------\n",
      "Ep done - 130280.\n",
      "Ep done - 130290.\n",
      "Ep done - 130300.\n",
      "Ep done - 130310.\n",
      "Ep done - 130320.\n",
      "Ep done - 130330.\n",
      "Ep done - 130340.\n",
      "Ep done - 130350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 601      |\n",
      "|    time_elapsed    | 13017    |\n",
      "|    total_timesteps | 3692544  |\n",
      "---------------------------------\n",
      "Ep done - 130360.\n",
      "Ep done - 130370.\n",
      "Ep done - 130380.\n",
      "Ep done - 130390.\n",
      "Ep done - 130400.\n",
      "Ep done - 130410.\n",
      "Ep done - 130420.\n",
      "Ep done - 130430.\n",
      "Ep done - 130440.\n",
      "Ep done - 130450.\n",
      "Ep done - 130460.\n",
      "Ep done - 130470.\n",
      "Ep done - 130480.\n",
      "Ep done - 130490.\n",
      "Ep done - 130500.\n",
      "Ep done - 130510.\n",
      "Ep done - 130520.\n",
      "Ep done - 130530.\n",
      "Ep done - 130540.\n",
      "Ep done - 130550.\n",
      "Ep done - 130560.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.56      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 602        |\n",
      "|    time_elapsed         | 13034      |\n",
      "|    total_timesteps      | 3698688    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82139796 |\n",
      "|    clip_fraction        | 0.0646     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00257   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.106      |\n",
      "|    n_updates            | 25240      |\n",
      "|    policy_gradient_loss | -0.00918   |\n",
      "|    value_loss           | 0.239      |\n",
      "----------------------------------------\n",
      "Ep done - 130570.\n",
      "Ep done - 130580.\n",
      "Ep done - 130590.\n",
      "Ep done - 130600.\n",
      "Ep done - 36910.\n",
      "Ep done - 36920.\n",
      "Ep done - 36930.\n",
      "Ep done - 36940.\n",
      "Ep done - 36950.\n",
      "Ep done - 36960.\n",
      "Ep done - 36970.\n",
      "Ep done - 36980.\n",
      "Ep done - 36990.\n",
      "Ep done - 37000.\n",
      "Eval num_timesteps=3700000, episode_reward=-0.68 +/- 0.73\n",
      "Episode length: 30.02 +/- 0.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.68      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3700000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39727363 |\n",
      "|    clip_fraction        | 0.0333     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000836  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.119      |\n",
      "|    n_updates            | 25250      |\n",
      "|    policy_gradient_loss | -0.00342   |\n",
      "|    value_loss           | 0.234      |\n",
      "----------------------------------------\n",
      "Ep done - 130610.\n",
      "Ep done - 130620.\n",
      "Ep done - 130630.\n",
      "Ep done - 130640.\n",
      "Ep done - 130650.\n",
      "Ep done - 130660.\n",
      "Ep done - 130670.\n",
      "Ep done - 130680.\n",
      "Ep done - 130690.\n",
      "Ep done - 130700.\n",
      "Ep done - 130710.\n",
      "Ep done - 130720.\n",
      "Ep done - 130730.\n",
      "Ep done - 130740.\n",
      "Ep done - 130750.\n",
      "Ep done - 130760.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 603      |\n",
      "|    time_elapsed    | 13062    |\n",
      "|    total_timesteps | 3704832  |\n",
      "---------------------------------\n",
      "Ep done - 130770.\n",
      "Ep done - 130780.\n",
      "Ep done - 130790.\n",
      "Ep done - 130800.\n",
      "Ep done - 130810.\n",
      "Ep done - 130820.\n",
      "Ep done - 130830.\n",
      "Ep done - 130840.\n",
      "Ep done - 130850.\n",
      "Ep done - 130860.\n",
      "Ep done - 130870.\n",
      "Ep done - 130880.\n",
      "Ep done - 130890.\n",
      "Ep done - 130900.\n",
      "Ep done - 130910.\n",
      "Ep done - 130920.\n",
      "Ep done - 130930.\n",
      "Ep done - 130940.\n",
      "Ep done - 37010.\n",
      "Ep done - 37020.\n",
      "Ep done - 37030.\n",
      "Ep done - 37040.\n",
      "Ep done - 37050.\n",
      "Ep done - 37060.\n",
      "Ep done - 37070.\n",
      "Ep done - 37080.\n",
      "Ep done - 37090.\n",
      "Ep done - 37100.\n",
      "Eval num_timesteps=3710000, episode_reward=-0.48 +/- 0.88\n",
      "Episode length: 30.02 +/- 0.14\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.48    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 3710000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.620319 |\n",
      "|    clip_fraction        | 0.113    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00278 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0606   |\n",
      "|    n_updates            | 25260    |\n",
      "|    policy_gradient_loss | -0.0096  |\n",
      "|    value_loss           | 0.183    |\n",
      "--------------------------------------\n",
      "Ep done - 130950.\n",
      "Ep done - 130960.\n",
      "Ep done - 130970.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 604      |\n",
      "|    time_elapsed    | 13084    |\n",
      "|    total_timesteps | 3710976  |\n",
      "---------------------------------\n",
      "Ep done - 130980.\n",
      "Ep done - 130990.\n",
      "Ep done - 131000.\n",
      "Ep done - 131010.\n",
      "Ep done - 131020.\n",
      "Ep done - 131030.\n",
      "Ep done - 131040.\n",
      "Ep done - 131050.\n",
      "Ep done - 131060.\n",
      "Ep done - 131070.\n",
      "Ep done - 131080.\n",
      "Ep done - 131090.\n",
      "Ep done - 131100.\n",
      "Ep done - 131110.\n",
      "Ep done - 131120.\n",
      "Ep done - 131130.\n",
      "Ep done - 131140.\n",
      "Ep done - 131150.\n",
      "Ep done - 131160.\n",
      "Ep done - 131170.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -0.46     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 605       |\n",
      "|    time_elapsed         | 13101     |\n",
      "|    total_timesteps      | 3717120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3185211 |\n",
      "|    clip_fraction        | 0.0798    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0036   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0821    |\n",
      "|    n_updates            | 25270     |\n",
      "|    policy_gradient_loss | 0.0121    |\n",
      "|    value_loss           | 0.25      |\n",
      "---------------------------------------\n",
      "Ep done - 131180.\n",
      "Ep done - 131190.\n",
      "Ep done - 131200.\n",
      "Ep done - 131210.\n",
      "Ep done - 131220.\n",
      "Ep done - 131230.\n",
      "Ep done - 131240.\n",
      "Ep done - 131250.\n",
      "Ep done - 131260.\n",
      "Ep done - 131270.\n",
      "Ep done - 37110.\n",
      "Ep done - 37120.\n",
      "Ep done - 37130.\n",
      "Ep done - 37140.\n",
      "Ep done - 37150.\n",
      "Ep done - 37160.\n",
      "Ep done - 37170.\n",
      "Ep done - 37180.\n",
      "Ep done - 37190.\n",
      "Ep done - 37200.\n",
      "Eval num_timesteps=3720000, episode_reward=-0.26 +/- 0.97\n",
      "Episode length: 29.98 +/- 0.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.26     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3720000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7826762 |\n",
      "|    clip_fraction        | 0.0583    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000933 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0935    |\n",
      "|    n_updates            | 25280     |\n",
      "|    policy_gradient_loss | -0.00527  |\n",
      "|    value_loss           | 0.248     |\n",
      "---------------------------------------\n",
      "Ep done - 131280.\n",
      "Ep done - 131290.\n",
      "Ep done - 131300.\n",
      "Ep done - 131310.\n",
      "Ep done - 131320.\n",
      "Ep done - 131330.\n",
      "Ep done - 131340.\n",
      "Ep done - 131350.\n",
      "Ep done - 131360.\n",
      "Ep done - 131370.\n",
      "Ep done - 131380.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 606      |\n",
      "|    time_elapsed    | 13123    |\n",
      "|    total_timesteps | 3723264  |\n",
      "---------------------------------\n",
      "Ep done - 131390.\n",
      "Ep done - 131400.\n",
      "Ep done - 131410.\n",
      "Ep done - 131420.\n",
      "Ep done - 131430.\n",
      "Ep done - 131440.\n",
      "Ep done - 131450.\n",
      "Ep done - 131460.\n",
      "Ep done - 131470.\n",
      "Ep done - 131480.\n",
      "Ep done - 131490.\n",
      "Ep done - 131500.\n",
      "Ep done - 131510.\n",
      "Ep done - 131520.\n",
      "Ep done - 131530.\n",
      "Ep done - 131540.\n",
      "Ep done - 131550.\n",
      "Ep done - 131560.\n",
      "Ep done - 131570.\n",
      "Ep done - 131580.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -0.28      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 607        |\n",
      "|    time_elapsed         | 13141      |\n",
      "|    total_timesteps      | 3729408    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39636788 |\n",
      "|    clip_fraction        | 0.0301     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000906  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.101      |\n",
      "|    n_updates            | 25290      |\n",
      "|    policy_gradient_loss | -0.000586  |\n",
      "|    value_loss           | 0.269      |\n",
      "----------------------------------------\n",
      "Ep done - 131590.\n",
      "Ep done - 131600.\n",
      "Ep done - 37210.\n",
      "Ep done - 37220.\n",
      "Ep done - 37230.\n",
      "Ep done - 37240.\n",
      "Ep done - 37250.\n",
      "Ep done - 37260.\n",
      "Ep done - 37270.\n",
      "Ep done - 37280.\n",
      "Ep done - 37290.\n",
      "Ep done - 37300.\n",
      "Eval num_timesteps=3730000, episode_reward=-0.36 +/- 0.93\n",
      "Episode length: 30.02 +/- 0.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.36      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3730000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61762756 |\n",
      "|    clip_fraction        | 0.019      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0031    |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.158      |\n",
      "|    n_updates            | 25300      |\n",
      "|    policy_gradient_loss | 0.00213    |\n",
      "|    value_loss           | 0.29       |\n",
      "----------------------------------------\n",
      "Ep done - 131610.\n",
      "Ep done - 131620.\n",
      "Ep done - 131630.\n",
      "Ep done - 131640.\n",
      "Ep done - 131650.\n",
      "Ep done - 131660.\n",
      "Ep done - 131670.\n",
      "Ep done - 131680.\n",
      "Ep done - 131690.\n",
      "Ep done - 131700.\n",
      "Ep done - 131710.\n",
      "Ep done - 131720.\n",
      "Ep done - 131730.\n",
      "Ep done - 131740.\n",
      "Ep done - 131750.\n",
      "Ep done - 131760.\n",
      "Ep done - 131770.\n",
      "Ep done - 131780.\n",
      "Ep done - 131790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 608      |\n",
      "|    time_elapsed    | 13163    |\n",
      "|    total_timesteps | 3735552  |\n",
      "---------------------------------\n",
      "Ep done - 131800.\n",
      "Ep done - 131810.\n",
      "Ep done - 131820.\n",
      "Ep done - 131830.\n",
      "Ep done - 131840.\n",
      "Ep done - 131850.\n",
      "Ep done - 131860.\n",
      "Ep done - 131870.\n",
      "Ep done - 131880.\n",
      "Ep done - 131890.\n",
      "Ep done - 131900.\n",
      "Ep done - 131910.\n",
      "Ep done - 131920.\n",
      "Ep done - 131930.\n",
      "Ep done - 131940.\n",
      "Ep done - 37310.\n",
      "Ep done - 37320.\n",
      "Ep done - 37330.\n",
      "Ep done - 37340.\n",
      "Ep done - 37350.\n",
      "Ep done - 37360.\n",
      "Ep done - 37370.\n",
      "Ep done - 37380.\n",
      "Ep done - 37390.\n",
      "Ep done - 37400.\n",
      "Eval num_timesteps=3740000, episode_reward=-0.52 +/- 0.85\n",
      "Episode length: 30.02 +/- 0.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.52      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3740000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20547736 |\n",
      "|    clip_fraction        | 0.0256     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00366   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.125      |\n",
      "|    n_updates            | 25310      |\n",
      "|    policy_gradient_loss | -0.00242   |\n",
      "|    value_loss           | 0.258      |\n",
      "----------------------------------------\n",
      "Ep done - 131950.\n",
      "Ep done - 131960.\n",
      "Ep done - 131970.\n",
      "Ep done - 131980.\n",
      "Ep done - 131990.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 609      |\n",
      "|    time_elapsed    | 13185    |\n",
      "|    total_timesteps | 3741696  |\n",
      "---------------------------------\n",
      "Ep done - 132000.\n",
      "Ep done - 132010.\n",
      "Ep done - 132020.\n",
      "Ep done - 132030.\n",
      "Ep done - 132040.\n",
      "Ep done - 132050.\n",
      "Ep done - 132060.\n",
      "Ep done - 132070.\n",
      "Ep done - 132080.\n",
      "Ep done - 132090.\n",
      "Ep done - 132100.\n",
      "Ep done - 132110.\n",
      "Ep done - 132120.\n",
      "Ep done - 132130.\n",
      "Ep done - 132140.\n",
      "Ep done - 132150.\n",
      "Ep done - 132160.\n",
      "Ep done - 132170.\n",
      "Ep done - 132180.\n",
      "Ep done - 132190.\n",
      "Ep done - 132200.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.43      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 610       |\n",
      "|    time_elapsed         | 13202     |\n",
      "|    total_timesteps      | 3747840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5777989 |\n",
      "|    clip_fraction        | 0.0338    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00172  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.148     |\n",
      "|    n_updates            | 25320     |\n",
      "|    policy_gradient_loss | -0.0037   |\n",
      "|    value_loss           | 0.268     |\n",
      "---------------------------------------\n",
      "Ep done - 132210.\n",
      "Ep done - 132220.\n",
      "Ep done - 132230.\n",
      "Ep done - 132240.\n",
      "Ep done - 132250.\n",
      "Ep done - 132260.\n",
      "Ep done - 132270.\n",
      "Ep done - 37410.\n",
      "Ep done - 37420.\n",
      "Ep done - 37430.\n",
      "Ep done - 37440.\n",
      "Ep done - 37450.\n",
      "Ep done - 37460.\n",
      "Ep done - 37470.\n",
      "Ep done - 37480.\n",
      "Ep done - 37490.\n",
      "Ep done - 37500.\n",
      "Eval num_timesteps=3750000, episode_reward=0.40 +/- 0.92\n",
      "Episode length: 30.02 +/- 0.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.4        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3750000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29759365 |\n",
      "|    clip_fraction        | 0.0236     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000994  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.113      |\n",
      "|    n_updates            | 25330      |\n",
      "|    policy_gradient_loss | 0.0117     |\n",
      "|    value_loss           | 0.234      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.4\n",
      "SELFPLAY: new best model, bumping up generation to 120\n",
      "Ep done - 132280.\n",
      "Ep done - 132290.\n",
      "Ep done - 132300.\n",
      "Ep done - 132310.\n",
      "Ep done - 132320.\n",
      "Ep done - 132330.\n",
      "Ep done - 132340.\n",
      "Ep done - 132350.\n",
      "Ep done - 132360.\n",
      "Ep done - 132370.\n",
      "Ep done - 132380.\n",
      "Ep done - 132390.\n",
      "Ep done - 132400.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.6     |\n",
      "|    ep_rew_mean     | 0.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 611      |\n",
      "|    time_elapsed    | 13224    |\n",
      "|    total_timesteps | 3753984  |\n",
      "---------------------------------\n",
      "Ep done - 132410.\n",
      "Ep done - 132420.\n",
      "Ep done - 132430.\n",
      "Ep done - 132440.\n",
      "Ep done - 132450.\n",
      "Ep done - 132460.\n",
      "Ep done - 132470.\n",
      "Ep done - 132480.\n",
      "Ep done - 132490.\n",
      "Ep done - 132500.\n",
      "Ep done - 132510.\n",
      "Ep done - 132520.\n",
      "Ep done - 132530.\n",
      "Ep done - 132540.\n",
      "Ep done - 132550.\n",
      "Ep done - 132560.\n",
      "Ep done - 132570.\n",
      "Ep done - 132580.\n",
      "Ep done - 132590.\n",
      "Ep done - 132600.\n",
      "Ep done - 37510.\n",
      "Ep done - 37520.\n",
      "Ep done - 37530.\n",
      "Ep done - 37540.\n",
      "Ep done - 37550.\n",
      "Ep done - 37560.\n",
      "Ep done - 37570.\n",
      "Ep done - 37580.\n",
      "Ep done - 37590.\n",
      "Ep done - 37600.\n",
      "Eval num_timesteps=3760000, episode_reward=0.38 +/- 0.91\n",
      "Episode length: 30.70 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.7       |\n",
      "|    mean_reward          | 0.38       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3760000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12382448 |\n",
      "|    clip_fraction        | 0.0128     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000901  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0854     |\n",
      "|    n_updates            | 25340      |\n",
      "|    policy_gradient_loss | -0.00312   |\n",
      "|    value_loss           | 0.267      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.38\n",
      "SELFPLAY: new best model, bumping up generation to 121\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.6     |\n",
      "|    ep_rew_mean     | 0.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 612      |\n",
      "|    time_elapsed    | 13245    |\n",
      "|    total_timesteps | 3760128  |\n",
      "---------------------------------\n",
      "Ep done - 132610.\n",
      "Ep done - 132620.\n",
      "Ep done - 132630.\n",
      "Ep done - 132640.\n",
      "Ep done - 132650.\n",
      "Ep done - 132660.\n",
      "Ep done - 132670.\n",
      "Ep done - 132680.\n",
      "Ep done - 132690.\n",
      "Ep done - 132700.\n",
      "Ep done - 132710.\n",
      "Ep done - 132720.\n",
      "Ep done - 132730.\n",
      "Ep done - 132740.\n",
      "Ep done - 132750.\n",
      "Ep done - 132760.\n",
      "Ep done - 132770.\n",
      "Ep done - 132780.\n",
      "Ep done - 132790.\n",
      "Ep done - 132800.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | -0.46      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 613        |\n",
      "|    time_elapsed         | 13262      |\n",
      "|    total_timesteps      | 3766272    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.62415785 |\n",
      "|    clip_fraction        | 0.0371     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0021    |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.122      |\n",
      "|    n_updates            | 25350      |\n",
      "|    policy_gradient_loss | -0.00867   |\n",
      "|    value_loss           | 0.249      |\n",
      "----------------------------------------\n",
      "Ep done - 132810.\n",
      "Ep done - 132820.\n",
      "Ep done - 132830.\n",
      "Ep done - 132840.\n",
      "Ep done - 132850.\n",
      "Ep done - 132860.\n",
      "Ep done - 132870.\n",
      "Ep done - 132880.\n",
      "Ep done - 132890.\n",
      "Ep done - 132900.\n",
      "Ep done - 132910.\n",
      "Ep done - 132920.\n",
      "Ep done - 132930.\n",
      "Ep done - 37610.\n",
      "Ep done - 37620.\n",
      "Ep done - 37630.\n",
      "Ep done - 37640.\n",
      "Ep done - 37650.\n",
      "Ep done - 37660.\n",
      "Ep done - 37670.\n",
      "Ep done - 37680.\n",
      "Ep done - 37690.\n",
      "Ep done - 37700.\n",
      "Eval num_timesteps=3770000, episode_reward=-0.02 +/- 0.60\n",
      "Episode length: 30.15 +/- 0.36\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.1     |\n",
      "|    mean_reward          | -0.02    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 3770000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.463524 |\n",
      "|    clip_fraction        | 0.0534   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00214 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0923   |\n",
      "|    n_updates            | 25360    |\n",
      "|    policy_gradient_loss | -0.00679 |\n",
      "|    value_loss           | 0.201    |\n",
      "--------------------------------------\n",
      "Ep done - 132940.\n",
      "Ep done - 132950.\n",
      "Ep done - 132960.\n",
      "Ep done - 132970.\n",
      "Ep done - 132980.\n",
      "Ep done - 132990.\n",
      "Ep done - 133000.\n",
      "Ep done - 133010.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 614      |\n",
      "|    time_elapsed    | 13284    |\n",
      "|    total_timesteps | 3772416  |\n",
      "---------------------------------\n",
      "Ep done - 133020.\n",
      "Ep done - 133030.\n",
      "Ep done - 133040.\n",
      "Ep done - 133050.\n",
      "Ep done - 133060.\n",
      "Ep done - 133070.\n",
      "Ep done - 133080.\n",
      "Ep done - 133090.\n",
      "Ep done - 133100.\n",
      "Ep done - 133110.\n",
      "Ep done - 133120.\n",
      "Ep done - 133130.\n",
      "Ep done - 133140.\n",
      "Ep done - 133150.\n",
      "Ep done - 133160.\n",
      "Ep done - 133170.\n",
      "Ep done - 133180.\n",
      "Ep done - 133190.\n",
      "Ep done - 133200.\n",
      "Ep done - 133210.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.12       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 284        |\n",
      "|    iterations           | 615        |\n",
      "|    time_elapsed         | 13300      |\n",
      "|    total_timesteps      | 3778560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12426438 |\n",
      "|    clip_fraction        | 0.00448    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000414  |\n",
      "|    explained_variance   | -2.38e-07  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0364     |\n",
      "|    n_updates            | 25370      |\n",
      "|    policy_gradient_loss | -0.00182   |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "Ep done - 133220.\n",
      "Ep done - 133230.\n",
      "Ep done - 133240.\n",
      "Ep done - 133250.\n",
      "Ep done - 133260.\n",
      "Ep done - 37710.\n",
      "Ep done - 37720.\n",
      "Ep done - 37730.\n",
      "Ep done - 37740.\n",
      "Ep done - 37750.\n",
      "Ep done - 37760.\n",
      "Ep done - 37770.\n",
      "Ep done - 37780.\n",
      "Ep done - 37790.\n",
      "Ep done - 37800.\n",
      "Eval num_timesteps=3780000, episode_reward=0.74 +/- 0.58\n",
      "Episode length: 30.65 +/- 0.48\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.6      |\n",
      "|    mean_reward          | 0.74      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3780000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8914396 |\n",
      "|    clip_fraction        | 0.0375    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000944 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0476    |\n",
      "|    n_updates            | 25380     |\n",
      "|    policy_gradient_loss | -0.00262  |\n",
      "|    value_loss           | 0.113     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.74\n",
      "SELFPLAY: new best model, bumping up generation to 122\n",
      "Ep done - 133270.\n",
      "Ep done - 133280.\n",
      "Ep done - 133290.\n",
      "Ep done - 133300.\n",
      "Ep done - 133310.\n",
      "Ep done - 133320.\n",
      "Ep done - 133330.\n",
      "Ep done - 133340.\n",
      "Ep done - 133350.\n",
      "Ep done - 133360.\n",
      "Ep done - 133370.\n",
      "Ep done - 133380.\n",
      "Ep done - 133390.\n",
      "Ep done - 133400.\n",
      "Ep done - 133410.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 616      |\n",
      "|    time_elapsed    | 13322    |\n",
      "|    total_timesteps | 3784704  |\n",
      "---------------------------------\n",
      "Ep done - 133420.\n",
      "Ep done - 133430.\n",
      "Ep done - 133440.\n",
      "Ep done - 133450.\n",
      "Ep done - 133460.\n",
      "Ep done - 133470.\n",
      "Ep done - 133480.\n",
      "Ep done - 133490.\n",
      "Ep done - 133500.\n",
      "Ep done - 133510.\n",
      "Ep done - 133520.\n",
      "Ep done - 133530.\n",
      "Ep done - 133540.\n",
      "Ep done - 133550.\n",
      "Ep done - 133560.\n",
      "Ep done - 133570.\n",
      "Ep done - 133580.\n",
      "Ep done - 133590.\n",
      "Ep done - 37810.\n",
      "Ep done - 37820.\n",
      "Ep done - 37830.\n",
      "Ep done - 37840.\n",
      "Ep done - 37850.\n",
      "Ep done - 37860.\n",
      "Ep done - 37870.\n",
      "Ep done - 37880.\n",
      "Ep done - 37890.\n",
      "Ep done - 37900.\n",
      "Eval num_timesteps=3790000, episode_reward=0.64 +/- 0.77\n",
      "Episode length: 30.11 +/- 0.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3790000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101109914 |\n",
      "|    clip_fraction        | 0.00757     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000193   |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0857      |\n",
      "|    n_updates            | 25390       |\n",
      "|    policy_gradient_loss | 0.0076      |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.64\n",
      "SELFPLAY: new best model, bumping up generation to 123\n",
      "Ep done - 133600.\n",
      "Ep done - 133610.\n",
      "Ep done - 133620.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 617      |\n",
      "|    time_elapsed    | 13343    |\n",
      "|    total_timesteps | 3790848  |\n",
      "---------------------------------\n",
      "Ep done - 133630.\n",
      "Ep done - 133640.\n",
      "Ep done - 133650.\n",
      "Ep done - 133660.\n",
      "Ep done - 133670.\n",
      "Ep done - 133680.\n",
      "Ep done - 133690.\n",
      "Ep done - 133700.\n",
      "Ep done - 133710.\n",
      "Ep done - 133720.\n",
      "Ep done - 133730.\n",
      "Ep done - 133740.\n",
      "Ep done - 133750.\n",
      "Ep done - 133760.\n",
      "Ep done - 133770.\n",
      "Ep done - 133780.\n",
      "Ep done - 133790.\n",
      "Ep done - 133800.\n",
      "Ep done - 133810.\n",
      "Ep done - 133820.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | -0.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 618       |\n",
      "|    time_elapsed         | 13359     |\n",
      "|    total_timesteps      | 3796992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5469186 |\n",
      "|    clip_fraction        | 0.033     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000584 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0717    |\n",
      "|    n_updates            | 25400     |\n",
      "|    policy_gradient_loss | -0.00327  |\n",
      "|    value_loss           | 0.221     |\n",
      "---------------------------------------\n",
      "Ep done - 133830.\n",
      "Ep done - 133840.\n",
      "Ep done - 133850.\n",
      "Ep done - 133860.\n",
      "Ep done - 133870.\n",
      "Ep done - 133880.\n",
      "Ep done - 133890.\n",
      "Ep done - 133900.\n",
      "Ep done - 133910.\n",
      "Ep done - 133920.\n",
      "Ep done - 37910.\n",
      "Ep done - 37920.\n",
      "Ep done - 37930.\n",
      "Ep done - 37940.\n",
      "Ep done - 37950.\n",
      "Ep done - 37960.\n",
      "Ep done - 37970.\n",
      "Ep done - 37980.\n",
      "Ep done - 37990.\n",
      "Ep done - 38000.\n",
      "Eval num_timesteps=3800000, episode_reward=-0.05 +/- 0.43\n",
      "Episode length: 30.07 +/- 0.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | -0.05      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3800000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81217605 |\n",
      "|    clip_fraction        | 0.0289     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000171  |\n",
      "|    explained_variance   | 2.38e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.104      |\n",
      "|    n_updates            | 25410      |\n",
      "|    policy_gradient_loss | -0.00447   |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "Ep done - 133930.\n",
      "Ep done - 133940.\n",
      "Ep done - 133950.\n",
      "Ep done - 133960.\n",
      "Ep done - 133970.\n",
      "Ep done - 133980.\n",
      "Ep done - 133990.\n",
      "Ep done - 134000.\n",
      "Ep done - 134010.\n",
      "Ep done - 134020.\n",
      "Ep done - 134030.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 619      |\n",
      "|    time_elapsed    | 13382    |\n",
      "|    total_timesteps | 3803136  |\n",
      "---------------------------------\n",
      "Ep done - 134040.\n",
      "Ep done - 134050.\n",
      "Ep done - 134060.\n",
      "Ep done - 134070.\n",
      "Ep done - 134080.\n",
      "Ep done - 134090.\n",
      "Ep done - 134100.\n",
      "Ep done - 134110.\n",
      "Ep done - 134120.\n",
      "Ep done - 134130.\n",
      "Ep done - 134140.\n",
      "Ep done - 134150.\n",
      "Ep done - 134160.\n",
      "Ep done - 134170.\n",
      "Ep done - 134180.\n",
      "Ep done - 134190.\n",
      "Ep done - 134200.\n",
      "Ep done - 134210.\n",
      "Ep done - 134220.\n",
      "Ep done - 134230.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | -0.04       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 284         |\n",
      "|    iterations           | 620         |\n",
      "|    time_elapsed         | 13399       |\n",
      "|    total_timesteps      | 3809280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004603972 |\n",
      "|    clip_fraction        | 0.000391    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.67e-05   |\n",
      "|    explained_variance   | 0.000184    |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 25420       |\n",
      "|    policy_gradient_loss | -0.000135   |\n",
      "|    value_loss           | 0.0576      |\n",
      "-----------------------------------------\n",
      "Ep done - 134240.\n",
      "Ep done - 134250.\n",
      "Ep done - 38010.\n",
      "Ep done - 38020.\n",
      "Ep done - 38030.\n",
      "Ep done - 38040.\n",
      "Ep done - 38050.\n",
      "Ep done - 38060.\n",
      "Ep done - 38070.\n",
      "Ep done - 38080.\n",
      "Ep done - 38090.\n",
      "Ep done - 38100.\n",
      "Eval num_timesteps=3810000, episode_reward=-0.05 +/- 0.41\n",
      "Episode length: 30.06 +/- 0.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | -0.05       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3810000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020437533 |\n",
      "|    clip_fraction        | 0.00146     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.49e-05   |\n",
      "|    explained_variance   | 0.0454      |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0319      |\n",
      "|    n_updates            | 25430       |\n",
      "|    policy_gradient_loss | -0.000531   |\n",
      "|    value_loss           | 0.0654      |\n",
      "-----------------------------------------\n",
      "Ep done - 134260.\n",
      "Ep done - 134270.\n",
      "Ep done - 134280.\n",
      "Ep done - 134290.\n",
      "Ep done - 134300.\n",
      "Ep done - 134310.\n",
      "Ep done - 134320.\n",
      "Ep done - 134330.\n",
      "Ep done - 134340.\n",
      "Ep done - 134350.\n",
      "Ep done - 134360.\n",
      "Ep done - 134370.\n",
      "Ep done - 134380.\n",
      "Ep done - 134390.\n",
      "Ep done - 134400.\n",
      "Ep done - 134410.\n",
      "Ep done - 134420.\n",
      "Ep done - 134430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 621      |\n",
      "|    time_elapsed    | 13420    |\n",
      "|    total_timesteps | 3815424  |\n",
      "---------------------------------\n",
      "Ep done - 134440.\n",
      "Ep done - 134450.\n",
      "Ep done - 134460.\n",
      "Ep done - 134470.\n",
      "Ep done - 134480.\n",
      "Ep done - 134490.\n",
      "Ep done - 134500.\n",
      "Ep done - 134510.\n",
      "Ep done - 134520.\n",
      "Ep done - 134530.\n",
      "Ep done - 134540.\n",
      "Ep done - 134550.\n",
      "Ep done - 134560.\n",
      "Ep done - 134570.\n",
      "Ep done - 134580.\n",
      "Ep done - 134590.\n",
      "Ep done - 38110.\n",
      "Ep done - 38120.\n",
      "Ep done - 38130.\n",
      "Ep done - 38140.\n",
      "Ep done - 38150.\n",
      "Ep done - 38160.\n",
      "Ep done - 38170.\n",
      "Ep done - 38180.\n",
      "Ep done - 38190.\n",
      "Ep done - 38200.\n",
      "Eval num_timesteps=3820000, episode_reward=-0.04 +/- 0.40\n",
      "Episode length: 30.10 +/- 0.30\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.04     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3820000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.39e-06 |\n",
      "|    explained_variance   | 0.0556    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0352    |\n",
      "|    n_updates            | 25440     |\n",
      "|    policy_gradient_loss | 1.39e-09  |\n",
      "|    value_loss           | 0.053     |\n",
      "---------------------------------------\n",
      "Ep done - 134600.\n",
      "Ep done - 134610.\n",
      "Ep done - 134620.\n",
      "Ep done - 134630.\n",
      "Ep done - 134640.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 622      |\n",
      "|    time_elapsed    | 13442    |\n",
      "|    total_timesteps | 3821568  |\n",
      "---------------------------------\n",
      "Ep done - 134650.\n",
      "Ep done - 134660.\n",
      "Ep done - 134670.\n",
      "Ep done - 134680.\n",
      "Ep done - 134690.\n",
      "Ep done - 134700.\n",
      "Ep done - 134710.\n",
      "Ep done - 134720.\n",
      "Ep done - 134730.\n",
      "Ep done - 134740.\n",
      "Ep done - 134750.\n",
      "Ep done - 134760.\n",
      "Ep done - 134770.\n",
      "Ep done - 134780.\n",
      "Ep done - 134790.\n",
      "Ep done - 134800.\n",
      "Ep done - 134810.\n",
      "Ep done - 134820.\n",
      "Ep done - 134830.\n",
      "Ep done - 134840.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | -0.11     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 623       |\n",
      "|    time_elapsed         | 13459     |\n",
      "|    total_timesteps      | 3827712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.58e-05 |\n",
      "|    explained_variance   | 0.0795    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0186    |\n",
      "|    n_updates            | 25450     |\n",
      "|    policy_gradient_loss | -1.89e-08 |\n",
      "|    value_loss           | 0.0572    |\n",
      "---------------------------------------\n",
      "Ep done - 134850.\n",
      "Ep done - 134860.\n",
      "Ep done - 134870.\n",
      "Ep done - 134880.\n",
      "Ep done - 134890.\n",
      "Ep done - 134900.\n",
      "Ep done - 134910.\n",
      "Ep done - 134920.\n",
      "Ep done - 38210.\n",
      "Ep done - 38220.\n",
      "Ep done - 38230.\n",
      "Ep done - 38240.\n",
      "Ep done - 38250.\n",
      "Ep done - 38260.\n",
      "Ep done - 38270.\n",
      "Ep done - 38280.\n",
      "Ep done - 38290.\n",
      "Ep done - 38300.\n",
      "Eval num_timesteps=3830000, episode_reward=-0.01 +/- 0.46\n",
      "Episode length: 30.10 +/- 0.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | -0.01       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3830000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009953287 |\n",
      "|    clip_fraction        | 0.00101     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.74e-05   |\n",
      "|    explained_variance   | 0.0889      |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0309      |\n",
      "|    n_updates            | 25460       |\n",
      "|    policy_gradient_loss | -0.000363   |\n",
      "|    value_loss           | 0.0548      |\n",
      "-----------------------------------------\n",
      "Ep done - 134930.\n",
      "Ep done - 134940.\n",
      "Ep done - 134950.\n",
      "Ep done - 134960.\n",
      "Ep done - 134970.\n",
      "Ep done - 134980.\n",
      "Ep done - 134990.\n",
      "Ep done - 135000.\n",
      "Ep done - 135010.\n",
      "Ep done - 135020.\n",
      "Ep done - 135030.\n",
      "Ep done - 135040.\n",
      "Ep done - 135050.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 624      |\n",
      "|    time_elapsed    | 13481    |\n",
      "|    total_timesteps | 3833856  |\n",
      "---------------------------------\n",
      "Ep done - 135060.\n",
      "Ep done - 135070.\n",
      "Ep done - 135080.\n",
      "Ep done - 135090.\n",
      "Ep done - 135100.\n",
      "Ep done - 135110.\n",
      "Ep done - 135120.\n",
      "Ep done - 135130.\n",
      "Ep done - 135140.\n",
      "Ep done - 135150.\n",
      "Ep done - 135160.\n",
      "Ep done - 135170.\n",
      "Ep done - 135180.\n",
      "Ep done - 135190.\n",
      "Ep done - 135200.\n",
      "Ep done - 135210.\n",
      "Ep done - 135220.\n",
      "Ep done - 135230.\n",
      "Ep done - 135240.\n",
      "Ep done - 135250.\n",
      "Ep done - 38310.\n",
      "Ep done - 38320.\n",
      "Ep done - 38330.\n",
      "Ep done - 38340.\n",
      "Ep done - 38350.\n",
      "Ep done - 38360.\n",
      "Ep done - 38370.\n",
      "Ep done - 38380.\n",
      "Ep done - 38390.\n",
      "Ep done - 38400.\n",
      "Eval num_timesteps=3840000, episode_reward=-0.02 +/- 0.45\n",
      "Episode length: 30.08 +/- 0.27\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.02     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3840000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.31e-05 |\n",
      "|    explained_variance   | 0.158     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0112    |\n",
      "|    n_updates            | 25470     |\n",
      "|    policy_gradient_loss | -1.8e-10  |\n",
      "|    value_loss           | 0.0502    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 625      |\n",
      "|    time_elapsed    | 13507    |\n",
      "|    total_timesteps | 3840000  |\n",
      "---------------------------------\n",
      "Ep done - 135260.\n",
      "Ep done - 135270.\n",
      "Ep done - 135280.\n",
      "Ep done - 135290.\n",
      "Ep done - 135300.\n",
      "Ep done - 135310.\n",
      "Ep done - 135320.\n",
      "Ep done - 135330.\n",
      "Ep done - 135340.\n",
      "Ep done - 135350.\n",
      "Ep done - 135360.\n",
      "Ep done - 135370.\n",
      "Ep done - 135380.\n",
      "Ep done - 135390.\n",
      "Ep done - 135400.\n",
      "Ep done - 135410.\n",
      "Ep done - 135420.\n",
      "Ep done - 135430.\n",
      "Ep done - 135440.\n",
      "Ep done - 135450.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30.1     |\n",
      "|    ep_rew_mean          | 0.05     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 284      |\n",
      "|    iterations           | 626      |\n",
      "|    time_elapsed         | 13528    |\n",
      "|    total_timesteps      | 3846144  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -6.1e-05 |\n",
      "|    explained_variance   | 0.124    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0206   |\n",
      "|    n_updates            | 25480    |\n",
      "|    policy_gradient_loss | 9.2e-07  |\n",
      "|    value_loss           | 0.0591   |\n",
      "--------------------------------------\n",
      "Ep done - 135460.\n",
      "Ep done - 135470.\n",
      "Ep done - 135480.\n",
      "Ep done - 135490.\n",
      "Ep done - 135500.\n",
      "Ep done - 135510.\n",
      "Ep done - 135520.\n",
      "Ep done - 135530.\n",
      "Ep done - 135540.\n",
      "Ep done - 135550.\n",
      "Ep done - 135560.\n",
      "Ep done - 135570.\n",
      "Ep done - 135580.\n",
      "Ep done - 38410.\n",
      "Ep done - 38420.\n",
      "Ep done - 38430.\n",
      "Ep done - 38440.\n",
      "Ep done - 38450.\n",
      "Ep done - 38460.\n",
      "Ep done - 38470.\n",
      "Ep done - 38480.\n",
      "Ep done - 38490.\n",
      "Ep done - 38500.\n",
      "Eval num_timesteps=3850000, episode_reward=0.01 +/- 0.39\n",
      "Episode length: 30.08 +/- 0.27\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | 0.01      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3850000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.54e-06 |\n",
      "|    explained_variance   | 0.15      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.018     |\n",
      "|    n_updates            | 25490     |\n",
      "|    policy_gradient_loss | 6.39e-10  |\n",
      "|    value_loss           | 0.0509    |\n",
      "---------------------------------------\n",
      "Ep done - 135590.\n",
      "Ep done - 135600.\n",
      "Ep done - 135610.\n",
      "Ep done - 135620.\n",
      "Ep done - 135630.\n",
      "Ep done - 135640.\n",
      "Ep done - 135650.\n",
      "Ep done - 135660.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 627      |\n",
      "|    time_elapsed    | 13546    |\n",
      "|    total_timesteps | 3852288  |\n",
      "---------------------------------\n",
      "Ep done - 135670.\n",
      "Ep done - 135680.\n",
      "Ep done - 135690.\n",
      "Ep done - 135700.\n",
      "Ep done - 135710.\n",
      "Ep done - 135720.\n",
      "Ep done - 135730.\n",
      "Ep done - 135740.\n",
      "Ep done - 135750.\n",
      "Ep done - 135760.\n",
      "Ep done - 135770.\n",
      "Ep done - 135780.\n",
      "Ep done - 135790.\n",
      "Ep done - 135800.\n",
      "Ep done - 135810.\n",
      "Ep done - 135820.\n",
      "Ep done - 135830.\n",
      "Ep done - 135840.\n",
      "Ep done - 135850.\n",
      "Ep done - 135860.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | -0.01     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 628       |\n",
      "|    time_elapsed         | 13568     |\n",
      "|    total_timesteps      | 3858432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.12e-05 |\n",
      "|    explained_variance   | 0.151     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0241    |\n",
      "|    n_updates            | 25500     |\n",
      "|    policy_gradient_loss | 1.92e-10  |\n",
      "|    value_loss           | 0.0412    |\n",
      "---------------------------------------\n",
      "Ep done - 135870.\n",
      "Ep done - 135880.\n",
      "Ep done - 135890.\n",
      "Ep done - 135900.\n",
      "Ep done - 135910.\n",
      "Ep done - 38510.\n",
      "Ep done - 38520.\n",
      "Ep done - 38530.\n",
      "Ep done - 38540.\n",
      "Ep done - 38550.\n",
      "Ep done - 38560.\n",
      "Ep done - 38570.\n",
      "Ep done - 38580.\n",
      "Ep done - 38590.\n",
      "Ep done - 38600.\n",
      "Eval num_timesteps=3860000, episode_reward=0.90 +/- 0.44\n",
      "Episode length: 30.95 +/- 0.22\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.9      |\n",
      "|    mean_reward          | 0.9       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3860000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5260005 |\n",
      "|    clip_fraction        | 0.0266    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.03e-05 |\n",
      "|    explained_variance   | 0.193     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0266    |\n",
      "|    n_updates            | 25510     |\n",
      "|    policy_gradient_loss | 0.00118   |\n",
      "|    value_loss           | 0.058     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.9\n",
      "SELFPLAY: new best model, bumping up generation to 124\n",
      "Ep done - 135920.\n",
      "Ep done - 135930.\n",
      "Ep done - 135940.\n",
      "Ep done - 135950.\n",
      "Ep done - 135960.\n",
      "Ep done - 135970.\n",
      "Ep done - 135980.\n",
      "Ep done - 135990.\n",
      "Ep done - 136000.\n",
      "Ep done - 136010.\n",
      "Ep done - 136020.\n",
      "Ep done - 136030.\n",
      "Ep done - 136040.\n",
      "Ep done - 136050.\n",
      "Ep done - 136060.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31       |\n",
      "|    ep_rew_mean     | 0.94     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 629      |\n",
      "|    time_elapsed    | 13591    |\n",
      "|    total_timesteps | 3864576  |\n",
      "---------------------------------\n",
      "Ep done - 136070.\n",
      "Ep done - 136080.\n",
      "Ep done - 136090.\n",
      "Ep done - 136100.\n",
      "Ep done - 136110.\n",
      "Ep done - 136120.\n",
      "Ep done - 136130.\n",
      "Ep done - 136140.\n",
      "Ep done - 136150.\n",
      "Ep done - 136160.\n",
      "Ep done - 136170.\n",
      "Ep done - 136180.\n",
      "Ep done - 136190.\n",
      "Ep done - 136200.\n",
      "Ep done - 136210.\n",
      "Ep done - 136220.\n",
      "Ep done - 136230.\n",
      "Ep done - 136240.\n",
      "Ep done - 38610.\n",
      "Ep done - 38620.\n",
      "Ep done - 38630.\n",
      "Ep done - 38640.\n",
      "Ep done - 38650.\n",
      "Ep done - 38660.\n",
      "Ep done - 38670.\n",
      "Ep done - 38680.\n",
      "Ep done - 38690.\n",
      "Ep done - 38700.\n",
      "Eval num_timesteps=3870000, episode_reward=-0.80 +/- 0.57\n",
      "Episode length: 30.13 +/- 0.34\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.1      |\n",
      "|    mean_reward          | -0.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3870000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9613165 |\n",
      "|    clip_fraction        | 0.0605    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000132 |\n",
      "|    explained_variance   | 0.0885    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.025     |\n",
      "|    n_updates            | 25520     |\n",
      "|    policy_gradient_loss | -0.0107   |\n",
      "|    value_loss           | 0.082     |\n",
      "---------------------------------------\n",
      "Ep done - 136250.\n",
      "Ep done - 136260.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 630      |\n",
      "|    time_elapsed    | 13614    |\n",
      "|    total_timesteps | 3870720  |\n",
      "---------------------------------\n",
      "Ep done - 136270.\n",
      "Ep done - 136280.\n",
      "Ep done - 136290.\n",
      "Ep done - 136300.\n",
      "Ep done - 136310.\n",
      "Ep done - 136320.\n",
      "Ep done - 136330.\n",
      "Ep done - 136340.\n",
      "Ep done - 136350.\n",
      "Ep done - 136360.\n",
      "Ep done - 136370.\n",
      "Ep done - 136380.\n",
      "Ep done - 136390.\n",
      "Ep done - 136400.\n",
      "Ep done - 136410.\n",
      "Ep done - 136420.\n",
      "Ep done - 136430.\n",
      "Ep done - 136440.\n",
      "Ep done - 136450.\n",
      "Ep done - 136460.\n",
      "Ep done - 136470.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.09      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 631       |\n",
      "|    time_elapsed         | 13633     |\n",
      "|    total_timesteps      | 3876864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3601636 |\n",
      "|    clip_fraction        | 0.0993    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000479 |\n",
      "|    explained_variance   | -1.37     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0388    |\n",
      "|    n_updates            | 25530     |\n",
      "|    policy_gradient_loss | 0.00625   |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "Ep done - 136480.\n",
      "Ep done - 136490.\n",
      "Ep done - 136500.\n",
      "Ep done - 136510.\n",
      "Ep done - 136520.\n",
      "Ep done - 136530.\n",
      "Ep done - 136540.\n",
      "Ep done - 136550.\n",
      "Ep done - 136560.\n",
      "Ep done - 136570.\n",
      "Ep done - 38710.\n",
      "Ep done - 38720.\n",
      "Ep done - 38730.\n",
      "Ep done - 38740.\n",
      "Ep done - 38750.\n",
      "Ep done - 38760.\n",
      "Ep done - 38770.\n",
      "Ep done - 38780.\n",
      "Ep done - 38790.\n",
      "Ep done - 38800.\n",
      "Eval num_timesteps=3880000, episode_reward=0.75 +/- 0.62\n",
      "Episode length: 30.85 +/- 0.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.9       |\n",
      "|    mean_reward          | 0.75       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3880000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56487197 |\n",
      "|    clip_fraction        | 0.00382    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000344  |\n",
      "|    explained_variance   | -6.08e-05  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0159     |\n",
      "|    n_updates            | 25540      |\n",
      "|    policy_gradient_loss | -7.39e-05  |\n",
      "|    value_loss           | 0.0409     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.75\n",
      "SELFPLAY: new best model, bumping up generation to 125\n",
      "Ep done - 136580.\n",
      "Ep done - 136590.\n",
      "Ep done - 136600.\n",
      "Ep done - 136610.\n",
      "Ep done - 136620.\n",
      "Ep done - 136630.\n",
      "Ep done - 136640.\n",
      "Ep done - 136650.\n",
      "Ep done - 136660.\n",
      "Ep done - 136670.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 632      |\n",
      "|    time_elapsed    | 13659    |\n",
      "|    total_timesteps | 3883008  |\n",
      "---------------------------------\n",
      "Ep done - 136680.\n",
      "Ep done - 136690.\n",
      "Ep done - 136700.\n",
      "Ep done - 136710.\n",
      "Ep done - 136720.\n",
      "Ep done - 136730.\n",
      "Ep done - 136740.\n",
      "Ep done - 136750.\n",
      "Ep done - 136760.\n",
      "Ep done - 136770.\n",
      "Ep done - 136780.\n",
      "Ep done - 136790.\n",
      "Ep done - 136800.\n",
      "Ep done - 136810.\n",
      "Ep done - 136820.\n",
      "Ep done - 136830.\n",
      "Ep done - 136840.\n",
      "Ep done - 136850.\n",
      "Ep done - 136860.\n",
      "Ep done - 136870.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.3      |\n",
      "|    ep_rew_mean          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 633       |\n",
      "|    time_elapsed         | 13674     |\n",
      "|    total_timesteps      | 3889152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4585665 |\n",
      "|    clip_fraction        | 0.078     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000231 |\n",
      "|    explained_variance   | 2.38e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.128     |\n",
      "|    n_updates            | 25550     |\n",
      "|    policy_gradient_loss | -0.0107   |\n",
      "|    value_loss           | 0.286     |\n",
      "---------------------------------------\n",
      "Ep done - 136880.\n",
      "Ep done - 136890.\n",
      "Ep done - 136900.\n",
      "Ep done - 38810.\n",
      "Ep done - 38820.\n",
      "Ep done - 38830.\n",
      "Ep done - 38840.\n",
      "Ep done - 38850.\n",
      "Ep done - 38860.\n",
      "Ep done - 38870.\n",
      "Ep done - 38880.\n",
      "Ep done - 38890.\n",
      "Ep done - 38900.\n",
      "Eval num_timesteps=3890000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3890000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.503202  |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000633 |\n",
      "|    explained_variance   | -0.137    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00553   |\n",
      "|    n_updates            | 25560     |\n",
      "|    policy_gradient_loss | -0.0235   |\n",
      "|    value_loss           | 0.0762    |\n",
      "---------------------------------------\n",
      "Ep done - 136910.\n",
      "Ep done - 136920.\n",
      "Ep done - 136930.\n",
      "Ep done - 136940.\n",
      "Ep done - 136950.\n",
      "Ep done - 136960.\n",
      "Ep done - 136970.\n",
      "Ep done - 136980.\n",
      "Ep done - 136990.\n",
      "Ep done - 137000.\n",
      "Ep done - 137010.\n",
      "Ep done - 137020.\n",
      "Ep done - 137030.\n",
      "Ep done - 137040.\n",
      "Ep done - 137050.\n",
      "Ep done - 137060.\n",
      "Ep done - 137070.\n",
      "Ep done - 137080.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 634      |\n",
      "|    time_elapsed    | 13697    |\n",
      "|    total_timesteps | 3895296  |\n",
      "---------------------------------\n",
      "Ep done - 137090.\n",
      "Ep done - 137100.\n",
      "Ep done - 137110.\n",
      "Ep done - 137120.\n",
      "Ep done - 137130.\n",
      "Ep done - 137140.\n",
      "Ep done - 137150.\n",
      "Ep done - 137160.\n",
      "Ep done - 137170.\n",
      "Ep done - 137180.\n",
      "Ep done - 137190.\n",
      "Ep done - 137200.\n",
      "Ep done - 137210.\n",
      "Ep done - 137220.\n",
      "Ep done - 137230.\n",
      "Ep done - 38910.\n",
      "Ep done - 38920.\n",
      "Ep done - 38930.\n",
      "Ep done - 38940.\n",
      "Ep done - 38950.\n",
      "Ep done - 38960.\n",
      "Ep done - 38970.\n",
      "Ep done - 38980.\n",
      "Ep done - 38990.\n",
      "Ep done - 39000.\n",
      "Eval num_timesteps=3900000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.02 +/- 0.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3900000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44831228 |\n",
      "|    clip_fraction        | 0.0325     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000205  |\n",
      "|    explained_variance   | -2.62e-05  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0106     |\n",
      "|    n_updates            | 25570      |\n",
      "|    policy_gradient_loss | -0.00169   |\n",
      "|    value_loss           | 0.0328     |\n",
      "----------------------------------------\n",
      "Ep done - 137240.\n",
      "Ep done - 137250.\n",
      "Ep done - 137260.\n",
      "Ep done - 137270.\n",
      "Ep done - 137280.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 635      |\n",
      "|    time_elapsed    | 13719    |\n",
      "|    total_timesteps | 3901440  |\n",
      "---------------------------------\n",
      "Ep done - 137290.\n",
      "Ep done - 137300.\n",
      "Ep done - 137310.\n",
      "Ep done - 137320.\n",
      "Ep done - 137330.\n",
      "Ep done - 137340.\n",
      "Ep done - 137350.\n",
      "Ep done - 137360.\n",
      "Ep done - 137370.\n",
      "Ep done - 137380.\n",
      "Ep done - 137390.\n",
      "Ep done - 137400.\n",
      "Ep done - 137410.\n",
      "Ep done - 137420.\n",
      "Ep done - 137430.\n",
      "Ep done - 137440.\n",
      "Ep done - 137450.\n",
      "Ep done - 137460.\n",
      "Ep done - 137470.\n",
      "Ep done - 137480.\n",
      "Ep done - 137490.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 636       |\n",
      "|    time_elapsed         | 13736     |\n",
      "|    total_timesteps      | 3907584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6097945 |\n",
      "|    clip_fraction        | 0.00452   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.24e-05 |\n",
      "|    explained_variance   | -1.12e-05 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00088  |\n",
      "|    n_updates            | 25580     |\n",
      "|    policy_gradient_loss | -0.00151  |\n",
      "|    value_loss           | 0.0197    |\n",
      "---------------------------------------\n",
      "Ep done - 137500.\n",
      "Ep done - 137510.\n",
      "Ep done - 137520.\n",
      "Ep done - 137530.\n",
      "Ep done - 137540.\n",
      "Ep done - 137550.\n",
      "Ep done - 137560.\n",
      "Ep done - 137570.\n",
      "Ep done - 39010.\n",
      "Ep done - 39020.\n",
      "Ep done - 39030.\n",
      "Ep done - 39040.\n",
      "Ep done - 39050.\n",
      "Ep done - 39060.\n",
      "Ep done - 39070.\n",
      "Ep done - 39080.\n",
      "Ep done - 39090.\n",
      "Ep done - 39100.\n",
      "Eval num_timesteps=3910000, episode_reward=-0.96 +/- 0.28\n",
      "Episode length: 30.03 +/- 0.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.96     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3910000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.4e-06  |\n",
      "|    explained_variance   | -5.96e-06 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0111    |\n",
      "|    n_updates            | 25590     |\n",
      "|    policy_gradient_loss | 1.36e-09  |\n",
      "|    value_loss           | 0.0184    |\n",
      "---------------------------------------\n",
      "Ep done - 137580.\n",
      "Ep done - 137590.\n",
      "Ep done - 137600.\n",
      "Ep done - 137610.\n",
      "Ep done - 137620.\n",
      "Ep done - 137630.\n",
      "Ep done - 137640.\n",
      "Ep done - 137650.\n",
      "Ep done - 137660.\n",
      "Ep done - 137670.\n",
      "Ep done - 137680.\n",
      "Ep done - 137690.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 637      |\n",
      "|    time_elapsed    | 13756    |\n",
      "|    total_timesteps | 3913728  |\n",
      "---------------------------------\n",
      "Ep done - 137700.\n",
      "Ep done - 137710.\n",
      "Ep done - 137720.\n",
      "Ep done - 137730.\n",
      "Ep done - 137740.\n",
      "Ep done - 137750.\n",
      "Ep done - 137760.\n",
      "Ep done - 137770.\n",
      "Ep done - 137780.\n",
      "Ep done - 137790.\n",
      "Ep done - 137800.\n",
      "Ep done - 137810.\n",
      "Ep done - 137820.\n",
      "Ep done - 137830.\n",
      "Ep done - 137840.\n",
      "Ep done - 137850.\n",
      "Ep done - 137860.\n",
      "Ep done - 137870.\n",
      "Ep done - 137880.\n",
      "Ep done - 137890.\n",
      "Ep done - 137900.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 638       |\n",
      "|    time_elapsed         | 13770     |\n",
      "|    total_timesteps      | 3919872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.19e-06 |\n",
      "|    explained_variance   | -7.39e-06 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00444   |\n",
      "|    n_updates            | 25600     |\n",
      "|    policy_gradient_loss | 3.21e-10  |\n",
      "|    value_loss           | 0.00999   |\n",
      "---------------------------------------\n",
      "Ep done - 39110.\n",
      "Ep done - 39120.\n",
      "Ep done - 39130.\n",
      "Ep done - 39140.\n",
      "Ep done - 39150.\n",
      "Ep done - 39160.\n",
      "Ep done - 39170.\n",
      "Ep done - 39180.\n",
      "Ep done - 39190.\n",
      "Ep done - 39200.\n",
      "Eval num_timesteps=3920000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.05 +/- 0.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30.1         |\n",
      "|    mean_reward          | -0.98        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3920000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.761022e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.54e-06    |\n",
      "|    explained_variance   | -6.32e-06    |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00287      |\n",
      "|    n_updates            | 25610        |\n",
      "|    policy_gradient_loss | -1.13e-07    |\n",
      "|    value_loss           | 0.00678      |\n",
      "------------------------------------------\n",
      "Ep done - 137910.\n",
      "Ep done - 137920.\n",
      "Ep done - 137930.\n",
      "Ep done - 137940.\n",
      "Ep done - 137950.\n",
      "Ep done - 137960.\n",
      "Ep done - 137970.\n",
      "Ep done - 137980.\n",
      "Ep done - 137990.\n",
      "Ep done - 138000.\n",
      "Ep done - 138010.\n",
      "Ep done - 138020.\n",
      "Ep done - 138030.\n",
      "Ep done - 138040.\n",
      "Ep done - 138050.\n",
      "Ep done - 138060.\n",
      "Ep done - 138070.\n",
      "Ep done - 138080.\n",
      "Ep done - 138090.\n",
      "Ep done - 138100.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 639      |\n",
      "|    time_elapsed    | 13808    |\n",
      "|    total_timesteps | 3926016  |\n",
      "---------------------------------\n",
      "Ep done - 138110.\n",
      "Ep done - 138120.\n",
      "Ep done - 138130.\n",
      "Ep done - 138140.\n",
      "Ep done - 138150.\n",
      "Ep done - 138160.\n",
      "Ep done - 138170.\n",
      "Ep done - 138180.\n",
      "Ep done - 138190.\n",
      "Ep done - 138200.\n",
      "Ep done - 138210.\n",
      "Ep done - 138220.\n",
      "Ep done - 138230.\n",
      "Ep done - 39210.\n",
      "Ep done - 39220.\n",
      "Ep done - 39230.\n",
      "Ep done - 39240.\n",
      "Ep done - 39250.\n",
      "Ep done - 39260.\n",
      "Ep done - 39270.\n",
      "Ep done - 39280.\n",
      "Ep done - 39290.\n",
      "Ep done - 39300.\n",
      "Eval num_timesteps=3930000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.03 +/- 0.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3930000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44911075 |\n",
      "|    clip_fraction        | 0.0232     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000242  |\n",
      "|    explained_variance   | -2.03e-06  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00274    |\n",
      "|    n_updates            | 25620      |\n",
      "|    policy_gradient_loss | -0.00562   |\n",
      "|    value_loss           | 0.011      |\n",
      "----------------------------------------\n",
      "Ep done - 138240.\n",
      "Ep done - 138250.\n",
      "Ep done - 138260.\n",
      "Ep done - 138270.\n",
      "Ep done - 138280.\n",
      "Ep done - 138290.\n",
      "Ep done - 138300.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 640      |\n",
      "|    time_elapsed    | 13841    |\n",
      "|    total_timesteps | 3932160  |\n",
      "---------------------------------\n",
      "Ep done - 138310.\n",
      "Ep done - 138320.\n",
      "Ep done - 138330.\n",
      "Ep done - 138340.\n",
      "Ep done - 138350.\n",
      "Ep done - 138360.\n",
      "Ep done - 138370.\n",
      "Ep done - 138380.\n",
      "Ep done - 138390.\n",
      "Ep done - 138400.\n",
      "Ep done - 138410.\n",
      "Ep done - 138420.\n",
      "Ep done - 138430.\n",
      "Ep done - 138440.\n",
      "Ep done - 138450.\n",
      "Ep done - 138460.\n",
      "Ep done - 138470.\n",
      "Ep done - 138480.\n",
      "Ep done - 138490.\n",
      "Ep done - 138500.\n",
      "Ep done - 138510.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 641       |\n",
      "|    time_elapsed         | 13872     |\n",
      "|    total_timesteps      | 3938304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8706294 |\n",
      "|    clip_fraction        | 0.0578    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000225 |\n",
      "|    explained_variance   | -5.96e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0317    |\n",
      "|    n_updates            | 25630     |\n",
      "|    policy_gradient_loss | -0.00911  |\n",
      "|    value_loss           | 0.01      |\n",
      "---------------------------------------\n",
      "Ep done - 138520.\n",
      "Ep done - 138530.\n",
      "Ep done - 138540.\n",
      "Ep done - 138550.\n",
      "Ep done - 138560.\n",
      "Ep done - 138570.\n",
      "Ep done - 39310.\n",
      "Ep done - 39320.\n",
      "Ep done - 39330.\n",
      "Ep done - 39340.\n",
      "Ep done - 39350.\n",
      "Ep done - 39360.\n",
      "Ep done - 39370.\n",
      "Ep done - 39380.\n",
      "Ep done - 39390.\n",
      "Ep done - 39400.\n",
      "Eval num_timesteps=3940000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3940000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.013916  |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000321 |\n",
      "|    explained_variance   | -4.53e-06 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0444   |\n",
      "|    n_updates            | 25640     |\n",
      "|    policy_gradient_loss | -0.0181   |\n",
      "|    value_loss           | 0.00771   |\n",
      "---------------------------------------\n",
      "Ep done - 138580.\n",
      "Ep done - 138590.\n",
      "Ep done - 138600.\n",
      "Ep done - 138610.\n",
      "Ep done - 138620.\n",
      "Ep done - 138630.\n",
      "Ep done - 138640.\n",
      "Ep done - 138650.\n",
      "Ep done - 138660.\n",
      "Ep done - 138670.\n",
      "Ep done - 138680.\n",
      "Ep done - 138690.\n",
      "Ep done - 138700.\n",
      "Ep done - 138710.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 642      |\n",
      "|    time_elapsed    | 13906    |\n",
      "|    total_timesteps | 3944448  |\n",
      "---------------------------------\n",
      "Ep done - 138720.\n",
      "Ep done - 138730.\n",
      "Ep done - 138740.\n",
      "Ep done - 138750.\n",
      "Ep done - 138760.\n",
      "Ep done - 138770.\n",
      "Ep done - 138780.\n",
      "Ep done - 138790.\n",
      "Ep done - 138800.\n",
      "Ep done - 138810.\n",
      "Ep done - 138820.\n",
      "Ep done - 138830.\n",
      "Ep done - 138840.\n",
      "Ep done - 138850.\n",
      "Ep done - 138860.\n",
      "Ep done - 138870.\n",
      "Ep done - 138880.\n",
      "Ep done - 138890.\n",
      "Ep done - 138900.\n",
      "Ep done - 39410.\n",
      "Ep done - 39420.\n",
      "Ep done - 39430.\n",
      "Ep done - 39440.\n",
      "Ep done - 39450.\n",
      "Ep done - 39460.\n",
      "Ep done - 39470.\n",
      "Ep done - 39480.\n",
      "Ep done - 39490.\n",
      "Ep done - 39500.\n",
      "Eval num_timesteps=3950000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.99 +/- 0.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -1            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 3950000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7163574e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -5.57e-06     |\n",
      "|    explained_variance   | -1.55e-06     |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.0149        |\n",
      "|    n_updates            | 25650         |\n",
      "|    policy_gradient_loss | -3.3e-08      |\n",
      "|    value_loss           | 0.0121        |\n",
      "-------------------------------------------\n",
      "Ep done - 138910.\n",
      "Ep done - 138920.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 643      |\n",
      "|    time_elapsed    | 13928    |\n",
      "|    total_timesteps | 3950592  |\n",
      "---------------------------------\n",
      "Ep done - 138930.\n",
      "Ep done - 138940.\n",
      "Ep done - 138950.\n",
      "Ep done - 138960.\n",
      "Ep done - 138970.\n",
      "Ep done - 138980.\n",
      "Ep done - 138990.\n",
      "Ep done - 139000.\n",
      "Ep done - 139010.\n",
      "Ep done - 139020.\n",
      "Ep done - 139030.\n",
      "Ep done - 139040.\n",
      "Ep done - 139050.\n",
      "Ep done - 139060.\n",
      "Ep done - 139070.\n",
      "Ep done - 139080.\n",
      "Ep done - 139090.\n",
      "Ep done - 139100.\n",
      "Ep done - 139110.\n",
      "Ep done - 139120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -1          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 644         |\n",
      "|    time_elapsed         | 13975       |\n",
      "|    total_timesteps      | 3956736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023801802 |\n",
      "|    clip_fraction        | 0.00114     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.13e-05   |\n",
      "|    explained_variance   | -1.19e-06   |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.00699     |\n",
      "|    n_updates            | 25660       |\n",
      "|    policy_gradient_loss | -4.67e-05   |\n",
      "|    value_loss           | 0.0147      |\n",
      "-----------------------------------------\n",
      "Ep done - 139130.\n",
      "Ep done - 139140.\n",
      "Ep done - 139150.\n",
      "Ep done - 139160.\n",
      "Ep done - 139170.\n",
      "Ep done - 139180.\n",
      "Ep done - 139190.\n",
      "Ep done - 139200.\n",
      "Ep done - 139210.\n",
      "Ep done - 139220.\n",
      "Ep done - 139230.\n",
      "Ep done - 39510.\n",
      "Ep done - 39520.\n",
      "Ep done - 39530.\n",
      "Ep done - 39540.\n",
      "Ep done - 39550.\n",
      "Ep done - 39560.\n",
      "Ep done - 39570.\n",
      "Ep done - 39580.\n",
      "Ep done - 39590.\n",
      "Ep done - 39600.\n",
      "Eval num_timesteps=3960000, episode_reward=0.94 +/- 0.34\n",
      "Episode length: 30.30 +/- 0.48\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | 0.94      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3960000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7795666 |\n",
      "|    clip_fraction        | 0.0239    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.79e-05 |\n",
      "|    explained_variance   | -1.43e-06 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00482  |\n",
      "|    n_updates            | 25670     |\n",
      "|    policy_gradient_loss | -0.00203  |\n",
      "|    value_loss           | 0.00552   |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.94\n",
      "SELFPLAY: new best model, bumping up generation to 126\n",
      "Ep done - 139240.\n",
      "Ep done - 139250.\n",
      "Ep done - 139260.\n",
      "Ep done - 139270.\n",
      "Ep done - 139280.\n",
      "Ep done - 139290.\n",
      "Ep done - 139300.\n",
      "Ep done - 139310.\n",
      "Ep done - 139320.\n",
      "Ep done - 139330.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 645      |\n",
      "|    time_elapsed    | 13999    |\n",
      "|    total_timesteps | 3962880  |\n",
      "---------------------------------\n",
      "Ep done - 139340.\n",
      "Ep done - 139350.\n",
      "Ep done - 139360.\n",
      "Ep done - 139370.\n",
      "Ep done - 139380.\n",
      "Ep done - 139390.\n",
      "Ep done - 139400.\n",
      "Ep done - 139410.\n",
      "Ep done - 139420.\n",
      "Ep done - 139430.\n",
      "Ep done - 139440.\n",
      "Ep done - 139450.\n",
      "Ep done - 139460.\n",
      "Ep done - 139470.\n",
      "Ep done - 139480.\n",
      "Ep done - 139490.\n",
      "Ep done - 139500.\n",
      "Ep done - 139510.\n",
      "Ep done - 139520.\n",
      "Ep done - 139530.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.26       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 646        |\n",
      "|    time_elapsed         | 14015      |\n",
      "|    total_timesteps      | 3969024    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06736153 |\n",
      "|    clip_fraction        | 0.00863    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000299  |\n",
      "|    explained_variance   | -2.38e-07  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.119      |\n",
      "|    n_updates            | 25680      |\n",
      "|    policy_gradient_loss | -0.00186   |\n",
      "|    value_loss           | 0.258      |\n",
      "----------------------------------------\n",
      "Ep done - 139540.\n",
      "Ep done - 139550.\n",
      "Ep done - 139560.\n",
      "Ep done - 39610.\n",
      "Ep done - 39620.\n",
      "Ep done - 39630.\n",
      "Ep done - 39640.\n",
      "Ep done - 39650.\n",
      "Ep done - 39660.\n",
      "Ep done - 39670.\n",
      "Ep done - 39680.\n",
      "Ep done - 39690.\n",
      "Ep done - 39700.\n",
      "Eval num_timesteps=3970000, episode_reward=0.38 +/- 0.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.38      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3970000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6234537 |\n",
      "|    clip_fraction        | 0.0588    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000124 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.106     |\n",
      "|    n_updates            | 25690     |\n",
      "|    policy_gradient_loss | -0.0157   |\n",
      "|    value_loss           | 0.287     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.38\n",
      "SELFPLAY: new best model, bumping up generation to 127\n",
      "Ep done - 139570.\n",
      "Ep done - 139580.\n",
      "Ep done - 139590.\n",
      "Ep done - 139600.\n",
      "Ep done - 139610.\n",
      "Ep done - 139620.\n",
      "Ep done - 139630.\n",
      "Ep done - 139640.\n",
      "Ep done - 139650.\n",
      "Ep done - 139660.\n",
      "Ep done - 139670.\n",
      "Ep done - 139680.\n",
      "Ep done - 139690.\n",
      "Ep done - 139700.\n",
      "Ep done - 139710.\n",
      "Ep done - 139720.\n",
      "Ep done - 139730.\n",
      "Ep done - 139740.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 647      |\n",
      "|    time_elapsed    | 14040    |\n",
      "|    total_timesteps | 3975168  |\n",
      "---------------------------------\n",
      "Ep done - 139750.\n",
      "Ep done - 139760.\n",
      "Ep done - 139770.\n",
      "Ep done - 139780.\n",
      "Ep done - 139790.\n",
      "Ep done - 139800.\n",
      "Ep done - 139810.\n",
      "Ep done - 139820.\n",
      "Ep done - 139830.\n",
      "Ep done - 139840.\n",
      "Ep done - 139850.\n",
      "Ep done - 139860.\n",
      "Ep done - 139870.\n",
      "Ep done - 139880.\n",
      "Ep done - 139890.\n",
      "Ep done - 139900.\n",
      "Ep done - 39710.\n",
      "Ep done - 39720.\n",
      "Ep done - 39730.\n",
      "Ep done - 39740.\n",
      "Ep done - 39750.\n",
      "Ep done - 39760.\n",
      "Ep done - 39770.\n",
      "Ep done - 39780.\n",
      "Ep done - 39790.\n",
      "Ep done - 39800.\n",
      "Eval num_timesteps=3980000, episode_reward=0.48 +/- 0.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.48      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3980000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7667472 |\n",
      "|    clip_fraction        | 0.0334    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00147  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.142     |\n",
      "|    n_updates            | 25700     |\n",
      "|    policy_gradient_loss | -0.0113   |\n",
      "|    value_loss           | 0.254     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.48\n",
      "SELFPLAY: new best model, bumping up generation to 128\n",
      "Ep done - 139910.\n",
      "Ep done - 139920.\n",
      "Ep done - 139930.\n",
      "Ep done - 139940.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 648      |\n",
      "|    time_elapsed    | 14066    |\n",
      "|    total_timesteps | 3981312  |\n",
      "---------------------------------\n",
      "Ep done - 139950.\n",
      "Ep done - 139960.\n",
      "Ep done - 139970.\n",
      "Ep done - 139980.\n",
      "Ep done - 139990.\n",
      "Ep done - 140000.\n",
      "Ep done - 140010.\n",
      "Ep done - 140020.\n",
      "Ep done - 140030.\n",
      "Ep done - 140040.\n",
      "Ep done - 140050.\n",
      "Ep done - 140060.\n",
      "Ep done - 140070.\n",
      "Ep done - 140080.\n",
      "Ep done - 140090.\n",
      "Ep done - 140100.\n",
      "Ep done - 140110.\n",
      "Ep done - 140120.\n",
      "Ep done - 140130.\n",
      "Ep done - 140140.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.5      |\n",
      "|    ep_rew_mean          | -0.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 649       |\n",
      "|    time_elapsed         | 14108     |\n",
      "|    total_timesteps      | 3987456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7604852 |\n",
      "|    clip_fraction        | 0.0339    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.122     |\n",
      "|    n_updates            | 25710     |\n",
      "|    policy_gradient_loss | -0.00766  |\n",
      "|    value_loss           | 0.291     |\n",
      "---------------------------------------\n",
      "Ep done - 140150.\n",
      "Ep done - 140160.\n",
      "Ep done - 140170.\n",
      "Ep done - 140180.\n",
      "Ep done - 140190.\n",
      "Ep done - 140200.\n",
      "Ep done - 140210.\n",
      "Ep done - 140220.\n",
      "Ep done - 140230.\n",
      "Ep done - 39810.\n",
      "Ep done - 39820.\n",
      "Ep done - 39830.\n",
      "Ep done - 39840.\n",
      "Ep done - 39850.\n",
      "Ep done - 39860.\n",
      "Ep done - 39870.\n",
      "Ep done - 39880.\n",
      "Ep done - 39890.\n",
      "Ep done - 39900.\n",
      "Eval num_timesteps=3990000, episode_reward=-0.44 +/- 0.90\n",
      "Episode length: 30.30 +/- 0.46\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | -0.44     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3990000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0407466 |\n",
      "|    clip_fraction        | 0.0349    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000745 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.072     |\n",
      "|    n_updates            | 25720     |\n",
      "|    policy_gradient_loss | -0.00452  |\n",
      "|    value_loss           | 0.204     |\n",
      "---------------------------------------\n",
      "Ep done - 140240.\n",
      "Ep done - 140250.\n",
      "Ep done - 140260.\n",
      "Ep done - 140270.\n",
      "Ep done - 140280.\n",
      "Ep done - 140290.\n",
      "Ep done - 140300.\n",
      "Ep done - 140310.\n",
      "Ep done - 140320.\n",
      "Ep done - 140330.\n",
      "Ep done - 140340.\n",
      "Ep done - 140350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 650      |\n",
      "|    time_elapsed    | 14137    |\n",
      "|    total_timesteps | 3993600  |\n",
      "---------------------------------\n",
      "Ep done - 140360.\n",
      "Ep done - 140370.\n",
      "Ep done - 140380.\n",
      "Ep done - 140390.\n",
      "Ep done - 140400.\n",
      "Ep done - 140410.\n",
      "Ep done - 140420.\n",
      "Ep done - 140430.\n",
      "Ep done - 140440.\n",
      "Ep done - 140450.\n",
      "Ep done - 140460.\n",
      "Ep done - 140470.\n",
      "Ep done - 140480.\n",
      "Ep done - 140490.\n",
      "Ep done - 140500.\n",
      "Ep done - 140510.\n",
      "Ep done - 140520.\n",
      "Ep done - 140530.\n",
      "Ep done - 140540.\n",
      "Ep done - 140550.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | -0.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 651       |\n",
      "|    time_elapsed         | 14157     |\n",
      "|    total_timesteps      | 3999744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6779943 |\n",
      "|    clip_fraction        | 0.0753    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000451 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.151     |\n",
      "|    n_updates            | 25730     |\n",
      "|    policy_gradient_loss | -0.00608  |\n",
      "|    value_loss           | 0.197     |\n",
      "---------------------------------------\n",
      "Ep done - 140560.\n",
      "Ep done - 39910.\n",
      "Ep done - 39920.\n",
      "Ep done - 39930.\n",
      "Ep done - 39940.\n",
      "Ep done - 39950.\n",
      "Ep done - 39960.\n",
      "Ep done - 39970.\n",
      "Ep done - 39980.\n",
      "Ep done - 39990.\n",
      "Ep done - 40000.\n",
      "Eval num_timesteps=4000000, episode_reward=-0.36 +/- 0.93\n",
      "Episode length: 30.33 +/- 0.47\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | -0.36     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4000000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.150884  |\n",
      "|    clip_fraction        | 0.126     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000355 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.167     |\n",
      "|    n_updates            | 25740     |\n",
      "|    policy_gradient_loss | -0.00256  |\n",
      "|    value_loss           | 0.212     |\n",
      "---------------------------------------\n",
      "Ep done - 140570.\n",
      "Ep done - 140580.\n",
      "Ep done - 140590.\n",
      "Ep done - 140600.\n",
      "Ep done - 140610.\n",
      "Ep done - 140620.\n",
      "Ep done - 140630.\n",
      "Ep done - 140640.\n",
      "Ep done - 140650.\n",
      "Ep done - 140660.\n",
      "Ep done - 140670.\n",
      "Ep done - 140680.\n",
      "Ep done - 140690.\n",
      "Ep done - 140700.\n",
      "Ep done - 140710.\n",
      "Ep done - 140720.\n",
      "Ep done - 140730.\n",
      "Ep done - 140740.\n",
      "Ep done - 140750.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 652      |\n",
      "|    time_elapsed    | 14188    |\n",
      "|    total_timesteps | 4005888  |\n",
      "---------------------------------\n",
      "Ep done - 140760.\n",
      "Ep done - 140770.\n",
      "Ep done - 140780.\n",
      "Ep done - 140790.\n",
      "Ep done - 140800.\n",
      "Ep done - 140810.\n",
      "Ep done - 140820.\n",
      "Ep done - 140830.\n",
      "Ep done - 140840.\n",
      "Ep done - 140850.\n",
      "Ep done - 140860.\n",
      "Ep done - 140870.\n",
      "Ep done - 140880.\n",
      "Ep done - 140890.\n",
      "Ep done - 40010.\n",
      "Ep done - 40020.\n",
      "Ep done - 40030.\n",
      "Ep done - 40040.\n",
      "Ep done - 40050.\n",
      "Ep done - 40060.\n",
      "Ep done - 40070.\n",
      "Ep done - 40080.\n",
      "Ep done - 40090.\n",
      "Ep done - 40100.\n",
      "Eval num_timesteps=4010000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 30.19 +/- 0.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | -0.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4010000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13768335 |\n",
      "|    clip_fraction        | 0.0273     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000232  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.132      |\n",
      "|    n_updates            | 25750      |\n",
      "|    policy_gradient_loss | 0.0049     |\n",
      "|    value_loss           | 0.203      |\n",
      "----------------------------------------\n",
      "Ep done - 140900.\n",
      "Ep done - 140910.\n",
      "Ep done - 140920.\n",
      "Ep done - 140930.\n",
      "Ep done - 140940.\n",
      "Ep done - 140950.\n",
      "Ep done - 140960.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 653      |\n",
      "|    time_elapsed    | 14213    |\n",
      "|    total_timesteps | 4012032  |\n",
      "---------------------------------\n",
      "Ep done - 140970.\n",
      "Ep done - 140980.\n",
      "Ep done - 140990.\n",
      "Ep done - 141000.\n",
      "Ep done - 141010.\n",
      "Ep done - 141020.\n",
      "Ep done - 141030.\n",
      "Ep done - 141040.\n",
      "Ep done - 141050.\n",
      "Ep done - 141060.\n",
      "Ep done - 141070.\n",
      "Ep done - 141080.\n",
      "Ep done - 141090.\n",
      "Ep done - 141100.\n",
      "Ep done - 141110.\n",
      "Ep done - 141120.\n",
      "Ep done - 141130.\n",
      "Ep done - 141140.\n",
      "Ep done - 141150.\n",
      "Ep done - 141160.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | -0.48     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 654       |\n",
      "|    time_elapsed         | 14231     |\n",
      "|    total_timesteps      | 4018176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.43e-05 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.115     |\n",
      "|    n_updates            | 25760     |\n",
      "|    policy_gradient_loss | 4.17e-10  |\n",
      "|    value_loss           | 0.23      |\n",
      "---------------------------------------\n",
      "Ep done - 141170.\n",
      "Ep done - 141180.\n",
      "Ep done - 141190.\n",
      "Ep done - 141200.\n",
      "Ep done - 141210.\n",
      "Ep done - 141220.\n",
      "Ep done - 40110.\n",
      "Ep done - 40120.\n",
      "Ep done - 40130.\n",
      "Ep done - 40140.\n",
      "Ep done - 40150.\n",
      "Ep done - 40160.\n",
      "Ep done - 40170.\n",
      "Ep done - 40180.\n",
      "Ep done - 40190.\n",
      "Ep done - 40200.\n",
      "Eval num_timesteps=4020000, episode_reward=-0.42 +/- 0.91\n",
      "Episode length: 30.29 +/- 0.45\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | -0.42     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4020000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8164993 |\n",
      "|    clip_fraction        | 0.0341    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000168 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.118     |\n",
      "|    n_updates            | 25770     |\n",
      "|    policy_gradient_loss | 0.00117   |\n",
      "|    value_loss           | 0.225     |\n",
      "---------------------------------------\n",
      "Ep done - 141230.\n",
      "Ep done - 141240.\n",
      "Ep done - 141250.\n",
      "Ep done - 141260.\n",
      "Ep done - 141270.\n",
      "Ep done - 141280.\n",
      "Ep done - 141290.\n",
      "Ep done - 141300.\n",
      "Ep done - 141310.\n",
      "Ep done - 141320.\n",
      "Ep done - 141330.\n",
      "Ep done - 141340.\n",
      "Ep done - 141350.\n",
      "Ep done - 141360.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | -0.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 655      |\n",
      "|    time_elapsed    | 14255    |\n",
      "|    total_timesteps | 4024320  |\n",
      "---------------------------------\n",
      "Ep done - 141370.\n",
      "Ep done - 141380.\n",
      "Ep done - 141390.\n",
      "Ep done - 141400.\n",
      "Ep done - 141410.\n",
      "Ep done - 141420.\n",
      "Ep done - 141430.\n",
      "Ep done - 141440.\n",
      "Ep done - 141450.\n",
      "Ep done - 141460.\n",
      "Ep done - 141470.\n",
      "Ep done - 141480.\n",
      "Ep done - 141490.\n",
      "Ep done - 141500.\n",
      "Ep done - 141510.\n",
      "Ep done - 141520.\n",
      "Ep done - 141530.\n",
      "Ep done - 141540.\n",
      "Ep done - 141550.\n",
      "Ep done - 40210.\n",
      "Ep done - 40220.\n",
      "Ep done - 40230.\n",
      "Ep done - 40240.\n",
      "Ep done - 40250.\n",
      "Ep done - 40260.\n",
      "Ep done - 40270.\n",
      "Ep done - 40280.\n",
      "Ep done - 40290.\n",
      "Ep done - 40300.\n",
      "Eval num_timesteps=4030000, episode_reward=-0.54 +/- 0.84\n",
      "Episode length: 30.21 +/- 0.41\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.2      |\n",
      "|    mean_reward          | -0.54     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4030000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.92e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.112     |\n",
      "|    n_updates            | 25780     |\n",
      "|    policy_gradient_loss | 1.97e-09  |\n",
      "|    value_loss           | 0.231     |\n",
      "---------------------------------------\n",
      "Ep done - 141560.\n",
      "Ep done - 141570.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | -0.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 656      |\n",
      "|    time_elapsed    | 14279    |\n",
      "|    total_timesteps | 4030464  |\n",
      "---------------------------------\n",
      "Ep done - 141580.\n",
      "Ep done - 141590.\n",
      "Ep done - 141600.\n",
      "Ep done - 141610.\n",
      "Ep done - 141620.\n",
      "Ep done - 141630.\n",
      "Ep done - 141640.\n",
      "Ep done - 141650.\n",
      "Ep done - 141660.\n",
      "Ep done - 141670.\n",
      "Ep done - 141680.\n",
      "Ep done - 141690.\n",
      "Ep done - 141700.\n",
      "Ep done - 141710.\n",
      "Ep done - 141720.\n",
      "Ep done - 141730.\n",
      "Ep done - 141740.\n",
      "Ep done - 141750.\n",
      "Ep done - 141760.\n",
      "Ep done - 141770.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | -0.46      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 657        |\n",
      "|    time_elapsed         | 14297      |\n",
      "|    total_timesteps      | 4036608    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33046544 |\n",
      "|    clip_fraction        | 0.0138     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.16e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.18       |\n",
      "|    n_updates            | 25790      |\n",
      "|    policy_gradient_loss | -0.000607  |\n",
      "|    value_loss           | 0.228      |\n",
      "----------------------------------------\n",
      "Ep done - 141780.\n",
      "Ep done - 141790.\n",
      "Ep done - 141800.\n",
      "Ep done - 141810.\n",
      "Ep done - 141820.\n",
      "Ep done - 141830.\n",
      "Ep done - 141840.\n",
      "Ep done - 141850.\n",
      "Ep done - 141860.\n",
      "Ep done - 141870.\n",
      "Ep done - 141880.\n",
      "Ep done - 40310.\n",
      "Ep done - 40320.\n",
      "Ep done - 40330.\n",
      "Ep done - 40340.\n",
      "Ep done - 40350.\n",
      "Ep done - 40360.\n",
      "Ep done - 40370.\n",
      "Ep done - 40380.\n",
      "Ep done - 40390.\n",
      "Ep done - 40400.\n",
      "Eval num_timesteps=4040000, episode_reward=0.08 +/- 0.66\n",
      "Episode length: 30.26 +/- 0.44\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.3      |\n",
      "|    mean_reward          | 0.08      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4040000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8845453 |\n",
      "|    clip_fraction        | 0.0486    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000907 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0953    |\n",
      "|    n_updates            | 25800     |\n",
      "|    policy_gradient_loss | 0.0101    |\n",
      "|    value_loss           | 0.235     |\n",
      "---------------------------------------\n",
      "Ep done - 141890.\n",
      "Ep done - 141900.\n",
      "Ep done - 141910.\n",
      "Ep done - 141920.\n",
      "Ep done - 141930.\n",
      "Ep done - 141940.\n",
      "Ep done - 141950.\n",
      "Ep done - 141960.\n",
      "Ep done - 141970.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 658      |\n",
      "|    time_elapsed    | 14320    |\n",
      "|    total_timesteps | 4042752  |\n",
      "---------------------------------\n",
      "Ep done - 141980.\n",
      "Ep done - 141990.\n",
      "Ep done - 142000.\n",
      "Ep done - 142010.\n",
      "Ep done - 142020.\n",
      "Ep done - 142030.\n",
      "Ep done - 142040.\n",
      "Ep done - 142050.\n",
      "Ep done - 142060.\n",
      "Ep done - 142070.\n",
      "Ep done - 142080.\n",
      "Ep done - 142090.\n",
      "Ep done - 142100.\n",
      "Ep done - 142110.\n",
      "Ep done - 142120.\n",
      "Ep done - 142130.\n",
      "Ep done - 142140.\n",
      "Ep done - 142150.\n",
      "Ep done - 142160.\n",
      "Ep done - 142170.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | -0.56     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 659       |\n",
      "|    time_elapsed         | 14338     |\n",
      "|    total_timesteps      | 4048896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7125201 |\n",
      "|    clip_fraction        | 0.0429    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000449 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0762    |\n",
      "|    n_updates            | 25810     |\n",
      "|    policy_gradient_loss | 0.00102   |\n",
      "|    value_loss           | 0.162     |\n",
      "---------------------------------------\n",
      "Ep done - 142180.\n",
      "Ep done - 142190.\n",
      "Ep done - 142200.\n",
      "Ep done - 142210.\n",
      "Ep done - 40410.\n",
      "Ep done - 40420.\n",
      "Ep done - 40430.\n",
      "Ep done - 40440.\n",
      "Ep done - 40450.\n",
      "Ep done - 40460.\n",
      "Ep done - 40470.\n",
      "Ep done - 40480.\n",
      "Ep done - 40490.\n",
      "Ep done - 40500.\n",
      "Eval num_timesteps=4050000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.99 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 31        |\n",
      "|    mean_reward          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4050000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8343741 |\n",
      "|    clip_fraction        | 0.0285    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000281 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.087     |\n",
      "|    n_updates            | 25820     |\n",
      "|    policy_gradient_loss | -0.00113  |\n",
      "|    value_loss           | 0.212     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 129\n",
      "Ep done - 142220.\n",
      "Ep done - 142230.\n",
      "Ep done - 142240.\n",
      "Ep done - 142250.\n",
      "Ep done - 142260.\n",
      "Ep done - 142270.\n",
      "Ep done - 142280.\n",
      "Ep done - 142290.\n",
      "Ep done - 142300.\n",
      "Ep done - 142310.\n",
      "Ep done - 142320.\n",
      "Ep done - 142330.\n",
      "Ep done - 142340.\n",
      "Ep done - 142350.\n",
      "Ep done - 142360.\n",
      "Ep done - 142370.\n",
      "Ep done - 142380.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.5     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 660      |\n",
      "|    time_elapsed    | 14360    |\n",
      "|    total_timesteps | 4055040  |\n",
      "---------------------------------\n",
      "Ep done - 142390.\n",
      "Ep done - 142400.\n",
      "Ep done - 142410.\n",
      "Ep done - 142420.\n",
      "Ep done - 142430.\n",
      "Ep done - 142440.\n",
      "Ep done - 142450.\n",
      "Ep done - 142460.\n",
      "Ep done - 142470.\n",
      "Ep done - 142480.\n",
      "Ep done - 142490.\n",
      "Ep done - 142500.\n",
      "Ep done - 142510.\n",
      "Ep done - 142520.\n",
      "Ep done - 142530.\n",
      "Ep done - 142540.\n",
      "Ep done - 40510.\n",
      "Ep done - 40520.\n",
      "Ep done - 40530.\n",
      "Ep done - 40540.\n",
      "Ep done - 40550.\n",
      "Ep done - 40560.\n",
      "Ep done - 40570.\n",
      "Ep done - 40580.\n",
      "Ep done - 40590.\n",
      "Ep done - 40600.\n",
      "Eval num_timesteps=4060000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -1       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 4060000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.894892 |\n",
      "|    clip_fraction        | 0.183    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00132 |\n",
      "|    explained_variance   | 4.34e-05 |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.121    |\n",
      "|    n_updates            | 25830    |\n",
      "|    policy_gradient_loss | 0.0232   |\n",
      "|    value_loss           | 0.172    |\n",
      "--------------------------------------\n",
      "Ep done - 142550.\n",
      "Ep done - 142560.\n",
      "Ep done - 142570.\n",
      "Ep done - 142580.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 661      |\n",
      "|    time_elapsed    | 14383    |\n",
      "|    total_timesteps | 4061184  |\n",
      "---------------------------------\n",
      "Ep done - 142590.\n",
      "Ep done - 142600.\n",
      "Ep done - 142610.\n",
      "Ep done - 142620.\n",
      "Ep done - 142630.\n",
      "Ep done - 142640.\n",
      "Ep done - 142650.\n",
      "Ep done - 142660.\n",
      "Ep done - 142670.\n",
      "Ep done - 142680.\n",
      "Ep done - 142690.\n",
      "Ep done - 142700.\n",
      "Ep done - 142710.\n",
      "Ep done - 142720.\n",
      "Ep done - 142730.\n",
      "Ep done - 142740.\n",
      "Ep done - 142750.\n",
      "Ep done - 142760.\n",
      "Ep done - 142770.\n",
      "Ep done - 142780.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 662       |\n",
      "|    time_elapsed         | 14402     |\n",
      "|    total_timesteps      | 4067328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8029674 |\n",
      "|    clip_fraction        | 0.0749    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000352 |\n",
      "|    explained_variance   | 2.38e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00333  |\n",
      "|    n_updates            | 25840     |\n",
      "|    policy_gradient_loss | -0.0108   |\n",
      "|    value_loss           | 0.0191    |\n",
      "---------------------------------------\n",
      "Ep done - 142790.\n",
      "Ep done - 142800.\n",
      "Ep done - 142810.\n",
      "Ep done - 142820.\n",
      "Ep done - 142830.\n",
      "Ep done - 142840.\n",
      "Ep done - 142850.\n",
      "Ep done - 142860.\n",
      "Ep done - 142870.\n",
      "Ep done - 40610.\n",
      "Ep done - 40620.\n",
      "Ep done - 40630.\n",
      "Ep done - 40640.\n",
      "Ep done - 40650.\n",
      "Ep done - 40660.\n",
      "Ep done - 40670.\n",
      "Ep done - 40680.\n",
      "Ep done - 40690.\n",
      "Ep done - 40700.\n",
      "Eval num_timesteps=4070000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4070000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.7669952 |\n",
      "|    clip_fraction        | 0.084     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000291 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0121   |\n",
      "|    n_updates            | 25850     |\n",
      "|    policy_gradient_loss | -0.00292  |\n",
      "|    value_loss           | 0.0145    |\n",
      "---------------------------------------\n",
      "Ep done - 142880.\n",
      "Ep done - 142890.\n",
      "Ep done - 142900.\n",
      "Ep done - 142910.\n",
      "Ep done - 142920.\n",
      "Ep done - 142930.\n",
      "Ep done - 142940.\n",
      "Ep done - 142950.\n",
      "Ep done - 142960.\n",
      "Ep done - 142970.\n",
      "Ep done - 142980.\n",
      "Ep done - 142990.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 663      |\n",
      "|    time_elapsed    | 14426    |\n",
      "|    total_timesteps | 4073472  |\n",
      "---------------------------------\n",
      "Ep done - 143000.\n",
      "Ep done - 143010.\n",
      "Ep done - 143020.\n",
      "Ep done - 143030.\n",
      "Ep done - 143040.\n",
      "Ep done - 143050.\n",
      "Ep done - 143060.\n",
      "Ep done - 143070.\n",
      "Ep done - 143080.\n",
      "Ep done - 143090.\n",
      "Ep done - 143100.\n",
      "Ep done - 143110.\n",
      "Ep done - 143120.\n",
      "Ep done - 143130.\n",
      "Ep done - 143140.\n",
      "Ep done - 143150.\n",
      "Ep done - 143160.\n",
      "Ep done - 143170.\n",
      "Ep done - 143180.\n",
      "Ep done - 143190.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 664       |\n",
      "|    time_elapsed         | 14444     |\n",
      "|    total_timesteps      | 4079616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.2637405 |\n",
      "|    clip_fraction        | 0.0877    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000248 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0169   |\n",
      "|    n_updates            | 25860     |\n",
      "|    policy_gradient_loss | -0.0119   |\n",
      "|    value_loss           | 0.0114    |\n",
      "---------------------------------------\n",
      "Ep done - 143200.\n",
      "Ep done - 143210.\n",
      "Ep done - 40710.\n",
      "Ep done - 40720.\n",
      "Ep done - 40730.\n",
      "Ep done - 40740.\n",
      "Ep done - 40750.\n",
      "Ep done - 40760.\n",
      "Ep done - 40770.\n",
      "Ep done - 40780.\n",
      "Ep done - 40790.\n",
      "Ep done - 40800.\n",
      "Eval num_timesteps=4080000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4080000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0405678 |\n",
      "|    clip_fraction        | 0.0504    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000229 |\n",
      "|    explained_variance   | 2.98e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0132   |\n",
      "|    n_updates            | 25870     |\n",
      "|    policy_gradient_loss | 0.0505    |\n",
      "|    value_loss           | 0.00745   |\n",
      "---------------------------------------\n",
      "Ep done - 143220.\n",
      "Ep done - 143230.\n",
      "Ep done - 143240.\n",
      "Ep done - 143250.\n",
      "Ep done - 143260.\n",
      "Ep done - 143270.\n",
      "Ep done - 143280.\n",
      "Ep done - 143290.\n",
      "Ep done - 143300.\n",
      "Ep done - 143310.\n",
      "Ep done - 143320.\n",
      "Ep done - 143330.\n",
      "Ep done - 143340.\n",
      "Ep done - 143350.\n",
      "Ep done - 143360.\n",
      "Ep done - 143370.\n",
      "Ep done - 143380.\n",
      "Ep done - 143390.\n",
      "Ep done - 143400.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 665      |\n",
      "|    time_elapsed    | 14468    |\n",
      "|    total_timesteps | 4085760  |\n",
      "---------------------------------\n",
      "Ep done - 143410.\n",
      "Ep done - 143420.\n",
      "Ep done - 143430.\n",
      "Ep done - 143440.\n",
      "Ep done - 143450.\n",
      "Ep done - 143460.\n",
      "Ep done - 143470.\n",
      "Ep done - 143480.\n",
      "Ep done - 143490.\n",
      "Ep done - 143500.\n",
      "Ep done - 143510.\n",
      "Ep done - 143520.\n",
      "Ep done - 143530.\n",
      "Ep done - 143540.\n",
      "Ep done - 40810.\n",
      "Ep done - 40820.\n",
      "Ep done - 40830.\n",
      "Ep done - 40840.\n",
      "Ep done - 40850.\n",
      "Ep done - 40860.\n",
      "Ep done - 40870.\n",
      "Ep done - 40880.\n",
      "Ep done - 40890.\n",
      "Ep done - 40900.\n",
      "Eval num_timesteps=4090000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4090000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.47e-06 |\n",
      "|    explained_variance   | 3.58e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00311   |\n",
      "|    n_updates            | 25880     |\n",
      "|    policy_gradient_loss | -3.54e-11 |\n",
      "|    value_loss           | 0.00659   |\n",
      "---------------------------------------\n",
      "Ep done - 143550.\n",
      "Ep done - 143560.\n",
      "Ep done - 143570.\n",
      "Ep done - 143580.\n",
      "Ep done - 143590.\n",
      "Ep done - 143600.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 666      |\n",
      "|    time_elapsed    | 14491    |\n",
      "|    total_timesteps | 4091904  |\n",
      "---------------------------------\n",
      "Ep done - 143610.\n",
      "Ep done - 143620.\n",
      "Ep done - 143630.\n",
      "Ep done - 143640.\n",
      "Ep done - 143650.\n",
      "Ep done - 143660.\n",
      "Ep done - 143670.\n",
      "Ep done - 143680.\n",
      "Ep done - 143690.\n",
      "Ep done - 143700.\n",
      "Ep done - 143710.\n",
      "Ep done - 143720.\n",
      "Ep done - 143730.\n",
      "Ep done - 143740.\n",
      "Ep done - 143750.\n",
      "Ep done - 143760.\n",
      "Ep done - 143770.\n",
      "Ep done - 143780.\n",
      "Ep done - 143790.\n",
      "Ep done - 143800.\n",
      "Ep done - 143810.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 667       |\n",
      "|    time_elapsed         | 14509     |\n",
      "|    total_timesteps      | 4098048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.34e-06 |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00186   |\n",
      "|    n_updates            | 25890     |\n",
      "|    policy_gradient_loss | -2.28e-09 |\n",
      "|    value_loss           | 0.00546   |\n",
      "---------------------------------------\n",
      "Ep done - 143820.\n",
      "Ep done - 143830.\n",
      "Ep done - 143840.\n",
      "Ep done - 143850.\n",
      "Ep done - 143860.\n",
      "Ep done - 143870.\n",
      "Ep done - 40910.\n",
      "Ep done - 40920.\n",
      "Ep done - 40930.\n",
      "Ep done - 40940.\n",
      "Ep done - 40950.\n",
      "Ep done - 40960.\n",
      "Ep done - 40970.\n",
      "Ep done - 40980.\n",
      "Ep done - 40990.\n",
      "Ep done - 41000.\n",
      "Eval num_timesteps=4100000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4100000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.25e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00242   |\n",
      "|    n_updates            | 25900     |\n",
      "|    policy_gradient_loss | -6.2e-10  |\n",
      "|    value_loss           | 0.00586   |\n",
      "---------------------------------------\n",
      "Ep done - 143880.\n",
      "Ep done - 143890.\n",
      "Ep done - 143900.\n",
      "Ep done - 143910.\n",
      "Ep done - 143920.\n",
      "Ep done - 143930.\n",
      "Ep done - 143940.\n",
      "Ep done - 143950.\n",
      "Ep done - 143960.\n",
      "Ep done - 143970.\n",
      "Ep done - 143980.\n",
      "Ep done - 143990.\n",
      "Ep done - 144000.\n",
      "Ep done - 144010.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 668      |\n",
      "|    time_elapsed    | 14533    |\n",
      "|    total_timesteps | 4104192  |\n",
      "---------------------------------\n",
      "Ep done - 144020.\n",
      "Ep done - 144030.\n",
      "Ep done - 144040.\n",
      "Ep done - 144050.\n",
      "Ep done - 144060.\n",
      "Ep done - 144070.\n",
      "Ep done - 144080.\n",
      "Ep done - 144090.\n",
      "Ep done - 144100.\n",
      "Ep done - 144110.\n",
      "Ep done - 144120.\n",
      "Ep done - 144130.\n",
      "Ep done - 144140.\n",
      "Ep done - 144150.\n",
      "Ep done - 144160.\n",
      "Ep done - 144170.\n",
      "Ep done - 144180.\n",
      "Ep done - 144190.\n",
      "Ep done - 144200.\n",
      "Ep done - 144210.\n",
      "Ep done - 41010.\n",
      "Ep done - 41020.\n",
      "Ep done - 41030.\n",
      "Ep done - 41040.\n",
      "Ep done - 41050.\n",
      "Ep done - 41060.\n",
      "Ep done - 41070.\n",
      "Ep done - 41080.\n",
      "Ep done - 41090.\n",
      "Ep done - 41100.\n",
      "Eval num_timesteps=4110000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4110000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.15e-06 |\n",
      "|    explained_variance   | 4.29e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00351   |\n",
      "|    n_updates            | 25910     |\n",
      "|    policy_gradient_loss | -8.21e-10 |\n",
      "|    value_loss           | 0.006     |\n",
      "---------------------------------------\n",
      "Ep done - 144220.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 669      |\n",
      "|    time_elapsed    | 14556    |\n",
      "|    total_timesteps | 4110336  |\n",
      "---------------------------------\n",
      "Ep done - 144230.\n",
      "Ep done - 144240.\n",
      "Ep done - 144250.\n",
      "Ep done - 144260.\n",
      "Ep done - 144270.\n",
      "Ep done - 144280.\n",
      "Ep done - 144290.\n",
      "Ep done - 144300.\n",
      "Ep done - 144310.\n",
      "Ep done - 144320.\n",
      "Ep done - 144330.\n",
      "Ep done - 144340.\n",
      "Ep done - 144350.\n",
      "Ep done - 144360.\n",
      "Ep done - 144370.\n",
      "Ep done - 144380.\n",
      "Ep done - 144390.\n",
      "Ep done - 144400.\n",
      "Ep done - 144410.\n",
      "Ep done - 144420.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 670       |\n",
      "|    time_elapsed         | 14575     |\n",
      "|    total_timesteps      | 4116480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.87e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00367   |\n",
      "|    n_updates            | 25920     |\n",
      "|    policy_gradient_loss | -1.13e-09 |\n",
      "|    value_loss           | 0.00589   |\n",
      "---------------------------------------\n",
      "Ep done - 144430.\n",
      "Ep done - 144440.\n",
      "Ep done - 144450.\n",
      "Ep done - 144460.\n",
      "Ep done - 144470.\n",
      "Ep done - 144480.\n",
      "Ep done - 144490.\n",
      "Ep done - 144500.\n",
      "Ep done - 144510.\n",
      "Ep done - 144520.\n",
      "Ep done - 144530.\n",
      "Ep done - 144540.\n",
      "Ep done - 41110.\n",
      "Ep done - 41120.\n",
      "Ep done - 41130.\n",
      "Ep done - 41140.\n",
      "Ep done - 41150.\n",
      "Ep done - 41160.\n",
      "Ep done - 41170.\n",
      "Ep done - 41180.\n",
      "Ep done - 41190.\n",
      "Ep done - 41200.\n",
      "Eval num_timesteps=4120000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4120000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.91e-06 |\n",
      "|    explained_variance   | 4.17e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00283   |\n",
      "|    n_updates            | 25930     |\n",
      "|    policy_gradient_loss | 5.16e-10  |\n",
      "|    value_loss           | 0.00692   |\n",
      "---------------------------------------\n",
      "Ep done - 144550.\n",
      "Ep done - 144560.\n",
      "Ep done - 144570.\n",
      "Ep done - 144580.\n",
      "Ep done - 144590.\n",
      "Ep done - 144600.\n",
      "Ep done - 144610.\n",
      "Ep done - 144620.\n",
      "Ep done - 144630.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 671      |\n",
      "|    time_elapsed    | 14599    |\n",
      "|    total_timesteps | 4122624  |\n",
      "---------------------------------\n",
      "Ep done - 144640.\n",
      "Ep done - 144650.\n",
      "Ep done - 144660.\n",
      "Ep done - 144670.\n",
      "Ep done - 144680.\n",
      "Ep done - 144690.\n",
      "Ep done - 144700.\n",
      "Ep done - 144710.\n",
      "Ep done - 144720.\n",
      "Ep done - 144730.\n",
      "Ep done - 144740.\n",
      "Ep done - 144750.\n",
      "Ep done - 144760.\n",
      "Ep done - 144770.\n",
      "Ep done - 144780.\n",
      "Ep done - 144790.\n",
      "Ep done - 144800.\n",
      "Ep done - 144810.\n",
      "Ep done - 144820.\n",
      "Ep done - 144830.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 672       |\n",
      "|    time_elapsed         | 14618     |\n",
      "|    total_timesteps      | 4128768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.81e-06 |\n",
      "|    explained_variance   | 0.0014    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00337   |\n",
      "|    n_updates            | 25940     |\n",
      "|    policy_gradient_loss | 3.97e-10  |\n",
      "|    value_loss           | 0.00729   |\n",
      "---------------------------------------\n",
      "Ep done - 144840.\n",
      "Ep done - 144850.\n",
      "Ep done - 144860.\n",
      "Ep done - 144870.\n",
      "Ep done - 41210.\n",
      "Ep done - 41220.\n",
      "Ep done - 41230.\n",
      "Ep done - 41240.\n",
      "Ep done - 41250.\n",
      "Ep done - 41260.\n",
      "Ep done - 41270.\n",
      "Ep done - 41280.\n",
      "Ep done - 41290.\n",
      "Ep done - 41300.\n",
      "Eval num_timesteps=4130000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4130000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.77e-06 |\n",
      "|    explained_variance   | 2.38e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00253   |\n",
      "|    n_updates            | 25950     |\n",
      "|    policy_gradient_loss | 1.18e-09  |\n",
      "|    value_loss           | 0.00727   |\n",
      "---------------------------------------\n",
      "Ep done - 144880.\n",
      "Ep done - 144890.\n",
      "Ep done - 144900.\n",
      "Ep done - 144910.\n",
      "Ep done - 144920.\n",
      "Ep done - 144930.\n",
      "Ep done - 144940.\n",
      "Ep done - 144950.\n",
      "Ep done - 144960.\n",
      "Ep done - 144970.\n",
      "Ep done - 144980.\n",
      "Ep done - 144990.\n",
      "Ep done - 145000.\n",
      "Ep done - 145010.\n",
      "Ep done - 145020.\n",
      "Ep done - 145030.\n",
      "Ep done - 145040.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 673      |\n",
      "|    time_elapsed    | 14642    |\n",
      "|    total_timesteps | 4134912  |\n",
      "---------------------------------\n",
      "Ep done - 145050.\n",
      "Ep done - 145060.\n",
      "Ep done - 145070.\n",
      "Ep done - 145080.\n",
      "Ep done - 145090.\n",
      "Ep done - 145100.\n",
      "Ep done - 145110.\n",
      "Ep done - 145120.\n",
      "Ep done - 145130.\n",
      "Ep done - 145140.\n",
      "Ep done - 145150.\n",
      "Ep done - 145160.\n",
      "Ep done - 145170.\n",
      "Ep done - 145180.\n",
      "Ep done - 145190.\n",
      "Ep done - 145200.\n",
      "Ep done - 145210.\n",
      "Ep done - 41310.\n",
      "Ep done - 41320.\n",
      "Ep done - 41330.\n",
      "Ep done - 41340.\n",
      "Ep done - 41350.\n",
      "Ep done - 41360.\n",
      "Ep done - 41370.\n",
      "Ep done - 41380.\n",
      "Ep done - 41390.\n",
      "Ep done - 41400.\n",
      "Eval num_timesteps=4140000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4140000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.49e-06 |\n",
      "|    explained_variance   | 1.79e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00304   |\n",
      "|    n_updates            | 25960     |\n",
      "|    policy_gradient_loss | 3.03e-09  |\n",
      "|    value_loss           | 0.00659   |\n",
      "---------------------------------------\n",
      "Ep done - 145220.\n",
      "Ep done - 145230.\n",
      "Ep done - 145240.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 674      |\n",
      "|    time_elapsed    | 14671    |\n",
      "|    total_timesteps | 4141056  |\n",
      "---------------------------------\n",
      "Ep done - 145250.\n",
      "Ep done - 145260.\n",
      "Ep done - 145270.\n",
      "Ep done - 145280.\n",
      "Ep done - 145290.\n",
      "Ep done - 145300.\n",
      "Ep done - 145310.\n",
      "Ep done - 145320.\n",
      "Ep done - 145330.\n",
      "Ep done - 145340.\n",
      "Ep done - 145350.\n",
      "Ep done - 145360.\n",
      "Ep done - 145370.\n",
      "Ep done - 145380.\n",
      "Ep done - 145390.\n",
      "Ep done - 145400.\n",
      "Ep done - 145410.\n",
      "Ep done - 145420.\n",
      "Ep done - 145430.\n",
      "Ep done - 145440.\n",
      "Ep done - 145450.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 675       |\n",
      "|    time_elapsed         | 14692     |\n",
      "|    total_timesteps      | 4147200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.39e-06 |\n",
      "|    explained_variance   | 1.79e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00392   |\n",
      "|    n_updates            | 25970     |\n",
      "|    policy_gradient_loss | 3.87e-10  |\n",
      "|    value_loss           | 0.00813   |\n",
      "---------------------------------------\n",
      "Ep done - 145460.\n",
      "Ep done - 145470.\n",
      "Ep done - 145480.\n",
      "Ep done - 145490.\n",
      "Ep done - 145500.\n",
      "Ep done - 145510.\n",
      "Ep done - 145520.\n",
      "Ep done - 145530.\n",
      "Ep done - 145540.\n",
      "Ep done - 41410.\n",
      "Ep done - 41420.\n",
      "Ep done - 41430.\n",
      "Ep done - 41440.\n",
      "Ep done - 41450.\n",
      "Ep done - 41460.\n",
      "Ep done - 41470.\n",
      "Ep done - 41480.\n",
      "Ep done - 41490.\n",
      "Ep done - 41500.\n",
      "Eval num_timesteps=4150000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4150000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.28e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00234   |\n",
      "|    n_updates            | 25980     |\n",
      "|    policy_gradient_loss | -1.07e-09 |\n",
      "|    value_loss           | 0.00546   |\n",
      "---------------------------------------\n",
      "Ep done - 145550.\n",
      "Ep done - 145560.\n",
      "Ep done - 145570.\n",
      "Ep done - 145580.\n",
      "Ep done - 145590.\n",
      "Ep done - 145600.\n",
      "Ep done - 145610.\n",
      "Ep done - 145620.\n",
      "Ep done - 145630.\n",
      "Ep done - 145640.\n",
      "Ep done - 145650.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 676      |\n",
      "|    time_elapsed    | 14715    |\n",
      "|    total_timesteps | 4153344  |\n",
      "---------------------------------\n",
      "Ep done - 145660.\n",
      "Ep done - 145670.\n",
      "Ep done - 145680.\n",
      "Ep done - 145690.\n",
      "Ep done - 145700.\n",
      "Ep done - 145710.\n",
      "Ep done - 145720.\n",
      "Ep done - 145730.\n",
      "Ep done - 145740.\n",
      "Ep done - 145750.\n",
      "Ep done - 145760.\n",
      "Ep done - 145770.\n",
      "Ep done - 145780.\n",
      "Ep done - 145790.\n",
      "Ep done - 145800.\n",
      "Ep done - 145810.\n",
      "Ep done - 145820.\n",
      "Ep done - 145830.\n",
      "Ep done - 145840.\n",
      "Ep done - 145850.\n",
      "Ep done - 145860.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 677       |\n",
      "|    time_elapsed         | 14736     |\n",
      "|    total_timesteps      | 4159488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.21e-06 |\n",
      "|    explained_variance   | 4.77e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00456   |\n",
      "|    n_updates            | 25990     |\n",
      "|    policy_gradient_loss | -6.66e-10 |\n",
      "|    value_loss           | 0.00701   |\n",
      "---------------------------------------\n",
      "Ep done - 145870.\n",
      "Ep done - 41510.\n",
      "Ep done - 41520.\n",
      "Ep done - 41530.\n",
      "Ep done - 41540.\n",
      "Ep done - 41550.\n",
      "Ep done - 41560.\n",
      "Ep done - 41570.\n",
      "Ep done - 41580.\n",
      "Ep done - 41590.\n",
      "Ep done - 41600.\n",
      "Eval num_timesteps=4160000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4160000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.15e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0041    |\n",
      "|    n_updates            | 26000     |\n",
      "|    policy_gradient_loss | 1.03e-09  |\n",
      "|    value_loss           | 0.00798   |\n",
      "---------------------------------------\n",
      "Ep done - 145880.\n",
      "Ep done - 145890.\n",
      "Ep done - 145900.\n",
      "Ep done - 145910.\n",
      "Ep done - 145920.\n",
      "Ep done - 145930.\n",
      "Ep done - 145940.\n",
      "Ep done - 145950.\n",
      "Ep done - 145960.\n",
      "Ep done - 145970.\n",
      "Ep done - 145980.\n",
      "Ep done - 145990.\n",
      "Ep done - 146000.\n",
      "Ep done - 146010.\n",
      "Ep done - 146020.\n",
      "Ep done - 146030.\n",
      "Ep done - 146040.\n",
      "Ep done - 146050.\n",
      "Ep done - 146060.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 678      |\n",
      "|    time_elapsed    | 14761    |\n",
      "|    total_timesteps | 4165632  |\n",
      "---------------------------------\n",
      "Ep done - 146070.\n",
      "Ep done - 146080.\n",
      "Ep done - 146090.\n",
      "Ep done - 146100.\n",
      "Ep done - 146110.\n",
      "Ep done - 146120.\n",
      "Ep done - 146130.\n",
      "Ep done - 146140.\n",
      "Ep done - 146150.\n",
      "Ep done - 146160.\n",
      "Ep done - 146170.\n",
      "Ep done - 146180.\n",
      "Ep done - 146190.\n",
      "Ep done - 146200.\n",
      "Ep done - 146210.\n",
      "Ep done - 41610.\n",
      "Ep done - 41620.\n",
      "Ep done - 41630.\n",
      "Ep done - 41640.\n",
      "Ep done - 41650.\n",
      "Ep done - 41660.\n",
      "Ep done - 41670.\n",
      "Ep done - 41680.\n",
      "Ep done - 41690.\n",
      "Ep done - 41700.\n",
      "Eval num_timesteps=4170000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4170000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.08e-06 |\n",
      "|    explained_variance   | 4.17e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00515   |\n",
      "|    n_updates            | 26010     |\n",
      "|    policy_gradient_loss | -5.33e-10 |\n",
      "|    value_loss           | 0.00799   |\n",
      "---------------------------------------\n",
      "Ep done - 146220.\n",
      "Ep done - 146230.\n",
      "Ep done - 146240.\n",
      "Ep done - 146250.\n",
      "Ep done - 146260.\n",
      "Ep done - 146270.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 679      |\n",
      "|    time_elapsed    | 14786    |\n",
      "|    total_timesteps | 4171776  |\n",
      "---------------------------------\n",
      "Ep done - 146280.\n",
      "Ep done - 146290.\n",
      "Ep done - 146300.\n",
      "Ep done - 146310.\n",
      "Ep done - 146320.\n",
      "Ep done - 146330.\n",
      "Ep done - 146340.\n",
      "Ep done - 146350.\n",
      "Ep done - 146360.\n",
      "Ep done - 146370.\n",
      "Ep done - 146380.\n",
      "Ep done - 146390.\n",
      "Ep done - 146400.\n",
      "Ep done - 146410.\n",
      "Ep done - 146420.\n",
      "Ep done - 146430.\n",
      "Ep done - 146440.\n",
      "Ep done - 146450.\n",
      "Ep done - 146460.\n",
      "Ep done - 146470.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 680       |\n",
      "|    time_elapsed         | 14806     |\n",
      "|    total_timesteps      | 4177920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.09e-06 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00213   |\n",
      "|    n_updates            | 26020     |\n",
      "|    policy_gradient_loss | 1.53e-10  |\n",
      "|    value_loss           | 0.00774   |\n",
      "---------------------------------------\n",
      "Ep done - 146480.\n",
      "Ep done - 146490.\n",
      "Ep done - 146500.\n",
      "Ep done - 146510.\n",
      "Ep done - 146520.\n",
      "Ep done - 146530.\n",
      "Ep done - 146540.\n",
      "Ep done - 41710.\n",
      "Ep done - 41720.\n",
      "Ep done - 41730.\n",
      "Ep done - 41740.\n",
      "Ep done - 41750.\n",
      "Ep done - 41760.\n",
      "Ep done - 41770.\n",
      "Ep done - 41780.\n",
      "Ep done - 41790.\n",
      "Ep done - 41800.\n",
      "Eval num_timesteps=4180000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -1            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4180000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9402554e-11 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.54e-06     |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.00365       |\n",
      "|    n_updates            | 26030         |\n",
      "|    policy_gradient_loss | -3.29e-08     |\n",
      "|    value_loss           | 0.0067        |\n",
      "-------------------------------------------\n",
      "Ep done - 146550.\n",
      "Ep done - 146560.\n",
      "Ep done - 146570.\n",
      "Ep done - 146580.\n",
      "Ep done - 146590.\n",
      "Ep done - 146600.\n",
      "Ep done - 146610.\n",
      "Ep done - 146620.\n",
      "Ep done - 146630.\n",
      "Ep done - 146640.\n",
      "Ep done - 146650.\n",
      "Ep done - 146660.\n",
      "Ep done - 146670.\n",
      "Ep done - 146680.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 681      |\n",
      "|    time_elapsed    | 14831    |\n",
      "|    total_timesteps | 4184064  |\n",
      "---------------------------------\n",
      "Ep done - 146690.\n",
      "Ep done - 146700.\n",
      "Ep done - 146710.\n",
      "Ep done - 146720.\n",
      "Ep done - 146730.\n",
      "Ep done - 146740.\n",
      "Ep done - 146750.\n",
      "Ep done - 146760.\n",
      "Ep done - 146770.\n",
      "Ep done - 146780.\n",
      "Ep done - 146790.\n",
      "Ep done - 146800.\n",
      "Ep done - 146810.\n",
      "Ep done - 146820.\n",
      "Ep done - 146830.\n",
      "Ep done - 146840.\n",
      "Ep done - 146850.\n",
      "Ep done - 146860.\n",
      "Ep done - 146870.\n",
      "Ep done - 41810.\n",
      "Ep done - 41820.\n",
      "Ep done - 41830.\n",
      "Ep done - 41840.\n",
      "Ep done - 41850.\n",
      "Ep done - 41860.\n",
      "Ep done - 41870.\n",
      "Ep done - 41880.\n",
      "Ep done - 41890.\n",
      "Ep done - 41900.\n",
      "Eval num_timesteps=4190000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4190000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.88e-07 |\n",
      "|    explained_variance   | 1.79e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00289   |\n",
      "|    n_updates            | 26040     |\n",
      "|    policy_gradient_loss | 4.47e-10  |\n",
      "|    value_loss           | 0.00587   |\n",
      "---------------------------------------\n",
      "Ep done - 146880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 682      |\n",
      "|    time_elapsed    | 14856    |\n",
      "|    total_timesteps | 4190208  |\n",
      "---------------------------------\n",
      "Ep done - 146890.\n",
      "Ep done - 146900.\n",
      "Ep done - 146910.\n",
      "Ep done - 146920.\n",
      "Ep done - 146930.\n",
      "Ep done - 146940.\n",
      "Ep done - 146950.\n",
      "Ep done - 146960.\n",
      "Ep done - 146970.\n",
      "Ep done - 146980.\n",
      "Ep done - 146990.\n",
      "Ep done - 147000.\n",
      "Ep done - 147010.\n",
      "Ep done - 147020.\n",
      "Ep done - 147030.\n",
      "Ep done - 147040.\n",
      "Ep done - 147050.\n",
      "Ep done - 147060.\n",
      "Ep done - 147070.\n",
      "Ep done - 147080.\n",
      "Ep done - 147090.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 683       |\n",
      "|    time_elapsed         | 14875     |\n",
      "|    total_timesteps      | 4196352   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.44e-07 |\n",
      "|    explained_variance   | 4.17e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00375   |\n",
      "|    n_updates            | 26050     |\n",
      "|    policy_gradient_loss | 4.07e-11  |\n",
      "|    value_loss           | 0.00657   |\n",
      "---------------------------------------\n",
      "Ep done - 147100.\n",
      "Ep done - 147110.\n",
      "Ep done - 147120.\n",
      "Ep done - 147130.\n",
      "Ep done - 147140.\n",
      "Ep done - 147150.\n",
      "Ep done - 147160.\n",
      "Ep done - 147170.\n",
      "Ep done - 147180.\n",
      "Ep done - 147190.\n",
      "Ep done - 147200.\n",
      "Ep done - 147210.\n",
      "Ep done - 41910.\n",
      "Ep done - 41920.\n",
      "Ep done - 41930.\n",
      "Ep done - 41940.\n",
      "Ep done - 41950.\n",
      "Ep done - 41960.\n",
      "Ep done - 41970.\n",
      "Ep done - 41980.\n",
      "Ep done - 41990.\n",
      "Ep done - 42000.\n",
      "Eval num_timesteps=4200000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4200000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.75e-07 |\n",
      "|    explained_variance   | 5.96e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0027    |\n",
      "|    n_updates            | 26060     |\n",
      "|    policy_gradient_loss | 1.45e-09  |\n",
      "|    value_loss           | 0.00484   |\n",
      "---------------------------------------\n",
      "Ep done - 147220.\n",
      "Ep done - 147230.\n",
      "Ep done - 147240.\n",
      "Ep done - 147250.\n",
      "Ep done - 147260.\n",
      "Ep done - 147270.\n",
      "Ep done - 147280.\n",
      "Ep done - 147290.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 684      |\n",
      "|    time_elapsed    | 14899    |\n",
      "|    total_timesteps | 4202496  |\n",
      "---------------------------------\n",
      "Ep done - 147300.\n",
      "Ep done - 147310.\n",
      "Ep done - 147320.\n",
      "Ep done - 147330.\n",
      "Ep done - 147340.\n",
      "Ep done - 147350.\n",
      "Ep done - 147360.\n",
      "Ep done - 147370.\n",
      "Ep done - 147380.\n",
      "Ep done - 147390.\n",
      "Ep done - 147400.\n",
      "Ep done - 147410.\n",
      "Ep done - 147420.\n",
      "Ep done - 147430.\n",
      "Ep done - 147440.\n",
      "Ep done - 147450.\n",
      "Ep done - 147460.\n",
      "Ep done - 147470.\n",
      "Ep done - 147480.\n",
      "Ep done - 147490.\n",
      "Ep done - 147500.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -1            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 282           |\n",
      "|    iterations           | 685           |\n",
      "|    time_elapsed         | 14918         |\n",
      "|    total_timesteps      | 4208640       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00057410257 |\n",
      "|    clip_fraction        | 0.000114      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.01e-05     |\n",
      "|    explained_variance   | 1.55e-06      |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.0024        |\n",
      "|    n_updates            | 26070         |\n",
      "|    policy_gradient_loss | -1.91e-05     |\n",
      "|    value_loss           | 0.0041        |\n",
      "-------------------------------------------\n",
      "Ep done - 147510.\n",
      "Ep done - 147520.\n",
      "Ep done - 147530.\n",
      "Ep done - 147540.\n",
      "Ep done - 42010.\n",
      "Ep done - 42020.\n",
      "Ep done - 42030.\n",
      "Ep done - 42040.\n",
      "Ep done - 42050.\n",
      "Ep done - 42060.\n",
      "Ep done - 42070.\n",
      "Ep done - 42080.\n",
      "Ep done - 42090.\n",
      "Ep done - 42100.\n",
      "Eval num_timesteps=4210000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4210000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.11e-07 |\n",
      "|    explained_variance   | 1.55e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00288   |\n",
      "|    n_updates            | 26080     |\n",
      "|    policy_gradient_loss | -1.04e-09 |\n",
      "|    value_loss           | 0.00582   |\n",
      "---------------------------------------\n",
      "Ep done - 147550.\n",
      "Ep done - 147560.\n",
      "Ep done - 147570.\n",
      "Ep done - 147580.\n",
      "Ep done - 147590.\n",
      "Ep done - 147600.\n",
      "Ep done - 147610.\n",
      "Ep done - 147620.\n",
      "Ep done - 147630.\n",
      "Ep done - 147640.\n",
      "Ep done - 147650.\n",
      "Ep done - 147660.\n",
      "Ep done - 147670.\n",
      "Ep done - 147680.\n",
      "Ep done - 147690.\n",
      "Ep done - 147700.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 686      |\n",
      "|    time_elapsed    | 14942    |\n",
      "|    total_timesteps | 4214784  |\n",
      "---------------------------------\n",
      "Ep done - 147710.\n",
      "Ep done - 147720.\n",
      "Ep done - 147730.\n",
      "Ep done - 147740.\n",
      "Ep done - 147750.\n",
      "Ep done - 147760.\n",
      "Ep done - 147770.\n",
      "Ep done - 147780.\n",
      "Ep done - 147790.\n",
      "Ep done - 147800.\n",
      "Ep done - 147810.\n",
      "Ep done - 147820.\n",
      "Ep done - 147830.\n",
      "Ep done - 147840.\n",
      "Ep done - 147850.\n",
      "Ep done - 147860.\n",
      "Ep done - 147870.\n",
      "Ep done - 42110.\n",
      "Ep done - 42120.\n",
      "Ep done - 42130.\n",
      "Ep done - 42140.\n",
      "Ep done - 42150.\n",
      "Ep done - 42160.\n",
      "Ep done - 42170.\n",
      "Ep done - 42180.\n",
      "Ep done - 42190.\n",
      "Ep done - 42200.\n",
      "Eval num_timesteps=4220000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4220000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.42e-07 |\n",
      "|    explained_variance   | 5.42e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0027    |\n",
      "|    n_updates            | 26090     |\n",
      "|    policy_gradient_loss | -3.09e-10 |\n",
      "|    value_loss           | 0.00554   |\n",
      "---------------------------------------\n",
      "Ep done - 147880.\n",
      "Ep done - 147890.\n",
      "Ep done - 147900.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 687      |\n",
      "|    time_elapsed    | 14967    |\n",
      "|    total_timesteps | 4220928  |\n",
      "---------------------------------\n",
      "Ep done - 147910.\n",
      "Ep done - 147920.\n",
      "Ep done - 147930.\n",
      "Ep done - 147940.\n",
      "Ep done - 147950.\n",
      "Ep done - 147960.\n",
      "Ep done - 147970.\n",
      "Ep done - 147980.\n",
      "Ep done - 147990.\n",
      "Ep done - 148000.\n",
      "Ep done - 148010.\n",
      "Ep done - 148020.\n",
      "Ep done - 148030.\n",
      "Ep done - 148040.\n",
      "Ep done - 148050.\n",
      "Ep done - 148060.\n",
      "Ep done - 148070.\n",
      "Ep done - 148080.\n",
      "Ep done - 148090.\n",
      "Ep done - 148100.\n",
      "Ep done - 148110.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 688       |\n",
      "|    time_elapsed         | 14986     |\n",
      "|    total_timesteps      | 4227072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.17e-07 |\n",
      "|    explained_variance   | 0.000136  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00284   |\n",
      "|    n_updates            | 26100     |\n",
      "|    policy_gradient_loss | 3.35e-10  |\n",
      "|    value_loss           | 0.00614   |\n",
      "---------------------------------------\n",
      "Ep done - 148120.\n",
      "Ep done - 148130.\n",
      "Ep done - 148140.\n",
      "Ep done - 148150.\n",
      "Ep done - 148160.\n",
      "Ep done - 148170.\n",
      "Ep done - 148180.\n",
      "Ep done - 148190.\n",
      "Ep done - 148200.\n",
      "Ep done - 148210.\n",
      "Ep done - 42210.\n",
      "Ep done - 42220.\n",
      "Ep done - 42230.\n",
      "Ep done - 42240.\n",
      "Ep done - 42250.\n",
      "Ep done - 42260.\n",
      "Ep done - 42270.\n",
      "Ep done - 42280.\n",
      "Ep done - 42290.\n",
      "Ep done - 42300.\n",
      "Eval num_timesteps=4230000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -1            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4230000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00076493033 |\n",
      "|    clip_fraction        | 0.000309      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -5.69e-05     |\n",
      "|    explained_variance   | 1.55e-06      |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.00232       |\n",
      "|    n_updates            | 26110         |\n",
      "|    policy_gradient_loss | -4.68e-05     |\n",
      "|    value_loss           | 0.00562       |\n",
      "-------------------------------------------\n",
      "Ep done - 148220.\n",
      "Ep done - 148230.\n",
      "Ep done - 148240.\n",
      "Ep done - 148250.\n",
      "Ep done - 148260.\n",
      "Ep done - 148270.\n",
      "Ep done - 148280.\n",
      "Ep done - 148290.\n",
      "Ep done - 148300.\n",
      "Ep done - 148310.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 689      |\n",
      "|    time_elapsed    | 15011    |\n",
      "|    total_timesteps | 4233216  |\n",
      "---------------------------------\n",
      "Ep done - 148320.\n",
      "Ep done - 148330.\n",
      "Ep done - 148340.\n",
      "Ep done - 148350.\n",
      "Ep done - 148360.\n",
      "Ep done - 148370.\n",
      "Ep done - 148380.\n",
      "Ep done - 148390.\n",
      "Ep done - 148400.\n",
      "Ep done - 148410.\n",
      "Ep done - 148420.\n",
      "Ep done - 148430.\n",
      "Ep done - 148440.\n",
      "Ep done - 148450.\n",
      "Ep done - 148460.\n",
      "Ep done - 148470.\n",
      "Ep done - 148480.\n",
      "Ep done - 148490.\n",
      "Ep done - 148500.\n",
      "Ep done - 148510.\n",
      "Ep done - 148520.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 690       |\n",
      "|    time_elapsed         | 15030     |\n",
      "|    total_timesteps      | 4239360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.01e-07 |\n",
      "|    explained_variance   | 1.31e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00292   |\n",
      "|    n_updates            | 26120     |\n",
      "|    policy_gradient_loss | -3.86e-11 |\n",
      "|    value_loss           | 0.00592   |\n",
      "---------------------------------------\n",
      "Ep done - 148530.\n",
      "Ep done - 148540.\n",
      "Ep done - 42310.\n",
      "Ep done - 42320.\n",
      "Ep done - 42330.\n",
      "Ep done - 42340.\n",
      "Ep done - 42350.\n",
      "Ep done - 42360.\n",
      "Ep done - 42370.\n",
      "Ep done - 42380.\n",
      "Ep done - 42390.\n",
      "Ep done - 42400.\n",
      "Eval num_timesteps=4240000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4240000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.81e-07 |\n",
      "|    explained_variance   | 1.67e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0029    |\n",
      "|    n_updates            | 26130     |\n",
      "|    policy_gradient_loss | 1.79e-09  |\n",
      "|    value_loss           | 0.00625   |\n",
      "---------------------------------------\n",
      "Ep done - 148550.\n",
      "Ep done - 148560.\n",
      "Ep done - 148570.\n",
      "Ep done - 148580.\n",
      "Ep done - 148590.\n",
      "Ep done - 148600.\n",
      "Ep done - 148610.\n",
      "Ep done - 148620.\n",
      "Ep done - 148630.\n",
      "Ep done - 148640.\n",
      "Ep done - 148650.\n",
      "Ep done - 148660.\n",
      "Ep done - 148670.\n",
      "Ep done - 148680.\n",
      "Ep done - 148690.\n",
      "Ep done - 148700.\n",
      "Ep done - 148710.\n",
      "Ep done - 148720.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 691      |\n",
      "|    time_elapsed    | 15054    |\n",
      "|    total_timesteps | 4245504  |\n",
      "---------------------------------\n",
      "Ep done - 148730.\n",
      "Ep done - 148740.\n",
      "Ep done - 148750.\n",
      "Ep done - 148760.\n",
      "Ep done - 148770.\n",
      "Ep done - 148780.\n",
      "Ep done - 148790.\n",
      "Ep done - 148800.\n",
      "Ep done - 148810.\n",
      "Ep done - 148820.\n",
      "Ep done - 148830.\n",
      "Ep done - 148840.\n",
      "Ep done - 148850.\n",
      "Ep done - 148860.\n",
      "Ep done - 148870.\n",
      "Ep done - 42410.\n",
      "Ep done - 42420.\n",
      "Ep done - 42430.\n",
      "Ep done - 42440.\n",
      "Ep done - 42450.\n",
      "Ep done - 42460.\n",
      "Ep done - 42470.\n",
      "Ep done - 42480.\n",
      "Ep done - 42490.\n",
      "Ep done - 42500.\n",
      "Eval num_timesteps=4250000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4250000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00204436 |\n",
      "|    clip_fraction        | 0.000146   |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.58e-06  |\n",
      "|    explained_variance   | 2.56e-06   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00159    |\n",
      "|    n_updates            | 26140      |\n",
      "|    policy_gradient_loss | -2.11e-05  |\n",
      "|    value_loss           | 0.00465    |\n",
      "----------------------------------------\n",
      "Ep done - 148880.\n",
      "Ep done - 148890.\n",
      "Ep done - 148900.\n",
      "Ep done - 148910.\n",
      "Ep done - 148920.\n",
      "Ep done - 148930.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 692      |\n",
      "|    time_elapsed    | 15078    |\n",
      "|    total_timesteps | 4251648  |\n",
      "---------------------------------\n",
      "Ep done - 148940.\n",
      "Ep done - 148950.\n",
      "Ep done - 148960.\n",
      "Ep done - 148970.\n",
      "Ep done - 148980.\n",
      "Ep done - 148990.\n",
      "Ep done - 149000.\n",
      "Ep done - 149010.\n",
      "Ep done - 149020.\n",
      "Ep done - 149030.\n",
      "Ep done - 149040.\n",
      "Ep done - 149050.\n",
      "Ep done - 149060.\n",
      "Ep done - 149070.\n",
      "Ep done - 149080.\n",
      "Ep done - 149090.\n",
      "Ep done - 149100.\n",
      "Ep done - 149110.\n",
      "Ep done - 149120.\n",
      "Ep done - 149130.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 693       |\n",
      "|    time_elapsed         | 15097     |\n",
      "|    total_timesteps      | 4257792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.75e-07 |\n",
      "|    explained_variance   | 3.04e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0028    |\n",
      "|    n_updates            | 26150     |\n",
      "|    policy_gradient_loss | 8.36e-10  |\n",
      "|    value_loss           | 0.00629   |\n",
      "---------------------------------------\n",
      "Ep done - 149140.\n",
      "Ep done - 149150.\n",
      "Ep done - 149160.\n",
      "Ep done - 149170.\n",
      "Ep done - 149180.\n",
      "Ep done - 149190.\n",
      "Ep done - 149200.\n",
      "Ep done - 149210.\n",
      "Ep done - 42510.\n",
      "Ep done - 42520.\n",
      "Ep done - 42530.\n",
      "Ep done - 42540.\n",
      "Ep done - 42550.\n",
      "Ep done - 42560.\n",
      "Ep done - 42570.\n",
      "Ep done - 42580.\n",
      "Ep done - 42590.\n",
      "Ep done - 42600.\n",
      "Eval num_timesteps=4260000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4260000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.16e-07 |\n",
      "|    explained_variance   | 1.08e-05  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00287   |\n",
      "|    n_updates            | 26160     |\n",
      "|    policy_gradient_loss | -2.36e-09 |\n",
      "|    value_loss           | 0.00642   |\n",
      "---------------------------------------\n",
      "Ep done - 149220.\n",
      "Ep done - 149230.\n",
      "Ep done - 149240.\n",
      "Ep done - 149250.\n",
      "Ep done - 149260.\n",
      "Ep done - 149270.\n",
      "Ep done - 149280.\n",
      "Ep done - 149290.\n",
      "Ep done - 149300.\n",
      "Ep done - 149310.\n",
      "Ep done - 149320.\n",
      "Ep done - 149330.\n",
      "Ep done - 149340.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 694      |\n",
      "|    time_elapsed    | 15121    |\n",
      "|    total_timesteps | 4263936  |\n",
      "---------------------------------\n",
      "Ep done - 149350.\n",
      "Ep done - 149360.\n",
      "Ep done - 149370.\n",
      "Ep done - 149380.\n",
      "Ep done - 149390.\n",
      "Ep done - 149400.\n",
      "Ep done - 149410.\n",
      "Ep done - 149420.\n",
      "Ep done - 149430.\n",
      "Ep done - 149440.\n",
      "Ep done - 149450.\n",
      "Ep done - 149460.\n",
      "Ep done - 149470.\n",
      "Ep done - 149480.\n",
      "Ep done - 149490.\n",
      "Ep done - 149500.\n",
      "Ep done - 149510.\n",
      "Ep done - 149520.\n",
      "Ep done - 149530.\n",
      "Ep done - 149540.\n",
      "Ep done - 42610.\n",
      "Ep done - 42620.\n",
      "Ep done - 42630.\n",
      "Ep done - 42640.\n",
      "Ep done - 42650.\n",
      "Ep done - 42660.\n",
      "Ep done - 42670.\n",
      "Ep done - 42680.\n",
      "Ep done - 42690.\n",
      "Ep done - 42700.\n",
      "Eval num_timesteps=4270000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4270000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.32e-07 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0074    |\n",
      "|    n_updates            | 26170     |\n",
      "|    policy_gradient_loss | -1.14e-09 |\n",
      "|    value_loss           | 0.00632   |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 695      |\n",
      "|    time_elapsed    | 15145    |\n",
      "|    total_timesteps | 4270080  |\n",
      "---------------------------------\n",
      "Ep done - 149550.\n",
      "Ep done - 149560.\n",
      "Ep done - 149570.\n",
      "Ep done - 149580.\n",
      "Ep done - 149590.\n",
      "Ep done - 149600.\n",
      "Ep done - 149610.\n",
      "Ep done - 149620.\n",
      "Ep done - 149630.\n",
      "Ep done - 149640.\n",
      "Ep done - 149650.\n",
      "Ep done - 149660.\n",
      "Ep done - 149670.\n",
      "Ep done - 149680.\n",
      "Ep done - 149690.\n",
      "Ep done - 149700.\n",
      "Ep done - 149710.\n",
      "Ep done - 149720.\n",
      "Ep done - 149730.\n",
      "Ep done - 149740.\n",
      "Ep done - 149750.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 696       |\n",
      "|    time_elapsed         | 15164     |\n",
      "|    total_timesteps      | 4276224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.11e-06 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00361   |\n",
      "|    n_updates            | 26180     |\n",
      "|    policy_gradient_loss | -9.73e-09 |\n",
      "|    value_loss           | 0.00912   |\n",
      "---------------------------------------\n",
      "Ep done - 149760.\n",
      "Ep done - 149770.\n",
      "Ep done - 149780.\n",
      "Ep done - 149790.\n",
      "Ep done - 149800.\n",
      "Ep done - 149810.\n",
      "Ep done - 149820.\n",
      "Ep done - 149830.\n",
      "Ep done - 149840.\n",
      "Ep done - 149850.\n",
      "Ep done - 149860.\n",
      "Ep done - 149870.\n",
      "Ep done - 42710.\n",
      "Ep done - 42720.\n",
      "Ep done - 42730.\n",
      "Ep done - 42740.\n",
      "Ep done - 42750.\n",
      "Ep done - 42760.\n",
      "Ep done - 42770.\n",
      "Ep done - 42780.\n",
      "Ep done - 42790.\n",
      "Ep done - 42800.\n",
      "Eval num_timesteps=4280000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4280000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.08e-06 |\n",
      "|    explained_variance   | 0.00141   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00418   |\n",
      "|    n_updates            | 26190     |\n",
      "|    policy_gradient_loss | -2.94e-10 |\n",
      "|    value_loss           | 0.00748   |\n",
      "---------------------------------------\n",
      "Ep done - 149880.\n",
      "Ep done - 149890.\n",
      "Ep done - 149900.\n",
      "Ep done - 149910.\n",
      "Ep done - 149920.\n",
      "Ep done - 149930.\n",
      "Ep done - 149940.\n",
      "Ep done - 149950.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 697      |\n",
      "|    time_elapsed    | 15188    |\n",
      "|    total_timesteps | 4282368  |\n",
      "---------------------------------\n",
      "Ep done - 149960.\n",
      "Ep done - 149970.\n",
      "Ep done - 149980.\n",
      "Ep done - 149990.\n",
      "Ep done - 150000.\n",
      "Ep done - 150010.\n",
      "Ep done - 150020.\n",
      "Ep done - 150030.\n",
      "Ep done - 150040.\n",
      "Ep done - 150050.\n",
      "Ep done - 150060.\n",
      "Ep done - 150070.\n",
      "Ep done - 150080.\n",
      "Ep done - 150090.\n",
      "Ep done - 150100.\n",
      "Ep done - 150110.\n",
      "Ep done - 150120.\n",
      "Ep done - 150130.\n",
      "Ep done - 150140.\n",
      "Ep done - 150150.\n",
      "Ep done - 150160.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 698       |\n",
      "|    time_elapsed         | 15211     |\n",
      "|    total_timesteps      | 4288512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.78e-07 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00358   |\n",
      "|    n_updates            | 26200     |\n",
      "|    policy_gradient_loss | -3.79e-10 |\n",
      "|    value_loss           | 0.00867   |\n",
      "---------------------------------------\n",
      "Ep done - 150170.\n",
      "Ep done - 150180.\n",
      "Ep done - 150190.\n",
      "Ep done - 150200.\n",
      "Ep done - 150210.\n",
      "Ep done - 42810.\n",
      "Ep done - 42820.\n",
      "Ep done - 42830.\n",
      "Ep done - 42840.\n",
      "Ep done - 42850.\n",
      "Ep done - 42860.\n",
      "Ep done - 42870.\n",
      "Ep done - 42880.\n",
      "Ep done - 42890.\n",
      "Ep done - 42900.\n",
      "Eval num_timesteps=4290000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4290000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.05e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0029    |\n",
      "|    n_updates            | 26210     |\n",
      "|    policy_gradient_loss | 3.06e-10  |\n",
      "|    value_loss           | 0.00512   |\n",
      "---------------------------------------\n",
      "Ep done - 150220.\n",
      "Ep done - 150230.\n",
      "Ep done - 150240.\n",
      "Ep done - 150250.\n",
      "Ep done - 150260.\n",
      "Ep done - 150270.\n",
      "Ep done - 150280.\n",
      "Ep done - 150290.\n",
      "Ep done - 150300.\n",
      "Ep done - 150310.\n",
      "Ep done - 150320.\n",
      "Ep done - 150330.\n",
      "Ep done - 150340.\n",
      "Ep done - 150350.\n",
      "Ep done - 150360.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 699      |\n",
      "|    time_elapsed    | 15244    |\n",
      "|    total_timesteps | 4294656  |\n",
      "---------------------------------\n",
      "Ep done - 150370.\n",
      "Ep done - 150380.\n",
      "Ep done - 150390.\n",
      "Ep done - 150400.\n",
      "Ep done - 150410.\n",
      "Ep done - 150420.\n",
      "Ep done - 150430.\n",
      "Ep done - 150440.\n",
      "Ep done - 150450.\n",
      "Ep done - 150460.\n",
      "Ep done - 150470.\n",
      "Ep done - 150480.\n",
      "Ep done - 150490.\n",
      "Ep done - 150500.\n",
      "Ep done - 150510.\n",
      "Ep done - 150520.\n",
      "Ep done - 150530.\n",
      "Ep done - 150540.\n",
      "Ep done - 42910.\n",
      "Ep done - 42920.\n",
      "Ep done - 42930.\n",
      "Ep done - 42940.\n",
      "Ep done - 42950.\n",
      "Ep done - 42960.\n",
      "Ep done - 42970.\n",
      "Ep done - 42980.\n",
      "Ep done - 42990.\n",
      "Ep done - 43000.\n",
      "Eval num_timesteps=4300000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4300000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.89e-07 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00272   |\n",
      "|    n_updates            | 26220     |\n",
      "|    policy_gradient_loss | 1.93e-09  |\n",
      "|    value_loss           | 0.00551   |\n",
      "---------------------------------------\n",
      "Ep done - 150550.\n",
      "Ep done - 150560.\n",
      "Ep done - 150570.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 700      |\n",
      "|    time_elapsed    | 15270    |\n",
      "|    total_timesteps | 4300800  |\n",
      "---------------------------------\n",
      "Ep done - 150580.\n",
      "Ep done - 150590.\n",
      "Ep done - 150600.\n",
      "Ep done - 150610.\n",
      "Ep done - 150620.\n",
      "Ep done - 150630.\n",
      "Ep done - 150640.\n",
      "Ep done - 150650.\n",
      "Ep done - 150660.\n",
      "Ep done - 150670.\n",
      "Ep done - 150680.\n",
      "Ep done - 150690.\n",
      "Ep done - 150700.\n",
      "Ep done - 150710.\n",
      "Ep done - 150720.\n",
      "Ep done - 150730.\n",
      "Ep done - 150740.\n",
      "Ep done - 150750.\n",
      "Ep done - 150760.\n",
      "Ep done - 150770.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 701       |\n",
      "|    time_elapsed         | 15290     |\n",
      "|    total_timesteps      | 4306944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.85e-07 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0027    |\n",
      "|    n_updates            | 26230     |\n",
      "|    policy_gradient_loss | 1.05e-10  |\n",
      "|    value_loss           | 0.00641   |\n",
      "---------------------------------------\n",
      "Ep done - 150780.\n",
      "Ep done - 150790.\n",
      "Ep done - 150800.\n",
      "Ep done - 150810.\n",
      "Ep done - 150820.\n",
      "Ep done - 150830.\n",
      "Ep done - 150840.\n",
      "Ep done - 150850.\n",
      "Ep done - 150860.\n",
      "Ep done - 150870.\n",
      "Ep done - 43010.\n",
      "Ep done - 43020.\n",
      "Ep done - 43030.\n",
      "Ep done - 43040.\n",
      "Ep done - 43050.\n",
      "Ep done - 43060.\n",
      "Ep done - 43070.\n",
      "Ep done - 43080.\n",
      "Ep done - 43090.\n",
      "Ep done - 43100.\n",
      "Eval num_timesteps=4310000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4310000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.53e-07 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00209   |\n",
      "|    n_updates            | 26240     |\n",
      "|    policy_gradient_loss | -3.25e-09 |\n",
      "|    value_loss           | 0.00467   |\n",
      "---------------------------------------\n",
      "Ep done - 150880.\n",
      "Ep done - 150890.\n",
      "Ep done - 150900.\n",
      "Ep done - 150910.\n",
      "Ep done - 150920.\n",
      "Ep done - 150930.\n",
      "Ep done - 150940.\n",
      "Ep done - 150950.\n",
      "Ep done - 150960.\n",
      "Ep done - 150970.\n",
      "Ep done - 150980.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 702      |\n",
      "|    time_elapsed    | 15311    |\n",
      "|    total_timesteps | 4313088  |\n",
      "---------------------------------\n",
      "Ep done - 150990.\n",
      "Ep done - 151000.\n",
      "Ep done - 151010.\n",
      "Ep done - 151020.\n",
      "Ep done - 151030.\n",
      "Ep done - 151040.\n",
      "Ep done - 151050.\n",
      "Ep done - 151060.\n",
      "Ep done - 151070.\n",
      "Ep done - 151080.\n",
      "Ep done - 151090.\n",
      "Ep done - 151100.\n",
      "Ep done - 151110.\n",
      "Ep done - 151120.\n",
      "Ep done - 151130.\n",
      "Ep done - 151140.\n",
      "Ep done - 151150.\n",
      "Ep done - 151160.\n",
      "Ep done - 151170.\n",
      "Ep done - 151180.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 703       |\n",
      "|    time_elapsed         | 15327     |\n",
      "|    total_timesteps      | 4319232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.13e-06 |\n",
      "|    explained_variance   | 4.17e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00179   |\n",
      "|    n_updates            | 26250     |\n",
      "|    policy_gradient_loss | -5.18e-09 |\n",
      "|    value_loss           | 0.00498   |\n",
      "---------------------------------------\n",
      "Ep done - 151190.\n",
      "Ep done - 151200.\n",
      "Ep done - 151210.\n",
      "Ep done - 43110.\n",
      "Ep done - 43120.\n",
      "Ep done - 43130.\n",
      "Ep done - 43140.\n",
      "Ep done - 43150.\n",
      "Ep done - 43160.\n",
      "Ep done - 43170.\n",
      "Ep done - 43180.\n",
      "Ep done - 43190.\n",
      "Ep done - 43200.\n",
      "Eval num_timesteps=4320000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4320000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.22e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00251   |\n",
      "|    n_updates            | 26260     |\n",
      "|    policy_gradient_loss | 4.03e-10  |\n",
      "|    value_loss           | 0.00676   |\n",
      "---------------------------------------\n",
      "Ep done - 151220.\n",
      "Ep done - 151230.\n",
      "Ep done - 151240.\n",
      "Ep done - 151250.\n",
      "Ep done - 151260.\n",
      "Ep done - 151270.\n",
      "Ep done - 151280.\n",
      "Ep done - 151290.\n",
      "Ep done - 151300.\n",
      "Ep done - 151310.\n",
      "Ep done - 151320.\n",
      "Ep done - 151330.\n",
      "Ep done - 151340.\n",
      "Ep done - 151350.\n",
      "Ep done - 151360.\n",
      "Ep done - 151370.\n",
      "Ep done - 151380.\n",
      "Ep done - 151390.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 704      |\n",
      "|    time_elapsed    | 15348    |\n",
      "|    total_timesteps | 4325376  |\n",
      "---------------------------------\n",
      "Ep done - 151400.\n",
      "Ep done - 151410.\n",
      "Ep done - 151420.\n",
      "Ep done - 151430.\n",
      "Ep done - 151440.\n",
      "Ep done - 151450.\n",
      "Ep done - 151460.\n",
      "Ep done - 151470.\n",
      "Ep done - 151480.\n",
      "Ep done - 151490.\n",
      "Ep done - 151500.\n",
      "Ep done - 151510.\n",
      "Ep done - 151520.\n",
      "Ep done - 151530.\n",
      "Ep done - 151540.\n",
      "Ep done - 43210.\n",
      "Ep done - 43220.\n",
      "Ep done - 43230.\n",
      "Ep done - 43240.\n",
      "Ep done - 43250.\n",
      "Ep done - 43260.\n",
      "Ep done - 43270.\n",
      "Ep done - 43280.\n",
      "Ep done - 43290.\n",
      "Ep done - 43300.\n",
      "Eval num_timesteps=4330000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.34 +/- 0.47\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.3      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4330000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7920952 |\n",
      "|    clip_fraction        | 0.0267    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.26e-05 |\n",
      "|    explained_variance   | 6.56e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00566  |\n",
      "|    n_updates            | 26270     |\n",
      "|    policy_gradient_loss | -0.00776  |\n",
      "|    value_loss           | 0.00698   |\n",
      "---------------------------------------\n",
      "Ep done - 151550.\n",
      "Ep done - 151560.\n",
      "Ep done - 151570.\n",
      "Ep done - 151580.\n",
      "Ep done - 151590.\n",
      "Ep done - 151600.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 705      |\n",
      "|    time_elapsed    | 15371    |\n",
      "|    total_timesteps | 4331520  |\n",
      "---------------------------------\n",
      "Ep done - 151610.\n",
      "Ep done - 151620.\n",
      "Ep done - 151630.\n",
      "Ep done - 151640.\n",
      "Ep done - 151650.\n",
      "Ep done - 151660.\n",
      "Ep done - 151670.\n",
      "Ep done - 151680.\n",
      "Ep done - 151690.\n",
      "Ep done - 151700.\n",
      "Ep done - 151710.\n",
      "Ep done - 151720.\n",
      "Ep done - 151730.\n",
      "Ep done - 151740.\n",
      "Ep done - 151750.\n",
      "Ep done - 151760.\n",
      "Ep done - 151770.\n",
      "Ep done - 151780.\n",
      "Ep done - 151790.\n",
      "Ep done - 151800.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.4      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 706       |\n",
      "|    time_elapsed         | 15393     |\n",
      "|    total_timesteps      | 4337664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.12e-07 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00146   |\n",
      "|    n_updates            | 26280     |\n",
      "|    policy_gradient_loss | -1.94e-09 |\n",
      "|    value_loss           | 0.00563   |\n",
      "---------------------------------------\n",
      "Ep done - 151810.\n",
      "Ep done - 151820.\n",
      "Ep done - 151830.\n",
      "Ep done - 151840.\n",
      "Ep done - 151850.\n",
      "Ep done - 151860.\n",
      "Ep done - 151870.\n",
      "Ep done - 151880.\n",
      "Ep done - 43310.\n",
      "Ep done - 43320.\n",
      "Ep done - 43330.\n",
      "Ep done - 43340.\n",
      "Ep done - 43350.\n",
      "Ep done - 43360.\n",
      "Ep done - 43370.\n",
      "Ep done - 43380.\n",
      "Ep done - 43390.\n",
      "Ep done - 43400.\n",
      "Eval num_timesteps=4340000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.37 +/- 0.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 29.4         |\n",
      "|    mean_reward          | -1           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4340000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015998316 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.27e-06    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00266      |\n",
      "|    n_updates            | 26290        |\n",
      "|    policy_gradient_loss | -7.96e-06    |\n",
      "|    value_loss           | 0.00499      |\n",
      "------------------------------------------\n",
      "Ep done - 151890.\n",
      "Ep done - 151900.\n",
      "Ep done - 151910.\n",
      "Ep done - 151920.\n",
      "Ep done - 151930.\n",
      "Ep done - 151940.\n",
      "Ep done - 151950.\n",
      "Ep done - 151960.\n",
      "Ep done - 151970.\n",
      "Ep done - 151980.\n",
      "Ep done - 151990.\n",
      "Ep done - 152000.\n",
      "Ep done - 152010.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 707      |\n",
      "|    time_elapsed    | 15420    |\n",
      "|    total_timesteps | 4343808  |\n",
      "---------------------------------\n",
      "Ep done - 152020.\n",
      "Ep done - 152030.\n",
      "Ep done - 152040.\n",
      "Ep done - 152050.\n",
      "Ep done - 152060.\n",
      "Ep done - 152070.\n",
      "Ep done - 152080.\n",
      "Ep done - 152090.\n",
      "Ep done - 152100.\n",
      "Ep done - 152110.\n",
      "Ep done - 152120.\n",
      "Ep done - 152130.\n",
      "Ep done - 152140.\n",
      "Ep done - 152150.\n",
      "Ep done - 152160.\n",
      "Ep done - 152170.\n",
      "Ep done - 152180.\n",
      "Ep done - 152190.\n",
      "Ep done - 152200.\n",
      "Ep done - 152210.\n",
      "Ep done - 152220.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 29.4         |\n",
      "|    ep_rew_mean          | -1           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 708          |\n",
      "|    time_elapsed         | 15440        |\n",
      "|    total_timesteps      | 4349952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006150296 |\n",
      "|    clip_fraction        | 0.00013      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.28e-06    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00352      |\n",
      "|    n_updates            | 26300        |\n",
      "|    policy_gradient_loss | -3.7e-05     |\n",
      "|    value_loss           | 0.00629      |\n",
      "------------------------------------------\n",
      "Ep done - 43410.\n",
      "Ep done - 43420.\n",
      "Ep done - 43430.\n",
      "Ep done - 43440.\n",
      "Ep done - 43450.\n",
      "Ep done - 43460.\n",
      "Ep done - 43470.\n",
      "Ep done - 43480.\n",
      "Ep done - 43490.\n",
      "Ep done - 43500.\n",
      "Eval num_timesteps=4350000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.48 +/- 0.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 29.5         |\n",
      "|    mean_reward          | -1           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4350000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009219738 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.12e-06    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00286      |\n",
      "|    n_updates            | 26310        |\n",
      "|    policy_gradient_loss | -4.67e-05    |\n",
      "|    value_loss           | 0.00661      |\n",
      "------------------------------------------\n",
      "Ep done - 152230.\n",
      "Ep done - 152240.\n",
      "Ep done - 152250.\n",
      "Ep done - 152260.\n",
      "Ep done - 152270.\n",
      "Ep done - 152280.\n",
      "Ep done - 152290.\n",
      "Ep done - 152300.\n",
      "Ep done - 152310.\n",
      "Ep done - 152320.\n",
      "Ep done - 152330.\n",
      "Ep done - 152340.\n",
      "Ep done - 152350.\n",
      "Ep done - 152360.\n",
      "Ep done - 152370.\n",
      "Ep done - 152380.\n",
      "Ep done - 152390.\n",
      "Ep done - 152400.\n",
      "Ep done - 152410.\n",
      "Ep done - 152420.\n",
      "Ep done - 152430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 709      |\n",
      "|    time_elapsed    | 15465    |\n",
      "|    total_timesteps | 4356096  |\n",
      "---------------------------------\n",
      "Ep done - 152440.\n",
      "Ep done - 152450.\n",
      "Ep done - 152460.\n",
      "Ep done - 152470.\n",
      "Ep done - 152480.\n",
      "Ep done - 152490.\n",
      "Ep done - 152500.\n",
      "Ep done - 152510.\n",
      "Ep done - 152520.\n",
      "Ep done - 152530.\n",
      "Ep done - 152540.\n",
      "Ep done - 152550.\n",
      "Ep done - 152560.\n",
      "Ep done - 43510.\n",
      "Ep done - 43520.\n",
      "Ep done - 43530.\n",
      "Ep done - 43540.\n",
      "Ep done - 43550.\n",
      "Ep done - 43560.\n",
      "Ep done - 43570.\n",
      "Ep done - 43580.\n",
      "Ep done - 43590.\n",
      "Ep done - 43600.\n",
      "Eval num_timesteps=4360000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.43 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.4      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4360000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.11e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00429   |\n",
      "|    n_updates            | 26320     |\n",
      "|    policy_gradient_loss | -2.29e-08 |\n",
      "|    value_loss           | 0.00926   |\n",
      "---------------------------------------\n",
      "Ep done - 152570.\n",
      "Ep done - 152580.\n",
      "Ep done - 152590.\n",
      "Ep done - 152600.\n",
      "Ep done - 152610.\n",
      "Ep done - 152620.\n",
      "Ep done - 152630.\n",
      "Ep done - 152640.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 710      |\n",
      "|    time_elapsed    | 15491    |\n",
      "|    total_timesteps | 4362240  |\n",
      "---------------------------------\n",
      "Ep done - 152650.\n",
      "Ep done - 152660.\n",
      "Ep done - 152670.\n",
      "Ep done - 152680.\n",
      "Ep done - 152690.\n",
      "Ep done - 152700.\n",
      "Ep done - 152710.\n",
      "Ep done - 152720.\n",
      "Ep done - 152730.\n",
      "Ep done - 152740.\n",
      "Ep done - 152750.\n",
      "Ep done - 152760.\n",
      "Ep done - 152770.\n",
      "Ep done - 152780.\n",
      "Ep done - 152790.\n",
      "Ep done - 152800.\n",
      "Ep done - 152810.\n",
      "Ep done - 152820.\n",
      "Ep done - 152830.\n",
      "Ep done - 152840.\n",
      "Ep done - 152850.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.4      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 711       |\n",
      "|    time_elapsed         | 15512     |\n",
      "|    total_timesteps      | 4368384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.04e-07 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00372   |\n",
      "|    n_updates            | 26330     |\n",
      "|    policy_gradient_loss | 1.75e-09  |\n",
      "|    value_loss           | 0.00809   |\n",
      "---------------------------------------\n",
      "Ep done - 152860.\n",
      "Ep done - 152870.\n",
      "Ep done - 152880.\n",
      "Ep done - 152890.\n",
      "Ep done - 152900.\n",
      "Ep done - 43610.\n",
      "Ep done - 43620.\n",
      "Ep done - 43630.\n",
      "Ep done - 43640.\n",
      "Ep done - 43650.\n",
      "Ep done - 43660.\n",
      "Ep done - 43670.\n",
      "Ep done - 43680.\n",
      "Ep done - 43690.\n",
      "Ep done - 43700.\n",
      "Eval num_timesteps=4370000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.49 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.5      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4370000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.07e-06 |\n",
      "|    explained_variance   | 8.34e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00466   |\n",
      "|    n_updates            | 26340     |\n",
      "|    policy_gradient_loss | -4.63e-10 |\n",
      "|    value_loss           | 0.00655   |\n",
      "---------------------------------------\n",
      "Ep done - 152910.\n",
      "Ep done - 152920.\n",
      "Ep done - 152930.\n",
      "Ep done - 152940.\n",
      "Ep done - 152950.\n",
      "Ep done - 152960.\n",
      "Ep done - 152970.\n",
      "Ep done - 152980.\n",
      "Ep done - 152990.\n",
      "Ep done - 153000.\n",
      "Ep done - 153010.\n",
      "Ep done - 153020.\n",
      "Ep done - 153030.\n",
      "Ep done - 153040.\n",
      "Ep done - 153050.\n",
      "Ep done - 153060.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.3     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 712      |\n",
      "|    time_elapsed    | 15539    |\n",
      "|    total_timesteps | 4374528  |\n",
      "---------------------------------\n",
      "Ep done - 153070.\n",
      "Ep done - 153080.\n",
      "Ep done - 153090.\n",
      "Ep done - 153100.\n",
      "Ep done - 153110.\n",
      "Ep done - 153120.\n",
      "Ep done - 153130.\n",
      "Ep done - 153140.\n",
      "Ep done - 153150.\n",
      "Ep done - 153160.\n",
      "Ep done - 153170.\n",
      "Ep done - 153180.\n",
      "Ep done - 153190.\n",
      "Ep done - 153200.\n",
      "Ep done - 153210.\n",
      "Ep done - 153220.\n",
      "Ep done - 153230.\n",
      "Ep done - 153240.\n",
      "Ep done - 43710.\n",
      "Ep done - 43720.\n",
      "Ep done - 43730.\n",
      "Ep done - 43740.\n",
      "Ep done - 43750.\n",
      "Ep done - 43760.\n",
      "Ep done - 43770.\n",
      "Ep done - 43780.\n",
      "Ep done - 43790.\n",
      "Ep done - 43800.\n",
      "Eval num_timesteps=4380000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.45 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.4      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4380000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.35e-07 |\n",
      "|    explained_variance   | -3.58e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00219   |\n",
      "|    n_updates            | 26350     |\n",
      "|    policy_gradient_loss | -1.89e-09 |\n",
      "|    value_loss           | 0.00538   |\n",
      "---------------------------------------\n",
      "Ep done - 153250.\n",
      "Ep done - 153260.\n",
      "Ep done - 153270.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 713      |\n",
      "|    time_elapsed    | 15564    |\n",
      "|    total_timesteps | 4380672  |\n",
      "---------------------------------\n",
      "Ep done - 153280.\n",
      "Ep done - 153290.\n",
      "Ep done - 153300.\n",
      "Ep done - 153310.\n",
      "Ep done - 153320.\n",
      "Ep done - 153330.\n",
      "Ep done - 153340.\n",
      "Ep done - 153350.\n",
      "Ep done - 153360.\n",
      "Ep done - 153370.\n",
      "Ep done - 153380.\n",
      "Ep done - 153390.\n",
      "Ep done - 153400.\n",
      "Ep done - 153410.\n",
      "Ep done - 153420.\n",
      "Ep done - 153430.\n",
      "Ep done - 153440.\n",
      "Ep done - 153450.\n",
      "Ep done - 153460.\n",
      "Ep done - 153470.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 29.4         |\n",
      "|    ep_rew_mean          | -1           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 714          |\n",
      "|    time_elapsed         | 15584        |\n",
      "|    total_timesteps      | 4386816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006776086 |\n",
      "|    clip_fraction        | 0.00013      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.59e-06    |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00416      |\n",
      "|    n_updates            | 26360        |\n",
      "|    policy_gradient_loss | -4.1e-05     |\n",
      "|    value_loss           | 0.00628      |\n",
      "------------------------------------------\n",
      "Ep done - 153480.\n",
      "Ep done - 153490.\n",
      "Ep done - 153500.\n",
      "Ep done - 153510.\n",
      "Ep done - 153520.\n",
      "Ep done - 153530.\n",
      "Ep done - 153540.\n",
      "Ep done - 153550.\n",
      "Ep done - 153560.\n",
      "Ep done - 153570.\n",
      "Ep done - 153580.\n",
      "Ep done - 43810.\n",
      "Ep done - 43820.\n",
      "Ep done - 43830.\n",
      "Ep done - 43840.\n",
      "Ep done - 43850.\n",
      "Ep done - 43860.\n",
      "Ep done - 43870.\n",
      "Ep done - 43880.\n",
      "Ep done - 43890.\n",
      "Ep done - 43900.\n",
      "Eval num_timesteps=4390000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.48 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.5      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4390000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.27e-07 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00314   |\n",
      "|    n_updates            | 26370     |\n",
      "|    policy_gradient_loss | -3.7e-10  |\n",
      "|    value_loss           | 0.00586   |\n",
      "---------------------------------------\n",
      "Ep done - 153590.\n",
      "Ep done - 153600.\n",
      "Ep done - 153610.\n",
      "Ep done - 153620.\n",
      "Ep done - 153630.\n",
      "Ep done - 153640.\n",
      "Ep done - 153650.\n",
      "Ep done - 153660.\n",
      "Ep done - 153670.\n",
      "Ep done - 153680.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 715      |\n",
      "|    time_elapsed    | 15612    |\n",
      "|    total_timesteps | 4392960  |\n",
      "---------------------------------\n",
      "Ep done - 153690.\n",
      "Ep done - 153700.\n",
      "Ep done - 153710.\n",
      "Ep done - 153720.\n",
      "Ep done - 153730.\n",
      "Ep done - 153740.\n",
      "Ep done - 153750.\n",
      "Ep done - 153760.\n",
      "Ep done - 153770.\n",
      "Ep done - 153780.\n",
      "Ep done - 153790.\n",
      "Ep done - 153800.\n",
      "Ep done - 153810.\n",
      "Ep done - 153820.\n",
      "Ep done - 153830.\n",
      "Ep done - 153840.\n",
      "Ep done - 153850.\n",
      "Ep done - 153860.\n",
      "Ep done - 153870.\n",
      "Ep done - 153880.\n",
      "Ep done - 153890.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.4      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 716       |\n",
      "|    time_elapsed         | 15631     |\n",
      "|    total_timesteps      | 4399104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.2e-07  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00334   |\n",
      "|    n_updates            | 26380     |\n",
      "|    policy_gradient_loss | -4.74e-09 |\n",
      "|    value_loss           | 0.00623   |\n",
      "---------------------------------------\n",
      "Ep done - 153900.\n",
      "Ep done - 153910.\n",
      "Ep done - 153920.\n",
      "Ep done - 43910.\n",
      "Ep done - 43920.\n",
      "Ep done - 43930.\n",
      "Ep done - 43940.\n",
      "Ep done - 43950.\n",
      "Ep done - 43960.\n",
      "Ep done - 43970.\n",
      "Ep done - 43980.\n",
      "Ep done - 43990.\n",
      "Ep done - 44000.\n",
      "Eval num_timesteps=4400000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.55 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.6      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4400000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.26e-06 |\n",
      "|    explained_variance   | 9.06e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0028    |\n",
      "|    n_updates            | 26390     |\n",
      "|    policy_gradient_loss | -4.51e-10 |\n",
      "|    value_loss           | 0.00565   |\n",
      "---------------------------------------\n",
      "Ep done - 153930.\n",
      "Ep done - 153940.\n",
      "Ep done - 153950.\n",
      "Ep done - 153960.\n",
      "Ep done - 153970.\n",
      "Ep done - 153980.\n",
      "Ep done - 153990.\n",
      "Ep done - 154000.\n",
      "Ep done - 154010.\n",
      "Ep done - 154020.\n",
      "Ep done - 154030.\n",
      "Ep done - 154040.\n",
      "Ep done - 154050.\n",
      "Ep done - 154060.\n",
      "Ep done - 154070.\n",
      "Ep done - 154080.\n",
      "Ep done - 154090.\n",
      "Ep done - 154100.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 717      |\n",
      "|    time_elapsed    | 15658    |\n",
      "|    total_timesteps | 4405248  |\n",
      "---------------------------------\n",
      "Ep done - 154110.\n",
      "Ep done - 154120.\n",
      "Ep done - 154130.\n",
      "Ep done - 154140.\n",
      "Ep done - 154150.\n",
      "Ep done - 154160.\n",
      "Ep done - 154170.\n",
      "Ep done - 154180.\n",
      "Ep done - 154190.\n",
      "Ep done - 154200.\n",
      "Ep done - 154210.\n",
      "Ep done - 154220.\n",
      "Ep done - 154230.\n",
      "Ep done - 154240.\n",
      "Ep done - 154250.\n",
      "Ep done - 154260.\n",
      "Ep done - 44010.\n",
      "Ep done - 44020.\n",
      "Ep done - 44030.\n",
      "Ep done - 44040.\n",
      "Ep done - 44050.\n",
      "Ep done - 44060.\n",
      "Ep done - 44070.\n",
      "Ep done - 44080.\n",
      "Ep done - 44090.\n",
      "Ep done - 44100.\n",
      "Eval num_timesteps=4410000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.41 +/- 0.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 29.4         |\n",
      "|    mean_reward          | -1           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4410000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011078317 |\n",
      "|    clip_fraction        | 0.000309     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.25e-05    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0022       |\n",
      "|    n_updates            | 26400        |\n",
      "|    policy_gradient_loss | -0.000103    |\n",
      "|    value_loss           | 0.00556      |\n",
      "------------------------------------------\n",
      "Ep done - 154270.\n",
      "Ep done - 154280.\n",
      "Ep done - 154290.\n",
      "Ep done - 154300.\n",
      "Ep done - 154310.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 718      |\n",
      "|    time_elapsed    | 15684    |\n",
      "|    total_timesteps | 4411392  |\n",
      "---------------------------------\n",
      "Ep done - 154320.\n",
      "Ep done - 154330.\n",
      "Ep done - 154340.\n",
      "Ep done - 154350.\n",
      "Ep done - 154360.\n",
      "Ep done - 154370.\n",
      "Ep done - 154380.\n",
      "Ep done - 154390.\n",
      "Ep done - 154400.\n",
      "Ep done - 154410.\n",
      "Ep done - 154420.\n",
      "Ep done - 154430.\n",
      "Ep done - 154440.\n",
      "Ep done - 154450.\n",
      "Ep done - 154460.\n",
      "Ep done - 154470.\n",
      "Ep done - 154480.\n",
      "Ep done - 154490.\n",
      "Ep done - 154500.\n",
      "Ep done - 154510.\n",
      "Ep done - 154520.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.4      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 719       |\n",
      "|    time_elapsed         | 15706     |\n",
      "|    total_timesteps      | 4417536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.91e-07 |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00262   |\n",
      "|    n_updates            | 26410     |\n",
      "|    policy_gradient_loss | 7.57e-11  |\n",
      "|    value_loss           | 0.00571   |\n",
      "---------------------------------------\n",
      "Ep done - 154530.\n",
      "Ep done - 154540.\n",
      "Ep done - 154550.\n",
      "Ep done - 154560.\n",
      "Ep done - 154570.\n",
      "Ep done - 154580.\n",
      "Ep done - 154590.\n",
      "Ep done - 154600.\n",
      "Ep done - 44110.\n",
      "Ep done - 44120.\n",
      "Ep done - 44130.\n",
      "Ep done - 44140.\n",
      "Ep done - 44150.\n",
      "Ep done - 44160.\n",
      "Ep done - 44170.\n",
      "Ep done - 44180.\n",
      "Ep done - 44190.\n",
      "Ep done - 44200.\n",
      "Eval num_timesteps=4420000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.54 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.5      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4420000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.01e-06 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00286   |\n",
      "|    n_updates            | 26420     |\n",
      "|    policy_gradient_loss | 1.6e-11   |\n",
      "|    value_loss           | 0.00725   |\n",
      "---------------------------------------\n",
      "Ep done - 154610.\n",
      "Ep done - 154620.\n",
      "Ep done - 154630.\n",
      "Ep done - 154640.\n",
      "Ep done - 154650.\n",
      "Ep done - 154660.\n",
      "Ep done - 154670.\n",
      "Ep done - 154680.\n",
      "Ep done - 154690.\n",
      "Ep done - 154700.\n",
      "Ep done - 154710.\n",
      "Ep done - 154720.\n",
      "Ep done - 154730.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 720      |\n",
      "|    time_elapsed    | 15731    |\n",
      "|    total_timesteps | 4423680  |\n",
      "---------------------------------\n",
      "Ep done - 154740.\n",
      "Ep done - 154750.\n",
      "Ep done - 154760.\n",
      "Ep done - 154770.\n",
      "Ep done - 154780.\n",
      "Ep done - 154790.\n",
      "Ep done - 154800.\n",
      "Ep done - 154810.\n",
      "Ep done - 154820.\n",
      "Ep done - 154830.\n",
      "Ep done - 154840.\n",
      "Ep done - 154850.\n",
      "Ep done - 154860.\n",
      "Ep done - 154870.\n",
      "Ep done - 154880.\n",
      "Ep done - 154890.\n",
      "Ep done - 154900.\n",
      "Ep done - 154910.\n",
      "Ep done - 154920.\n",
      "Ep done - 154930.\n",
      "Ep done - 154940.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.4      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 721       |\n",
      "|    time_elapsed         | 15751     |\n",
      "|    total_timesteps      | 4429824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.92e-07 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00411   |\n",
      "|    n_updates            | 26430     |\n",
      "|    policy_gradient_loss | -7.61e-10 |\n",
      "|    value_loss           | 0.00865   |\n",
      "---------------------------------------\n",
      "Ep done - 44210.\n",
      "Ep done - 44220.\n",
      "Ep done - 44230.\n",
      "Ep done - 44240.\n",
      "Ep done - 44250.\n",
      "Ep done - 44260.\n",
      "Ep done - 44270.\n",
      "Ep done - 44280.\n",
      "Ep done - 44290.\n",
      "Ep done - 44300.\n",
      "Eval num_timesteps=4430000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.43 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.4        |\n",
      "|    mean_reward          | -1          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4430000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002401705 |\n",
      "|    clip_fraction        | 0.000309    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.35e-06   |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.00264     |\n",
      "|    n_updates            | 26440       |\n",
      "|    policy_gradient_loss | -8.92e-05   |\n",
      "|    value_loss           | 0.00691     |\n",
      "-----------------------------------------\n",
      "Ep done - 154950.\n",
      "Ep done - 154960.\n",
      "Ep done - 154970.\n",
      "Ep done - 154980.\n",
      "Ep done - 154990.\n",
      "Ep done - 155000.\n",
      "Ep done - 155010.\n",
      "Ep done - 155020.\n",
      "Ep done - 155030.\n",
      "Ep done - 155040.\n",
      "Ep done - 155050.\n",
      "Ep done - 155060.\n",
      "Ep done - 155070.\n",
      "Ep done - 155080.\n",
      "Ep done - 155090.\n",
      "Ep done - 155100.\n",
      "Ep done - 155110.\n",
      "Ep done - 155120.\n",
      "Ep done - 155130.\n",
      "Ep done - 155140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 722      |\n",
      "|    time_elapsed    | 15782    |\n",
      "|    total_timesteps | 4435968  |\n",
      "---------------------------------\n",
      "Ep done - 155150.\n",
      "Ep done - 155160.\n",
      "Ep done - 155170.\n",
      "Ep done - 155180.\n",
      "Ep done - 155190.\n",
      "Ep done - 155200.\n",
      "Ep done - 155210.\n",
      "Ep done - 155220.\n",
      "Ep done - 155230.\n",
      "Ep done - 155240.\n",
      "Ep done - 155250.\n",
      "Ep done - 155260.\n",
      "Ep done - 155270.\n",
      "Ep done - 155280.\n",
      "Ep done - 44310.\n",
      "Ep done - 44320.\n",
      "Ep done - 44330.\n",
      "Ep done - 44340.\n",
      "Ep done - 44350.\n",
      "Ep done - 44360.\n",
      "Ep done - 44370.\n",
      "Ep done - 44380.\n",
      "Ep done - 44390.\n",
      "Ep done - 44400.\n",
      "Eval num_timesteps=4440000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.50 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.5      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4440000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.82e-07 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00364   |\n",
      "|    n_updates            | 26450     |\n",
      "|    policy_gradient_loss | 2.95e-09  |\n",
      "|    value_loss           | 0.00743   |\n",
      "---------------------------------------\n",
      "Ep done - 155290.\n",
      "Ep done - 155300.\n",
      "Ep done - 155310.\n",
      "Ep done - 155320.\n",
      "Ep done - 155330.\n",
      "Ep done - 155340.\n",
      "Ep done - 155350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 723      |\n",
      "|    time_elapsed    | 15806    |\n",
      "|    total_timesteps | 4442112  |\n",
      "---------------------------------\n",
      "Ep done - 155360.\n",
      "Ep done - 155370.\n",
      "Ep done - 155380.\n",
      "Ep done - 155390.\n",
      "Ep done - 155400.\n",
      "Ep done - 155410.\n",
      "Ep done - 155420.\n",
      "Ep done - 155430.\n",
      "Ep done - 155440.\n",
      "Ep done - 155450.\n",
      "Ep done - 155460.\n",
      "Ep done - 155470.\n",
      "Ep done - 155480.\n",
      "Ep done - 155490.\n",
      "Ep done - 155500.\n",
      "Ep done - 155510.\n",
      "Ep done - 155520.\n",
      "Ep done - 155530.\n",
      "Ep done - 155540.\n",
      "Ep done - 155550.\n",
      "Ep done - 155560.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.4      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 724       |\n",
      "|    time_elapsed         | 15827     |\n",
      "|    total_timesteps      | 4448256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.17e-06 |\n",
      "|    explained_variance   | 2.19e-05  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00297   |\n",
      "|    n_updates            | 26460     |\n",
      "|    policy_gradient_loss | 8.55e-10  |\n",
      "|    value_loss           | 0.00667   |\n",
      "---------------------------------------\n",
      "Ep done - 155570.\n",
      "Ep done - 155580.\n",
      "Ep done - 155590.\n",
      "Ep done - 155600.\n",
      "Ep done - 155610.\n",
      "Ep done - 155620.\n",
      "Ep done - 44410.\n",
      "Ep done - 44420.\n",
      "Ep done - 44430.\n",
      "Ep done - 44440.\n",
      "Ep done - 44450.\n",
      "Ep done - 44460.\n",
      "Ep done - 44470.\n",
      "Ep done - 44480.\n",
      "Ep done - 44490.\n",
      "Ep done - 44500.\n",
      "Eval num_timesteps=4450000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.40 +/- 0.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.4      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4450000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.01e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00226   |\n",
      "|    n_updates            | 26470     |\n",
      "|    policy_gradient_loss | 1.7e-10   |\n",
      "|    value_loss           | 0.00454   |\n",
      "---------------------------------------\n",
      "Ep done - 155630.\n",
      "Ep done - 155640.\n",
      "Ep done - 155650.\n",
      "Ep done - 155660.\n",
      "Ep done - 155670.\n",
      "Ep done - 155680.\n",
      "Ep done - 155690.\n",
      "Ep done - 155700.\n",
      "Ep done - 155710.\n",
      "Ep done - 155720.\n",
      "Ep done - 155730.\n",
      "Ep done - 155740.\n",
      "Ep done - 155750.\n",
      "Ep done - 155760.\n",
      "Ep done - 155770.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 725      |\n",
      "|    time_elapsed    | 15853    |\n",
      "|    total_timesteps | 4454400  |\n",
      "---------------------------------\n",
      "Ep done - 155780.\n",
      "Ep done - 155790.\n",
      "Ep done - 155800.\n",
      "Ep done - 155810.\n",
      "Ep done - 155820.\n",
      "Ep done - 155830.\n",
      "Ep done - 155840.\n",
      "Ep done - 155850.\n",
      "Ep done - 155860.\n",
      "Ep done - 155870.\n",
      "Ep done - 155880.\n",
      "Ep done - 155890.\n",
      "Ep done - 155900.\n",
      "Ep done - 155910.\n",
      "Ep done - 155920.\n",
      "Ep done - 155930.\n",
      "Ep done - 155940.\n",
      "Ep done - 155950.\n",
      "Ep done - 155960.\n",
      "Ep done - 44510.\n",
      "Ep done - 44520.\n",
      "Ep done - 44530.\n",
      "Ep done - 44540.\n",
      "Ep done - 44550.\n",
      "Ep done - 44560.\n",
      "Ep done - 44570.\n",
      "Ep done - 44580.\n",
      "Ep done - 44590.\n",
      "Ep done - 44600.\n",
      "Eval num_timesteps=4460000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.36 +/- 0.48\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.4      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4460000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.01e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00402   |\n",
      "|    n_updates            | 26480     |\n",
      "|    policy_gradient_loss | 1.64e-09  |\n",
      "|    value_loss           | 0.00587   |\n",
      "---------------------------------------\n",
      "Ep done - 155970.\n",
      "Ep done - 155980.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 726      |\n",
      "|    time_elapsed    | 15875    |\n",
      "|    total_timesteps | 4460544  |\n",
      "---------------------------------\n",
      "Ep done - 155990.\n",
      "Ep done - 156000.\n",
      "Ep done - 156010.\n",
      "Ep done - 156020.\n",
      "Ep done - 156030.\n",
      "Ep done - 156040.\n",
      "Ep done - 156050.\n",
      "Ep done - 156060.\n",
      "Ep done - 156070.\n",
      "Ep done - 156080.\n",
      "Ep done - 156090.\n",
      "Ep done - 156100.\n",
      "Ep done - 156110.\n",
      "Ep done - 156120.\n",
      "Ep done - 156130.\n",
      "Ep done - 156140.\n",
      "Ep done - 156150.\n",
      "Ep done - 156160.\n",
      "Ep done - 156170.\n",
      "Ep done - 156180.\n",
      "Ep done - 156190.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.4        |\n",
      "|    ep_rew_mean          | -1          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 727         |\n",
      "|    time_elapsed         | 15894       |\n",
      "|    total_timesteps      | 4466688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000778218 |\n",
      "|    clip_fraction        | 0.000228    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.97e-05   |\n",
      "|    explained_variance   | 2.98e-07    |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.00484     |\n",
      "|    n_updates            | 26490       |\n",
      "|    policy_gradient_loss | -5.2e-05    |\n",
      "|    value_loss           | 0.00651     |\n",
      "-----------------------------------------\n",
      "Ep done - 156200.\n",
      "Ep done - 156210.\n",
      "Ep done - 156220.\n",
      "Ep done - 156230.\n",
      "Ep done - 156240.\n",
      "Ep done - 156250.\n",
      "Ep done - 156260.\n",
      "Ep done - 156270.\n",
      "Ep done - 156280.\n",
      "Ep done - 156290.\n",
      "Ep done - 156300.\n",
      "Ep done - 44610.\n",
      "Ep done - 44620.\n",
      "Ep done - 44630.\n",
      "Ep done - 44640.\n",
      "Ep done - 44650.\n",
      "Ep done - 44660.\n",
      "Ep done - 44670.\n",
      "Ep done - 44680.\n",
      "Ep done - 44690.\n",
      "Ep done - 44700.\n",
      "Eval num_timesteps=4470000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.52 +/- 0.50\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 29.5          |\n",
      "|    mean_reward          | -1            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4470000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096916803 |\n",
      "|    clip_fraction        | 0.00013       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -4e-06        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.00254       |\n",
      "|    n_updates            | 26500         |\n",
      "|    policy_gradient_loss | -1.47e-05     |\n",
      "|    value_loss           | 0.00581       |\n",
      "-------------------------------------------\n",
      "Ep done - 156310.\n",
      "Ep done - 156320.\n",
      "Ep done - 156330.\n",
      "Ep done - 156340.\n",
      "Ep done - 156350.\n",
      "Ep done - 156360.\n",
      "Ep done - 156370.\n",
      "Ep done - 156380.\n",
      "Ep done - 156390.\n",
      "Ep done - 156400.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 728      |\n",
      "|    time_elapsed    | 15919    |\n",
      "|    total_timesteps | 4472832  |\n",
      "---------------------------------\n",
      "Ep done - 156410.\n",
      "Ep done - 156420.\n",
      "Ep done - 156430.\n",
      "Ep done - 156440.\n",
      "Ep done - 156450.\n",
      "Ep done - 156460.\n",
      "Ep done - 156470.\n",
      "Ep done - 156480.\n",
      "Ep done - 156490.\n",
      "Ep done - 156500.\n",
      "Ep done - 156510.\n",
      "Ep done - 156520.\n",
      "Ep done - 156530.\n",
      "Ep done - 156540.\n",
      "Ep done - 156550.\n",
      "Ep done - 156560.\n",
      "Ep done - 156570.\n",
      "Ep done - 156580.\n",
      "Ep done - 156590.\n",
      "Ep done - 156600.\n",
      "Ep done - 156610.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.5      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 729       |\n",
      "|    time_elapsed         | 15937     |\n",
      "|    total_timesteps      | 4478976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.03e-06 |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00327   |\n",
      "|    n_updates            | 26510     |\n",
      "|    policy_gradient_loss | 3.01e-11  |\n",
      "|    value_loss           | 0.00723   |\n",
      "---------------------------------------\n",
      "Ep done - 156620.\n",
      "Ep done - 156630.\n",
      "Ep done - 156640.\n",
      "Ep done - 44710.\n",
      "Ep done - 44720.\n",
      "Ep done - 44730.\n",
      "Ep done - 44740.\n",
      "Ep done - 44750.\n",
      "Ep done - 44760.\n",
      "Ep done - 44770.\n",
      "Ep done - 44780.\n",
      "Ep done - 44790.\n",
      "Ep done - 44800.\n",
      "Eval num_timesteps=4480000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.46 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.5      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4480000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.04e-06 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00258   |\n",
      "|    n_updates            | 26520     |\n",
      "|    policy_gradient_loss | 1.29e-09  |\n",
      "|    value_loss           | 0.0063    |\n",
      "---------------------------------------\n",
      "Ep done - 156650.\n",
      "Ep done - 156660.\n",
      "Ep done - 156670.\n",
      "Ep done - 156680.\n",
      "Ep done - 156690.\n",
      "Ep done - 156700.\n",
      "Ep done - 156710.\n",
      "Ep done - 156720.\n",
      "Ep done - 156730.\n",
      "Ep done - 156740.\n",
      "Ep done - 156750.\n",
      "Ep done - 156760.\n",
      "Ep done - 156770.\n",
      "Ep done - 156780.\n",
      "Ep done - 156790.\n",
      "Ep done - 156800.\n",
      "Ep done - 156810.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 730      |\n",
      "|    time_elapsed    | 15962    |\n",
      "|    total_timesteps | 4485120  |\n",
      "---------------------------------\n",
      "Ep done - 156820.\n",
      "Ep done - 156830.\n",
      "Ep done - 156840.\n",
      "Ep done - 156850.\n",
      "Ep done - 156860.\n",
      "Ep done - 156870.\n",
      "Ep done - 156880.\n",
      "Ep done - 156890.\n",
      "Ep done - 156900.\n",
      "Ep done - 156910.\n",
      "Ep done - 156920.\n",
      "Ep done - 156930.\n",
      "Ep done - 156940.\n",
      "Ep done - 156950.\n",
      "Ep done - 156960.\n",
      "Ep done - 156970.\n",
      "Ep done - 156980.\n",
      "Ep done - 44810.\n",
      "Ep done - 44820.\n",
      "Ep done - 44830.\n",
      "Ep done - 44840.\n",
      "Ep done - 44850.\n",
      "Ep done - 44860.\n",
      "Ep done - 44870.\n",
      "Ep done - 44880.\n",
      "Ep done - 44890.\n",
      "Ep done - 44900.\n",
      "Eval num_timesteps=4490000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.50 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 29.5      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4490000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.06e-06 |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00283   |\n",
      "|    n_updates            | 26530     |\n",
      "|    policy_gradient_loss | -1.23e-09 |\n",
      "|    value_loss           | 0.00512   |\n",
      "---------------------------------------\n",
      "Ep done - 156990.\n",
      "Ep done - 157000.\n",
      "Ep done - 157010.\n",
      "Ep done - 157020.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 731      |\n",
      "|    time_elapsed    | 15986    |\n",
      "|    total_timesteps | 4491264  |\n",
      "---------------------------------\n",
      "Ep done - 157030.\n",
      "Ep done - 157040.\n",
      "Ep done - 157050.\n",
      "Ep done - 157060.\n",
      "Ep done - 157070.\n",
      "Ep done - 157080.\n",
      "Ep done - 157090.\n",
      "Ep done - 157100.\n",
      "Ep done - 157110.\n",
      "Ep done - 157120.\n",
      "Ep done - 157130.\n",
      "Ep done - 157140.\n",
      "Ep done - 157150.\n",
      "Ep done - 157160.\n",
      "Ep done - 157170.\n",
      "Ep done - 157180.\n",
      "Ep done - 157190.\n",
      "Ep done - 157200.\n",
      "Ep done - 157210.\n",
      "Ep done - 157220.\n",
      "Ep done - 157230.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.5      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 732       |\n",
      "|    time_elapsed         | 16005     |\n",
      "|    total_timesteps      | 4497408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.1e-06  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00306   |\n",
      "|    n_updates            | 26540     |\n",
      "|    policy_gradient_loss | -1.82e-11 |\n",
      "|    value_loss           | 0.00467   |\n",
      "---------------------------------------\n",
      "Ep done - 157240.\n",
      "Ep done - 157250.\n",
      "Ep done - 157260.\n",
      "Ep done - 157270.\n",
      "Ep done - 157280.\n",
      "Ep done - 157290.\n",
      "Ep done - 157300.\n",
      "Ep done - 157310.\n",
      "Ep done - 157320.\n",
      "Ep done - 44910.\n",
      "Ep done - 44920.\n",
      "Ep done - 44930.\n",
      "Ep done - 44940.\n",
      "Ep done - 44950.\n",
      "Ep done - 44960.\n",
      "Ep done - 44970.\n",
      "Ep done - 44980.\n",
      "Ep done - 44990.\n",
      "Ep done - 45000.\n",
      "Eval num_timesteps=4500000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.49 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.5       |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34243593 |\n",
      "|    clip_fraction        | 0.00924    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.68e-05  |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00367   |\n",
      "|    n_updates            | 26550      |\n",
      "|    policy_gradient_loss | -0.0034    |\n",
      "|    value_loss           | 0.00524    |\n",
      "----------------------------------------\n",
      "Ep done - 157330.\n",
      "Ep done - 157340.\n",
      "Ep done - 157350.\n",
      "Ep done - 157360.\n",
      "Ep done - 157370.\n",
      "Ep done - 157380.\n",
      "Ep done - 157390.\n",
      "Ep done - 157400.\n",
      "Ep done - 157410.\n",
      "Ep done - 157420.\n",
      "Ep done - 157430.\n",
      "Ep done - 157440.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 733      |\n",
      "|    time_elapsed    | 16030    |\n",
      "|    total_timesteps | 4503552  |\n",
      "---------------------------------\n",
      "Ep done - 157450.\n",
      "Ep done - 157460.\n",
      "Ep done - 157470.\n",
      "Ep done - 157480.\n",
      "Ep done - 157490.\n",
      "Ep done - 157500.\n",
      "Ep done - 157510.\n",
      "Ep done - 157520.\n",
      "Ep done - 157530.\n",
      "Ep done - 157540.\n",
      "Ep done - 157550.\n",
      "Ep done - 157560.\n",
      "Ep done - 157570.\n",
      "Ep done - 157580.\n",
      "Ep done - 157590.\n",
      "Ep done - 157600.\n",
      "Ep done - 157610.\n",
      "Ep done - 157620.\n",
      "Ep done - 157630.\n",
      "Ep done - 157640.\n",
      "Ep done - 157650.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 29.5      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 734       |\n",
      "|    time_elapsed         | 16050     |\n",
      "|    total_timesteps      | 4509696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.4e-06  |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0032    |\n",
      "|    n_updates            | 26560     |\n",
      "|    policy_gradient_loss | -8.73e-11 |\n",
      "|    value_loss           | 0.00571   |\n",
      "---------------------------------------\n",
      "Ep done - 157660.\n",
      "Ep done - 45010.\n",
      "Ep done - 45020.\n",
      "Ep done - 45030.\n",
      "Ep done - 45040.\n",
      "Ep done - 45050.\n",
      "Ep done - 45060.\n",
      "Ep done - 45070.\n",
      "Ep done - 45080.\n",
      "Ep done - 45090.\n",
      "Ep done - 45100.\n",
      "Eval num_timesteps=4510000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.32 +/- 0.47\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 29.3     |\n",
      "|    mean_reward          | -1       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 4510000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.8e-06 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.00335  |\n",
      "|    n_updates            | 26570    |\n",
      "|    policy_gradient_loss | 6.24e-10 |\n",
      "|    value_loss           | 0.00659  |\n",
      "--------------------------------------\n",
      "Ep done - 157670.\n",
      "Ep done - 157680.\n",
      "Ep done - 157690.\n",
      "Ep done - 157700.\n",
      "Ep done - 157710.\n",
      "Ep done - 157720.\n",
      "Ep done - 157730.\n",
      "Ep done - 157740.\n",
      "Ep done - 157750.\n",
      "Ep done - 157760.\n",
      "Ep done - 157770.\n",
      "Ep done - 157780.\n",
      "Ep done - 157790.\n",
      "Ep done - 157800.\n",
      "Ep done - 157810.\n",
      "Ep done - 157820.\n",
      "Ep done - 157830.\n",
      "Ep done - 157840.\n",
      "Ep done - 157850.\n",
      "Ep done - 157860.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 735      |\n",
      "|    time_elapsed    | 16074    |\n",
      "|    total_timesteps | 4515840  |\n",
      "---------------------------------\n",
      "Ep done - 157870.\n",
      "Ep done - 157880.\n",
      "Ep done - 157890.\n",
      "Ep done - 157900.\n",
      "Ep done - 157910.\n",
      "Ep done - 157920.\n",
      "Ep done - 157930.\n",
      "Ep done - 157940.\n",
      "Ep done - 157950.\n",
      "Ep done - 157960.\n",
      "Ep done - 157970.\n",
      "Ep done - 157980.\n",
      "Ep done - 157990.\n",
      "Ep done - 158000.\n",
      "Ep done - 45110.\n",
      "Ep done - 45120.\n",
      "Ep done - 45130.\n",
      "Ep done - 45140.\n",
      "Ep done - 45150.\n",
      "Ep done - 45160.\n",
      "Ep done - 45170.\n",
      "Ep done - 45180.\n",
      "Ep done - 45190.\n",
      "Ep done - 45200.\n",
      "Eval num_timesteps=4520000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4520000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7597211 |\n",
      "|    clip_fraction        | 0.0185    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00174  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00792  |\n",
      "|    n_updates            | 26580     |\n",
      "|    policy_gradient_loss | -0.00128  |\n",
      "|    value_loss           | 0.00728   |\n",
      "---------------------------------------\n",
      "Ep done - 158010.\n",
      "Ep done - 158020.\n",
      "Ep done - 158030.\n",
      "Ep done - 158040.\n",
      "Ep done - 158050.\n",
      "Ep done - 158060.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 736      |\n",
      "|    time_elapsed    | 16099    |\n",
      "|    total_timesteps | 4521984  |\n",
      "---------------------------------\n",
      "Ep done - 158070.\n",
      "Ep done - 158080.\n",
      "Ep done - 158090.\n",
      "Ep done - 158100.\n",
      "Ep done - 158110.\n",
      "Ep done - 158120.\n",
      "Ep done - 158130.\n",
      "Ep done - 158140.\n",
      "Ep done - 158150.\n",
      "Ep done - 158160.\n",
      "Ep done - 158170.\n",
      "Ep done - 158180.\n",
      "Ep done - 158190.\n",
      "Ep done - 158200.\n",
      "Ep done - 158210.\n",
      "Ep done - 158220.\n",
      "Ep done - 158230.\n",
      "Ep done - 158240.\n",
      "Ep done - 158250.\n",
      "Ep done - 158260.\n",
      "Ep done - 158270.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 737       |\n",
      "|    time_elapsed         | 16119     |\n",
      "|    total_timesteps      | 4528128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.5395396 |\n",
      "|    clip_fraction        | 0.0745    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000448 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0407   |\n",
      "|    n_updates            | 26590     |\n",
      "|    policy_gradient_loss | -0.00846  |\n",
      "|    value_loss           | 0.00802   |\n",
      "---------------------------------------\n",
      "Ep done - 158280.\n",
      "Ep done - 158290.\n",
      "Ep done - 158300.\n",
      "Ep done - 158310.\n",
      "Ep done - 158320.\n",
      "Ep done - 158330.\n",
      "Ep done - 45210.\n",
      "Ep done - 45220.\n",
      "Ep done - 45230.\n",
      "Ep done - 45240.\n",
      "Ep done - 45250.\n",
      "Ep done - 45260.\n",
      "Ep done - 45270.\n",
      "Ep done - 45280.\n",
      "Ep done - 45290.\n",
      "Ep done - 45300.\n",
      "Eval num_timesteps=4530000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4530000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67304784 |\n",
      "|    clip_fraction        | 0.00262    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.61e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00114   |\n",
      "|    n_updates            | 26600      |\n",
      "|    policy_gradient_loss | -0.00058   |\n",
      "|    value_loss           | 0.00643    |\n",
      "----------------------------------------\n",
      "Ep done - 158340.\n",
      "Ep done - 158350.\n",
      "Ep done - 158360.\n",
      "Ep done - 158370.\n",
      "Ep done - 158380.\n",
      "Ep done - 158390.\n",
      "Ep done - 158400.\n",
      "Ep done - 158410.\n",
      "Ep done - 158420.\n",
      "Ep done - 158430.\n",
      "Ep done - 158440.\n",
      "Ep done - 158450.\n",
      "Ep done - 158460.\n",
      "Ep done - 158470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 738      |\n",
      "|    time_elapsed    | 16143    |\n",
      "|    total_timesteps | 4534272  |\n",
      "---------------------------------\n",
      "Ep done - 158480.\n",
      "Ep done - 158490.\n",
      "Ep done - 158500.\n",
      "Ep done - 158510.\n",
      "Ep done - 158520.\n",
      "Ep done - 158530.\n",
      "Ep done - 158540.\n",
      "Ep done - 158550.\n",
      "Ep done - 158560.\n",
      "Ep done - 158570.\n",
      "Ep done - 158580.\n",
      "Ep done - 158590.\n",
      "Ep done - 158600.\n",
      "Ep done - 158610.\n",
      "Ep done - 158620.\n",
      "Ep done - 158630.\n",
      "Ep done - 158640.\n",
      "Ep done - 158650.\n",
      "Ep done - 158660.\n",
      "Ep done - 45310.\n",
      "Ep done - 45320.\n",
      "Ep done - 45330.\n",
      "Ep done - 45340.\n",
      "Ep done - 45350.\n",
      "Ep done - 45360.\n",
      "Ep done - 45370.\n",
      "Ep done - 45380.\n",
      "Ep done - 45390.\n",
      "Ep done - 45400.\n",
      "Eval num_timesteps=4540000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4540000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.16e-05 |\n",
      "|    explained_variance   | 1.79e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00405   |\n",
      "|    n_updates            | 26610     |\n",
      "|    policy_gradient_loss | -1.34e-09 |\n",
      "|    value_loss           | 0.00536   |\n",
      "---------------------------------------\n",
      "Ep done - 158670.\n",
      "Ep done - 158680.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 739      |\n",
      "|    time_elapsed    | 16174    |\n",
      "|    total_timesteps | 4540416  |\n",
      "---------------------------------\n",
      "Ep done - 158690.\n",
      "Ep done - 158700.\n",
      "Ep done - 158710.\n",
      "Ep done - 158720.\n",
      "Ep done - 158730.\n",
      "Ep done - 158740.\n",
      "Ep done - 158750.\n",
      "Ep done - 158760.\n",
      "Ep done - 158770.\n",
      "Ep done - 158780.\n",
      "Ep done - 158790.\n",
      "Ep done - 158800.\n",
      "Ep done - 158810.\n",
      "Ep done - 158820.\n",
      "Ep done - 158830.\n",
      "Ep done - 158840.\n",
      "Ep done - 158850.\n",
      "Ep done - 158860.\n",
      "Ep done - 158870.\n",
      "Ep done - 158880.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 740       |\n",
      "|    time_elapsed         | 16194     |\n",
      "|    total_timesteps      | 4546560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.54e-06 |\n",
      "|    explained_variance   | 4.17e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00175   |\n",
      "|    n_updates            | 26620     |\n",
      "|    policy_gradient_loss | 7.37e-11  |\n",
      "|    value_loss           | 0.0044    |\n",
      "---------------------------------------\n",
      "Ep done - 158890.\n",
      "Ep done - 158900.\n",
      "Ep done - 158910.\n",
      "Ep done - 158920.\n",
      "Ep done - 158930.\n",
      "Ep done - 158940.\n",
      "Ep done - 158950.\n",
      "Ep done - 158960.\n",
      "Ep done - 158970.\n",
      "Ep done - 158980.\n",
      "Ep done - 158990.\n",
      "Ep done - 159000.\n",
      "Ep done - 45410.\n",
      "Ep done - 45420.\n",
      "Ep done - 45430.\n",
      "Ep done - 45440.\n",
      "Ep done - 45450.\n",
      "Ep done - 45460.\n",
      "Ep done - 45470.\n",
      "Ep done - 45480.\n",
      "Ep done - 45490.\n",
      "Ep done - 45500.\n",
      "Eval num_timesteps=4550000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4550000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.79e-06 |\n",
      "|    explained_variance   | 2.62e-05  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00308   |\n",
      "|    n_updates            | 26630     |\n",
      "|    policy_gradient_loss | -1.69e-09 |\n",
      "|    value_loss           | 0.0072    |\n",
      "---------------------------------------\n",
      "Ep done - 159010.\n",
      "Ep done - 159020.\n",
      "Ep done - 159030.\n",
      "Ep done - 159040.\n",
      "Ep done - 159050.\n",
      "Ep done - 159060.\n",
      "Ep done - 159070.\n",
      "Ep done - 159080.\n",
      "Ep done - 159090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 741      |\n",
      "|    time_elapsed    | 16218    |\n",
      "|    total_timesteps | 4552704  |\n",
      "---------------------------------\n",
      "Ep done - 159100.\n",
      "Ep done - 159110.\n",
      "Ep done - 159120.\n",
      "Ep done - 159130.\n",
      "Ep done - 159140.\n",
      "Ep done - 159150.\n",
      "Ep done - 159160.\n",
      "Ep done - 159170.\n",
      "Ep done - 159180.\n",
      "Ep done - 159190.\n",
      "Ep done - 159200.\n",
      "Ep done - 159210.\n",
      "Ep done - 159220.\n",
      "Ep done - 159230.\n",
      "Ep done - 159240.\n",
      "Ep done - 159250.\n",
      "Ep done - 159260.\n",
      "Ep done - 159270.\n",
      "Ep done - 159280.\n",
      "Ep done - 159290.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 742       |\n",
      "|    time_elapsed         | 16237     |\n",
      "|    total_timesteps      | 4558848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.59e-06 |\n",
      "|    explained_variance   | -2.38e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00492   |\n",
      "|    n_updates            | 26640     |\n",
      "|    policy_gradient_loss | -5.31e-10 |\n",
      "|    value_loss           | 0.00887   |\n",
      "---------------------------------------\n",
      "Ep done - 159300.\n",
      "Ep done - 159310.\n",
      "Ep done - 159320.\n",
      "Ep done - 159330.\n",
      "Ep done - 45510.\n",
      "Ep done - 45520.\n",
      "Ep done - 45530.\n",
      "Ep done - 45540.\n",
      "Ep done - 45550.\n",
      "Ep done - 45560.\n",
      "Ep done - 45570.\n",
      "Ep done - 45580.\n",
      "Ep done - 45590.\n",
      "Ep done - 45600.\n",
      "Eval num_timesteps=4560000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4560000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.83e-06 |\n",
      "|    explained_variance   | 2.38e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00389   |\n",
      "|    n_updates            | 26650     |\n",
      "|    policy_gradient_loss | 8.51e-10  |\n",
      "|    value_loss           | 0.00525   |\n",
      "---------------------------------------\n",
      "Ep done - 159340.\n",
      "Ep done - 159350.\n",
      "Ep done - 159360.\n",
      "Ep done - 159370.\n",
      "Ep done - 159380.\n",
      "Ep done - 159390.\n",
      "Ep done - 159400.\n",
      "Ep done - 159410.\n",
      "Ep done - 159420.\n",
      "Ep done - 159430.\n",
      "Ep done - 159440.\n",
      "Ep done - 159450.\n",
      "Ep done - 159460.\n",
      "Ep done - 159470.\n",
      "Ep done - 159480.\n",
      "Ep done - 159490.\n",
      "Ep done - 159500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 743      |\n",
      "|    time_elapsed    | 16265    |\n",
      "|    total_timesteps | 4564992  |\n",
      "---------------------------------\n",
      "Ep done - 159510.\n",
      "Ep done - 159520.\n",
      "Ep done - 159530.\n",
      "Ep done - 159540.\n",
      "Ep done - 159550.\n",
      "Ep done - 159560.\n",
      "Ep done - 159570.\n",
      "Ep done - 159580.\n",
      "Ep done - 159590.\n",
      "Ep done - 159600.\n",
      "Ep done - 159610.\n",
      "Ep done - 159620.\n",
      "Ep done - 159630.\n",
      "Ep done - 159640.\n",
      "Ep done - 159650.\n",
      "Ep done - 159660.\n",
      "Ep done - 45610.\n",
      "Ep done - 45620.\n",
      "Ep done - 45630.\n",
      "Ep done - 45640.\n",
      "Ep done - 45650.\n",
      "Ep done - 45660.\n",
      "Ep done - 45670.\n",
      "Ep done - 45680.\n",
      "Ep done - 45690.\n",
      "Ep done - 45700.\n",
      "Eval num_timesteps=4570000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4570000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2081405 |\n",
      "|    clip_fraction        | 0.0342    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000164 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0057   |\n",
      "|    n_updates            | 26660     |\n",
      "|    policy_gradient_loss | -0.00199  |\n",
      "|    value_loss           | 0.00452   |\n",
      "---------------------------------------\n",
      "Ep done - 159670.\n",
      "Ep done - 159680.\n",
      "Ep done - 159690.\n",
      "Ep done - 159700.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 744      |\n",
      "|    time_elapsed    | 16299    |\n",
      "|    total_timesteps | 4571136  |\n",
      "---------------------------------\n",
      "Ep done - 159710.\n",
      "Ep done - 159720.\n",
      "Ep done - 159730.\n",
      "Ep done - 159740.\n",
      "Ep done - 159750.\n",
      "Ep done - 159760.\n",
      "Ep done - 159770.\n",
      "Ep done - 159780.\n",
      "Ep done - 159790.\n",
      "Ep done - 159800.\n",
      "Ep done - 159810.\n",
      "Ep done - 159820.\n",
      "Ep done - 159830.\n",
      "Ep done - 159840.\n",
      "Ep done - 159850.\n",
      "Ep done - 159860.\n",
      "Ep done - 159870.\n",
      "Ep done - 159880.\n",
      "Ep done - 159890.\n",
      "Ep done - 159900.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.5      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 745       |\n",
      "|    time_elapsed         | 16332     |\n",
      "|    total_timesteps      | 4577280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4992849 |\n",
      "|    clip_fraction        | 0.0403    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000262 |\n",
      "|    explained_variance   | 1.32e-05  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00364   |\n",
      "|    n_updates            | 26670     |\n",
      "|    policy_gradient_loss | -0.000241 |\n",
      "|    value_loss           | 0.00492   |\n",
      "---------------------------------------\n",
      "Ep done - 159910.\n",
      "Ep done - 159920.\n",
      "Ep done - 159930.\n",
      "Ep done - 159940.\n",
      "Ep done - 159950.\n",
      "Ep done - 159960.\n",
      "Ep done - 159970.\n",
      "Ep done - 159980.\n",
      "Ep done - 159990.\n",
      "Ep done - 45710.\n",
      "Ep done - 45720.\n",
      "Ep done - 45730.\n",
      "Ep done - 45740.\n",
      "Ep done - 45750.\n",
      "Ep done - 45760.\n",
      "Ep done - 45770.\n",
      "Ep done - 45780.\n",
      "Ep done - 45790.\n",
      "Ep done - 45800.\n",
      "Eval num_timesteps=4580000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.47 +/- 0.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.5      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4580000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.942755  |\n",
      "|    clip_fraction        | 0.0456    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.33e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00728  |\n",
      "|    n_updates            | 26680     |\n",
      "|    policy_gradient_loss | -0.00536  |\n",
      "|    value_loss           | 0.00489   |\n",
      "---------------------------------------\n",
      "Ep done - 160000.\n",
      "Ep done - 160010.\n",
      "Ep done - 160020.\n",
      "Ep done - 160030.\n",
      "Ep done - 160040.\n",
      "Ep done - 160050.\n",
      "Ep done - 160060.\n",
      "Ep done - 160070.\n",
      "Ep done - 160080.\n",
      "Ep done - 160090.\n",
      "Ep done - 160100.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.5     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 746      |\n",
      "|    time_elapsed    | 16359    |\n",
      "|    total_timesteps | 4583424  |\n",
      "---------------------------------\n",
      "Ep done - 160110.\n",
      "Ep done - 160120.\n",
      "Ep done - 160130.\n",
      "Ep done - 160140.\n",
      "Ep done - 160150.\n",
      "Ep done - 160160.\n",
      "Ep done - 160170.\n",
      "Ep done - 160180.\n",
      "Ep done - 160190.\n",
      "Ep done - 160200.\n",
      "Ep done - 160210.\n",
      "Ep done - 160220.\n",
      "Ep done - 160230.\n",
      "Ep done - 160240.\n",
      "Ep done - 160250.\n",
      "Ep done - 160260.\n",
      "Ep done - 160270.\n",
      "Ep done - 160280.\n",
      "Ep done - 160290.\n",
      "Ep done - 160300.\n",
      "Ep done - 160310.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.6      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 747       |\n",
      "|    time_elapsed         | 16379     |\n",
      "|    total_timesteps      | 4589568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0295855 |\n",
      "|    clip_fraction        | 0.0417    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00011  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00637  |\n",
      "|    n_updates            | 26690     |\n",
      "|    policy_gradient_loss | -0.00716  |\n",
      "|    value_loss           | 0.00789   |\n",
      "---------------------------------------\n",
      "Ep done - 160320.\n",
      "Ep done - 45810.\n",
      "Ep done - 45820.\n",
      "Ep done - 45830.\n",
      "Ep done - 45840.\n",
      "Ep done - 45850.\n",
      "Ep done - 45860.\n",
      "Ep done - 45870.\n",
      "Ep done - 45880.\n",
      "Ep done - 45890.\n",
      "Ep done - 45900.\n",
      "Eval num_timesteps=4590000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4590000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.8381183 |\n",
      "|    clip_fraction        | 0.0723    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000275 |\n",
      "|    explained_variance   | 2.26e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0155   |\n",
      "|    n_updates            | 26700     |\n",
      "|    policy_gradient_loss | -0.0121   |\n",
      "|    value_loss           | 0.00538   |\n",
      "---------------------------------------\n",
      "Ep done - 160330.\n",
      "Ep done - 160340.\n",
      "Ep done - 160350.\n",
      "Ep done - 160360.\n",
      "Ep done - 160370.\n",
      "Ep done - 160380.\n",
      "Ep done - 160390.\n",
      "Ep done - 160400.\n",
      "Ep done - 160410.\n",
      "Ep done - 160420.\n",
      "Ep done - 160430.\n",
      "Ep done - 160440.\n",
      "Ep done - 160450.\n",
      "Ep done - 160460.\n",
      "Ep done - 160470.\n",
      "Ep done - 160480.\n",
      "Ep done - 160490.\n",
      "Ep done - 160500.\n",
      "Ep done - 160510.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 748      |\n",
      "|    time_elapsed    | 16417    |\n",
      "|    total_timesteps | 4595712  |\n",
      "---------------------------------\n",
      "Ep done - 160520.\n",
      "Ep done - 160530.\n",
      "Ep done - 160540.\n",
      "Ep done - 160550.\n",
      "Ep done - 160560.\n",
      "Ep done - 160570.\n",
      "Ep done - 160580.\n",
      "Ep done - 160590.\n",
      "Ep done - 160600.\n",
      "Ep done - 160610.\n",
      "Ep done - 160620.\n",
      "Ep done - 160630.\n",
      "Ep done - 160640.\n",
      "Ep done - 160650.\n",
      "Ep done - 45910.\n",
      "Ep done - 45920.\n",
      "Ep done - 45930.\n",
      "Ep done - 45940.\n",
      "Ep done - 45950.\n",
      "Ep done - 45960.\n",
      "Ep done - 45970.\n",
      "Ep done - 45980.\n",
      "Ep done - 45990.\n",
      "Ep done - 46000.\n",
      "Eval num_timesteps=4600000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4600000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.21e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.002     |\n",
      "|    n_updates            | 26710     |\n",
      "|    policy_gradient_loss | 9.59e-10  |\n",
      "|    value_loss           | 0.00443   |\n",
      "---------------------------------------\n",
      "Ep done - 160660.\n",
      "Ep done - 160670.\n",
      "Ep done - 160680.\n",
      "Ep done - 160690.\n",
      "Ep done - 160700.\n",
      "Ep done - 160710.\n",
      "Ep done - 160720.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 749      |\n",
      "|    time_elapsed    | 16447    |\n",
      "|    total_timesteps | 4601856  |\n",
      "---------------------------------\n",
      "Ep done - 160730.\n",
      "Ep done - 160740.\n",
      "Ep done - 160750.\n",
      "Ep done - 160760.\n",
      "Ep done - 160770.\n",
      "Ep done - 160780.\n",
      "Ep done - 160790.\n",
      "Ep done - 160800.\n",
      "Ep done - 160810.\n",
      "Ep done - 160820.\n",
      "Ep done - 160830.\n",
      "Ep done - 160840.\n",
      "Ep done - 160850.\n",
      "Ep done - 160860.\n",
      "Ep done - 160870.\n",
      "Ep done - 160880.\n",
      "Ep done - 160890.\n",
      "Ep done - 160900.\n",
      "Ep done - 160910.\n",
      "Ep done - 160920.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 750        |\n",
      "|    time_elapsed         | 16470      |\n",
      "|    total_timesteps      | 4608000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93421334 |\n",
      "|    clip_fraction        | 0.0176     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.35e-05  |\n",
      "|    explained_variance   | 1.79e-07   |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.0131    |\n",
      "|    n_updates            | 26720      |\n",
      "|    policy_gradient_loss | -0.00663   |\n",
      "|    value_loss           | 0.00546    |\n",
      "----------------------------------------\n",
      "Ep done - 160930.\n",
      "Ep done - 160940.\n",
      "Ep done - 160950.\n",
      "Ep done - 160960.\n",
      "Ep done - 160970.\n",
      "Ep done - 160980.\n",
      "Ep done - 160990.\n",
      "Ep done - 46010.\n",
      "Ep done - 46020.\n",
      "Ep done - 46030.\n",
      "Ep done - 46040.\n",
      "Ep done - 46050.\n",
      "Ep done - 46060.\n",
      "Ep done - 46070.\n",
      "Ep done - 46080.\n",
      "Ep done - 46090.\n",
      "Ep done - 46100.\n",
      "Eval num_timesteps=4610000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.99 +/- 0.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -1            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4610000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045847474 |\n",
      "|    clip_fraction        | 6.51e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.6e-05      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.00208       |\n",
      "|    n_updates            | 26730         |\n",
      "|    policy_gradient_loss | -1.85e-05     |\n",
      "|    value_loss           | 0.00466       |\n",
      "-------------------------------------------\n",
      "Ep done - 161000.\n",
      "Ep done - 161010.\n",
      "Ep done - 161020.\n",
      "Ep done - 161030.\n",
      "Ep done - 161040.\n",
      "Ep done - 161050.\n",
      "Ep done - 161060.\n",
      "Ep done - 161070.\n",
      "Ep done - 161080.\n",
      "Ep done - 161090.\n",
      "Ep done - 161100.\n",
      "Ep done - 161110.\n",
      "Ep done - 161120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 751      |\n",
      "|    time_elapsed    | 16500    |\n",
      "|    total_timesteps | 4614144  |\n",
      "---------------------------------\n",
      "Ep done - 161130.\n",
      "Ep done - 161140.\n",
      "Ep done - 161150.\n",
      "Ep done - 161160.\n",
      "Ep done - 161170.\n",
      "Ep done - 161180.\n",
      "Ep done - 161190.\n",
      "Ep done - 161200.\n",
      "Ep done - 161210.\n",
      "Ep done - 161220.\n",
      "Ep done - 161230.\n",
      "Ep done - 161240.\n",
      "Ep done - 161250.\n",
      "Ep done - 161260.\n",
      "Ep done - 161270.\n",
      "Ep done - 161280.\n",
      "Ep done - 161290.\n",
      "Ep done - 161300.\n",
      "Ep done - 161310.\n",
      "Ep done - 161320.\n",
      "Ep done - 46110.\n",
      "Ep done - 46120.\n",
      "Ep done - 46130.\n",
      "Ep done - 46140.\n",
      "Ep done - 46150.\n",
      "Ep done - 46160.\n",
      "Ep done - 46170.\n",
      "Ep done - 46180.\n",
      "Ep done - 46190.\n",
      "Ep done - 46200.\n",
      "Eval num_timesteps=4620000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4620000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64176863 |\n",
      "|    clip_fraction        | 0.0236     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.15e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00049    |\n",
      "|    n_updates            | 26740      |\n",
      "|    policy_gradient_loss | -0.00278   |\n",
      "|    value_loss           | 0.00598    |\n",
      "----------------------------------------\n",
      "Ep done - 161330.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 752      |\n",
      "|    time_elapsed    | 16529    |\n",
      "|    total_timesteps | 4620288  |\n",
      "---------------------------------\n",
      "Ep done - 161340.\n",
      "Ep done - 161350.\n",
      "Ep done - 161360.\n",
      "Ep done - 161370.\n",
      "Ep done - 161380.\n",
      "Ep done - 161390.\n",
      "Ep done - 161400.\n",
      "Ep done - 161410.\n",
      "Ep done - 161420.\n",
      "Ep done - 161430.\n",
      "Ep done - 161440.\n",
      "Ep done - 161450.\n",
      "Ep done - 161460.\n",
      "Ep done - 161470.\n",
      "Ep done - 161480.\n",
      "Ep done - 161490.\n",
      "Ep done - 161500.\n",
      "Ep done - 161510.\n",
      "Ep done - 161520.\n",
      "Ep done - 161530.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 753       |\n",
      "|    time_elapsed         | 16553     |\n",
      "|    total_timesteps      | 4626432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.54e-07 |\n",
      "|    explained_variance   | 4.77e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00319   |\n",
      "|    n_updates            | 26750     |\n",
      "|    policy_gradient_loss | 2.29e-09  |\n",
      "|    value_loss           | 0.00583   |\n",
      "---------------------------------------\n",
      "Ep done - 161540.\n",
      "Ep done - 161550.\n",
      "Ep done - 161560.\n",
      "Ep done - 161570.\n",
      "Ep done - 161580.\n",
      "Ep done - 161590.\n",
      "Ep done - 161600.\n",
      "Ep done - 161610.\n",
      "Ep done - 161620.\n",
      "Ep done - 161630.\n",
      "Ep done - 161640.\n",
      "Ep done - 161650.\n",
      "Ep done - 46210.\n",
      "Ep done - 46220.\n",
      "Ep done - 46230.\n",
      "Ep done - 46240.\n",
      "Ep done - 46250.\n",
      "Ep done - 46260.\n",
      "Ep done - 46270.\n",
      "Ep done - 46280.\n",
      "Ep done - 46290.\n",
      "Ep done - 46300.\n",
      "Eval num_timesteps=4630000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4630000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.39e-07 |\n",
      "|    explained_variance   | 4.17e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00424   |\n",
      "|    n_updates            | 26760     |\n",
      "|    policy_gradient_loss | 1.12e-10  |\n",
      "|    value_loss           | 0.00707   |\n",
      "---------------------------------------\n",
      "Ep done - 161660.\n",
      "Ep done - 161670.\n",
      "Ep done - 161680.\n",
      "Ep done - 161690.\n",
      "Ep done - 161700.\n",
      "Ep done - 161710.\n",
      "Ep done - 161720.\n",
      "Ep done - 161730.\n",
      "Ep done - 161740.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 754      |\n",
      "|    time_elapsed    | 16582    |\n",
      "|    total_timesteps | 4632576  |\n",
      "---------------------------------\n",
      "Ep done - 161750.\n",
      "Ep done - 161760.\n",
      "Ep done - 161770.\n",
      "Ep done - 161780.\n",
      "Ep done - 161790.\n",
      "Ep done - 161800.\n",
      "Ep done - 161810.\n",
      "Ep done - 161820.\n",
      "Ep done - 161830.\n",
      "Ep done - 161840.\n",
      "Ep done - 161850.\n",
      "Ep done - 161860.\n",
      "Ep done - 161870.\n",
      "Ep done - 161880.\n",
      "Ep done - 161890.\n",
      "Ep done - 161900.\n",
      "Ep done - 161910.\n",
      "Ep done - 161920.\n",
      "Ep done - 161930.\n",
      "Ep done - 161940.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 755       |\n",
      "|    time_elapsed         | 16606     |\n",
      "|    total_timesteps      | 4638720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.27e-07 |\n",
      "|    explained_variance   | 2.21e-06  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00153   |\n",
      "|    n_updates            | 26770     |\n",
      "|    policy_gradient_loss | 9.92e-11  |\n",
      "|    value_loss           | 0.0058    |\n",
      "---------------------------------------\n",
      "Ep done - 161950.\n",
      "Ep done - 161960.\n",
      "Ep done - 161970.\n",
      "Ep done - 161980.\n",
      "Ep done - 161990.\n",
      "Ep done - 46310.\n",
      "Ep done - 46320.\n",
      "Ep done - 46330.\n",
      "Ep done - 46340.\n",
      "Ep done - 46350.\n",
      "Ep done - 46360.\n",
      "Ep done - 46370.\n",
      "Ep done - 46380.\n",
      "Ep done - 46390.\n",
      "Ep done - 46400.\n",
      "Eval num_timesteps=4640000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4640000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.63e-07 |\n",
      "|    explained_variance   | 0.514     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00173   |\n",
      "|    n_updates            | 26780     |\n",
      "|    policy_gradient_loss | 1.17e-09  |\n",
      "|    value_loss           | 0.00376   |\n",
      "---------------------------------------\n",
      "Ep done - 162000.\n",
      "Ep done - 162010.\n",
      "Ep done - 162020.\n",
      "Ep done - 162030.\n",
      "Ep done - 162040.\n",
      "Ep done - 162050.\n",
      "Ep done - 162060.\n",
      "Ep done - 162070.\n",
      "Ep done - 162080.\n",
      "Ep done - 162090.\n",
      "Ep done - 162100.\n",
      "Ep done - 162110.\n",
      "Ep done - 162120.\n",
      "Ep done - 162130.\n",
      "Ep done - 162140.\n",
      "Ep done - 162150.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 756      |\n",
      "|    time_elapsed    | 16636    |\n",
      "|    total_timesteps | 4644864  |\n",
      "---------------------------------\n",
      "Ep done - 162160.\n",
      "Ep done - 162170.\n",
      "Ep done - 162180.\n",
      "Ep done - 162190.\n",
      "Ep done - 162200.\n",
      "Ep done - 162210.\n",
      "Ep done - 162220.\n",
      "Ep done - 162230.\n",
      "Ep done - 162240.\n",
      "Ep done - 162250.\n",
      "Ep done - 162260.\n",
      "Ep done - 162270.\n",
      "Ep done - 162280.\n",
      "Ep done - 162290.\n",
      "Ep done - 162300.\n",
      "Ep done - 162310.\n",
      "Ep done - 162320.\n",
      "Ep done - 46410.\n",
      "Ep done - 46420.\n",
      "Ep done - 46430.\n",
      "Ep done - 46440.\n",
      "Ep done - 46450.\n",
      "Ep done - 46460.\n",
      "Ep done - 46470.\n",
      "Ep done - 46480.\n",
      "Ep done - 46490.\n",
      "Ep done - 46500.\n",
      "Eval num_timesteps=4650000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 29.99 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4650000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.06e-07 |\n",
      "|    explained_variance   | 0.497     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0012    |\n",
      "|    n_updates            | 26790     |\n",
      "|    policy_gradient_loss | -2.38e-09 |\n",
      "|    value_loss           | 0.00314   |\n",
      "---------------------------------------\n",
      "Ep done - 162330.\n",
      "Ep done - 162340.\n",
      "Ep done - 162350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 757      |\n",
      "|    time_elapsed    | 16665    |\n",
      "|    total_timesteps | 4651008  |\n",
      "---------------------------------\n",
      "Ep done - 162360.\n",
      "Ep done - 162370.\n",
      "Ep done - 162380.\n",
      "Ep done - 162390.\n",
      "Ep done - 162400.\n",
      "Ep done - 162410.\n",
      "Ep done - 162420.\n",
      "Ep done - 162430.\n",
      "Ep done - 162440.\n",
      "Ep done - 162450.\n",
      "Ep done - 162460.\n",
      "Ep done - 162470.\n",
      "Ep done - 162480.\n",
      "Ep done - 162490.\n",
      "Ep done - 162500.\n",
      "Ep done - 162510.\n",
      "Ep done - 162520.\n",
      "Ep done - 162530.\n",
      "Ep done - 162540.\n",
      "Ep done - 162550.\n",
      "Ep done - 162560.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -1            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 279           |\n",
      "|    iterations           | 758           |\n",
      "|    time_elapsed         | 16690         |\n",
      "|    total_timesteps      | 4657152       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3967815e-05 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.16e-05     |\n",
      "|    explained_variance   | 0.521         |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.00134       |\n",
      "|    n_updates            | 26800         |\n",
      "|    policy_gradient_loss | 2.02e-07      |\n",
      "|    value_loss           | 0.00308       |\n",
      "-------------------------------------------\n",
      "Ep done - 162570.\n",
      "Ep done - 162580.\n",
      "Ep done - 162590.\n",
      "Ep done - 162600.\n",
      "Ep done - 162610.\n",
      "Ep done - 162620.\n",
      "Ep done - 162630.\n",
      "Ep done - 162640.\n",
      "Ep done - 162650.\n",
      "Ep done - 46510.\n",
      "Ep done - 46520.\n",
      "Ep done - 46530.\n",
      "Ep done - 46540.\n",
      "Ep done - 46550.\n",
      "Ep done - 46560.\n",
      "Ep done - 46570.\n",
      "Ep done - 46580.\n",
      "Ep done - 46590.\n",
      "Ep done - 46600.\n",
      "Eval num_timesteps=4660000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4660000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.26e-07 |\n",
      "|    explained_variance   | 0.518     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00179   |\n",
      "|    n_updates            | 26810     |\n",
      "|    policy_gradient_loss | 2.24e-10  |\n",
      "|    value_loss           | 0.00377   |\n",
      "---------------------------------------\n",
      "Ep done - 162660.\n",
      "Ep done - 162670.\n",
      "Ep done - 162680.\n",
      "Ep done - 162690.\n",
      "Ep done - 162700.\n",
      "Ep done - 162710.\n",
      "Ep done - 162720.\n",
      "Ep done - 162730.\n",
      "Ep done - 162740.\n",
      "Ep done - 162750.\n",
      "Ep done - 162760.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 759      |\n",
      "|    time_elapsed    | 16719    |\n",
      "|    total_timesteps | 4663296  |\n",
      "---------------------------------\n",
      "Ep done - 162770.\n",
      "Ep done - 162780.\n",
      "Ep done - 162790.\n",
      "Ep done - 162800.\n",
      "Ep done - 162810.\n",
      "Ep done - 162820.\n",
      "Ep done - 162830.\n",
      "Ep done - 162840.\n",
      "Ep done - 162850.\n",
      "Ep done - 162860.\n",
      "Ep done - 162870.\n",
      "Ep done - 162880.\n",
      "Ep done - 162890.\n",
      "Ep done - 162900.\n",
      "Ep done - 162910.\n",
      "Ep done - 162920.\n",
      "Ep done - 162930.\n",
      "Ep done - 162940.\n",
      "Ep done - 162950.\n",
      "Ep done - 162960.\n",
      "Ep done - 162970.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 760       |\n",
      "|    time_elapsed         | 16735     |\n",
      "|    total_timesteps      | 4669440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.38e-07 |\n",
      "|    explained_variance   | 0.516     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00287   |\n",
      "|    n_updates            | 26820     |\n",
      "|    policy_gradient_loss | -1.85e-10 |\n",
      "|    value_loss           | 0.00398   |\n",
      "---------------------------------------\n",
      "Ep done - 162980.\n",
      "Ep done - 162990.\n",
      "Ep done - 46610.\n",
      "Ep done - 46620.\n",
      "Ep done - 46630.\n",
      "Ep done - 46640.\n",
      "Ep done - 46650.\n",
      "Ep done - 46660.\n",
      "Ep done - 46670.\n",
      "Ep done - 46680.\n",
      "Ep done - 46690.\n",
      "Ep done - 46700.\n",
      "Eval num_timesteps=4670000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4670000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.55e-07 |\n",
      "|    explained_variance   | 0.51      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00216   |\n",
      "|    n_updates            | 26830     |\n",
      "|    policy_gradient_loss | -1.33e-10 |\n",
      "|    value_loss           | 0.00372   |\n",
      "---------------------------------------\n",
      "Ep done - 163000.\n",
      "Ep done - 163010.\n",
      "Ep done - 163020.\n",
      "Ep done - 163030.\n",
      "Ep done - 163040.\n",
      "Ep done - 163050.\n",
      "Ep done - 163060.\n",
      "Ep done - 163070.\n",
      "Ep done - 163080.\n",
      "Ep done - 163090.\n",
      "Ep done - 163100.\n",
      "Ep done - 163110.\n",
      "Ep done - 163120.\n",
      "Ep done - 163130.\n",
      "Ep done - 163140.\n",
      "Ep done - 163150.\n",
      "Ep done - 163160.\n",
      "Ep done - 163170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 761      |\n",
      "|    time_elapsed    | 16756    |\n",
      "|    total_timesteps | 4675584  |\n",
      "---------------------------------\n",
      "Ep done - 163180.\n",
      "Ep done - 163190.\n",
      "Ep done - 163200.\n",
      "Ep done - 163210.\n",
      "Ep done - 163220.\n",
      "Ep done - 163230.\n",
      "Ep done - 163240.\n",
      "Ep done - 163250.\n",
      "Ep done - 163260.\n",
      "Ep done - 163270.\n",
      "Ep done - 163280.\n",
      "Ep done - 163290.\n",
      "Ep done - 163300.\n",
      "Ep done - 163310.\n",
      "Ep done - 163320.\n",
      "Ep done - 46710.\n",
      "Ep done - 46720.\n",
      "Ep done - 46730.\n",
      "Ep done - 46740.\n",
      "Ep done - 46750.\n",
      "Ep done - 46760.\n",
      "Ep done - 46770.\n",
      "Ep done - 46780.\n",
      "Ep done - 46790.\n",
      "Ep done - 46800.\n",
      "Eval num_timesteps=4680000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4680000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.77e-07 |\n",
      "|    explained_variance   | 0.517     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00136   |\n",
      "|    n_updates            | 26840     |\n",
      "|    policy_gradient_loss | -3.98e-10 |\n",
      "|    value_loss           | 0.00384   |\n",
      "---------------------------------------\n",
      "Ep done - 163330.\n",
      "Ep done - 163340.\n",
      "Ep done - 163350.\n",
      "Ep done - 163360.\n",
      "Ep done - 163370.\n",
      "Ep done - 163380.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 762      |\n",
      "|    time_elapsed    | 16777    |\n",
      "|    total_timesteps | 4681728  |\n",
      "---------------------------------\n",
      "Ep done - 163390.\n",
      "Ep done - 163400.\n",
      "Ep done - 163410.\n",
      "Ep done - 163420.\n",
      "Ep done - 163430.\n",
      "Ep done - 163440.\n",
      "Ep done - 163450.\n",
      "Ep done - 163460.\n",
      "Ep done - 163470.\n",
      "Ep done - 163480.\n",
      "Ep done - 163490.\n",
      "Ep done - 163500.\n",
      "Ep done - 163510.\n",
      "Ep done - 163520.\n",
      "Ep done - 163530.\n",
      "Ep done - 163540.\n",
      "Ep done - 163550.\n",
      "Ep done - 163560.\n",
      "Ep done - 163570.\n",
      "Ep done - 163580.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 763       |\n",
      "|    time_elapsed         | 16793     |\n",
      "|    total_timesteps      | 4687872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.03e-07 |\n",
      "|    explained_variance   | 0.518     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00205   |\n",
      "|    n_updates            | 26850     |\n",
      "|    policy_gradient_loss | 8.83e-10  |\n",
      "|    value_loss           | 0.00394   |\n",
      "---------------------------------------\n",
      "Ep done - 163590.\n",
      "Ep done - 163600.\n",
      "Ep done - 163610.\n",
      "Ep done - 163620.\n",
      "Ep done - 163630.\n",
      "Ep done - 163640.\n",
      "Ep done - 163650.\n",
      "Ep done - 46810.\n",
      "Ep done - 46820.\n",
      "Ep done - 46830.\n",
      "Ep done - 46840.\n",
      "Ep done - 46850.\n",
      "Ep done - 46860.\n",
      "Ep done - 46870.\n",
      "Ep done - 46880.\n",
      "Ep done - 46890.\n",
      "Ep done - 46900.\n",
      "Eval num_timesteps=4690000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4690000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.67e-07 |\n",
      "|    explained_variance   | 0.521     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00159   |\n",
      "|    n_updates            | 26860     |\n",
      "|    policy_gradient_loss | -9.29e-10 |\n",
      "|    value_loss           | 0.00416   |\n",
      "---------------------------------------\n",
      "Ep done - 163660.\n",
      "Ep done - 163670.\n",
      "Ep done - 163680.\n",
      "Ep done - 163690.\n",
      "Ep done - 163700.\n",
      "Ep done - 163710.\n",
      "Ep done - 163720.\n",
      "Ep done - 163730.\n",
      "Ep done - 163740.\n",
      "Ep done - 163750.\n",
      "Ep done - 163760.\n",
      "Ep done - 163770.\n",
      "Ep done - 163780.\n",
      "Ep done - 163790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 764      |\n",
      "|    time_elapsed    | 16814    |\n",
      "|    total_timesteps | 4694016  |\n",
      "---------------------------------\n",
      "Ep done - 163800.\n",
      "Ep done - 163810.\n",
      "Ep done - 163820.\n",
      "Ep done - 163830.\n",
      "Ep done - 163840.\n",
      "Ep done - 163850.\n",
      "Ep done - 163860.\n",
      "Ep done - 163870.\n",
      "Ep done - 163880.\n",
      "Ep done - 163890.\n",
      "Ep done - 163900.\n",
      "Ep done - 163910.\n",
      "Ep done - 163920.\n",
      "Ep done - 163930.\n",
      "Ep done - 163940.\n",
      "Ep done - 163950.\n",
      "Ep done - 163960.\n",
      "Ep done - 163970.\n",
      "Ep done - 163980.\n",
      "Ep done - 163990.\n",
      "Ep done - 46910.\n",
      "Ep done - 46920.\n",
      "Ep done - 46930.\n",
      "Ep done - 46940.\n",
      "Ep done - 46950.\n",
      "Ep done - 46960.\n",
      "Ep done - 46970.\n",
      "Ep done - 46980.\n",
      "Ep done - 46990.\n",
      "Ep done - 47000.\n",
      "Eval num_timesteps=4700000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4700000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.82e-07 |\n",
      "|    explained_variance   | 0.515     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00141   |\n",
      "|    n_updates            | 26870     |\n",
      "|    policy_gradient_loss | -1.34e-09 |\n",
      "|    value_loss           | 0.00385   |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 765      |\n",
      "|    time_elapsed    | 16835    |\n",
      "|    total_timesteps | 4700160  |\n",
      "---------------------------------\n",
      "Ep done - 164000.\n",
      "Ep done - 164010.\n",
      "Ep done - 164020.\n",
      "Ep done - 164030.\n",
      "Ep done - 164040.\n",
      "Ep done - 164050.\n",
      "Ep done - 164060.\n",
      "Ep done - 164070.\n",
      "Ep done - 164080.\n",
      "Ep done - 164090.\n",
      "Ep done - 164100.\n",
      "Ep done - 164110.\n",
      "Ep done - 164120.\n",
      "Ep done - 164130.\n",
      "Ep done - 164140.\n",
      "Ep done - 164150.\n",
      "Ep done - 164160.\n",
      "Ep done - 164170.\n",
      "Ep done - 164180.\n",
      "Ep done - 164190.\n",
      "Ep done - 164200.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 766       |\n",
      "|    time_elapsed         | 16851     |\n",
      "|    total_timesteps      | 4706304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.77e-07 |\n",
      "|    explained_variance   | 0.521     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00224   |\n",
      "|    n_updates            | 26880     |\n",
      "|    policy_gradient_loss | 1.84e-10  |\n",
      "|    value_loss           | 0.00472   |\n",
      "---------------------------------------\n",
      "Ep done - 164210.\n",
      "Ep done - 164220.\n",
      "Ep done - 164230.\n",
      "Ep done - 164240.\n",
      "Ep done - 164250.\n",
      "Ep done - 164260.\n",
      "Ep done - 164270.\n",
      "Ep done - 164280.\n",
      "Ep done - 164290.\n",
      "Ep done - 164300.\n",
      "Ep done - 164310.\n",
      "Ep done - 164320.\n",
      "Ep done - 47010.\n",
      "Ep done - 47020.\n",
      "Ep done - 47030.\n",
      "Ep done - 47040.\n",
      "Ep done - 47050.\n",
      "Ep done - 47060.\n",
      "Ep done - 47070.\n",
      "Ep done - 47080.\n",
      "Ep done - 47090.\n",
      "Ep done - 47100.\n",
      "Eval num_timesteps=4710000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4710000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2823181 |\n",
      "|    clip_fraction        | 0.0194    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.35e-05 |\n",
      "|    explained_variance   | 0.501     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000716  |\n",
      "|    n_updates            | 26890     |\n",
      "|    policy_gradient_loss | -0.00862  |\n",
      "|    value_loss           | 0.00161   |\n",
      "---------------------------------------\n",
      "Ep done - 164330.\n",
      "Ep done - 164340.\n",
      "Ep done - 164350.\n",
      "Ep done - 164360.\n",
      "Ep done - 164370.\n",
      "Ep done - 164380.\n",
      "Ep done - 164390.\n",
      "Ep done - 164400.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 767      |\n",
      "|    time_elapsed    | 16873    |\n",
      "|    total_timesteps | 4712448  |\n",
      "---------------------------------\n",
      "Ep done - 164410.\n",
      "Ep done - 164420.\n",
      "Ep done - 164430.\n",
      "Ep done - 164440.\n",
      "Ep done - 164450.\n",
      "Ep done - 164460.\n",
      "Ep done - 164470.\n",
      "Ep done - 164480.\n",
      "Ep done - 164490.\n",
      "Ep done - 164500.\n",
      "Ep done - 164510.\n",
      "Ep done - 164520.\n",
      "Ep done - 164530.\n",
      "Ep done - 164540.\n",
      "Ep done - 164550.\n",
      "Ep done - 164560.\n",
      "Ep done - 164570.\n",
      "Ep done - 164580.\n",
      "Ep done - 164590.\n",
      "Ep done - 164600.\n",
      "Ep done - 164610.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 768       |\n",
      "|    time_elapsed         | 16889     |\n",
      "|    total_timesteps      | 4718592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8802464 |\n",
      "|    clip_fraction        | 0.0168    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.33e-05 |\n",
      "|    explained_variance   | 0.794     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00467  |\n",
      "|    n_updates            | 26900     |\n",
      "|    policy_gradient_loss | -0.00612  |\n",
      "|    value_loss           | 0.00154   |\n",
      "---------------------------------------\n",
      "Ep done - 164620.\n",
      "Ep done - 164630.\n",
      "Ep done - 164640.\n",
      "Ep done - 164650.\n",
      "Ep done - 47110.\n",
      "Ep done - 47120.\n",
      "Ep done - 47130.\n",
      "Ep done - 47140.\n",
      "Ep done - 47150.\n",
      "Ep done - 47160.\n",
      "Ep done - 47170.\n",
      "Ep done - 47180.\n",
      "Ep done - 47190.\n",
      "Ep done - 47200.\n",
      "Eval num_timesteps=4720000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4720000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.1263905 |\n",
      "|    clip_fraction        | 0.0965    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000323 |\n",
      "|    explained_variance   | 0.808     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.000417 |\n",
      "|    n_updates            | 26910     |\n",
      "|    policy_gradient_loss | -0.0105   |\n",
      "|    value_loss           | 0.00137   |\n",
      "---------------------------------------\n",
      "Ep done - 164660.\n",
      "Ep done - 164670.\n",
      "Ep done - 164680.\n",
      "Ep done - 164690.\n",
      "Ep done - 164700.\n",
      "Ep done - 164710.\n",
      "Ep done - 164720.\n",
      "Ep done - 164730.\n",
      "Ep done - 164740.\n",
      "Ep done - 164750.\n",
      "Ep done - 164760.\n",
      "Ep done - 164770.\n",
      "Ep done - 164780.\n",
      "Ep done - 164790.\n",
      "Ep done - 164800.\n",
      "Ep done - 164810.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 769      |\n",
      "|    time_elapsed    | 16909    |\n",
      "|    total_timesteps | 4724736  |\n",
      "---------------------------------\n",
      "Ep done - 164820.\n",
      "Ep done - 164830.\n",
      "Ep done - 164840.\n",
      "Ep done - 164850.\n",
      "Ep done - 164860.\n",
      "Ep done - 164870.\n",
      "Ep done - 164880.\n",
      "Ep done - 164890.\n",
      "Ep done - 164900.\n",
      "Ep done - 164910.\n",
      "Ep done - 164920.\n",
      "Ep done - 164930.\n",
      "Ep done - 164940.\n",
      "Ep done - 164950.\n",
      "Ep done - 164960.\n",
      "Ep done - 164970.\n",
      "Ep done - 164980.\n",
      "Ep done - 164990.\n",
      "Ep done - 47210.\n",
      "Ep done - 47220.\n",
      "Ep done - 47230.\n",
      "Ep done - 47240.\n",
      "Ep done - 47250.\n",
      "Ep done - 47260.\n",
      "Ep done - 47270.\n",
      "Ep done - 47280.\n",
      "Ep done - 47290.\n",
      "Ep done - 47300.\n",
      "Eval num_timesteps=4730000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4730000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.132822  |\n",
      "|    clip_fraction        | 0.0796    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000217 |\n",
      "|    explained_variance   | 0.785     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0242   |\n",
      "|    n_updates            | 26920     |\n",
      "|    policy_gradient_loss | -0.022    |\n",
      "|    value_loss           | 0.00119   |\n",
      "---------------------------------------\n",
      "Ep done - 165000.\n",
      "Ep done - 165010.\n",
      "Ep done - 165020.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 770      |\n",
      "|    time_elapsed    | 16930    |\n",
      "|    total_timesteps | 4730880  |\n",
      "---------------------------------\n",
      "Ep done - 165030.\n",
      "Ep done - 165040.\n",
      "Ep done - 165050.\n",
      "Ep done - 165060.\n",
      "Ep done - 165070.\n",
      "Ep done - 165080.\n",
      "Ep done - 165090.\n",
      "Ep done - 165100.\n",
      "Ep done - 165110.\n",
      "Ep done - 165120.\n",
      "Ep done - 165130.\n",
      "Ep done - 165140.\n",
      "Ep done - 165150.\n",
      "Ep done - 165160.\n",
      "Ep done - 165170.\n",
      "Ep done - 165180.\n",
      "Ep done - 165190.\n",
      "Ep done - 165200.\n",
      "Ep done - 165210.\n",
      "Ep done - 165220.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 771        |\n",
      "|    time_elapsed         | 16946      |\n",
      "|    total_timesteps      | 4737024    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38890553 |\n",
      "|    clip_fraction        | 0.0338     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000731  |\n",
      "|    explained_variance   | 0.855      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.000738   |\n",
      "|    n_updates            | 26930      |\n",
      "|    policy_gradient_loss | -0.00253   |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "Ep done - 165230.\n",
      "Ep done - 165240.\n",
      "Ep done - 165250.\n",
      "Ep done - 165260.\n",
      "Ep done - 165270.\n",
      "Ep done - 165280.\n",
      "Ep done - 165290.\n",
      "Ep done - 165300.\n",
      "Ep done - 165310.\n",
      "Ep done - 165320.\n",
      "Ep done - 47310.\n",
      "Ep done - 47320.\n",
      "Ep done - 47330.\n",
      "Ep done - 47340.\n",
      "Ep done - 47350.\n",
      "Ep done - 47360.\n",
      "Ep done - 47370.\n",
      "Ep done - 47380.\n",
      "Ep done - 47390.\n",
      "Ep done - 47400.\n",
      "Eval num_timesteps=4740000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4740000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56450945 |\n",
      "|    clip_fraction        | 0.0278     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000141  |\n",
      "|    explained_variance   | 0.832      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00213   |\n",
      "|    n_updates            | 26940      |\n",
      "|    policy_gradient_loss | -0.00216   |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "Ep done - 165330.\n",
      "Ep done - 165340.\n",
      "Ep done - 165350.\n",
      "Ep done - 165360.\n",
      "Ep done - 165370.\n",
      "Ep done - 165380.\n",
      "Ep done - 165390.\n",
      "Ep done - 165400.\n",
      "Ep done - 165410.\n",
      "Ep done - 165420.\n",
      "Ep done - 165430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 772      |\n",
      "|    time_elapsed    | 16967    |\n",
      "|    total_timesteps | 4743168  |\n",
      "---------------------------------\n",
      "Ep done - 165440.\n",
      "Ep done - 165450.\n",
      "Ep done - 165460.\n",
      "Ep done - 165470.\n",
      "Ep done - 165480.\n",
      "Ep done - 165490.\n",
      "Ep done - 165500.\n",
      "Ep done - 165510.\n",
      "Ep done - 165520.\n",
      "Ep done - 165530.\n",
      "Ep done - 165540.\n",
      "Ep done - 165550.\n",
      "Ep done - 165560.\n",
      "Ep done - 165570.\n",
      "Ep done - 165580.\n",
      "Ep done - 165590.\n",
      "Ep done - 165600.\n",
      "Ep done - 165610.\n",
      "Ep done - 165620.\n",
      "Ep done - 165630.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 279       |\n",
      "|    iterations           | 773       |\n",
      "|    time_elapsed         | 16984     |\n",
      "|    total_timesteps      | 4749312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.42e-06 |\n",
      "|    explained_variance   | 0.847     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000374  |\n",
      "|    n_updates            | 26950     |\n",
      "|    policy_gradient_loss | 4.34e-10  |\n",
      "|    value_loss           | 0.00119   |\n",
      "---------------------------------------\n",
      "Ep done - 165640.\n",
      "Ep done - 165650.\n",
      "Ep done - 47410.\n",
      "Ep done - 47420.\n",
      "Ep done - 47430.\n",
      "Ep done - 47440.\n",
      "Ep done - 47450.\n",
      "Ep done - 47460.\n",
      "Ep done - 47470.\n",
      "Ep done - 47480.\n",
      "Ep done - 47490.\n",
      "Ep done - 47500.\n",
      "Eval num_timesteps=4750000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | -1            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4750000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6485175e-05 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -4.6e-05      |\n",
      "|    explained_variance   | 0.856         |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.000531      |\n",
      "|    n_updates            | 26960         |\n",
      "|    policy_gradient_loss | -2.97e-05     |\n",
      "|    value_loss           | 0.00115       |\n",
      "-------------------------------------------\n",
      "Ep done - 165660.\n",
      "Ep done - 165670.\n",
      "Ep done - 165680.\n",
      "Ep done - 165690.\n",
      "Ep done - 165700.\n",
      "Ep done - 165710.\n",
      "Ep done - 165720.\n",
      "Ep done - 165730.\n",
      "Ep done - 165740.\n",
      "Ep done - 165750.\n",
      "Ep done - 165760.\n",
      "Ep done - 165770.\n",
      "Ep done - 165780.\n",
      "Ep done - 165790.\n",
      "Ep done - 165800.\n",
      "Ep done - 165810.\n",
      "Ep done - 165820.\n",
      "Ep done - 165830.\n",
      "Ep done - 165840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 774      |\n",
      "|    time_elapsed    | 17005    |\n",
      "|    total_timesteps | 4755456  |\n",
      "---------------------------------\n",
      "Ep done - 165850.\n",
      "Ep done - 165860.\n",
      "Ep done - 165870.\n",
      "Ep done - 165880.\n",
      "Ep done - 165890.\n",
      "Ep done - 165900.\n",
      "Ep done - 165910.\n",
      "Ep done - 165920.\n",
      "Ep done - 165930.\n",
      "Ep done - 165940.\n",
      "Ep done - 165950.\n",
      "Ep done - 165960.\n",
      "Ep done - 165970.\n",
      "Ep done - 165980.\n",
      "Ep done - 165990.\n",
      "Ep done - 47510.\n",
      "Ep done - 47520.\n",
      "Ep done - 47530.\n",
      "Ep done - 47540.\n",
      "Ep done - 47550.\n",
      "Ep done - 47560.\n",
      "Ep done - 47570.\n",
      "Ep done - 47580.\n",
      "Ep done - 47590.\n",
      "Ep done - 47600.\n",
      "Eval num_timesteps=4760000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4760000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20898797 |\n",
      "|    clip_fraction        | 0.00163    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.02e-05  |\n",
      "|    explained_variance   | 0.859      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00106   |\n",
      "|    n_updates            | 26970      |\n",
      "|    policy_gradient_loss | -5.9e-05   |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "Ep done - 166000.\n",
      "Ep done - 166010.\n",
      "Ep done - 166020.\n",
      "Ep done - 166030.\n",
      "Ep done - 166040.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 775      |\n",
      "|    time_elapsed    | 17027    |\n",
      "|    total_timesteps | 4761600  |\n",
      "---------------------------------\n",
      "Ep done - 166050.\n",
      "Ep done - 166060.\n",
      "Ep done - 166070.\n",
      "Ep done - 166080.\n",
      "Ep done - 166090.\n",
      "Ep done - 166100.\n",
      "Ep done - 166110.\n",
      "Ep done - 166120.\n",
      "Ep done - 166130.\n",
      "Ep done - 166140.\n",
      "Ep done - 166150.\n",
      "Ep done - 166160.\n",
      "Ep done - 166170.\n",
      "Ep done - 166180.\n",
      "Ep done - 166190.\n",
      "Ep done - 166200.\n",
      "Ep done - 166210.\n",
      "Ep done - 166220.\n",
      "Ep done - 166230.\n",
      "Ep done - 166240.\n",
      "Ep done - 166250.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 776        |\n",
      "|    time_elapsed         | 17044      |\n",
      "|    total_timesteps      | 4767744    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77856666 |\n",
      "|    clip_fraction        | 0.0453     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000334  |\n",
      "|    explained_variance   | 0.85       |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00456   |\n",
      "|    n_updates            | 26980      |\n",
      "|    policy_gradient_loss | -0.00515   |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "Ep done - 166260.\n",
      "Ep done - 166270.\n",
      "Ep done - 166280.\n",
      "Ep done - 166290.\n",
      "Ep done - 166300.\n",
      "Ep done - 166310.\n",
      "Ep done - 166320.\n",
      "Ep done - 47610.\n",
      "Ep done - 47620.\n",
      "Ep done - 47630.\n",
      "Ep done - 47640.\n",
      "Ep done - 47650.\n",
      "Ep done - 47660.\n",
      "Ep done - 47670.\n",
      "Ep done - 47680.\n",
      "Ep done - 47690.\n",
      "Ep done - 47700.\n",
      "Eval num_timesteps=4770000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4770000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.6365764 |\n",
      "|    clip_fraction        | 0.0655    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.08e-05 |\n",
      "|    explained_variance   | 0.853     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0287   |\n",
      "|    n_updates            | 26990     |\n",
      "|    policy_gradient_loss | -0.0216   |\n",
      "|    value_loss           | 0.00125   |\n",
      "---------------------------------------\n",
      "Ep done - 166330.\n",
      "Ep done - 166340.\n",
      "Ep done - 166350.\n",
      "Ep done - 166360.\n",
      "Ep done - 166370.\n",
      "Ep done - 166380.\n",
      "Ep done - 166390.\n",
      "Ep done - 166400.\n",
      "Ep done - 166410.\n",
      "Ep done - 166420.\n",
      "Ep done - 166430.\n",
      "Ep done - 166440.\n",
      "Ep done - 166450.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 777      |\n",
      "|    time_elapsed    | 17064    |\n",
      "|    total_timesteps | 4773888  |\n",
      "---------------------------------\n",
      "Ep done - 166460.\n",
      "Ep done - 166470.\n",
      "Ep done - 166480.\n",
      "Ep done - 166490.\n",
      "Ep done - 166500.\n",
      "Ep done - 166510.\n",
      "Ep done - 166520.\n",
      "Ep done - 166530.\n",
      "Ep done - 166540.\n",
      "Ep done - 166550.\n",
      "Ep done - 166560.\n",
      "Ep done - 166570.\n",
      "Ep done - 166580.\n",
      "Ep done - 166590.\n",
      "Ep done - 166600.\n",
      "Ep done - 166610.\n",
      "Ep done - 166620.\n",
      "Ep done - 166630.\n",
      "Ep done - 166640.\n",
      "Ep done - 166650.\n",
      "Ep done - 47710.\n",
      "Ep done - 47720.\n",
      "Ep done - 47730.\n",
      "Ep done - 47740.\n",
      "Ep done - 47750.\n",
      "Ep done - 47760.\n",
      "Ep done - 47770.\n",
      "Ep done - 47780.\n",
      "Ep done - 47790.\n",
      "Ep done - 47800.\n",
      "Eval num_timesteps=4780000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4780000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0218942 |\n",
      "|    clip_fraction        | 0.0659    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000175 |\n",
      "|    explained_variance   | 0.773     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.018    |\n",
      "|    n_updates            | 27000     |\n",
      "|    policy_gradient_loss | -0.00117  |\n",
      "|    value_loss           | 0.00159   |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 778      |\n",
      "|    time_elapsed    | 17085    |\n",
      "|    total_timesteps | 4780032  |\n",
      "---------------------------------\n",
      "Ep done - 166660.\n",
      "Ep done - 166670.\n",
      "Ep done - 166680.\n",
      "Ep done - 166690.\n",
      "Ep done - 166700.\n",
      "Ep done - 166710.\n",
      "Ep done - 166720.\n",
      "Ep done - 166730.\n",
      "Ep done - 166740.\n",
      "Ep done - 166750.\n",
      "Ep done - 166760.\n",
      "Ep done - 166770.\n",
      "Ep done - 166780.\n",
      "Ep done - 166790.\n",
      "Ep done - 166800.\n",
      "Ep done - 166810.\n",
      "Ep done - 166820.\n",
      "Ep done - 166830.\n",
      "Ep done - 166840.\n",
      "Ep done - 166850.\n",
      "Ep done - 166860.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -1           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 279          |\n",
      "|    iterations           | 779          |\n",
      "|    time_elapsed         | 17103        |\n",
      "|    total_timesteps      | 4786176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.958121e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.9e-05     |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.000377     |\n",
      "|    n_updates            | 27010        |\n",
      "|    policy_gradient_loss | -3.3e-06     |\n",
      "|    value_loss           | 0.00122      |\n",
      "------------------------------------------\n",
      "Ep done - 166870.\n",
      "Ep done - 166880.\n",
      "Ep done - 166890.\n",
      "Ep done - 166900.\n",
      "Ep done - 166910.\n",
      "Ep done - 166920.\n",
      "Ep done - 166930.\n",
      "Ep done - 166940.\n",
      "Ep done - 166950.\n",
      "Ep done - 166960.\n",
      "Ep done - 166970.\n",
      "Ep done - 166980.\n",
      "Ep done - 166990.\n",
      "Ep done - 47810.\n",
      "Ep done - 47820.\n",
      "Ep done - 47830.\n",
      "Ep done - 47840.\n",
      "Ep done - 47850.\n",
      "Ep done - 47860.\n",
      "Ep done - 47870.\n",
      "Ep done - 47880.\n",
      "Ep done - 47890.\n",
      "Ep done - 47900.\n",
      "Eval num_timesteps=4790000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4790000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29018995 |\n",
      "|    clip_fraction        | 0.05       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000484  |\n",
      "|    explained_variance   | 0.848      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.000521  |\n",
      "|    n_updates            | 27020      |\n",
      "|    policy_gradient_loss | -0.00285   |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "Ep done - 167000.\n",
      "Ep done - 167010.\n",
      "Ep done - 167020.\n",
      "Ep done - 167030.\n",
      "Ep done - 167040.\n",
      "Ep done - 167050.\n",
      "Ep done - 167060.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 780      |\n",
      "|    time_elapsed    | 17123    |\n",
      "|    total_timesteps | 4792320  |\n",
      "---------------------------------\n",
      "Ep done - 167070.\n",
      "Ep done - 167080.\n",
      "Ep done - 167090.\n",
      "Ep done - 167100.\n",
      "Ep done - 167110.\n",
      "Ep done - 167120.\n",
      "Ep done - 167130.\n",
      "Ep done - 167140.\n",
      "Ep done - 167150.\n",
      "Ep done - 167160.\n",
      "Ep done - 167170.\n",
      "Ep done - 167180.\n",
      "Ep done - 167190.\n",
      "Ep done - 167200.\n",
      "Ep done - 167210.\n",
      "Ep done - 167220.\n",
      "Ep done - 167230.\n",
      "Ep done - 167240.\n",
      "Ep done - 167250.\n",
      "Ep done - 167260.\n",
      "Ep done - 167270.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 781        |\n",
      "|    time_elapsed         | 17140      |\n",
      "|    total_timesteps      | 4798464    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30476284 |\n",
      "|    clip_fraction        | 0.0285     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000125  |\n",
      "|    explained_variance   | 0.448      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.000615   |\n",
      "|    n_updates            | 27030      |\n",
      "|    policy_gradient_loss | -6.82e-05  |\n",
      "|    value_loss           | 0.00737    |\n",
      "----------------------------------------\n",
      "Ep done - 167280.\n",
      "Ep done - 167290.\n",
      "Ep done - 167300.\n",
      "Ep done - 167310.\n",
      "Ep done - 167320.\n",
      "Ep done - 47910.\n",
      "Ep done - 47920.\n",
      "Ep done - 47930.\n",
      "Ep done - 47940.\n",
      "Ep done - 47950.\n",
      "Ep done - 47960.\n",
      "Ep done - 47970.\n",
      "Ep done - 47980.\n",
      "Ep done - 47990.\n",
      "Ep done - 48000.\n",
      "Eval num_timesteps=4800000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4800000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60019606 |\n",
      "|    clip_fraction        | 0.00783    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000104  |\n",
      "|    explained_variance   | 0.819      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00293   |\n",
      "|    n_updates            | 27040      |\n",
      "|    policy_gradient_loss | -0.000584  |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "Ep done - 167330.\n",
      "Ep done - 167340.\n",
      "Ep done - 167350.\n",
      "Ep done - 167360.\n",
      "Ep done - 167370.\n",
      "Ep done - 167380.\n",
      "Ep done - 167390.\n",
      "Ep done - 167400.\n",
      "Ep done - 167410.\n",
      "Ep done - 167420.\n",
      "Ep done - 167430.\n",
      "Ep done - 167440.\n",
      "Ep done - 167450.\n",
      "Ep done - 167460.\n",
      "Ep done - 167470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 782      |\n",
      "|    time_elapsed    | 17161    |\n",
      "|    total_timesteps | 4804608  |\n",
      "---------------------------------\n",
      "Ep done - 167480.\n",
      "Ep done - 167490.\n",
      "Ep done - 167500.\n",
      "Ep done - 167510.\n",
      "Ep done - 167520.\n",
      "Ep done - 167530.\n",
      "Ep done - 167540.\n",
      "Ep done - 167550.\n",
      "Ep done - 167560.\n",
      "Ep done - 167570.\n",
      "Ep done - 167580.\n",
      "Ep done - 167590.\n",
      "Ep done - 167600.\n",
      "Ep done - 167610.\n",
      "Ep done - 167620.\n",
      "Ep done - 167630.\n",
      "Ep done - 167640.\n",
      "Ep done - 167650.\n",
      "Ep done - 48010.\n",
      "Ep done - 48020.\n",
      "Ep done - 48030.\n",
      "Ep done - 48040.\n",
      "Ep done - 48050.\n",
      "Ep done - 48060.\n",
      "Ep done - 48070.\n",
      "Ep done - 48080.\n",
      "Ep done - 48090.\n",
      "Ep done - 48100.\n",
      "Eval num_timesteps=4810000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4810000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.3737342 |\n",
      "|    clip_fraction        | 0.0887    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00276  |\n",
      "|    explained_variance   | 0.637     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0185   |\n",
      "|    n_updates            | 27050     |\n",
      "|    policy_gradient_loss | -0.0123   |\n",
      "|    value_loss           | 0.00195   |\n",
      "---------------------------------------\n",
      "Ep done - 167660.\n",
      "Ep done - 167670.\n",
      "Ep done - 167680.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 783      |\n",
      "|    time_elapsed    | 17182    |\n",
      "|    total_timesteps | 4810752  |\n",
      "---------------------------------\n",
      "Ep done - 167690.\n",
      "Ep done - 167700.\n",
      "Ep done - 167710.\n",
      "Ep done - 167720.\n",
      "Ep done - 167730.\n",
      "Ep done - 167740.\n",
      "Ep done - 167750.\n",
      "Ep done - 167760.\n",
      "Ep done - 167770.\n",
      "Ep done - 167780.\n",
      "Ep done - 167790.\n",
      "Ep done - 167800.\n",
      "Ep done - 167810.\n",
      "Ep done - 167820.\n",
      "Ep done - 167830.\n",
      "Ep done - 167840.\n",
      "Ep done - 167850.\n",
      "Ep done - 167860.\n",
      "Ep done - 167870.\n",
      "Ep done - 167880.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 784       |\n",
      "|    time_elapsed         | 17198     |\n",
      "|    total_timesteps      | 4816896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2894616 |\n",
      "|    clip_fraction        | 0.055     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00171  |\n",
      "|    explained_variance   | 0.821     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0135   |\n",
      "|    n_updates            | 27060     |\n",
      "|    policy_gradient_loss | -0.0138   |\n",
      "|    value_loss           | 0.00147   |\n",
      "---------------------------------------\n",
      "Ep done - 167890.\n",
      "Ep done - 167900.\n",
      "Ep done - 167910.\n",
      "Ep done - 167920.\n",
      "Ep done - 167930.\n",
      "Ep done - 167940.\n",
      "Ep done - 167950.\n",
      "Ep done - 167960.\n",
      "Ep done - 167970.\n",
      "Ep done - 167980.\n",
      "Ep done - 167990.\n",
      "Ep done - 48110.\n",
      "Ep done - 48120.\n",
      "Ep done - 48130.\n",
      "Ep done - 48140.\n",
      "Ep done - 48150.\n",
      "Ep done - 48160.\n",
      "Ep done - 48170.\n",
      "Ep done - 48180.\n",
      "Ep done - 48190.\n",
      "Ep done - 48200.\n",
      "Eval num_timesteps=4820000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -1           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4820000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.298434e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07e-05    |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.000915     |\n",
      "|    n_updates            | 27070        |\n",
      "|    policy_gradient_loss | -2.34e-08    |\n",
      "|    value_loss           | 0.00141      |\n",
      "------------------------------------------\n",
      "Ep done - 168000.\n",
      "Ep done - 168010.\n",
      "Ep done - 168020.\n",
      "Ep done - 168030.\n",
      "Ep done - 168040.\n",
      "Ep done - 168050.\n",
      "Ep done - 168060.\n",
      "Ep done - 168070.\n",
      "Ep done - 168080.\n",
      "Ep done - 168090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 785      |\n",
      "|    time_elapsed    | 17219    |\n",
      "|    total_timesteps | 4823040  |\n",
      "---------------------------------\n",
      "Ep done - 168100.\n",
      "Ep done - 168110.\n",
      "Ep done - 168120.\n",
      "Ep done - 168130.\n",
      "Ep done - 168140.\n",
      "Ep done - 168150.\n",
      "Ep done - 168160.\n",
      "Ep done - 168170.\n",
      "Ep done - 168180.\n",
      "Ep done - 168190.\n",
      "Ep done - 168200.\n",
      "Ep done - 168210.\n",
      "Ep done - 168220.\n",
      "Ep done - 168230.\n",
      "Ep done - 168240.\n",
      "Ep done - 168250.\n",
      "Ep done - 168260.\n",
      "Ep done - 168270.\n",
      "Ep done - 168280.\n",
      "Ep done - 168290.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 786        |\n",
      "|    time_elapsed         | 17236      |\n",
      "|    total_timesteps      | 4829184    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86584425 |\n",
      "|    clip_fraction        | 0.0203     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000108  |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.000975  |\n",
      "|    n_updates            | 27080      |\n",
      "|    policy_gradient_loss | -0.0013    |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "Ep done - 168300.\n",
      "Ep done - 168310.\n",
      "Ep done - 168320.\n",
      "Ep done - 48210.\n",
      "Ep done - 48220.\n",
      "Ep done - 48230.\n",
      "Ep done - 48240.\n",
      "Ep done - 48250.\n",
      "Ep done - 48260.\n",
      "Ep done - 48270.\n",
      "Ep done - 48280.\n",
      "Ep done - 48290.\n",
      "Ep done - 48300.\n",
      "Eval num_timesteps=4830000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4830000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47403696 |\n",
      "|    clip_fraction        | 0.00461    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000195  |\n",
      "|    explained_variance   | 0.845      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00127    |\n",
      "|    n_updates            | 27090      |\n",
      "|    policy_gradient_loss | -0.000555  |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "Ep done - 168330.\n",
      "Ep done - 168340.\n",
      "Ep done - 168350.\n",
      "Ep done - 168360.\n",
      "Ep done - 168370.\n",
      "Ep done - 168380.\n",
      "Ep done - 168390.\n",
      "Ep done - 168400.\n",
      "Ep done - 168410.\n",
      "Ep done - 168420.\n",
      "Ep done - 168430.\n",
      "Ep done - 168440.\n",
      "Ep done - 168450.\n",
      "Ep done - 168460.\n",
      "Ep done - 168470.\n",
      "Ep done - 168480.\n",
      "Ep done - 168490.\n",
      "Ep done - 168500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 787      |\n",
      "|    time_elapsed    | 17257    |\n",
      "|    total_timesteps | 4835328  |\n",
      "---------------------------------\n",
      "Ep done - 168510.\n",
      "Ep done - 168520.\n",
      "Ep done - 168530.\n",
      "Ep done - 168540.\n",
      "Ep done - 168550.\n",
      "Ep done - 168560.\n",
      "Ep done - 168570.\n",
      "Ep done - 168580.\n",
      "Ep done - 168590.\n",
      "Ep done - 168600.\n",
      "Ep done - 168610.\n",
      "Ep done - 168620.\n",
      "Ep done - 168630.\n",
      "Ep done - 168640.\n",
      "Ep done - 168650.\n",
      "Ep done - 48310.\n",
      "Ep done - 48320.\n",
      "Ep done - 48330.\n",
      "Ep done - 48340.\n",
      "Ep done - 48350.\n",
      "Ep done - 48360.\n",
      "Ep done - 48370.\n",
      "Ep done - 48380.\n",
      "Ep done - 48390.\n",
      "Ep done - 48400.\n",
      "Eval num_timesteps=4840000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4840000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47339845 |\n",
      "|    clip_fraction        | 0.0293     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.53e-05  |\n",
      "|    explained_variance   | 0.774      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.000239  |\n",
      "|    n_updates            | 27100      |\n",
      "|    policy_gradient_loss | -0.00144   |\n",
      "|    value_loss           | 0.00179    |\n",
      "----------------------------------------\n",
      "Ep done - 168660.\n",
      "Ep done - 168670.\n",
      "Ep done - 168680.\n",
      "Ep done - 168690.\n",
      "Ep done - 168700.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 788      |\n",
      "|    time_elapsed    | 17280    |\n",
      "|    total_timesteps | 4841472  |\n",
      "---------------------------------\n",
      "Ep done - 168710.\n",
      "Ep done - 168720.\n",
      "Ep done - 168730.\n",
      "Ep done - 168740.\n",
      "Ep done - 168750.\n",
      "Ep done - 168760.\n",
      "Ep done - 168770.\n",
      "Ep done - 168780.\n",
      "Ep done - 168790.\n",
      "Ep done - 168800.\n",
      "Ep done - 168810.\n",
      "Ep done - 168820.\n",
      "Ep done - 168830.\n",
      "Ep done - 168840.\n",
      "Ep done - 168850.\n",
      "Ep done - 168860.\n",
      "Ep done - 168870.\n",
      "Ep done - 168880.\n",
      "Ep done - 168890.\n",
      "Ep done - 168900.\n",
      "Ep done - 168910.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -1            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 280           |\n",
      "|    iterations           | 789           |\n",
      "|    time_elapsed         | 17296         |\n",
      "|    total_timesteps      | 4847616       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8805106e-10 |\n",
      "|    clip_fraction        | 1.63e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -4.2e-05      |\n",
      "|    explained_variance   | 0.382         |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.00182       |\n",
      "|    n_updates            | 27110         |\n",
      "|    policy_gradient_loss | 9.05e-07      |\n",
      "|    value_loss           | 0.00831       |\n",
      "-------------------------------------------\n",
      "Ep done - 168920.\n",
      "Ep done - 168930.\n",
      "Ep done - 168940.\n",
      "Ep done - 168950.\n",
      "Ep done - 168960.\n",
      "Ep done - 168970.\n",
      "Ep done - 168980.\n",
      "Ep done - 168990.\n",
      "Ep done - 48410.\n",
      "Ep done - 48420.\n",
      "Ep done - 48430.\n",
      "Ep done - 48440.\n",
      "Ep done - 48450.\n",
      "Ep done - 48460.\n",
      "Ep done - 48470.\n",
      "Ep done - 48480.\n",
      "Ep done - 48490.\n",
      "Ep done - 48500.\n",
      "Eval num_timesteps=4850000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -1           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4850000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048132897 |\n",
      "|    clip_fraction        | 0.000423     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000142    |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.000402     |\n",
      "|    n_updates            | 27120        |\n",
      "|    policy_gradient_loss | -9.51e-05    |\n",
      "|    value_loss           | 0.00108      |\n",
      "------------------------------------------\n",
      "Ep done - 169000.\n",
      "Ep done - 169010.\n",
      "Ep done - 169020.\n",
      "Ep done - 169030.\n",
      "Ep done - 169040.\n",
      "Ep done - 169050.\n",
      "Ep done - 169060.\n",
      "Ep done - 169070.\n",
      "Ep done - 169080.\n",
      "Ep done - 169090.\n",
      "Ep done - 169100.\n",
      "Ep done - 169110.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 790      |\n",
      "|    time_elapsed    | 17317    |\n",
      "|    total_timesteps | 4853760  |\n",
      "---------------------------------\n",
      "Ep done - 169120.\n",
      "Ep done - 169130.\n",
      "Ep done - 169140.\n",
      "Ep done - 169150.\n",
      "Ep done - 169160.\n",
      "Ep done - 169170.\n",
      "Ep done - 169180.\n",
      "Ep done - 169190.\n",
      "Ep done - 169200.\n",
      "Ep done - 169210.\n",
      "Ep done - 169220.\n",
      "Ep done - 169230.\n",
      "Ep done - 169240.\n",
      "Ep done - 169250.\n",
      "Ep done - 169260.\n",
      "Ep done - 169270.\n",
      "Ep done - 169280.\n",
      "Ep done - 169290.\n",
      "Ep done - 169300.\n",
      "Ep done - 169310.\n",
      "Ep done - 169320.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 791       |\n",
      "|    time_elapsed         | 17334     |\n",
      "|    total_timesteps      | 4859904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.57e-06 |\n",
      "|    explained_variance   | 0.808     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000539  |\n",
      "|    n_updates            | 27130     |\n",
      "|    policy_gradient_loss | 1.39e-10  |\n",
      "|    value_loss           | 0.0014    |\n",
      "---------------------------------------\n",
      "Ep done - 48510.\n",
      "Ep done - 48520.\n",
      "Ep done - 48530.\n",
      "Ep done - 48540.\n",
      "Ep done - 48550.\n",
      "Ep done - 48560.\n",
      "Ep done - 48570.\n",
      "Ep done - 48580.\n",
      "Ep done - 48590.\n",
      "Ep done - 48600.\n",
      "Eval num_timesteps=4860000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | -1          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4860000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007353811 |\n",
      "|    clip_fraction        | 0.00112     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000577   |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.00101     |\n",
      "|    n_updates            | 27140       |\n",
      "|    policy_gradient_loss | -0.000195   |\n",
      "|    value_loss           | 0.00156     |\n",
      "-----------------------------------------\n",
      "Ep done - 169330.\n",
      "Ep done - 169340.\n",
      "Ep done - 169350.\n",
      "Ep done - 169360.\n",
      "Ep done - 169370.\n",
      "Ep done - 169380.\n",
      "Ep done - 169390.\n",
      "Ep done - 169400.\n",
      "Ep done - 169410.\n",
      "Ep done - 169420.\n",
      "Ep done - 169430.\n",
      "Ep done - 169440.\n",
      "Ep done - 169450.\n",
      "Ep done - 169460.\n",
      "Ep done - 169470.\n",
      "Ep done - 169480.\n",
      "Ep done - 169490.\n",
      "Ep done - 169500.\n",
      "Ep done - 169510.\n",
      "Ep done - 169520.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 792      |\n",
      "|    time_elapsed    | 17356    |\n",
      "|    total_timesteps | 4866048  |\n",
      "---------------------------------\n",
      "Ep done - 169530.\n",
      "Ep done - 169540.\n",
      "Ep done - 169550.\n",
      "Ep done - 169560.\n",
      "Ep done - 169570.\n",
      "Ep done - 169580.\n",
      "Ep done - 169590.\n",
      "Ep done - 169600.\n",
      "Ep done - 169610.\n",
      "Ep done - 169620.\n",
      "Ep done - 169630.\n",
      "Ep done - 169640.\n",
      "Ep done - 169650.\n",
      "Ep done - 48610.\n",
      "Ep done - 48620.\n",
      "Ep done - 48630.\n",
      "Ep done - 48640.\n",
      "Ep done - 48650.\n",
      "Ep done - 48660.\n",
      "Ep done - 48670.\n",
      "Ep done - 48680.\n",
      "Ep done - 48690.\n",
      "Ep done - 48700.\n",
      "Eval num_timesteps=4870000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4870000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6122384 |\n",
      "|    clip_fraction        | 0.0353    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000125 |\n",
      "|    explained_variance   | 0.804     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0153    |\n",
      "|    n_updates            | 27150     |\n",
      "|    policy_gradient_loss | 0.0102    |\n",
      "|    value_loss           | 0.00157   |\n",
      "---------------------------------------\n",
      "Ep done - 169660.\n",
      "Ep done - 169670.\n",
      "Ep done - 169680.\n",
      "Ep done - 169690.\n",
      "Ep done - 169700.\n",
      "Ep done - 169710.\n",
      "Ep done - 169720.\n",
      "Ep done - 169730.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 793      |\n",
      "|    time_elapsed    | 17377    |\n",
      "|    total_timesteps | 4872192  |\n",
      "---------------------------------\n",
      "Ep done - 169740.\n",
      "Ep done - 169750.\n",
      "Ep done - 169760.\n",
      "Ep done - 169770.\n",
      "Ep done - 169780.\n",
      "Ep done - 169790.\n",
      "Ep done - 169800.\n",
      "Ep done - 169810.\n",
      "Ep done - 169820.\n",
      "Ep done - 169830.\n",
      "Ep done - 169840.\n",
      "Ep done - 169850.\n",
      "Ep done - 169860.\n",
      "Ep done - 169870.\n",
      "Ep done - 169880.\n",
      "Ep done - 169890.\n",
      "Ep done - 169900.\n",
      "Ep done - 169910.\n",
      "Ep done - 169920.\n",
      "Ep done - 169930.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 794       |\n",
      "|    time_elapsed         | 17394     |\n",
      "|    total_timesteps      | 4878336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5881285 |\n",
      "|    clip_fraction        | 0.00321   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.77e-05 |\n",
      "|    explained_variance   | 0.41      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0113    |\n",
      "|    n_updates            | 27160     |\n",
      "|    policy_gradient_loss | 0.000435  |\n",
      "|    value_loss           | 0.0073    |\n",
      "---------------------------------------\n",
      "Ep done - 169940.\n",
      "Ep done - 169950.\n",
      "Ep done - 169960.\n",
      "Ep done - 169970.\n",
      "Ep done - 169980.\n",
      "Ep done - 169990.\n",
      "Ep done - 48710.\n",
      "Ep done - 48720.\n",
      "Ep done - 48730.\n",
      "Ep done - 48740.\n",
      "Ep done - 48750.\n",
      "Ep done - 48760.\n",
      "Ep done - 48770.\n",
      "Ep done - 48780.\n",
      "Ep done - 48790.\n",
      "Ep done - 48800.\n",
      "Eval num_timesteps=4880000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4880000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6525176 |\n",
      "|    clip_fraction        | 0.0322    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000123 |\n",
      "|    explained_variance   | 0.817     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00243  |\n",
      "|    n_updates            | 27170     |\n",
      "|    policy_gradient_loss | -0.00161  |\n",
      "|    value_loss           | 0.00136   |\n",
      "---------------------------------------\n",
      "Ep done - 170000.\n",
      "Ep done - 170010.\n",
      "Ep done - 170020.\n",
      "Ep done - 170030.\n",
      "Ep done - 170040.\n",
      "Ep done - 170050.\n",
      "Ep done - 170060.\n",
      "Ep done - 170070.\n",
      "Ep done - 170080.\n",
      "Ep done - 170090.\n",
      "Ep done - 170100.\n",
      "Ep done - 170110.\n",
      "Ep done - 170120.\n",
      "Ep done - 170130.\n",
      "Ep done - 170140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 795      |\n",
      "|    time_elapsed    | 17415    |\n",
      "|    total_timesteps | 4884480  |\n",
      "---------------------------------\n",
      "Ep done - 170150.\n",
      "Ep done - 170160.\n",
      "Ep done - 170170.\n",
      "Ep done - 170180.\n",
      "Ep done - 170190.\n",
      "Ep done - 170200.\n",
      "Ep done - 170210.\n",
      "Ep done - 170220.\n",
      "Ep done - 170230.\n",
      "Ep done - 170240.\n",
      "Ep done - 170250.\n",
      "Ep done - 170260.\n",
      "Ep done - 170270.\n",
      "Ep done - 170280.\n",
      "Ep done - 170290.\n",
      "Ep done - 170300.\n",
      "Ep done - 170310.\n",
      "Ep done - 170320.\n",
      "Ep done - 48810.\n",
      "Ep done - 48820.\n",
      "Ep done - 48830.\n",
      "Ep done - 48840.\n",
      "Ep done - 48850.\n",
      "Ep done - 48860.\n",
      "Ep done - 48870.\n",
      "Ep done - 48880.\n",
      "Ep done - 48890.\n",
      "Ep done - 48900.\n",
      "Eval num_timesteps=4890000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4890000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.35e-06 |\n",
      "|    explained_variance   | 0.826     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000863  |\n",
      "|    n_updates            | 27180     |\n",
      "|    policy_gradient_loss | -1.64e-10 |\n",
      "|    value_loss           | 0.00145   |\n",
      "---------------------------------------\n",
      "Ep done - 170330.\n",
      "Ep done - 170340.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 796      |\n",
      "|    time_elapsed    | 17436    |\n",
      "|    total_timesteps | 4890624  |\n",
      "---------------------------------\n",
      "Ep done - 170350.\n",
      "Ep done - 170360.\n",
      "Ep done - 170370.\n",
      "Ep done - 170380.\n",
      "Ep done - 170390.\n",
      "Ep done - 170400.\n",
      "Ep done - 170410.\n",
      "Ep done - 170420.\n",
      "Ep done - 170430.\n",
      "Ep done - 170440.\n",
      "Ep done - 170450.\n",
      "Ep done - 170460.\n",
      "Ep done - 170470.\n",
      "Ep done - 170480.\n",
      "Ep done - 170490.\n",
      "Ep done - 170500.\n",
      "Ep done - 170510.\n",
      "Ep done - 170520.\n",
      "Ep done - 170530.\n",
      "Ep done - 170540.\n",
      "Ep done - 170550.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 797       |\n",
      "|    time_elapsed         | 17453     |\n",
      "|    total_timesteps      | 4896768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.42e-06 |\n",
      "|    explained_variance   | 0.838     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000516  |\n",
      "|    n_updates            | 27190     |\n",
      "|    policy_gradient_loss | 7.79e-10  |\n",
      "|    value_loss           | 0.00134   |\n",
      "---------------------------------------\n",
      "Ep done - 170560.\n",
      "Ep done - 170570.\n",
      "Ep done - 170580.\n",
      "Ep done - 170590.\n",
      "Ep done - 170600.\n",
      "Ep done - 170610.\n",
      "Ep done - 170620.\n",
      "Ep done - 170630.\n",
      "Ep done - 170640.\n",
      "Ep done - 170650.\n",
      "Ep done - 48910.\n",
      "Ep done - 48920.\n",
      "Ep done - 48930.\n",
      "Ep done - 48940.\n",
      "Ep done - 48950.\n",
      "Ep done - 48960.\n",
      "Ep done - 48970.\n",
      "Ep done - 48980.\n",
      "Ep done - 48990.\n",
      "Ep done - 49000.\n",
      "Eval num_timesteps=4900000, episode_reward=-0.99 +/- 0.10\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.99     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4900000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7810354 |\n",
      "|    clip_fraction        | 0.0234    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.17e-05 |\n",
      "|    explained_variance   | 0.835     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0209   |\n",
      "|    n_updates            | 27200     |\n",
      "|    policy_gradient_loss | -0.00918  |\n",
      "|    value_loss           | 0.00137   |\n",
      "---------------------------------------\n",
      "Ep done - 170660.\n",
      "Ep done - 170670.\n",
      "Ep done - 170680.\n",
      "Ep done - 170690.\n",
      "Ep done - 170700.\n",
      "Ep done - 170710.\n",
      "Ep done - 170720.\n",
      "Ep done - 170730.\n",
      "Ep done - 170740.\n",
      "Ep done - 170750.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 798      |\n",
      "|    time_elapsed    | 17475    |\n",
      "|    total_timesteps | 4902912  |\n",
      "---------------------------------\n",
      "Ep done - 170760.\n",
      "Ep done - 170770.\n",
      "Ep done - 170780.\n",
      "Ep done - 170790.\n",
      "Ep done - 170800.\n",
      "Ep done - 170810.\n",
      "Ep done - 170820.\n",
      "Ep done - 170830.\n",
      "Ep done - 170840.\n",
      "Ep done - 170850.\n",
      "Ep done - 170860.\n",
      "Ep done - 170870.\n",
      "Ep done - 170880.\n",
      "Ep done - 170890.\n",
      "Ep done - 170900.\n",
      "Ep done - 170910.\n",
      "Ep done - 170920.\n",
      "Ep done - 170930.\n",
      "Ep done - 170940.\n",
      "Ep done - 170950.\n",
      "Ep done - 170960.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 799       |\n",
      "|    time_elapsed         | 17494     |\n",
      "|    total_timesteps      | 4909056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2222134 |\n",
      "|    clip_fraction        | 0.0673    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00248  |\n",
      "|    explained_variance   | 0.443     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00407  |\n",
      "|    n_updates            | 27210     |\n",
      "|    policy_gradient_loss | -0.00812  |\n",
      "|    value_loss           | 0.00772   |\n",
      "---------------------------------------\n",
      "Ep done - 170970.\n",
      "Ep done - 170980.\n",
      "Ep done - 170990.\n",
      "Ep done - 49010.\n",
      "Ep done - 49020.\n",
      "Ep done - 49030.\n",
      "Ep done - 49040.\n",
      "Ep done - 49050.\n",
      "Ep done - 49060.\n",
      "Ep done - 49070.\n",
      "Ep done - 49080.\n",
      "Ep done - 49090.\n",
      "Ep done - 49100.\n",
      "Eval num_timesteps=4910000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -1       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 4910000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.830157 |\n",
      "|    clip_fraction        | 0.121    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.00163 |\n",
      "|    explained_variance   | 0.806    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | -0.0245  |\n",
      "|    n_updates            | 27220    |\n",
      "|    policy_gradient_loss | -0.0222  |\n",
      "|    value_loss           | 0.00166  |\n",
      "--------------------------------------\n",
      "Ep done - 171000.\n",
      "Ep done - 171010.\n",
      "Ep done - 171020.\n",
      "Ep done - 171030.\n",
      "Ep done - 171040.\n",
      "Ep done - 171050.\n",
      "Ep done - 171060.\n",
      "Ep done - 171070.\n",
      "Ep done - 171080.\n",
      "Ep done - 171090.\n",
      "Ep done - 171100.\n",
      "Ep done - 171110.\n",
      "Ep done - 171120.\n",
      "Ep done - 171130.\n",
      "Ep done - 171140.\n",
      "Ep done - 171150.\n",
      "Ep done - 171160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 800      |\n",
      "|    time_elapsed    | 17515    |\n",
      "|    total_timesteps | 4915200  |\n",
      "---------------------------------\n",
      "Ep done - 171170.\n",
      "Ep done - 171180.\n",
      "Ep done - 171190.\n",
      "Ep done - 171200.\n",
      "Ep done - 171210.\n",
      "Ep done - 171220.\n",
      "Ep done - 171230.\n",
      "Ep done - 171240.\n",
      "Ep done - 171250.\n",
      "Ep done - 171260.\n",
      "Ep done - 171270.\n",
      "Ep done - 171280.\n",
      "Ep done - 171290.\n",
      "Ep done - 171300.\n",
      "Ep done - 171310.\n",
      "Ep done - 171320.\n",
      "Ep done - 49110.\n",
      "Ep done - 49120.\n",
      "Ep done - 49130.\n",
      "Ep done - 49140.\n",
      "Ep done - 49150.\n",
      "Ep done - 49160.\n",
      "Ep done - 49170.\n",
      "Ep done - 49180.\n",
      "Ep done - 49190.\n",
      "Ep done - 49200.\n",
      "Eval num_timesteps=4920000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.01 +/- 0.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4920000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41397607 |\n",
      "|    clip_fraction        | 0.0335     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000154  |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0283     |\n",
      "|    n_updates            | 27230      |\n",
      "|    policy_gradient_loss | 0.0235     |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "Ep done - 171330.\n",
      "Ep done - 171340.\n",
      "Ep done - 171350.\n",
      "Ep done - 171360.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 801      |\n",
      "|    time_elapsed    | 17537    |\n",
      "|    total_timesteps | 4921344  |\n",
      "---------------------------------\n",
      "Ep done - 171370.\n",
      "Ep done - 171380.\n",
      "Ep done - 171390.\n",
      "Ep done - 171400.\n",
      "Ep done - 171410.\n",
      "Ep done - 171420.\n",
      "Ep done - 171430.\n",
      "Ep done - 171440.\n",
      "Ep done - 171450.\n",
      "Ep done - 171460.\n",
      "Ep done - 171470.\n",
      "Ep done - 171480.\n",
      "Ep done - 171490.\n",
      "Ep done - 171500.\n",
      "Ep done - 171510.\n",
      "Ep done - 171520.\n",
      "Ep done - 171530.\n",
      "Ep done - 171540.\n",
      "Ep done - 171550.\n",
      "Ep done - 171560.\n",
      "Ep done - 171570.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 802        |\n",
      "|    time_elapsed         | 17553      |\n",
      "|    total_timesteps      | 4927488    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66544867 |\n",
      "|    clip_fraction        | 0.0278     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000122  |\n",
      "|    explained_variance   | 0.475      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.0101    |\n",
      "|    n_updates            | 27240      |\n",
      "|    policy_gradient_loss | -0.00233   |\n",
      "|    value_loss           | 0.00744    |\n",
      "----------------------------------------\n",
      "Ep done - 171580.\n",
      "Ep done - 171590.\n",
      "Ep done - 171600.\n",
      "Ep done - 171610.\n",
      "Ep done - 171620.\n",
      "Ep done - 171630.\n",
      "Ep done - 171640.\n",
      "Ep done - 171650.\n",
      "Ep done - 49210.\n",
      "Ep done - 49220.\n",
      "Ep done - 49230.\n",
      "Ep done - 49240.\n",
      "Ep done - 49250.\n",
      "Ep done - 49260.\n",
      "Ep done - 49270.\n",
      "Ep done - 49280.\n",
      "Ep done - 49290.\n",
      "Ep done - 49300.\n",
      "Eval num_timesteps=4930000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4930000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2513629 |\n",
      "|    clip_fraction        | 0.0645    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000136 |\n",
      "|    explained_variance   | 0.733     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00641  |\n",
      "|    n_updates            | 27250     |\n",
      "|    policy_gradient_loss | 0.000148  |\n",
      "|    value_loss           | 0.00228   |\n",
      "---------------------------------------\n",
      "Ep done - 171660.\n",
      "Ep done - 171670.\n",
      "Ep done - 171680.\n",
      "Ep done - 171690.\n",
      "Ep done - 171700.\n",
      "Ep done - 171710.\n",
      "Ep done - 171720.\n",
      "Ep done - 171730.\n",
      "Ep done - 171740.\n",
      "Ep done - 171750.\n",
      "Ep done - 171760.\n",
      "Ep done - 171770.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 803      |\n",
      "|    time_elapsed    | 17576    |\n",
      "|    total_timesteps | 4933632  |\n",
      "---------------------------------\n",
      "Ep done - 171780.\n",
      "Ep done - 171790.\n",
      "Ep done - 171800.\n",
      "Ep done - 171810.\n",
      "Ep done - 171820.\n",
      "Ep done - 171830.\n",
      "Ep done - 171840.\n",
      "Ep done - 171850.\n",
      "Ep done - 171860.\n",
      "Ep done - 171870.\n",
      "Ep done - 171880.\n",
      "Ep done - 171890.\n",
      "Ep done - 171900.\n",
      "Ep done - 171910.\n",
      "Ep done - 171920.\n",
      "Ep done - 171930.\n",
      "Ep done - 171940.\n",
      "Ep done - 171950.\n",
      "Ep done - 171960.\n",
      "Ep done - 171970.\n",
      "Ep done - 171980.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 804       |\n",
      "|    time_elapsed         | 17592     |\n",
      "|    total_timesteps      | 4939776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6735718 |\n",
      "|    clip_fraction        | 0.0294    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000124 |\n",
      "|    explained_variance   | 0.604     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.000811 |\n",
      "|    n_updates            | 27260     |\n",
      "|    policy_gradient_loss | -0.00564  |\n",
      "|    value_loss           | 0.00263   |\n",
      "---------------------------------------\n",
      "Ep done - 171990.\n",
      "Ep done - 49310.\n",
      "Ep done - 49320.\n",
      "Ep done - 49330.\n",
      "Ep done - 49340.\n",
      "Ep done - 49350.\n",
      "Ep done - 49360.\n",
      "Ep done - 49370.\n",
      "Ep done - 49380.\n",
      "Ep done - 49390.\n",
      "Ep done - 49400.\n",
      "Eval num_timesteps=4940000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -1           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4940000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.880511e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39e-05    |\n",
      "|    explained_variance   | 0.714        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00159      |\n",
      "|    n_updates            | 27270        |\n",
      "|    policy_gradient_loss | -5.44e-08    |\n",
      "|    value_loss           | 0.00147      |\n",
      "------------------------------------------\n",
      "Ep done - 172000.\n",
      "Ep done - 172010.\n",
      "Ep done - 172020.\n",
      "Ep done - 172030.\n",
      "Ep done - 172040.\n",
      "Ep done - 172050.\n",
      "Ep done - 172060.\n",
      "Ep done - 172070.\n",
      "Ep done - 172080.\n",
      "Ep done - 172090.\n",
      "Ep done - 172100.\n",
      "Ep done - 172110.\n",
      "Ep done - 172120.\n",
      "Ep done - 172130.\n",
      "Ep done - 172140.\n",
      "Ep done - 172150.\n",
      "Ep done - 172160.\n",
      "Ep done - 172170.\n",
      "Ep done - 172180.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 805      |\n",
      "|    time_elapsed    | 17613    |\n",
      "|    total_timesteps | 4945920  |\n",
      "---------------------------------\n",
      "Ep done - 172190.\n",
      "Ep done - 172200.\n",
      "Ep done - 172210.\n",
      "Ep done - 172220.\n",
      "Ep done - 172230.\n",
      "Ep done - 172240.\n",
      "Ep done - 172250.\n",
      "Ep done - 172260.\n",
      "Ep done - 172270.\n",
      "Ep done - 172280.\n",
      "Ep done - 172290.\n",
      "Ep done - 172300.\n",
      "Ep done - 172310.\n",
      "Ep done - 172320.\n",
      "Ep done - 49410.\n",
      "Ep done - 49420.\n",
      "Ep done - 49430.\n",
      "Ep done - 49440.\n",
      "Ep done - 49450.\n",
      "Ep done - 49460.\n",
      "Ep done - 49470.\n",
      "Ep done - 49480.\n",
      "Ep done - 49490.\n",
      "Ep done - 49500.\n",
      "Eval num_timesteps=4950000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4950000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.392436  |\n",
      "|    clip_fraction        | 0.0281    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000208 |\n",
      "|    explained_variance   | 0.851     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00428  |\n",
      "|    n_updates            | 27280     |\n",
      "|    policy_gradient_loss | -0.00223  |\n",
      "|    value_loss           | 0.00134   |\n",
      "---------------------------------------\n",
      "Ep done - 172330.\n",
      "Ep done - 172340.\n",
      "Ep done - 172350.\n",
      "Ep done - 172360.\n",
      "Ep done - 172370.\n",
      "Ep done - 172380.\n",
      "Ep done - 172390.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 806      |\n",
      "|    time_elapsed    | 17634    |\n",
      "|    total_timesteps | 4952064  |\n",
      "---------------------------------\n",
      "Ep done - 172400.\n",
      "Ep done - 172410.\n",
      "Ep done - 172420.\n",
      "Ep done - 172430.\n",
      "Ep done - 172440.\n",
      "Ep done - 172450.\n",
      "Ep done - 172460.\n",
      "Ep done - 172470.\n",
      "Ep done - 172480.\n",
      "Ep done - 172490.\n",
      "Ep done - 172500.\n",
      "Ep done - 172510.\n",
      "Ep done - 172520.\n",
      "Ep done - 172530.\n",
      "Ep done - 172540.\n",
      "Ep done - 172550.\n",
      "Ep done - 172560.\n",
      "Ep done - 172570.\n",
      "Ep done - 172580.\n",
      "Ep done - 172590.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 807       |\n",
      "|    time_elapsed         | 17656     |\n",
      "|    total_timesteps      | 4958208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.5782254 |\n",
      "|    clip_fraction        | 0.0684    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000238 |\n",
      "|    explained_variance   | 0.837     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0165   |\n",
      "|    n_updates            | 27290     |\n",
      "|    policy_gradient_loss | -0.0123   |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "Ep done - 172600.\n",
      "Ep done - 172610.\n",
      "Ep done - 172620.\n",
      "Ep done - 172630.\n",
      "Ep done - 172640.\n",
      "Ep done - 172650.\n",
      "Ep done - 49510.\n",
      "Ep done - 49520.\n",
      "Ep done - 49530.\n",
      "Ep done - 49540.\n",
      "Ep done - 49550.\n",
      "Ep done - 49560.\n",
      "Ep done - 49570.\n",
      "Ep done - 49580.\n",
      "Ep done - 49590.\n",
      "Ep done - 49600.\n",
      "Eval num_timesteps=4960000, episode_reward=-0.92 +/- 0.27\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.92     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4960000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6369437 |\n",
      "|    clip_fraction        | 0.0382    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000101 |\n",
      "|    explained_variance   | 0.445     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00277  |\n",
      "|    n_updates            | 27300     |\n",
      "|    policy_gradient_loss | 0.0467    |\n",
      "|    value_loss           | 0.00698   |\n",
      "---------------------------------------\n",
      "Ep done - 172660.\n",
      "Ep done - 172670.\n",
      "Ep done - 172680.\n",
      "Ep done - 172690.\n",
      "Ep done - 172700.\n",
      "Ep done - 172710.\n",
      "Ep done - 172720.\n",
      "Ep done - 172730.\n",
      "Ep done - 172740.\n",
      "Ep done - 172750.\n",
      "Ep done - 172760.\n",
      "Ep done - 172770.\n",
      "Ep done - 172780.\n",
      "Ep done - 172790.\n",
      "Ep done - 172800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.92    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 808      |\n",
      "|    time_elapsed    | 17679    |\n",
      "|    total_timesteps | 4964352  |\n",
      "---------------------------------\n",
      "Ep done - 172810.\n",
      "Ep done - 172820.\n",
      "Ep done - 172830.\n",
      "Ep done - 172840.\n",
      "Ep done - 172850.\n",
      "Ep done - 172860.\n",
      "Ep done - 172870.\n",
      "Ep done - 172880.\n",
      "Ep done - 172890.\n",
      "Ep done - 172900.\n",
      "Ep done - 172910.\n",
      "Ep done - 172920.\n",
      "Ep done - 172930.\n",
      "Ep done - 172940.\n",
      "Ep done - 172950.\n",
      "Ep done - 172960.\n",
      "Ep done - 172970.\n",
      "Ep done - 172980.\n",
      "Ep done - 172990.\n",
      "Ep done - 49610.\n",
      "Ep done - 49620.\n",
      "Ep done - 49630.\n",
      "Ep done - 49640.\n",
      "Ep done - 49650.\n",
      "Ep done - 49660.\n",
      "Ep done - 49670.\n",
      "Ep done - 49680.\n",
      "Ep done - 49690.\n",
      "Ep done - 49700.\n",
      "Eval num_timesteps=4970000, episode_reward=-0.99 +/- 0.10\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.99      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4970000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87139606 |\n",
      "|    clip_fraction        | 0.0232     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.64e-05  |\n",
      "|    explained_variance   | 0.346      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00506    |\n",
      "|    n_updates            | 27310      |\n",
      "|    policy_gradient_loss | 0.000791   |\n",
      "|    value_loss           | 0.0134     |\n",
      "----------------------------------------\n",
      "Ep done - 173000.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 809      |\n",
      "|    time_elapsed    | 17702    |\n",
      "|    total_timesteps | 4970496  |\n",
      "---------------------------------\n",
      "Ep done - 173010.\n",
      "Ep done - 173020.\n",
      "Ep done - 173030.\n",
      "Ep done - 173040.\n",
      "Ep done - 173050.\n",
      "Ep done - 173060.\n",
      "Ep done - 173070.\n",
      "Ep done - 173080.\n",
      "Ep done - 173090.\n",
      "Ep done - 173100.\n",
      "Ep done - 173110.\n",
      "Ep done - 173120.\n",
      "Ep done - 173130.\n",
      "Ep done - 173140.\n",
      "Ep done - 173150.\n",
      "Ep done - 173160.\n",
      "Ep done - 173170.\n",
      "Ep done - 173180.\n",
      "Ep done - 173190.\n",
      "Ep done - 173200.\n",
      "Ep done - 173210.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 810        |\n",
      "|    time_elapsed         | 17720      |\n",
      "|    total_timesteps      | 4976640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88031656 |\n",
      "|    clip_fraction        | 0.0303     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000334  |\n",
      "|    explained_variance   | 0.196      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00563   |\n",
      "|    n_updates            | 27320      |\n",
      "|    policy_gradient_loss | -0.000287  |\n",
      "|    value_loss           | 0.0137     |\n",
      "----------------------------------------\n",
      "Ep done - 173220.\n",
      "Ep done - 173230.\n",
      "Ep done - 173240.\n",
      "Ep done - 173250.\n",
      "Ep done - 173260.\n",
      "Ep done - 173270.\n",
      "Ep done - 173280.\n",
      "Ep done - 173290.\n",
      "Ep done - 173300.\n",
      "Ep done - 173310.\n",
      "Ep done - 173320.\n",
      "Ep done - 49710.\n",
      "Ep done - 49720.\n",
      "Ep done - 49730.\n",
      "Ep done - 49740.\n",
      "Ep done - 49750.\n",
      "Ep done - 49760.\n",
      "Ep done - 49770.\n",
      "Ep done - 49780.\n",
      "Ep done - 49790.\n",
      "Ep done - 49800.\n",
      "Eval num_timesteps=4980000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.95 +/- 0.22\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.9      |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4980000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.3898926 |\n",
      "|    clip_fraction        | 0.0893    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000327 |\n",
      "|    explained_variance   | 0.848     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0282   |\n",
      "|    n_updates            | 27330     |\n",
      "|    policy_gradient_loss | -0.0149   |\n",
      "|    value_loss           | 0.00157   |\n",
      "---------------------------------------\n",
      "Ep done - 173330.\n",
      "Ep done - 173340.\n",
      "Ep done - 173350.\n",
      "Ep done - 173360.\n",
      "Ep done - 173370.\n",
      "Ep done - 173380.\n",
      "Ep done - 173390.\n",
      "Ep done - 173400.\n",
      "Ep done - 173410.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.9     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 811      |\n",
      "|    time_elapsed    | 17741    |\n",
      "|    total_timesteps | 4982784  |\n",
      "---------------------------------\n",
      "Ep done - 173420.\n",
      "Ep done - 173430.\n",
      "Ep done - 173440.\n",
      "Ep done - 173450.\n",
      "Ep done - 173460.\n",
      "Ep done - 173470.\n",
      "Ep done - 173480.\n",
      "Ep done - 173490.\n",
      "Ep done - 173500.\n",
      "Ep done - 173510.\n",
      "Ep done - 173520.\n",
      "Ep done - 173530.\n",
      "Ep done - 173540.\n",
      "Ep done - 173550.\n",
      "Ep done - 173560.\n",
      "Ep done - 173570.\n",
      "Ep done - 173580.\n",
      "Ep done - 173590.\n",
      "Ep done - 173600.\n",
      "Ep done - 173610.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 280       |\n",
      "|    iterations           | 812       |\n",
      "|    time_elapsed         | 17755     |\n",
      "|    total_timesteps      | 4988928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6186448 |\n",
      "|    clip_fraction        | 0.0085    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.58e-05 |\n",
      "|    explained_variance   | 0.768     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00446  |\n",
      "|    n_updates            | 27340     |\n",
      "|    policy_gradient_loss | -0.000655 |\n",
      "|    value_loss           | 0.00185   |\n",
      "---------------------------------------\n",
      "Ep done - 173620.\n",
      "Ep done - 173630.\n",
      "Ep done - 173640.\n",
      "Ep done - 173650.\n",
      "Ep done - 49810.\n",
      "Ep done - 49820.\n",
      "Ep done - 49830.\n",
      "Ep done - 49840.\n",
      "Ep done - 49850.\n",
      "Ep done - 49860.\n",
      "Ep done - 49870.\n",
      "Ep done - 49880.\n",
      "Ep done - 49890.\n",
      "Ep done - 49900.\n",
      "Eval num_timesteps=4990000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4990000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94667274 |\n",
      "|    clip_fraction        | 0.0207     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.89e-05  |\n",
      "|    explained_variance   | 0.698      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00316   |\n",
      "|    n_updates            | 27350      |\n",
      "|    policy_gradient_loss | -0.00289   |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "Ep done - 173660.\n",
      "Ep done - 173670.\n",
      "Ep done - 173680.\n",
      "Ep done - 173690.\n",
      "Ep done - 173700.\n",
      "Ep done - 173710.\n",
      "Ep done - 173720.\n",
      "Ep done - 173730.\n",
      "Ep done - 173740.\n",
      "Ep done - 173750.\n",
      "Ep done - 173760.\n",
      "Ep done - 173770.\n",
      "Ep done - 173780.\n",
      "Ep done - 173790.\n",
      "Ep done - 173800.\n",
      "Ep done - 173810.\n",
      "Ep done - 173820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 813      |\n",
      "|    time_elapsed    | 17774    |\n",
      "|    total_timesteps | 4995072  |\n",
      "---------------------------------\n",
      "Ep done - 173830.\n",
      "Ep done - 173840.\n",
      "Ep done - 173850.\n",
      "Ep done - 173860.\n",
      "Ep done - 173870.\n",
      "Ep done - 173880.\n",
      "Ep done - 173890.\n",
      "Ep done - 173900.\n",
      "Ep done - 173910.\n",
      "Ep done - 173920.\n",
      "Ep done - 173930.\n",
      "Ep done - 173940.\n",
      "Ep done - 173950.\n",
      "Ep done - 173960.\n",
      "Ep done - 173970.\n",
      "Ep done - 173980.\n",
      "Ep done - 49910.\n",
      "Ep done - 49920.\n",
      "Ep done - 49930.\n",
      "Ep done - 49940.\n",
      "Ep done - 49950.\n",
      "Ep done - 49960.\n",
      "Ep done - 49970.\n",
      "Ep done - 49980.\n",
      "Ep done - 49990.\n",
      "Ep done - 50000.\n",
      "Eval num_timesteps=5000000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5000000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.55e-06 |\n",
      "|    explained_variance   | 0.841     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00114   |\n",
      "|    n_updates            | 27360     |\n",
      "|    policy_gradient_loss | -4.84e-10 |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "Ep done - 173990.\n",
      "Ep done - 174000.\n",
      "Ep done - 174010.\n",
      "Ep done - 174020.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 814      |\n",
      "|    time_elapsed    | 17792    |\n",
      "|    total_timesteps | 5001216  |\n",
      "---------------------------------\n",
      "Ep done - 174030.\n",
      "Ep done - 174040.\n",
      "Ep done - 174050.\n",
      "Ep done - 174060.\n",
      "Ep done - 174070.\n",
      "Ep done - 174080.\n",
      "Ep done - 174090.\n",
      "Ep done - 174100.\n",
      "Ep done - 174110.\n",
      "Ep done - 174120.\n",
      "Ep done - 174130.\n",
      "Ep done - 174140.\n",
      "Ep done - 174150.\n",
      "Ep done - 174160.\n",
      "Ep done - 174170.\n",
      "Ep done - 174180.\n",
      "Ep done - 174190.\n",
      "Ep done - 174200.\n",
      "Ep done - 174210.\n",
      "Ep done - 174220.\n",
      "Ep done - 174230.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -1            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 281           |\n",
      "|    iterations           | 815           |\n",
      "|    time_elapsed         | 17806         |\n",
      "|    total_timesteps      | 5007360       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022673694 |\n",
      "|    clip_fraction        | 0.000114      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -5.35e-05     |\n",
      "|    explained_variance   | 0.855         |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.000563      |\n",
      "|    n_updates            | 27370         |\n",
      "|    policy_gradient_loss | -7.15e-06     |\n",
      "|    value_loss           | 0.00141       |\n",
      "-------------------------------------------\n",
      "Ep done - 174240.\n",
      "Ep done - 174250.\n",
      "Ep done - 174260.\n",
      "Ep done - 174270.\n",
      "Ep done - 174280.\n",
      "Ep done - 174290.\n",
      "Ep done - 174300.\n",
      "Ep done - 174310.\n",
      "Ep done - 50010.\n",
      "Ep done - 50020.\n",
      "Ep done - 50030.\n",
      "Ep done - 50040.\n",
      "Ep done - 50050.\n",
      "Ep done - 50060.\n",
      "Ep done - 50070.\n",
      "Ep done - 50080.\n",
      "Ep done - 50090.\n",
      "Ep done - 50100.\n",
      "Eval num_timesteps=5010000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5010000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70164484 |\n",
      "|    clip_fraction        | 0.0313     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.12e-05  |\n",
      "|    explained_variance   | 0.834      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.0129    |\n",
      "|    n_updates            | 27380      |\n",
      "|    policy_gradient_loss | -0.00377   |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "Ep done - 174320.\n",
      "Ep done - 174330.\n",
      "Ep done - 174340.\n",
      "Ep done - 174350.\n",
      "Ep done - 174360.\n",
      "Ep done - 174370.\n",
      "Ep done - 174380.\n",
      "Ep done - 174390.\n",
      "Ep done - 174400.\n",
      "Ep done - 174410.\n",
      "Ep done - 174420.\n",
      "Ep done - 174430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 816      |\n",
      "|    time_elapsed    | 17824    |\n",
      "|    total_timesteps | 5013504  |\n",
      "---------------------------------\n",
      "Ep done - 174440.\n",
      "Ep done - 174450.\n",
      "Ep done - 174460.\n",
      "Ep done - 174470.\n",
      "Ep done - 174480.\n",
      "Ep done - 174490.\n",
      "Ep done - 174500.\n",
      "Ep done - 174510.\n",
      "Ep done - 174520.\n",
      "Ep done - 174530.\n",
      "Ep done - 174540.\n",
      "Ep done - 174550.\n",
      "Ep done - 174560.\n",
      "Ep done - 174570.\n",
      "Ep done - 174580.\n",
      "Ep done - 174590.\n",
      "Ep done - 174600.\n",
      "Ep done - 174610.\n",
      "Ep done - 174620.\n",
      "Ep done - 174630.\n",
      "Ep done - 174640.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -1          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 817         |\n",
      "|    time_elapsed         | 17838       |\n",
      "|    total_timesteps      | 5019648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008835667 |\n",
      "|    clip_fraction        | 0.00786     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000846   |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.000949    |\n",
      "|    n_updates            | 27390       |\n",
      "|    policy_gradient_loss | 0.00632     |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "Ep done - 174650.\n",
      "Ep done - 50110.\n",
      "Ep done - 50120.\n",
      "Ep done - 50130.\n",
      "Ep done - 50140.\n",
      "Ep done - 50150.\n",
      "Ep done - 50160.\n",
      "Ep done - 50170.\n",
      "Ep done - 50180.\n",
      "Ep done - 50190.\n",
      "Ep done - 50200.\n",
      "Eval num_timesteps=5020000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -1       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 5020000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.2e-05 |\n",
      "|    explained_variance   | 0.857    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.000406 |\n",
      "|    n_updates            | 27400    |\n",
      "|    policy_gradient_loss | 8.52e-10 |\n",
      "|    value_loss           | 0.00116  |\n",
      "--------------------------------------\n",
      "Ep done - 174660.\n",
      "Ep done - 174670.\n",
      "Ep done - 174680.\n",
      "Ep done - 174690.\n",
      "Ep done - 174700.\n",
      "Ep done - 174710.\n",
      "Ep done - 174720.\n",
      "Ep done - 174730.\n",
      "Ep done - 174740.\n",
      "Ep done - 174750.\n",
      "Ep done - 174760.\n",
      "Ep done - 174770.\n",
      "Ep done - 174780.\n",
      "Ep done - 174790.\n",
      "Ep done - 174800.\n",
      "Ep done - 174810.\n",
      "Ep done - 174820.\n",
      "Ep done - 174830.\n",
      "Ep done - 174840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 818      |\n",
      "|    time_elapsed    | 17857    |\n",
      "|    total_timesteps | 5025792  |\n",
      "---------------------------------\n",
      "Ep done - 174850.\n",
      "Ep done - 174860.\n",
      "Ep done - 174870.\n",
      "Ep done - 174880.\n",
      "Ep done - 174890.\n",
      "Ep done - 174900.\n",
      "Ep done - 174910.\n",
      "Ep done - 174920.\n",
      "Ep done - 174930.\n",
      "Ep done - 174940.\n",
      "Ep done - 174950.\n",
      "Ep done - 174960.\n",
      "Ep done - 174970.\n",
      "Ep done - 174980.\n",
      "Ep done - 50210.\n",
      "Ep done - 50220.\n",
      "Ep done - 50230.\n",
      "Ep done - 50240.\n",
      "Ep done - 50250.\n",
      "Ep done - 50260.\n",
      "Ep done - 50270.\n",
      "Ep done - 50280.\n",
      "Ep done - 50290.\n",
      "Ep done - 50300.\n",
      "Eval num_timesteps=5030000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5030000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2886735 |\n",
      "|    clip_fraction        | 0.00337   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000147 |\n",
      "|    explained_variance   | 0.856     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00163  |\n",
      "|    n_updates            | 27410     |\n",
      "|    policy_gradient_loss | -0.000153 |\n",
      "|    value_loss           | 0.00106   |\n",
      "---------------------------------------\n",
      "Ep done - 174990.\n",
      "Ep done - 175000.\n",
      "Ep done - 175010.\n",
      "Ep done - 175020.\n",
      "Ep done - 175030.\n",
      "Ep done - 175040.\n",
      "Ep done - 175050.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 819      |\n",
      "|    time_elapsed    | 17875    |\n",
      "|    total_timesteps | 5031936  |\n",
      "---------------------------------\n",
      "Ep done - 175060.\n",
      "Ep done - 175070.\n",
      "Ep done - 175080.\n",
      "Ep done - 175090.\n",
      "Ep done - 175100.\n",
      "Ep done - 175110.\n",
      "Ep done - 175120.\n",
      "Ep done - 175130.\n",
      "Ep done - 175140.\n",
      "Ep done - 175150.\n",
      "Ep done - 175160.\n",
      "Ep done - 175170.\n",
      "Ep done - 175180.\n",
      "Ep done - 175190.\n",
      "Ep done - 175200.\n",
      "Ep done - 175210.\n",
      "Ep done - 175220.\n",
      "Ep done - 175230.\n",
      "Ep done - 175240.\n",
      "Ep done - 175250.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 820       |\n",
      "|    time_elapsed         | 17889     |\n",
      "|    total_timesteps      | 5038080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.76e-06 |\n",
      "|    explained_variance   | 0.845     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000534  |\n",
      "|    n_updates            | 27420     |\n",
      "|    policy_gradient_loss | -1.09e-08 |\n",
      "|    value_loss           | 0.00134   |\n",
      "---------------------------------------\n",
      "Ep done - 175260.\n",
      "Ep done - 175270.\n",
      "Ep done - 175280.\n",
      "Ep done - 175290.\n",
      "Ep done - 175300.\n",
      "Ep done - 175310.\n",
      "Ep done - 50310.\n",
      "Ep done - 50320.\n",
      "Ep done - 50330.\n",
      "Ep done - 50340.\n",
      "Ep done - 50350.\n",
      "Ep done - 50360.\n",
      "Ep done - 50370.\n",
      "Ep done - 50380.\n",
      "Ep done - 50390.\n",
      "Ep done - 50400.\n",
      "Eval num_timesteps=5040000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5040000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5301152 |\n",
      "|    clip_fraction        | 0.0133    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000119 |\n",
      "|    explained_variance   | 0.853     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.000273 |\n",
      "|    n_updates            | 27430     |\n",
      "|    policy_gradient_loss | -0.000115 |\n",
      "|    value_loss           | 0.00141   |\n",
      "---------------------------------------\n",
      "Ep done - 175320.\n",
      "Ep done - 175330.\n",
      "Ep done - 175340.\n",
      "Ep done - 175350.\n",
      "Ep done - 175360.\n",
      "Ep done - 175370.\n",
      "Ep done - 175380.\n",
      "Ep done - 175390.\n",
      "Ep done - 175400.\n",
      "Ep done - 175410.\n",
      "Ep done - 175420.\n",
      "Ep done - 175430.\n",
      "Ep done - 175440.\n",
      "Ep done - 175450.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 821      |\n",
      "|    time_elapsed    | 17907    |\n",
      "|    total_timesteps | 5044224  |\n",
      "---------------------------------\n",
      "Ep done - 175460.\n",
      "Ep done - 175470.\n",
      "Ep done - 175480.\n",
      "Ep done - 175490.\n",
      "Ep done - 175500.\n",
      "Ep done - 175510.\n",
      "Ep done - 175520.\n",
      "Ep done - 175530.\n",
      "Ep done - 175540.\n",
      "Ep done - 175550.\n",
      "Ep done - 175560.\n",
      "Ep done - 175570.\n",
      "Ep done - 175580.\n",
      "Ep done - 175590.\n",
      "Ep done - 175600.\n",
      "Ep done - 175610.\n",
      "Ep done - 175620.\n",
      "Ep done - 175630.\n",
      "Ep done - 175640.\n",
      "Ep done - 175650.\n",
      "Ep done - 50410.\n",
      "Ep done - 50420.\n",
      "Ep done - 50430.\n",
      "Ep done - 50440.\n",
      "Ep done - 50450.\n",
      "Ep done - 50460.\n",
      "Ep done - 50470.\n",
      "Ep done - 50480.\n",
      "Ep done - 50490.\n",
      "Ep done - 50500.\n",
      "Eval num_timesteps=5050000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5050000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.68e-07 |\n",
      "|    explained_variance   | 0.852     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000481  |\n",
      "|    n_updates            | 27440     |\n",
      "|    policy_gradient_loss | 7.65e-10  |\n",
      "|    value_loss           | 0.00139   |\n",
      "---------------------------------------\n",
      "Ep done - 175660.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 822      |\n",
      "|    time_elapsed    | 17926    |\n",
      "|    total_timesteps | 5050368  |\n",
      "---------------------------------\n",
      "Ep done - 175670.\n",
      "Ep done - 175680.\n",
      "Ep done - 175690.\n",
      "Ep done - 175700.\n",
      "Ep done - 175710.\n",
      "Ep done - 175720.\n",
      "Ep done - 175730.\n",
      "Ep done - 175740.\n",
      "Ep done - 175750.\n",
      "Ep done - 175760.\n",
      "Ep done - 175770.\n",
      "Ep done - 175780.\n",
      "Ep done - 175790.\n",
      "Ep done - 175800.\n",
      "Ep done - 175810.\n",
      "Ep done - 175820.\n",
      "Ep done - 175830.\n",
      "Ep done - 175840.\n",
      "Ep done - 175850.\n",
      "Ep done - 175860.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 823       |\n",
      "|    time_elapsed         | 17940     |\n",
      "|    total_timesteps      | 5056512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.21e-07 |\n",
      "|    explained_variance   | 0.857     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000718  |\n",
      "|    n_updates            | 27450     |\n",
      "|    policy_gradient_loss | -3.87e-10 |\n",
      "|    value_loss           | 0.00145   |\n",
      "---------------------------------------\n",
      "Ep done - 175870.\n",
      "Ep done - 175880.\n",
      "Ep done - 175890.\n",
      "Ep done - 175900.\n",
      "Ep done - 175910.\n",
      "Ep done - 175920.\n",
      "Ep done - 175930.\n",
      "Ep done - 175940.\n",
      "Ep done - 175950.\n",
      "Ep done - 175960.\n",
      "Ep done - 175970.\n",
      "Ep done - 175980.\n",
      "Ep done - 50510.\n",
      "Ep done - 50520.\n",
      "Ep done - 50530.\n",
      "Ep done - 50540.\n",
      "Ep done - 50550.\n",
      "Ep done - 50560.\n",
      "Ep done - 50570.\n",
      "Ep done - 50580.\n",
      "Ep done - 50590.\n",
      "Ep done - 50600.\n",
      "Eval num_timesteps=5060000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5060000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.45e-07 |\n",
      "|    explained_variance   | 0.847     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000525  |\n",
      "|    n_updates            | 27460     |\n",
      "|    policy_gradient_loss | -1.39e-09 |\n",
      "|    value_loss           | 0.0015    |\n",
      "---------------------------------------\n",
      "Ep done - 175990.\n",
      "Ep done - 176000.\n",
      "Ep done - 176010.\n",
      "Ep done - 176020.\n",
      "Ep done - 176030.\n",
      "Ep done - 176040.\n",
      "Ep done - 176050.\n",
      "Ep done - 176060.\n",
      "Ep done - 176070.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 824      |\n",
      "|    time_elapsed    | 17959    |\n",
      "|    total_timesteps | 5062656  |\n",
      "---------------------------------\n",
      "Ep done - 176080.\n",
      "Ep done - 176090.\n",
      "Ep done - 176100.\n",
      "Ep done - 176110.\n",
      "Ep done - 176120.\n",
      "Ep done - 176130.\n",
      "Ep done - 176140.\n",
      "Ep done - 176150.\n",
      "Ep done - 176160.\n",
      "Ep done - 176170.\n",
      "Ep done - 176180.\n",
      "Ep done - 176190.\n",
      "Ep done - 176200.\n",
      "Ep done - 176210.\n",
      "Ep done - 176220.\n",
      "Ep done - 176230.\n",
      "Ep done - 176240.\n",
      "Ep done - 176250.\n",
      "Ep done - 176260.\n",
      "Ep done - 176270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -1          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 825         |\n",
      "|    time_elapsed         | 17973       |\n",
      "|    total_timesteps      | 5068800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 7.15896e-06 |\n",
      "|    clip_fraction        | 8.14e-05    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.83e-05   |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.000463    |\n",
      "|    n_updates            | 27470       |\n",
      "|    policy_gradient_loss | -1.51e-05   |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n",
      "Ep done - 176280.\n",
      "Ep done - 176290.\n",
      "Ep done - 176300.\n",
      "Ep done - 176310.\n",
      "Ep done - 50610.\n",
      "Ep done - 50620.\n",
      "Ep done - 50630.\n",
      "Ep done - 50640.\n",
      "Ep done - 50650.\n",
      "Ep done - 50660.\n",
      "Ep done - 50670.\n",
      "Ep done - 50680.\n",
      "Ep done - 50690.\n",
      "Ep done - 50700.\n",
      "Eval num_timesteps=5070000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5070000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2e-07    |\n",
      "|    explained_variance   | 0.851     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000407  |\n",
      "|    n_updates            | 27480     |\n",
      "|    policy_gradient_loss | -1.95e-09 |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n",
      "Ep done - 176320.\n",
      "Ep done - 176330.\n",
      "Ep done - 176340.\n",
      "Ep done - 176350.\n",
      "Ep done - 176360.\n",
      "Ep done - 176370.\n",
      "Ep done - 176380.\n",
      "Ep done - 176390.\n",
      "Ep done - 176400.\n",
      "Ep done - 176410.\n",
      "Ep done - 176420.\n",
      "Ep done - 176430.\n",
      "Ep done - 176440.\n",
      "Ep done - 176450.\n",
      "Ep done - 176460.\n",
      "Ep done - 176470.\n",
      "Ep done - 176480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 826      |\n",
      "|    time_elapsed    | 17991    |\n",
      "|    total_timesteps | 5074944  |\n",
      "---------------------------------\n",
      "Ep done - 176490.\n",
      "Ep done - 176500.\n",
      "Ep done - 176510.\n",
      "Ep done - 176520.\n",
      "Ep done - 176530.\n",
      "Ep done - 176540.\n",
      "Ep done - 176550.\n",
      "Ep done - 176560.\n",
      "Ep done - 176570.\n",
      "Ep done - 176580.\n",
      "Ep done - 176590.\n",
      "Ep done - 176600.\n",
      "Ep done - 176610.\n",
      "Ep done - 176620.\n",
      "Ep done - 176630.\n",
      "Ep done - 176640.\n",
      "Ep done - 176650.\n",
      "Ep done - 50710.\n",
      "Ep done - 50720.\n",
      "Ep done - 50730.\n",
      "Ep done - 50740.\n",
      "Ep done - 50750.\n",
      "Ep done - 50760.\n",
      "Ep done - 50770.\n",
      "Ep done - 50780.\n",
      "Ep done - 50790.\n",
      "Ep done - 50800.\n",
      "Eval num_timesteps=5080000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5080000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.08e-07 |\n",
      "|    explained_variance   | 0.856     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00054   |\n",
      "|    n_updates            | 27490     |\n",
      "|    policy_gradient_loss | 7.6e-11   |\n",
      "|    value_loss           | 0.00135   |\n",
      "---------------------------------------\n",
      "Ep done - 176660.\n",
      "Ep done - 176670.\n",
      "Ep done - 176680.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 827      |\n",
      "|    time_elapsed    | 18010    |\n",
      "|    total_timesteps | 5081088  |\n",
      "---------------------------------\n",
      "Ep done - 176690.\n",
      "Ep done - 176700.\n",
      "Ep done - 176710.\n",
      "Ep done - 176720.\n",
      "Ep done - 176730.\n",
      "Ep done - 176740.\n",
      "Ep done - 176750.\n",
      "Ep done - 176760.\n",
      "Ep done - 176770.\n",
      "Ep done - 176780.\n",
      "Ep done - 176790.\n",
      "Ep done - 176800.\n",
      "Ep done - 176810.\n",
      "Ep done - 176820.\n",
      "Ep done - 176830.\n",
      "Ep done - 176840.\n",
      "Ep done - 176850.\n",
      "Ep done - 176860.\n",
      "Ep done - 176870.\n",
      "Ep done - 176880.\n",
      "Ep done - 176890.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -1           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 282          |\n",
      "|    iterations           | 828          |\n",
      "|    time_elapsed         | 18024        |\n",
      "|    total_timesteps      | 5087232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042036916 |\n",
      "|    clip_fraction        | 0.000212     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.23e-05    |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00088      |\n",
      "|    n_updates            | 27500        |\n",
      "|    policy_gradient_loss | -2.32e-05    |\n",
      "|    value_loss           | 0.00133      |\n",
      "------------------------------------------\n",
      "Ep done - 176900.\n",
      "Ep done - 176910.\n",
      "Ep done - 176920.\n",
      "Ep done - 176930.\n",
      "Ep done - 176940.\n",
      "Ep done - 176950.\n",
      "Ep done - 176960.\n",
      "Ep done - 176970.\n",
      "Ep done - 176980.\n",
      "Ep done - 50810.\n",
      "Ep done - 50820.\n",
      "Ep done - 50830.\n",
      "Ep done - 50840.\n",
      "Ep done - 50850.\n",
      "Ep done - 50860.\n",
      "Ep done - 50870.\n",
      "Ep done - 50880.\n",
      "Ep done - 50890.\n",
      "Ep done - 50900.\n",
      "Eval num_timesteps=5090000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | -1           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5090000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.507252e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.03e-06    |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00129      |\n",
      "|    n_updates            | 27510        |\n",
      "|    policy_gradient_loss | -6.65e-09    |\n",
      "|    value_loss           | 0.00127      |\n",
      "------------------------------------------\n",
      "Ep done - 176990.\n",
      "Ep done - 177000.\n",
      "Ep done - 177010.\n",
      "Ep done - 177020.\n",
      "Ep done - 177030.\n",
      "Ep done - 177040.\n",
      "Ep done - 177050.\n",
      "Ep done - 177060.\n",
      "Ep done - 177070.\n",
      "Ep done - 177080.\n",
      "Ep done - 177090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 829      |\n",
      "|    time_elapsed    | 18043    |\n",
      "|    total_timesteps | 5093376  |\n",
      "---------------------------------\n",
      "Ep done - 177100.\n",
      "Ep done - 177110.\n",
      "Ep done - 177120.\n",
      "Ep done - 177130.\n",
      "Ep done - 177140.\n",
      "Ep done - 177150.\n",
      "Ep done - 177160.\n",
      "Ep done - 177170.\n",
      "Ep done - 177180.\n",
      "Ep done - 177190.\n",
      "Ep done - 177200.\n",
      "Ep done - 177210.\n",
      "Ep done - 177220.\n",
      "Ep done - 177230.\n",
      "Ep done - 177240.\n",
      "Ep done - 177250.\n",
      "Ep done - 177260.\n",
      "Ep done - 177270.\n",
      "Ep done - 177280.\n",
      "Ep done - 177290.\n",
      "Ep done - 177300.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | -1           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 282          |\n",
      "|    iterations           | 830          |\n",
      "|    time_elapsed         | 18057        |\n",
      "|    total_timesteps      | 5099520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074260426 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25e-05    |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.000876     |\n",
      "|    n_updates            | 27520        |\n",
      "|    policy_gradient_loss | -0.000222    |\n",
      "|    value_loss           | 0.0013       |\n",
      "------------------------------------------\n",
      "Ep done - 177310.\n",
      "Ep done - 50910.\n",
      "Ep done - 50920.\n",
      "Ep done - 50930.\n",
      "Ep done - 50940.\n",
      "Ep done - 50950.\n",
      "Ep done - 50960.\n",
      "Ep done - 50970.\n",
      "Ep done - 50980.\n",
      "Ep done - 50990.\n",
      "Ep done - 51000.\n",
      "Eval num_timesteps=5100000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5100000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.79e-07 |\n",
      "|    explained_variance   | 0.858     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000471  |\n",
      "|    n_updates            | 27530     |\n",
      "|    policy_gradient_loss | -2.24e-09 |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n",
      "Ep done - 177320.\n",
      "Ep done - 177330.\n",
      "Ep done - 177340.\n",
      "Ep done - 177350.\n",
      "Ep done - 177360.\n",
      "Ep done - 177370.\n",
      "Ep done - 177380.\n",
      "Ep done - 177390.\n",
      "Ep done - 177400.\n",
      "Ep done - 177410.\n",
      "Ep done - 177420.\n",
      "Ep done - 177430.\n",
      "Ep done - 177440.\n",
      "Ep done - 177450.\n",
      "Ep done - 177460.\n",
      "Ep done - 177470.\n",
      "Ep done - 177480.\n",
      "Ep done - 177490.\n",
      "Ep done - 177500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 831      |\n",
      "|    time_elapsed    | 18077    |\n",
      "|    total_timesteps | 5105664  |\n",
      "---------------------------------\n",
      "Ep done - 177510.\n",
      "Ep done - 177520.\n",
      "Ep done - 177530.\n",
      "Ep done - 177540.\n",
      "Ep done - 177550.\n",
      "Ep done - 177560.\n",
      "Ep done - 177570.\n",
      "Ep done - 177580.\n",
      "Ep done - 177590.\n",
      "Ep done - 177600.\n",
      "Ep done - 177610.\n",
      "Ep done - 177620.\n",
      "Ep done - 177630.\n",
      "Ep done - 177640.\n",
      "Ep done - 177650.\n",
      "Ep done - 51010.\n",
      "Ep done - 51020.\n",
      "Ep done - 51030.\n",
      "Ep done - 51040.\n",
      "Ep done - 51050.\n",
      "Ep done - 51060.\n",
      "Ep done - 51070.\n",
      "Ep done - 51080.\n",
      "Ep done - 51090.\n",
      "Ep done - 51100.\n",
      "Eval num_timesteps=5110000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.01 +/- 0.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5110000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5752886 |\n",
      "|    clip_fraction        | 0.0104    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000134 |\n",
      "|    explained_variance   | 0.482     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0288    |\n",
      "|    n_updates            | 27540     |\n",
      "|    policy_gradient_loss | -0.000744 |\n",
      "|    value_loss           | 0.00808   |\n",
      "---------------------------------------\n",
      "Ep done - 177660.\n",
      "Ep done - 177670.\n",
      "Ep done - 177680.\n",
      "Ep done - 177690.\n",
      "Ep done - 177700.\n",
      "Ep done - 177710.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 832      |\n",
      "|    time_elapsed    | 18100    |\n",
      "|    total_timesteps | 5111808  |\n",
      "---------------------------------\n",
      "Ep done - 177720.\n",
      "Ep done - 177730.\n",
      "Ep done - 177740.\n",
      "Ep done - 177750.\n",
      "Ep done - 177760.\n",
      "Ep done - 177770.\n",
      "Ep done - 177780.\n",
      "Ep done - 177790.\n",
      "Ep done - 177800.\n",
      "Ep done - 177810.\n",
      "Ep done - 177820.\n",
      "Ep done - 177830.\n",
      "Ep done - 177840.\n",
      "Ep done - 177850.\n",
      "Ep done - 177860.\n",
      "Ep done - 177870.\n",
      "Ep done - 177880.\n",
      "Ep done - 177890.\n",
      "Ep done - 177900.\n",
      "Ep done - 177910.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 833       |\n",
      "|    time_elapsed         | 18114     |\n",
      "|    total_timesteps      | 5117952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7748307 |\n",
      "|    clip_fraction        | 0.0649    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000213 |\n",
      "|    explained_variance   | 0.825     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00735  |\n",
      "|    n_updates            | 27550     |\n",
      "|    policy_gradient_loss | -0.00583  |\n",
      "|    value_loss           | 0.0016    |\n",
      "---------------------------------------\n",
      "Ep done - 177920.\n",
      "Ep done - 177930.\n",
      "Ep done - 177940.\n",
      "Ep done - 177950.\n",
      "Ep done - 177960.\n",
      "Ep done - 177970.\n",
      "Ep done - 177980.\n",
      "Ep done - 51110.\n",
      "Ep done - 51120.\n",
      "Ep done - 51130.\n",
      "Ep done - 51140.\n",
      "Ep done - 51150.\n",
      "Ep done - 51160.\n",
      "Ep done - 51170.\n",
      "Ep done - 51180.\n",
      "Ep done - 51190.\n",
      "Ep done - 51200.\n",
      "Eval num_timesteps=5120000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5120000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09590051 |\n",
      "|    clip_fraction        | 0.0325     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0012    |\n",
      "|    explained_variance   | 0.854      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00306   |\n",
      "|    n_updates            | 27560      |\n",
      "|    policy_gradient_loss | 0.00128    |\n",
      "|    value_loss           | 0.00165    |\n",
      "----------------------------------------\n",
      "Ep done - 177990.\n",
      "Ep done - 178000.\n",
      "Ep done - 178010.\n",
      "Ep done - 178020.\n",
      "Ep done - 178030.\n",
      "Ep done - 178040.\n",
      "Ep done - 178050.\n",
      "Ep done - 178060.\n",
      "Ep done - 178070.\n",
      "Ep done - 178080.\n",
      "Ep done - 178090.\n",
      "Ep done - 178100.\n",
      "Ep done - 178110.\n",
      "Ep done - 178120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 834      |\n",
      "|    time_elapsed    | 18133    |\n",
      "|    total_timesteps | 5124096  |\n",
      "---------------------------------\n",
      "Ep done - 178130.\n",
      "Ep done - 178140.\n",
      "Ep done - 178150.\n",
      "Ep done - 178160.\n",
      "Ep done - 178170.\n",
      "Ep done - 178180.\n",
      "Ep done - 178190.\n",
      "Ep done - 178200.\n",
      "Ep done - 178210.\n",
      "Ep done - 178220.\n",
      "Ep done - 178230.\n",
      "Ep done - 178240.\n",
      "Ep done - 178250.\n",
      "Ep done - 178260.\n",
      "Ep done - 178270.\n",
      "Ep done - 178280.\n",
      "Ep done - 178290.\n",
      "Ep done - 178300.\n",
      "Ep done - 178310.\n",
      "Ep done - 51210.\n",
      "Ep done - 51220.\n",
      "Ep done - 51230.\n",
      "Ep done - 51240.\n",
      "Ep done - 51250.\n",
      "Ep done - 51260.\n",
      "Ep done - 51270.\n",
      "Ep done - 51280.\n",
      "Ep done - 51290.\n",
      "Ep done - 51300.\n",
      "Eval num_timesteps=5130000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5130000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.31e-06 |\n",
      "|    explained_variance   | 0.814     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0009    |\n",
      "|    n_updates            | 27570     |\n",
      "|    policy_gradient_loss | -5.86e-10 |\n",
      "|    value_loss           | 0.00135   |\n",
      "---------------------------------------\n",
      "Ep done - 178320.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 835      |\n",
      "|    time_elapsed    | 18152    |\n",
      "|    total_timesteps | 5130240  |\n",
      "---------------------------------\n",
      "Ep done - 178330.\n",
      "Ep done - 178340.\n",
      "Ep done - 178350.\n",
      "Ep done - 178360.\n",
      "Ep done - 178370.\n",
      "Ep done - 178380.\n",
      "Ep done - 178390.\n",
      "Ep done - 178400.\n",
      "Ep done - 178410.\n",
      "Ep done - 178420.\n",
      "Ep done - 178430.\n",
      "Ep done - 178440.\n",
      "Ep done - 178450.\n",
      "Ep done - 178460.\n",
      "Ep done - 178470.\n",
      "Ep done - 178480.\n",
      "Ep done - 178490.\n",
      "Ep done - 178500.\n",
      "Ep done - 178510.\n",
      "Ep done - 178520.\n",
      "Ep done - 178530.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 836       |\n",
      "|    time_elapsed         | 18165     |\n",
      "|    total_timesteps      | 5136384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.12e-06 |\n",
      "|    explained_variance   | 0.857     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00058   |\n",
      "|    n_updates            | 27580     |\n",
      "|    policy_gradient_loss | 3.07e-10  |\n",
      "|    value_loss           | 0.00119   |\n",
      "---------------------------------------\n",
      "Ep done - 178540.\n",
      "Ep done - 178550.\n",
      "Ep done - 178560.\n",
      "Ep done - 178570.\n",
      "Ep done - 178580.\n",
      "Ep done - 178590.\n",
      "Ep done - 178600.\n",
      "Ep done - 178610.\n",
      "Ep done - 178620.\n",
      "Ep done - 178630.\n",
      "Ep done - 178640.\n",
      "Ep done - 178650.\n",
      "Ep done - 51310.\n",
      "Ep done - 51320.\n",
      "Ep done - 51330.\n",
      "Ep done - 51340.\n",
      "Ep done - 51350.\n",
      "Ep done - 51360.\n",
      "Ep done - 51370.\n",
      "Ep done - 51380.\n",
      "Ep done - 51390.\n",
      "Ep done - 51400.\n",
      "Eval num_timesteps=5140000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5140000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.02e-06 |\n",
      "|    explained_variance   | 0.861     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000465  |\n",
      "|    n_updates            | 27590     |\n",
      "|    policy_gradient_loss | -9.39e-10 |\n",
      "|    value_loss           | 0.0012    |\n",
      "---------------------------------------\n",
      "Ep done - 178660.\n",
      "Ep done - 178670.\n",
      "Ep done - 178680.\n",
      "Ep done - 178690.\n",
      "Ep done - 178700.\n",
      "Ep done - 178710.\n",
      "Ep done - 178720.\n",
      "Ep done - 178730.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 837      |\n",
      "|    time_elapsed    | 18184    |\n",
      "|    total_timesteps | 5142528  |\n",
      "---------------------------------\n",
      "Ep done - 178740.\n",
      "Ep done - 178750.\n",
      "Ep done - 178760.\n",
      "Ep done - 178770.\n",
      "Ep done - 178780.\n",
      "Ep done - 178790.\n",
      "Ep done - 178800.\n",
      "Ep done - 178810.\n",
      "Ep done - 178820.\n",
      "Ep done - 178830.\n",
      "Ep done - 178840.\n",
      "Ep done - 178850.\n",
      "Ep done - 178860.\n",
      "Ep done - 178870.\n",
      "Ep done - 178880.\n",
      "Ep done - 178890.\n",
      "Ep done - 178900.\n",
      "Ep done - 178910.\n",
      "Ep done - 178920.\n",
      "Ep done - 178930.\n",
      "Ep done - 178940.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -0.98         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 282           |\n",
      "|    iterations           | 838           |\n",
      "|    time_elapsed         | 18197         |\n",
      "|    total_timesteps      | 5148672       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2720391e-08 |\n",
      "|    clip_fraction        | 1.63e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -5.41e-05     |\n",
      "|    explained_variance   | 0.213         |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.00101       |\n",
      "|    n_updates            | 27600         |\n",
      "|    policy_gradient_loss | 5.94e-05      |\n",
      "|    value_loss           | 0.0195        |\n",
      "-------------------------------------------\n",
      "Ep done - 178950.\n",
      "Ep done - 178960.\n",
      "Ep done - 178970.\n",
      "Ep done - 178980.\n",
      "Ep done - 51410.\n",
      "Ep done - 51420.\n",
      "Ep done - 51430.\n",
      "Ep done - 51440.\n",
      "Ep done - 51450.\n",
      "Ep done - 51460.\n",
      "Ep done - 51470.\n",
      "Ep done - 51480.\n",
      "Ep done - 51490.\n",
      "Ep done - 51500.\n",
      "Eval num_timesteps=5150000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5150000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5712934 |\n",
      "|    clip_fraction        | 0.0253    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.73e-05 |\n",
      "|    explained_variance   | 0.312     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000689  |\n",
      "|    n_updates            | 27610     |\n",
      "|    policy_gradient_loss | -0.00251  |\n",
      "|    value_loss           | 0.0121    |\n",
      "---------------------------------------\n",
      "Ep done - 178990.\n",
      "Ep done - 179000.\n",
      "Ep done - 179010.\n",
      "Ep done - 179020.\n",
      "Ep done - 179030.\n",
      "Ep done - 179040.\n",
      "Ep done - 179050.\n",
      "Ep done - 179060.\n",
      "Ep done - 179070.\n",
      "Ep done - 179080.\n",
      "Ep done - 179090.\n",
      "Ep done - 179100.\n",
      "Ep done - 179110.\n",
      "Ep done - 179120.\n",
      "Ep done - 179130.\n",
      "Ep done - 179140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 839      |\n",
      "|    time_elapsed    | 18217    |\n",
      "|    total_timesteps | 5154816  |\n",
      "---------------------------------\n",
      "Ep done - 179150.\n",
      "Ep done - 179160.\n",
      "Ep done - 179170.\n",
      "Ep done - 179180.\n",
      "Ep done - 179190.\n",
      "Ep done - 179200.\n",
      "Ep done - 179210.\n",
      "Ep done - 179220.\n",
      "Ep done - 179230.\n",
      "Ep done - 179240.\n",
      "Ep done - 179250.\n",
      "Ep done - 179260.\n",
      "Ep done - 179270.\n",
      "Ep done - 179280.\n",
      "Ep done - 179290.\n",
      "Ep done - 179300.\n",
      "Ep done - 179310.\n",
      "Ep done - 51510.\n",
      "Ep done - 51520.\n",
      "Ep done - 51530.\n",
      "Ep done - 51540.\n",
      "Ep done - 51550.\n",
      "Ep done - 51560.\n",
      "Ep done - 51570.\n",
      "Ep done - 51580.\n",
      "Ep done - 51590.\n",
      "Ep done - 51600.\n",
      "Eval num_timesteps=5160000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5160000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.56e-07 |\n",
      "|    explained_variance   | 0.496     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00154   |\n",
      "|    n_updates            | 27620     |\n",
      "|    policy_gradient_loss | -2.45e-11 |\n",
      "|    value_loss           | 0.00703   |\n",
      "---------------------------------------\n",
      "Ep done - 179320.\n",
      "Ep done - 179330.\n",
      "Ep done - 179340.\n",
      "Ep done - 179350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 840      |\n",
      "|    time_elapsed    | 18241    |\n",
      "|    total_timesteps | 5160960  |\n",
      "---------------------------------\n",
      "Ep done - 179360.\n",
      "Ep done - 179370.\n",
      "Ep done - 179380.\n",
      "Ep done - 179390.\n",
      "Ep done - 179400.\n",
      "Ep done - 179410.\n",
      "Ep done - 179420.\n",
      "Ep done - 179430.\n",
      "Ep done - 179440.\n",
      "Ep done - 179450.\n",
      "Ep done - 179460.\n",
      "Ep done - 179470.\n",
      "Ep done - 179480.\n",
      "Ep done - 179490.\n",
      "Ep done - 179500.\n",
      "Ep done - 179510.\n",
      "Ep done - 179520.\n",
      "Ep done - 179530.\n",
      "Ep done - 179540.\n",
      "Ep done - 179550.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 282       |\n",
      "|    iterations           | 841       |\n",
      "|    time_elapsed         | 18259     |\n",
      "|    total_timesteps      | 5167104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.16e-07 |\n",
      "|    explained_variance   | 0.847     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0011    |\n",
      "|    n_updates            | 27630     |\n",
      "|    policy_gradient_loss | 2.51e-09  |\n",
      "|    value_loss           | 0.00151   |\n",
      "---------------------------------------\n",
      "Ep done - 179560.\n",
      "Ep done - 179570.\n",
      "Ep done - 179580.\n",
      "Ep done - 179590.\n",
      "Ep done - 179600.\n",
      "Ep done - 179610.\n",
      "Ep done - 179620.\n",
      "Ep done - 179630.\n",
      "Ep done - 179640.\n",
      "Ep done - 179650.\n",
      "Ep done - 51610.\n",
      "Ep done - 51620.\n",
      "Ep done - 51630.\n",
      "Ep done - 51640.\n",
      "Ep done - 51650.\n",
      "Ep done - 51660.\n",
      "Ep done - 51670.\n",
      "Ep done - 51680.\n",
      "Ep done - 51690.\n",
      "Ep done - 51700.\n",
      "Eval num_timesteps=5170000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.98     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5170000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.67e-07 |\n",
      "|    explained_variance   | 0.557     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00148   |\n",
      "|    n_updates            | 27640     |\n",
      "|    policy_gradient_loss | 2.12e-09  |\n",
      "|    value_loss           | 0.00714   |\n",
      "---------------------------------------\n",
      "Ep done - 179660.\n",
      "Ep done - 179670.\n",
      "Ep done - 179680.\n",
      "Ep done - 179690.\n",
      "Ep done - 179700.\n",
      "Ep done - 179710.\n",
      "Ep done - 179720.\n",
      "Ep done - 179730.\n",
      "Ep done - 179740.\n",
      "Ep done - 179750.\n",
      "Ep done - 179760.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 842      |\n",
      "|    time_elapsed    | 18278    |\n",
      "|    total_timesteps | 5173248  |\n",
      "---------------------------------\n",
      "Ep done - 179770.\n",
      "Ep done - 179780.\n",
      "Ep done - 179790.\n",
      "Ep done - 179800.\n",
      "Ep done - 179810.\n",
      "Ep done - 179820.\n",
      "Ep done - 179830.\n",
      "Ep done - 179840.\n",
      "Ep done - 179850.\n",
      "Ep done - 179860.\n",
      "Ep done - 179870.\n",
      "Ep done - 179880.\n",
      "Ep done - 179890.\n",
      "Ep done - 179900.\n",
      "Ep done - 179910.\n",
      "Ep done - 179920.\n",
      "Ep done - 179930.\n",
      "Ep done - 179940.\n",
      "Ep done - 179950.\n",
      "Ep done - 179960.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 843       |\n",
      "|    time_elapsed         | 18294     |\n",
      "|    total_timesteps      | 5179392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.38e-07 |\n",
      "|    explained_variance   | 0.817     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000933  |\n",
      "|    n_updates            | 27650     |\n",
      "|    policy_gradient_loss | 1.15e-09  |\n",
      "|    value_loss           | 0.00131   |\n",
      "---------------------------------------\n",
      "Ep done - 179970.\n",
      "Ep done - 179980.\n",
      "Ep done - 51710.\n",
      "Ep done - 51720.\n",
      "Ep done - 51730.\n",
      "Ep done - 51740.\n",
      "Ep done - 51750.\n",
      "Ep done - 51760.\n",
      "Ep done - 51770.\n",
      "Ep done - 51780.\n",
      "Ep done - 51790.\n",
      "Ep done - 51800.\n",
      "Eval num_timesteps=5180000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30       |\n",
      "|    mean_reward          | -0.98    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 5180000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -7.4e-07 |\n",
      "|    explained_variance   | 0.862    |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.000485 |\n",
      "|    n_updates            | 27660    |\n",
      "|    policy_gradient_loss | 7.97e-10 |\n",
      "|    value_loss           | 0.00124  |\n",
      "--------------------------------------\n",
      "Ep done - 179990.\n",
      "Ep done - 180000.\n",
      "Ep done - 180010.\n",
      "Ep done - 180020.\n",
      "Ep done - 180030.\n",
      "Ep done - 180040.\n",
      "Ep done - 180050.\n",
      "Ep done - 180060.\n",
      "Ep done - 180070.\n",
      "Ep done - 180080.\n",
      "Ep done - 180090.\n",
      "Ep done - 180100.\n",
      "Ep done - 180110.\n",
      "Ep done - 180120.\n",
      "Ep done - 180130.\n",
      "Ep done - 180140.\n",
      "Ep done - 180150.\n",
      "Ep done - 180160.\n",
      "Ep done - 180170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 844      |\n",
      "|    time_elapsed    | 18313    |\n",
      "|    total_timesteps | 5185536  |\n",
      "---------------------------------\n",
      "Ep done - 180180.\n",
      "Ep done - 180190.\n",
      "Ep done - 180200.\n",
      "Ep done - 180210.\n",
      "Ep done - 180220.\n",
      "Ep done - 180230.\n",
      "Ep done - 180240.\n",
      "Ep done - 180250.\n",
      "Ep done - 180260.\n",
      "Ep done - 180270.\n",
      "Ep done - 180280.\n",
      "Ep done - 180290.\n",
      "Ep done - 180300.\n",
      "Ep done - 180310.\n",
      "Ep done - 51810.\n",
      "Ep done - 51820.\n",
      "Ep done - 51830.\n",
      "Ep done - 51840.\n",
      "Ep done - 51850.\n",
      "Ep done - 51860.\n",
      "Ep done - 51870.\n",
      "Ep done - 51880.\n",
      "Ep done - 51890.\n",
      "Ep done - 51900.\n",
      "Eval num_timesteps=5190000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -1         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5190000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13317285 |\n",
      "|    clip_fraction        | 0.00586    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000601  |\n",
      "|    explained_variance   | 0.32       |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0083     |\n",
      "|    n_updates            | 27670      |\n",
      "|    policy_gradient_loss | -0.000356  |\n",
      "|    value_loss           | 0.0145     |\n",
      "----------------------------------------\n",
      "Ep done - 180320.\n",
      "Ep done - 180330.\n",
      "Ep done - 180340.\n",
      "Ep done - 180350.\n",
      "Ep done - 180360.\n",
      "Ep done - 180370.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 845      |\n",
      "|    time_elapsed    | 18332    |\n",
      "|    total_timesteps | 5191680  |\n",
      "---------------------------------\n",
      "Ep done - 180380.\n",
      "Ep done - 180390.\n",
      "Ep done - 180400.\n",
      "Ep done - 180410.\n",
      "Ep done - 180420.\n",
      "Ep done - 180430.\n",
      "Ep done - 180440.\n",
      "Ep done - 180450.\n",
      "Ep done - 180460.\n",
      "Ep done - 180470.\n",
      "Ep done - 180480.\n",
      "Ep done - 180490.\n",
      "Ep done - 180500.\n",
      "Ep done - 180510.\n",
      "Ep done - 180520.\n",
      "Ep done - 180530.\n",
      "Ep done - 180540.\n",
      "Ep done - 180550.\n",
      "Ep done - 180560.\n",
      "Ep done - 180570.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 846        |\n",
      "|    time_elapsed         | 18346      |\n",
      "|    total_timesteps      | 5197824    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47588336 |\n",
      "|    clip_fraction        | 0.0329     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000774  |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.00593   |\n",
      "|    n_updates            | 27680      |\n",
      "|    policy_gradient_loss | -0.00415   |\n",
      "|    value_loss           | 0.00273    |\n",
      "----------------------------------------\n",
      "Ep done - 180580.\n",
      "Ep done - 180590.\n",
      "Ep done - 180600.\n",
      "Ep done - 180610.\n",
      "Ep done - 180620.\n",
      "Ep done - 180630.\n",
      "Ep done - 180640.\n",
      "Ep done - 180650.\n",
      "Ep done - 51910.\n",
      "Ep done - 51920.\n",
      "Ep done - 51930.\n",
      "Ep done - 51940.\n",
      "Ep done - 51950.\n",
      "Ep done - 51960.\n",
      "Ep done - 51970.\n",
      "Ep done - 51980.\n",
      "Ep done - 51990.\n",
      "Ep done - 52000.\n",
      "Eval num_timesteps=5200000, episode_reward=-0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -0.98      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5200000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45057622 |\n",
      "|    clip_fraction        | 0.0332     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000171  |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00191    |\n",
      "|    n_updates            | 27690      |\n",
      "|    policy_gradient_loss | -0.00175   |\n",
      "|    value_loss           | 0.0071     |\n",
      "----------------------------------------\n",
      "Ep done - 180660.\n",
      "Ep done - 180670.\n",
      "Ep done - 180680.\n",
      "Ep done - 180690.\n",
      "Ep done - 180700.\n",
      "Ep done - 180710.\n",
      "Ep done - 180720.\n",
      "Ep done - 180730.\n",
      "Ep done - 180740.\n",
      "Ep done - 180750.\n",
      "Ep done - 180760.\n",
      "Ep done - 180770.\n",
      "Ep done - 180780.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 847      |\n",
      "|    time_elapsed    | 18366    |\n",
      "|    total_timesteps | 5203968  |\n",
      "---------------------------------\n",
      "Ep done - 180790.\n",
      "Ep done - 180800.\n",
      "Ep done - 180810.\n",
      "Ep done - 180820.\n",
      "Ep done - 180830.\n",
      "Ep done - 180840.\n",
      "Ep done - 180850.\n",
      "Ep done - 180860.\n",
      "Ep done - 180870.\n",
      "Ep done - 180880.\n",
      "Ep done - 180890.\n",
      "Ep done - 180900.\n",
      "Ep done - 180910.\n",
      "Ep done - 180920.\n",
      "Ep done - 180930.\n",
      "Ep done - 180940.\n",
      "Ep done - 180950.\n",
      "Ep done - 180960.\n",
      "Ep done - 180970.\n",
      "Ep done - 180980.\n",
      "Ep done - 52010.\n",
      "Ep done - 52020.\n",
      "Ep done - 52030.\n",
      "Ep done - 52040.\n",
      "Ep done - 52050.\n",
      "Ep done - 52060.\n",
      "Ep done - 52070.\n",
      "Ep done - 52080.\n",
      "Ep done - 52090.\n",
      "Ep done - 52100.\n",
      "Eval num_timesteps=5210000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5210000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8677275 |\n",
      "|    clip_fraction        | 0.0302    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.63e-05 |\n",
      "|    explained_variance   | 0.43      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00541  |\n",
      "|    n_updates            | 27700     |\n",
      "|    policy_gradient_loss | -0.00286  |\n",
      "|    value_loss           | 0.0071    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 848      |\n",
      "|    time_elapsed    | 18385    |\n",
      "|    total_timesteps | 5210112  |\n",
      "---------------------------------\n",
      "Ep done - 180990.\n",
      "Ep done - 181000.\n",
      "Ep done - 181010.\n",
      "Ep done - 181020.\n",
      "Ep done - 181030.\n",
      "Ep done - 181040.\n",
      "Ep done - 181050.\n",
      "Ep done - 181060.\n",
      "Ep done - 181070.\n",
      "Ep done - 181080.\n",
      "Ep done - 181090.\n",
      "Ep done - 181100.\n",
      "Ep done - 181110.\n",
      "Ep done - 181120.\n",
      "Ep done - 181130.\n",
      "Ep done - 181140.\n",
      "Ep done - 181150.\n",
      "Ep done - 181160.\n",
      "Ep done - 181170.\n",
      "Ep done - 181180.\n",
      "Ep done - 181190.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 849       |\n",
      "|    time_elapsed         | 18401     |\n",
      "|    total_timesteps      | 5216256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.51e-06 |\n",
      "|    explained_variance   | 0.825     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000463  |\n",
      "|    n_updates            | 27710     |\n",
      "|    policy_gradient_loss | -1.35e-10 |\n",
      "|    value_loss           | 0.00104   |\n",
      "---------------------------------------\n",
      "Ep done - 181200.\n",
      "Ep done - 181210.\n",
      "Ep done - 181220.\n",
      "Ep done - 181230.\n",
      "Ep done - 181240.\n",
      "Ep done - 181250.\n",
      "Ep done - 181260.\n",
      "Ep done - 181270.\n",
      "Ep done - 181280.\n",
      "Ep done - 181290.\n",
      "Ep done - 181300.\n",
      "Ep done - 181310.\n",
      "Ep done - 52110.\n",
      "Ep done - 52120.\n",
      "Ep done - 52130.\n",
      "Ep done - 52140.\n",
      "Ep done - 52150.\n",
      "Ep done - 52160.\n",
      "Ep done - 52170.\n",
      "Ep done - 52180.\n",
      "Ep done - 52190.\n",
      "Ep done - 52200.\n",
      "Eval num_timesteps=5220000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5220000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.19e-06 |\n",
      "|    explained_variance   | 0.851     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000502  |\n",
      "|    n_updates            | 27720     |\n",
      "|    policy_gradient_loss | 4e-10     |\n",
      "|    value_loss           | 0.00122   |\n",
      "---------------------------------------\n",
      "Ep done - 181320.\n",
      "Ep done - 181330.\n",
      "Ep done - 181340.\n",
      "Ep done - 181350.\n",
      "Ep done - 181360.\n",
      "Ep done - 181370.\n",
      "Ep done - 181380.\n",
      "Ep done - 181390.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 850      |\n",
      "|    time_elapsed    | 18420    |\n",
      "|    total_timesteps | 5222400  |\n",
      "---------------------------------\n",
      "Ep done - 181400.\n",
      "Ep done - 181410.\n",
      "Ep done - 181420.\n",
      "Ep done - 181430.\n",
      "Ep done - 181440.\n",
      "Ep done - 181450.\n",
      "Ep done - 181460.\n",
      "Ep done - 181470.\n",
      "Ep done - 181480.\n",
      "Ep done - 181490.\n",
      "Ep done - 181500.\n",
      "Ep done - 181510.\n",
      "Ep done - 181520.\n",
      "Ep done - 181530.\n",
      "Ep done - 181540.\n",
      "Ep done - 181550.\n",
      "Ep done - 181560.\n",
      "Ep done - 181570.\n",
      "Ep done - 181580.\n",
      "Ep done - 181590.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.9      |\n",
      "|    ep_rew_mean          | -1        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 851       |\n",
      "|    time_elapsed         | 18433     |\n",
      "|    total_timesteps      | 5228544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0419438 |\n",
      "|    clip_fraction        | 0.00737   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000131 |\n",
      "|    explained_variance   | 0.84      |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0212   |\n",
      "|    n_updates            | 27730     |\n",
      "|    policy_gradient_loss | -0.00229  |\n",
      "|    value_loss           | 0.00116   |\n",
      "---------------------------------------\n",
      "Ep done - 181600.\n",
      "Ep done - 181610.\n",
      "Ep done - 181620.\n",
      "Ep done - 181630.\n",
      "Ep done - 181640.\n",
      "Ep done - 52210.\n",
      "Ep done - 52220.\n",
      "Ep done - 52230.\n",
      "Ep done - 52240.\n",
      "Ep done - 52250.\n",
      "Ep done - 52260.\n",
      "Ep done - 52270.\n",
      "Ep done - 52280.\n",
      "Ep done - 52290.\n",
      "Ep done - 52300.\n",
      "Eval num_timesteps=5230000, episode_reward=-0.92 +/- 0.39\n",
      "Episode length: 30.03 +/- 0.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.92     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5230000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.022373  |\n",
      "|    clip_fraction        | 0.0492    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000201 |\n",
      "|    explained_variance   | 0.344     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0404   |\n",
      "|    n_updates            | 27740     |\n",
      "|    policy_gradient_loss | -0.0144   |\n",
      "|    value_loss           | 0.00756   |\n",
      "---------------------------------------\n",
      "Ep done - 181650.\n",
      "Ep done - 181660.\n",
      "Ep done - 181670.\n",
      "Ep done - 181680.\n",
      "Ep done - 181690.\n",
      "Ep done - 181700.\n",
      "Ep done - 181710.\n",
      "Ep done - 181720.\n",
      "Ep done - 181730.\n",
      "Ep done - 181740.\n",
      "Ep done - 181750.\n",
      "Ep done - 181760.\n",
      "Ep done - 181770.\n",
      "Ep done - 181780.\n",
      "Ep done - 181790.\n",
      "Ep done - 181800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 852      |\n",
      "|    time_elapsed    | 18452    |\n",
      "|    total_timesteps | 5234688  |\n",
      "---------------------------------\n",
      "Ep done - 181810.\n",
      "Ep done - 181820.\n",
      "Ep done - 181830.\n",
      "Ep done - 181840.\n",
      "Ep done - 181850.\n",
      "Ep done - 181860.\n",
      "Ep done - 181870.\n",
      "Ep done - 181880.\n",
      "Ep done - 181890.\n",
      "Ep done - 181900.\n",
      "Ep done - 181910.\n",
      "Ep done - 181920.\n",
      "Ep done - 181930.\n",
      "Ep done - 181940.\n",
      "Ep done - 181950.\n",
      "Ep done - 181960.\n",
      "Ep done - 181970.\n",
      "Ep done - 52310.\n",
      "Ep done - 52320.\n",
      "Ep done - 52330.\n",
      "Ep done - 52340.\n",
      "Ep done - 52350.\n",
      "Ep done - 52360.\n",
      "Ep done - 52370.\n",
      "Ep done - 52380.\n",
      "Ep done - 52390.\n",
      "Ep done - 52400.\n",
      "Eval num_timesteps=5240000, episode_reward=-0.96 +/- 0.28\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.96     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5240000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2486762 |\n",
      "|    clip_fraction        | 0.0235    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000115 |\n",
      "|    explained_variance   | 0.308     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0397    |\n",
      "|    n_updates            | 27750     |\n",
      "|    policy_gradient_loss | -0.0021   |\n",
      "|    value_loss           | 0.0174    |\n",
      "---------------------------------------\n",
      "Ep done - 181980.\n",
      "Ep done - 181990.\n",
      "Ep done - 182000.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 853      |\n",
      "|    time_elapsed    | 18479    |\n",
      "|    total_timesteps | 5240832  |\n",
      "---------------------------------\n",
      "Ep done - 182010.\n",
      "Ep done - 182020.\n",
      "Ep done - 182030.\n",
      "Ep done - 182040.\n",
      "Ep done - 182050.\n",
      "Ep done - 182060.\n",
      "Ep done - 182070.\n",
      "Ep done - 182080.\n",
      "Ep done - 182090.\n",
      "Ep done - 182100.\n",
      "Ep done - 182110.\n",
      "Ep done - 182120.\n",
      "Ep done - 182130.\n",
      "Ep done - 182140.\n",
      "Ep done - 182150.\n",
      "Ep done - 182160.\n",
      "Ep done - 182170.\n",
      "Ep done - 182180.\n",
      "Ep done - 182190.\n",
      "Ep done - 182200.\n",
      "Ep done - 182210.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | -1         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 854        |\n",
      "|    time_elapsed         | 18496      |\n",
      "|    total_timesteps      | 5246976    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.36795798 |\n",
      "|    clip_fraction        | 0.0325     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000163  |\n",
      "|    explained_variance   | 0.225      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0545     |\n",
      "|    n_updates            | 27760      |\n",
      "|    policy_gradient_loss | 0.00253    |\n",
      "|    value_loss           | 0.0426     |\n",
      "----------------------------------------\n",
      "Ep done - 182220.\n",
      "Ep done - 182230.\n",
      "Ep done - 182240.\n",
      "Ep done - 182250.\n",
      "Ep done - 182260.\n",
      "Ep done - 182270.\n",
      "Ep done - 182280.\n",
      "Ep done - 182290.\n",
      "Ep done - 182300.\n",
      "Ep done - 52410.\n",
      "Ep done - 52420.\n",
      "Ep done - 52430.\n",
      "Ep done - 52440.\n",
      "Ep done - 52450.\n",
      "Ep done - 52460.\n",
      "Ep done - 52470.\n",
      "Ep done - 52480.\n",
      "Ep done - 52490.\n",
      "Ep done - 52500.\n",
      "Eval num_timesteps=5250000, episode_reward=0.90 +/- 0.44\n",
      "Episode length: 30.94 +/- 0.24\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.9      |\n",
      "|    mean_reward          | 0.9       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5250000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9527076 |\n",
      "|    clip_fraction        | 0.0982    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000213 |\n",
      "|    explained_variance   | 0.355     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0145    |\n",
      "|    n_updates            | 27770     |\n",
      "|    policy_gradient_loss | -0.0159   |\n",
      "|    value_loss           | 0.00761   |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.9\n",
      "SELFPLAY: new best model, bumping up generation to 130\n",
      "Ep done - 182310.\n",
      "Ep done - 182320.\n",
      "Ep done - 182330.\n",
      "Ep done - 182340.\n",
      "Ep done - 182350.\n",
      "Ep done - 182360.\n",
      "Ep done - 182370.\n",
      "Ep done - 182380.\n",
      "Ep done - 182390.\n",
      "Ep done - 182400.\n",
      "Ep done - 182410.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 855      |\n",
      "|    time_elapsed    | 18515    |\n",
      "|    total_timesteps | 5253120  |\n",
      "---------------------------------\n",
      "Ep done - 182420.\n",
      "Ep done - 182430.\n",
      "Ep done - 182440.\n",
      "Ep done - 182450.\n",
      "Ep done - 182460.\n",
      "Ep done - 182470.\n",
      "Ep done - 182480.\n",
      "Ep done - 182490.\n",
      "Ep done - 182500.\n",
      "Ep done - 182510.\n",
      "Ep done - 182520.\n",
      "Ep done - 182530.\n",
      "Ep done - 182540.\n",
      "Ep done - 182550.\n",
      "Ep done - 182560.\n",
      "Ep done - 182570.\n",
      "Ep done - 182580.\n",
      "Ep done - 182590.\n",
      "Ep done - 182600.\n",
      "Ep done - 182610.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.8       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 856        |\n",
      "|    time_elapsed         | 18528      |\n",
      "|    total_timesteps      | 5259264    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55577344 |\n",
      "|    clip_fraction        | 0.0517     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00164   |\n",
      "|    explained_variance   | -0.202     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0315     |\n",
      "|    n_updates            | 27780      |\n",
      "|    policy_gradient_loss | -0.00633   |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "Ep done - 182620.\n",
      "Ep done - 182630.\n",
      "Ep done - 52510.\n",
      "Ep done - 52520.\n",
      "Ep done - 52530.\n",
      "Ep done - 52540.\n",
      "Ep done - 52550.\n",
      "Ep done - 52560.\n",
      "Ep done - 52570.\n",
      "Ep done - 52580.\n",
      "Ep done - 52590.\n",
      "Ep done - 52600.\n",
      "Eval num_timesteps=5260000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 30.13 +/- 0.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.2        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5260000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.83628845 |\n",
      "|    clip_fraction        | 0.066      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00286   |\n",
      "|    explained_variance   | 0.424      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0403     |\n",
      "|    n_updates            | 27790      |\n",
      "|    policy_gradient_loss | -0.00128   |\n",
      "|    value_loss           | 0.0613     |\n",
      "----------------------------------------\n",
      "Ep done - 182640.\n",
      "Ep done - 182650.\n",
      "Ep done - 182660.\n",
      "Ep done - 182670.\n",
      "Ep done - 182680.\n",
      "Ep done - 182690.\n",
      "Ep done - 182700.\n",
      "Ep done - 182710.\n",
      "Ep done - 182720.\n",
      "Ep done - 182730.\n",
      "Ep done - 182740.\n",
      "Ep done - 182750.\n",
      "Ep done - 182760.\n",
      "Ep done - 182770.\n",
      "Ep done - 182780.\n",
      "Ep done - 182790.\n",
      "Ep done - 182800.\n",
      "Ep done - 182810.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 857      |\n",
      "|    time_elapsed    | 18548    |\n",
      "|    total_timesteps | 5265408  |\n",
      "---------------------------------\n",
      "Ep done - 182820.\n",
      "Ep done - 182830.\n",
      "Ep done - 182840.\n",
      "Ep done - 182850.\n",
      "Ep done - 182860.\n",
      "Ep done - 182870.\n",
      "Ep done - 182880.\n",
      "Ep done - 182890.\n",
      "Ep done - 182900.\n",
      "Ep done - 182910.\n",
      "Ep done - 182920.\n",
      "Ep done - 182930.\n",
      "Ep done - 182940.\n",
      "Ep done - 182950.\n",
      "Ep done - 182960.\n",
      "Ep done - 52610.\n",
      "Ep done - 52620.\n",
      "Ep done - 52630.\n",
      "Ep done - 52640.\n",
      "Ep done - 52650.\n",
      "Ep done - 52660.\n",
      "Ep done - 52670.\n",
      "Ep done - 52680.\n",
      "Ep done - 52690.\n",
      "Ep done - 52700.\n",
      "Eval num_timesteps=5270000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.88 +/- 0.32\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.9      |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5270000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.053517  |\n",
      "|    clip_fraction        | 0.0786    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000445 |\n",
      "|    explained_variance   | -0.405    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.154     |\n",
      "|    n_updates            | 27800     |\n",
      "|    policy_gradient_loss | -0.00363  |\n",
      "|    value_loss           | 0.276     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 131\n",
      "Ep done - 182970.\n",
      "Ep done - 182980.\n",
      "Ep done - 182990.\n",
      "Ep done - 183000.\n",
      "Ep done - 183010.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31       |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 858      |\n",
      "|    time_elapsed    | 18570    |\n",
      "|    total_timesteps | 5271552  |\n",
      "---------------------------------\n",
      "Ep done - 183020.\n",
      "Ep done - 183030.\n",
      "Ep done - 183040.\n",
      "Ep done - 183050.\n",
      "Ep done - 183060.\n",
      "Ep done - 183070.\n",
      "Ep done - 183080.\n",
      "Ep done - 183090.\n",
      "Ep done - 183100.\n",
      "Ep done - 183110.\n",
      "Ep done - 183120.\n",
      "Ep done - 183130.\n",
      "Ep done - 183140.\n",
      "Ep done - 183150.\n",
      "Ep done - 183160.\n",
      "Ep done - 183170.\n",
      "Ep done - 183180.\n",
      "Ep done - 183190.\n",
      "Ep done - 183200.\n",
      "Ep done - 183210.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.96      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 859       |\n",
      "|    time_elapsed         | 18587     |\n",
      "|    total_timesteps      | 5277696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2708017 |\n",
      "|    clip_fraction        | 0.0939    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00498  |\n",
      "|    explained_variance   | 0.262     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.00329  |\n",
      "|    n_updates            | 27810     |\n",
      "|    policy_gradient_loss | -0.00632  |\n",
      "|    value_loss           | 0.00881   |\n",
      "---------------------------------------\n",
      "Ep done - 183220.\n",
      "Ep done - 183230.\n",
      "Ep done - 183240.\n",
      "Ep done - 183250.\n",
      "Ep done - 183260.\n",
      "Ep done - 183270.\n",
      "Ep done - 183280.\n",
      "Ep done - 183290.\n",
      "Ep done - 52710.\n",
      "Ep done - 52720.\n",
      "Ep done - 52730.\n",
      "Ep done - 52740.\n",
      "Ep done - 52750.\n",
      "Ep done - 52760.\n",
      "Ep done - 52770.\n",
      "Ep done - 52780.\n",
      "Ep done - 52790.\n",
      "Ep done - 52800.\n",
      "Eval num_timesteps=5280000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.02 +/- 0.14\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5280000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6688593 |\n",
      "|    clip_fraction        | 0.0345    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000354 |\n",
      "|    explained_variance   | 0.292     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0161   |\n",
      "|    n_updates            | 27820     |\n",
      "|    policy_gradient_loss | -0.008    |\n",
      "|    value_loss           | 0.0148    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 132\n",
      "Ep done - 183300.\n",
      "Ep done - 183310.\n",
      "Ep done - 183320.\n",
      "Ep done - 183330.\n",
      "Ep done - 183340.\n",
      "Ep done - 183350.\n",
      "Ep done - 183360.\n",
      "Ep done - 183370.\n",
      "Ep done - 183380.\n",
      "Ep done - 183390.\n",
      "Ep done - 183400.\n",
      "Ep done - 183410.\n",
      "Ep done - 183420.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.1     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 860      |\n",
      "|    time_elapsed    | 18607    |\n",
      "|    total_timesteps | 5283840  |\n",
      "---------------------------------\n",
      "Ep done - 183430.\n",
      "Ep done - 183440.\n",
      "Ep done - 183450.\n",
      "Ep done - 183460.\n",
      "Ep done - 183470.\n",
      "Ep done - 183480.\n",
      "Ep done - 183490.\n",
      "Ep done - 183500.\n",
      "Ep done - 183510.\n",
      "Ep done - 183520.\n",
      "Ep done - 183530.\n",
      "Ep done - 183540.\n",
      "Ep done - 183550.\n",
      "Ep done - 183560.\n",
      "Ep done - 183570.\n",
      "Ep done - 183580.\n",
      "Ep done - 183590.\n",
      "Ep done - 183600.\n",
      "Ep done - 183610.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.7       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 284        |\n",
      "|    iterations           | 861        |\n",
      "|    time_elapsed         | 18621      |\n",
      "|    total_timesteps      | 5289984    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.91509867 |\n",
      "|    clip_fraction        | 0.0458     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000525  |\n",
      "|    explained_variance   | 0.802      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 27830      |\n",
      "|    policy_gradient_loss | 0.00345    |\n",
      "|    value_loss           | 0.00174    |\n",
      "----------------------------------------\n",
      "Ep done - 183620.\n",
      "Ep done - 52810.\n",
      "Ep done - 52820.\n",
      "Ep done - 52830.\n",
      "Ep done - 52840.\n",
      "Ep done - 52850.\n",
      "Ep done - 52860.\n",
      "Ep done - 52870.\n",
      "Ep done - 52880.\n",
      "Ep done - 52890.\n",
      "Ep done - 52900.\n",
      "Eval num_timesteps=5290000, episode_reward=-0.10 +/- 0.99\n",
      "Episode length: 30.35 +/- 0.48\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30.4      |\n",
      "|    mean_reward          | -0.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5290000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1287904 |\n",
      "|    clip_fraction        | 0.0648    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000666 |\n",
      "|    explained_variance   | 0.872     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | -0.0151   |\n",
      "|    n_updates            | 27840     |\n",
      "|    policy_gradient_loss | -0.0138   |\n",
      "|    value_loss           | 0.00141   |\n",
      "---------------------------------------\n",
      "Ep done - 183630.\n",
      "Ep done - 183640.\n",
      "Ep done - 183650.\n",
      "Ep done - 183660.\n",
      "Ep done - 183670.\n",
      "Ep done - 183680.\n",
      "Ep done - 183690.\n",
      "Ep done - 183700.\n",
      "Ep done - 183710.\n",
      "Ep done - 183720.\n",
      "Ep done - 183730.\n",
      "Ep done - 183740.\n",
      "Ep done - 183750.\n",
      "Ep done - 183760.\n",
      "Ep done - 183770.\n",
      "Ep done - 183780.\n",
      "Ep done - 183790.\n",
      "Ep done - 183800.\n",
      "Ep done - 183810.\n",
      "Ep done - 183820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | -0.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 862      |\n",
      "|    time_elapsed    | 18640    |\n",
      "|    total_timesteps | 5296128  |\n",
      "---------------------------------\n",
      "Ep done - 183830.\n",
      "Ep done - 183840.\n",
      "Ep done - 183850.\n",
      "Ep done - 183860.\n",
      "Ep done - 183870.\n",
      "Ep done - 183880.\n",
      "Ep done - 183890.\n",
      "Ep done - 183900.\n",
      "Ep done - 183910.\n",
      "Ep done - 183920.\n",
      "Ep done - 183930.\n",
      "Ep done - 183940.\n",
      "Ep done - 183950.\n",
      "Ep done - 52910.\n",
      "Ep done - 52920.\n",
      "Ep done - 52930.\n",
      "Ep done - 52940.\n",
      "Ep done - 52950.\n",
      "Ep done - 52960.\n",
      "Ep done - 52970.\n",
      "Ep done - 52980.\n",
      "Ep done - 52990.\n",
      "Ep done - 53000.\n",
      "Eval num_timesteps=5300000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.32 +/- 0.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.3       |\n",
      "|    mean_reward          | 0.98       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5300000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81008655 |\n",
      "|    clip_fraction        | 0.0341     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000321  |\n",
      "|    explained_variance   | -0.128     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.135      |\n",
      "|    n_updates            | 27850      |\n",
      "|    policy_gradient_loss | -0.00594   |\n",
      "|    value_loss           | 0.292      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 133\n",
      "Ep done - 183960.\n",
      "Ep done - 183970.\n",
      "Ep done - 183980.\n",
      "Ep done - 183990.\n",
      "Ep done - 184000.\n",
      "Ep done - 184010.\n",
      "Ep done - 184020.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 863      |\n",
      "|    time_elapsed    | 18660    |\n",
      "|    total_timesteps | 5302272  |\n",
      "---------------------------------\n",
      "Ep done - 184030.\n",
      "Ep done - 184040.\n",
      "Ep done - 184050.\n",
      "Ep done - 184060.\n",
      "Ep done - 184070.\n",
      "Ep done - 184080.\n",
      "Ep done - 184090.\n",
      "Ep done - 184100.\n",
      "Ep done - 184110.\n",
      "Ep done - 184120.\n",
      "Ep done - 184130.\n",
      "Ep done - 184140.\n",
      "Ep done - 184150.\n",
      "Ep done - 184160.\n",
      "Ep done - 184170.\n",
      "Ep done - 184180.\n",
      "Ep done - 184190.\n",
      "Ep done - 184200.\n",
      "Ep done - 184210.\n",
      "Ep done - 184220.\n",
      "Ep done - 184230.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | -0.98         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 284           |\n",
      "|    iterations           | 864           |\n",
      "|    time_elapsed         | 18674         |\n",
      "|    total_timesteps      | 5308416       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1847933e-06 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -5.21e-05     |\n",
      "|    explained_variance   | -0.144        |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.0536        |\n",
      "|    n_updates            | 27860         |\n",
      "|    policy_gradient_loss | -4.69e-05     |\n",
      "|    value_loss           | 0.218         |\n",
      "-------------------------------------------\n",
      "Ep done - 184240.\n",
      "Ep done - 184250.\n",
      "Ep done - 184260.\n",
      "Ep done - 184270.\n",
      "Ep done - 184280.\n",
      "Ep done - 53010.\n",
      "Ep done - 53020.\n",
      "Ep done - 53030.\n",
      "Ep done - 53040.\n",
      "Ep done - 53050.\n",
      "Ep done - 53060.\n",
      "Ep done - 53070.\n",
      "Ep done - 53080.\n",
      "Ep done - 53090.\n",
      "Ep done - 53100.\n",
      "Eval num_timesteps=5310000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5310000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4367633 |\n",
      "|    clip_fraction        | 0.0506    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000171 |\n",
      "|    explained_variance   | 0.684     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0188    |\n",
      "|    n_updates            | 27870     |\n",
      "|    policy_gradient_loss | -0.00413  |\n",
      "|    value_loss           | 0.0379    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 134\n",
      "Ep done - 184290.\n",
      "Ep done - 184300.\n",
      "Ep done - 184310.\n",
      "Ep done - 184320.\n",
      "Ep done - 184330.\n",
      "Ep done - 184340.\n",
      "Ep done - 184350.\n",
      "Ep done - 184360.\n",
      "Ep done - 184370.\n",
      "Ep done - 184380.\n",
      "Ep done - 184390.\n",
      "Ep done - 184400.\n",
      "Ep done - 184410.\n",
      "Ep done - 184420.\n",
      "Ep done - 184430.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 865      |\n",
      "|    time_elapsed    | 18693    |\n",
      "|    total_timesteps | 5314560  |\n",
      "---------------------------------\n",
      "Ep done - 184440.\n",
      "Ep done - 184450.\n",
      "Ep done - 184460.\n",
      "Ep done - 184470.\n",
      "Ep done - 184480.\n",
      "Ep done - 184490.\n",
      "Ep done - 184500.\n",
      "Ep done - 184510.\n",
      "Ep done - 184520.\n",
      "Ep done - 184530.\n",
      "Ep done - 184540.\n",
      "Ep done - 184550.\n",
      "Ep done - 184560.\n",
      "Ep done - 184570.\n",
      "Ep done - 184580.\n",
      "Ep done - 184590.\n",
      "Ep done - 184600.\n",
      "Ep done - 184610.\n",
      "Ep done - 53110.\n",
      "Ep done - 53120.\n",
      "Ep done - 53130.\n",
      "Ep done - 53140.\n",
      "Ep done - 53150.\n",
      "Ep done - 53160.\n",
      "Ep done - 53170.\n",
      "Ep done - 53180.\n",
      "Ep done - 53190.\n",
      "Ep done - 53200.\n",
      "Eval num_timesteps=5320000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 0.98         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5320000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019919479 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.29e-06    |\n",
      "|    explained_variance   | -2.15        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.00613      |\n",
      "|    n_updates            | 27880        |\n",
      "|    policy_gradient_loss | -6.21e-05    |\n",
      "|    value_loss           | 0.0395       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 135\n",
      "Ep done - 184620.\n",
      "Ep done - 184630.\n",
      "Ep done - 184640.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 866      |\n",
      "|    time_elapsed    | 18712    |\n",
      "|    total_timesteps | 5320704  |\n",
      "---------------------------------\n",
      "Ep done - 184650.\n",
      "Ep done - 184660.\n",
      "Ep done - 184670.\n",
      "Ep done - 184680.\n",
      "Ep done - 184690.\n",
      "Ep done - 184700.\n",
      "Ep done - 184710.\n",
      "Ep done - 184720.\n",
      "Ep done - 184730.\n",
      "Ep done - 184740.\n",
      "Ep done - 184750.\n",
      "Ep done - 184760.\n",
      "Ep done - 184770.\n",
      "Ep done - 184780.\n",
      "Ep done - 184790.\n",
      "Ep done - 184800.\n",
      "Ep done - 184810.\n",
      "Ep done - 184820.\n",
      "Ep done - 184830.\n",
      "Ep done - 184840.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 867       |\n",
      "|    time_elapsed         | 18727     |\n",
      "|    total_timesteps      | 5326848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.65e-06 |\n",
      "|    explained_variance   | 0.529     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00765   |\n",
      "|    n_updates            | 27890     |\n",
      "|    policy_gradient_loss | -5.57e-09 |\n",
      "|    value_loss           | 0.0205    |\n",
      "---------------------------------------\n",
      "Ep done - 184850.\n",
      "Ep done - 184860.\n",
      "Ep done - 184870.\n",
      "Ep done - 184880.\n",
      "Ep done - 184890.\n",
      "Ep done - 184900.\n",
      "Ep done - 184910.\n",
      "Ep done - 184920.\n",
      "Ep done - 184930.\n",
      "Ep done - 184940.\n",
      "Ep done - 184950.\n",
      "Ep done - 53210.\n",
      "Ep done - 53220.\n",
      "Ep done - 53230.\n",
      "Ep done - 53240.\n",
      "Ep done - 53250.\n",
      "Ep done - 53260.\n",
      "Ep done - 53270.\n",
      "Ep done - 53280.\n",
      "Ep done - 53290.\n",
      "Ep done - 53300.\n",
      "Eval num_timesteps=5330000, episode_reward=0.98 +/- 0.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5330000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6066403 |\n",
      "|    clip_fraction        | 0.0163    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.25e-05 |\n",
      "|    explained_variance   | 0.333     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00357   |\n",
      "|    n_updates            | 27900     |\n",
      "|    policy_gradient_loss | -0.00353  |\n",
      "|    value_loss           | 0.0187    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.98\n",
      "SELFPLAY: new best model, bumping up generation to 136\n",
      "Ep done - 184960.\n",
      "Ep done - 184970.\n",
      "Ep done - 184980.\n",
      "Ep done - 184990.\n",
      "Ep done - 185000.\n",
      "Ep done - 185010.\n",
      "Ep done - 185020.\n",
      "Ep done - 185030.\n",
      "Ep done - 185040.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.92     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 868      |\n",
      "|    time_elapsed    | 18746    |\n",
      "|    total_timesteps | 5332992  |\n",
      "---------------------------------\n",
      "Ep done - 185050.\n",
      "Ep done - 185060.\n",
      "Ep done - 185070.\n",
      "Ep done - 185080.\n",
      "Ep done - 185090.\n",
      "Ep done - 185100.\n",
      "Ep done - 185110.\n",
      "Ep done - 185120.\n",
      "Ep done - 185130.\n",
      "Ep done - 185140.\n",
      "Ep done - 185150.\n",
      "Ep done - 185160.\n",
      "Ep done - 185170.\n",
      "Ep done - 185180.\n",
      "Ep done - 185190.\n",
      "Ep done - 185200.\n",
      "Ep done - 185210.\n",
      "Ep done - 185220.\n",
      "Ep done - 185230.\n",
      "Ep done - 185240.\n",
      "Ep done - 185250.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 30            |\n",
      "|    ep_rew_mean          | 0.96          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 284           |\n",
      "|    iterations           | 869           |\n",
      "|    time_elapsed         | 18761         |\n",
      "|    total_timesteps      | 5339136       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085942494 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -4.03e-05     |\n",
      "|    explained_variance   | 0.133         |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.0107        |\n",
      "|    n_updates            | 27910         |\n",
      "|    policy_gradient_loss | -0.000184     |\n",
      "|    value_loss           | 0.0358        |\n",
      "-------------------------------------------\n",
      "Ep done - 185260.\n",
      "Ep done - 185270.\n",
      "Ep done - 185280.\n",
      "Ep done - 53310.\n",
      "Ep done - 53320.\n",
      "Ep done - 53330.\n",
      "Ep done - 53340.\n",
      "Ep done - 53350.\n",
      "Ep done - 53360.\n",
      "Ep done - 53370.\n",
      "Ep done - 53380.\n",
      "Ep done - 53390.\n",
      "Ep done - 53400.\n",
      "Eval num_timesteps=5340000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -1        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5340000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8391376 |\n",
      "|    clip_fraction        | 0.0097    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.26e-05 |\n",
      "|    explained_variance   | 0.241     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.00308   |\n",
      "|    n_updates            | 27920     |\n",
      "|    policy_gradient_loss | 0.000359  |\n",
      "|    value_loss           | 0.0247    |\n",
      "---------------------------------------\n",
      "Ep done - 185290.\n",
      "Ep done - 185300.\n",
      "Ep done - 185310.\n",
      "Ep done - 185320.\n",
      "Ep done - 185330.\n",
      "Ep done - 185340.\n",
      "Ep done - 185350.\n",
      "Ep done - 185360.\n",
      "Ep done - 185370.\n",
      "Ep done - 185380.\n",
      "Ep done - 185390.\n",
      "Ep done - 185400.\n",
      "Ep done - 185410.\n",
      "Ep done - 185420.\n",
      "Ep done - 185430.\n",
      "Ep done - 185440.\n",
      "Ep done - 185450.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 870      |\n",
      "|    time_elapsed    | 18779    |\n",
      "|    total_timesteps | 5345280  |\n",
      "---------------------------------\n",
      "Ep done - 185460.\n",
      "Ep done - 185470.\n",
      "Ep done - 185480.\n",
      "Ep done - 185490.\n",
      "Ep done - 185500.\n",
      "Ep done - 185510.\n",
      "Ep done - 185520.\n",
      "Ep done - 185530.\n",
      "Ep done - 185540.\n",
      "Ep done - 185550.\n",
      "Ep done - 185560.\n",
      "Ep done - 185570.\n",
      "Ep done - 185580.\n",
      "Ep done - 185590.\n",
      "Ep done - 185600.\n",
      "Ep done - 185610.\n",
      "Ep done - 53410.\n",
      "Ep done - 53420.\n",
      "Ep done - 53430.\n",
      "Ep done - 53440.\n",
      "Ep done - 53450.\n",
      "Ep done - 53460.\n",
      "Ep done - 53470.\n",
      "Ep done - 53480.\n",
      "Ep done - 53490.\n",
      "Ep done - 53500.\n",
      "Eval num_timesteps=5350000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 1         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5350000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.3249    |\n",
      "|    clip_fraction        | 0.0937    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000523 |\n",
      "|    explained_variance   | -0.387    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.000804  |\n",
      "|    n_updates            | 27930     |\n",
      "|    policy_gradient_loss | -0.0199   |\n",
      "|    value_loss           | 0.0707    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 137\n",
      "Ep done - 185620.\n",
      "Ep done - 185630.\n",
      "Ep done - 185640.\n",
      "Ep done - 185650.\n",
      "Ep done - 185660.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 871      |\n",
      "|    time_elapsed    | 18798    |\n",
      "|    total_timesteps | 5351424  |\n",
      "---------------------------------\n",
      "Ep done - 185670.\n",
      "Ep done - 185680.\n",
      "Ep done - 185690.\n",
      "Ep done - 185700.\n",
      "Ep done - 185710.\n",
      "Ep done - 185720.\n",
      "Ep done - 185730.\n",
      "Ep done - 185740.\n",
      "Ep done - 185750.\n",
      "Ep done - 185760.\n",
      "Ep done - 185770.\n",
      "Ep done - 185780.\n",
      "Ep done - 185790.\n",
      "Ep done - 185800.\n",
      "Ep done - 185810.\n",
      "Ep done - 185820.\n",
      "Ep done - 185830.\n",
      "Ep done - 185840.\n",
      "Ep done - 185850.\n",
      "Ep done - 185860.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 284          |\n",
      "|    iterations           | 872          |\n",
      "|    time_elapsed         | 18812        |\n",
      "|    total_timesteps      | 5357568      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013418718 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.1e-05     |\n",
      "|    explained_variance   | -3.66        |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.0259       |\n",
      "|    n_updates            | 27940        |\n",
      "|    policy_gradient_loss | -1.27e-05    |\n",
      "|    value_loss           | 0.0244       |\n",
      "------------------------------------------\n",
      "Ep done - 185870.\n",
      "Ep done - 185880.\n",
      "Ep done - 185890.\n",
      "Ep done - 185900.\n",
      "Ep done - 185910.\n",
      "Ep done - 185920.\n",
      "Ep done - 185930.\n",
      "Ep done - 185940.\n",
      "Ep done - 185950.\n",
      "Ep done - 53510.\n",
      "Ep done - 53520.\n",
      "Ep done - 53530.\n",
      "Ep done - 53540.\n",
      "Ep done - 53550.\n",
      "Ep done - 53560.\n",
      "Ep done - 53570.\n",
      "Ep done - 53580.\n",
      "Ep done - 53590.\n",
      "Ep done - 53600.\n",
      "Eval num_timesteps=5360000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5360000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09728032 |\n",
      "|    clip_fraction        | 0.00708    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.17e-05  |\n",
      "|    explained_variance   | 0.673      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.00356    |\n",
      "|    n_updates            | 27950      |\n",
      "|    policy_gradient_loss | -0.000902  |\n",
      "|    value_loss           | 0.00894    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 138\n",
      "Ep done - 185960.\n",
      "Ep done - 185970.\n",
      "Ep done - 185980.\n",
      "Ep done - 185990.\n",
      "Ep done - 186000.\n",
      "Ep done - 186010.\n",
      "Ep done - 186020.\n",
      "Ep done - 186030.\n",
      "Ep done - 186040.\n",
      "Ep done - 186050.\n",
      "Ep done - 186060.\n",
      "Ep done - 186070.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 873      |\n",
      "|    time_elapsed    | 18837    |\n",
      "|    total_timesteps | 5363712  |\n",
      "---------------------------------\n",
      "Ep done - 186080.\n",
      "Ep done - 186090.\n",
      "Ep done - 186100.\n",
      "Ep done - 186110.\n",
      "Ep done - 186120.\n",
      "Ep done - 186130.\n",
      "Ep done - 186140.\n",
      "Ep done - 186150.\n",
      "Ep done - 186160.\n",
      "Ep done - 186170.\n",
      "Ep done - 186180.\n",
      "Ep done - 186190.\n",
      "Ep done - 186200.\n",
      "Ep done - 186210.\n",
      "Ep done - 186220.\n",
      "Ep done - 186230.\n",
      "Ep done - 186240.\n",
      "Ep done - 186250.\n",
      "Ep done - 186260.\n",
      "Ep done - 186270.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.16      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 874       |\n",
      "|    time_elapsed         | 18862     |\n",
      "|    total_timesteps      | 5369856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3109986 |\n",
      "|    clip_fraction        | 0.0156    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.88e-05 |\n",
      "|    explained_variance   | -0.138    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.127     |\n",
      "|    n_updates            | 27960     |\n",
      "|    policy_gradient_loss | 0.0468    |\n",
      "|    value_loss           | 0.24      |\n",
      "---------------------------------------\n",
      "Ep done - 186280.\n",
      "Ep done - 53610.\n",
      "Ep done - 53620.\n",
      "Ep done - 53630.\n",
      "Ep done - 53640.\n",
      "Ep done - 53650.\n",
      "Ep done - 53660.\n",
      "Ep done - 53670.\n",
      "Ep done - 53680.\n",
      "Ep done - 53690.\n",
      "Ep done - 53700.\n",
      "Eval num_timesteps=5370000, episode_reward=0.02 +/- 1.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.02      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5370000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.53e-06 |\n",
      "|    explained_variance   | 0.00748   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.071     |\n",
      "|    n_updates            | 27970     |\n",
      "|    policy_gradient_loss | 7.92e-10  |\n",
      "|    value_loss           | 0.263     |\n",
      "---------------------------------------\n",
      "Ep done - 186290.\n",
      "Ep done - 186300.\n",
      "Ep done - 186310.\n",
      "Ep done - 186320.\n",
      "Ep done - 186330.\n",
      "Ep done - 186340.\n",
      "Ep done - 186350.\n",
      "Ep done - 186360.\n",
      "Ep done - 186370.\n",
      "Ep done - 186380.\n",
      "Ep done - 186390.\n",
      "Ep done - 186400.\n",
      "Ep done - 186410.\n",
      "Ep done - 186420.\n",
      "Ep done - 186430.\n",
      "Ep done - 186440.\n",
      "Ep done - 186450.\n",
      "Ep done - 186460.\n",
      "Ep done - 186470.\n",
      "Ep done - 186480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 875      |\n",
      "|    time_elapsed    | 18887    |\n",
      "|    total_timesteps | 5376000  |\n",
      "---------------------------------\n",
      "Ep done - 186490.\n",
      "Ep done - 186500.\n",
      "Ep done - 186510.\n",
      "Ep done - 186520.\n",
      "Ep done - 186530.\n",
      "Ep done - 186540.\n",
      "Ep done - 186550.\n",
      "Ep done - 186560.\n",
      "Ep done - 186570.\n",
      "Ep done - 186580.\n",
      "Ep done - 186590.\n",
      "Ep done - 186600.\n",
      "Ep done - 186610.\n",
      "Ep done - 53710.\n",
      "Ep done - 53720.\n",
      "Ep done - 53730.\n",
      "Ep done - 53740.\n",
      "Ep done - 53750.\n",
      "Ep done - 53760.\n",
      "Ep done - 53770.\n",
      "Ep done - 53780.\n",
      "Ep done - 53790.\n",
      "Ep done - 53800.\n",
      "Eval num_timesteps=5380000, episode_reward=0.32 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.32      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5380000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.96e-06 |\n",
      "|    explained_variance   | 0.00938   |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.119     |\n",
      "|    n_updates            | 27980     |\n",
      "|    policy_gradient_loss | -1.06e-09 |\n",
      "|    value_loss           | 0.295     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.32\n",
      "SELFPLAY: new best model, bumping up generation to 139\n",
      "Ep done - 186620.\n",
      "Ep done - 186630.\n",
      "Ep done - 186640.\n",
      "Ep done - 186650.\n",
      "Ep done - 186660.\n",
      "Ep done - 186670.\n",
      "Ep done - 186680.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 876      |\n",
      "|    time_elapsed    | 18913    |\n",
      "|    total_timesteps | 5382144  |\n",
      "---------------------------------\n",
      "Ep done - 186690.\n",
      "Ep done - 186700.\n",
      "Ep done - 186710.\n",
      "Ep done - 186720.\n",
      "Ep done - 186730.\n",
      "Ep done - 186740.\n",
      "Ep done - 186750.\n",
      "Ep done - 186760.\n",
      "Ep done - 186770.\n",
      "Ep done - 186780.\n",
      "Ep done - 186790.\n",
      "Ep done - 186800.\n",
      "Ep done - 186810.\n",
      "Ep done - 186820.\n",
      "Ep done - 186830.\n",
      "Ep done - 186840.\n",
      "Ep done - 186850.\n",
      "Ep done - 186860.\n",
      "Ep done - 186870.\n",
      "Ep done - 186880.\n",
      "Ep done - 186890.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.26      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 877       |\n",
      "|    time_elapsed         | 18934     |\n",
      "|    total_timesteps      | 5388288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.77e-06 |\n",
      "|    explained_variance   | 0.0118    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.11      |\n",
      "|    n_updates            | 27990     |\n",
      "|    policy_gradient_loss | -2.55e-10 |\n",
      "|    value_loss           | 0.227     |\n",
      "---------------------------------------\n",
      "Ep done - 186900.\n",
      "Ep done - 186910.\n",
      "Ep done - 186920.\n",
      "Ep done - 186930.\n",
      "Ep done - 186940.\n",
      "Ep done - 186950.\n",
      "Ep done - 53810.\n",
      "Ep done - 53820.\n",
      "Ep done - 53830.\n",
      "Ep done - 53840.\n",
      "Ep done - 53850.\n",
      "Ep done - 53860.\n",
      "Ep done - 53870.\n",
      "Ep done - 53880.\n",
      "Ep done - 53890.\n",
      "Ep done - 53900.\n",
      "Eval num_timesteps=5390000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5390000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34515405 |\n",
      "|    clip_fraction        | 0.00931    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.1e-05   |\n",
      "|    explained_variance   | 0.302      |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.101      |\n",
      "|    n_updates            | 28000      |\n",
      "|    policy_gradient_loss | -0.00286   |\n",
      "|    value_loss           | 0.242      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 140\n",
      "Ep done - 186960.\n",
      "Ep done - 186970.\n",
      "Ep done - 186980.\n",
      "Ep done - 186990.\n",
      "Ep done - 187000.\n",
      "Ep done - 187010.\n",
      "Ep done - 187020.\n",
      "Ep done - 187030.\n",
      "Ep done - 187040.\n",
      "Ep done - 187050.\n",
      "Ep done - 187060.\n",
      "Ep done - 187070.\n",
      "Ep done - 187080.\n",
      "Ep done - 187090.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 878      |\n",
      "|    time_elapsed    | 18959    |\n",
      "|    total_timesteps | 5394432  |\n",
      "---------------------------------\n",
      "Ep done - 187100.\n",
      "Ep done - 187110.\n",
      "Ep done - 187120.\n",
      "Ep done - 187130.\n",
      "Ep done - 187140.\n",
      "Ep done - 187150.\n",
      "Ep done - 187160.\n",
      "Ep done - 187170.\n",
      "Ep done - 187180.\n",
      "Ep done - 187190.\n",
      "Ep done - 187200.\n",
      "Ep done - 187210.\n",
      "Ep done - 187220.\n",
      "Ep done - 187230.\n",
      "Ep done - 187240.\n",
      "Ep done - 187250.\n",
      "Ep done - 187260.\n",
      "Ep done - 187270.\n",
      "Ep done - 187280.\n",
      "Ep done - 53910.\n",
      "Ep done - 53920.\n",
      "Ep done - 53930.\n",
      "Ep done - 53940.\n",
      "Ep done - 53950.\n",
      "Ep done - 53960.\n",
      "Ep done - 53970.\n",
      "Ep done - 53980.\n",
      "Ep done - 53990.\n",
      "Ep done - 54000.\n",
      "Eval num_timesteps=5400000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 1          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5400000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54092425 |\n",
      "|    clip_fraction        | 0.0152     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.65e-05  |\n",
      "|    explained_variance   | -0.0223    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.0159     |\n",
      "|    n_updates            | 28010      |\n",
      "|    policy_gradient_loss | 0.215      |\n",
      "|    value_loss           | 0.0259     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 1.0\n",
      "SELFPLAY: new best model, bumping up generation to 141\n",
      "Ep done - 187290.\n",
      "Ep done - 187300.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.84     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 879      |\n",
      "|    time_elapsed    | 18987    |\n",
      "|    total_timesteps | 5400576  |\n",
      "---------------------------------\n",
      "Ep done - 187310.\n",
      "Ep done - 187320.\n",
      "Ep done - 187330.\n",
      "Ep done - 187340.\n",
      "Ep done - 187350.\n",
      "Ep done - 187360.\n",
      "Ep done - 187370.\n",
      "Ep done - 187380.\n",
      "Ep done - 187390.\n",
      "Ep done - 187400.\n",
      "Ep done - 187410.\n",
      "Ep done - 187420.\n",
      "Ep done - 187430.\n",
      "Ep done - 187440.\n",
      "Ep done - 187450.\n",
      "Ep done - 187460.\n",
      "Ep done - 187470.\n",
      "Ep done - 187480.\n",
      "Ep done - 187490.\n",
      "Ep done - 187500.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.08       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 284        |\n",
      "|    iterations           | 880        |\n",
      "|    time_elapsed         | 19010      |\n",
      "|    total_timesteps      | 5406720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.25724658 |\n",
      "|    clip_fraction        | 0.0293     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000914  |\n",
      "|    explained_variance   | -0.0623    |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.049      |\n",
      "|    n_updates            | 28020      |\n",
      "|    policy_gradient_loss | -0.0046    |\n",
      "|    value_loss           | 0.0513     |\n",
      "----------------------------------------\n",
      "Ep done - 187510.\n",
      "Ep done - 187520.\n",
      "Ep done - 187530.\n",
      "Ep done - 187540.\n",
      "Ep done - 187550.\n",
      "Ep done - 187560.\n",
      "Ep done - 187570.\n",
      "Ep done - 187580.\n",
      "Ep done - 187590.\n",
      "Ep done - 187600.\n",
      "Ep done - 187610.\n",
      "Ep done - 54010.\n",
      "Ep done - 54020.\n",
      "Ep done - 54030.\n",
      "Ep done - 54040.\n",
      "Ep done - 54050.\n",
      "Ep done - 54060.\n",
      "Ep done - 54070.\n",
      "Ep done - 54080.\n",
      "Ep done - 54090.\n",
      "Ep done - 54100.\n",
      "Eval num_timesteps=5410000, episode_reward=0.54 +/- 0.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5410000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.37925968 |\n",
      "|    clip_fraction        | 0.0256     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.000181  |\n",
      "|    explained_variance   | -0.152     |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.18       |\n",
      "|    n_updates            | 28030      |\n",
      "|    policy_gradient_loss | 0.00372    |\n",
      "|    value_loss           | 0.297      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.54\n",
      "SELFPLAY: new best model, bumping up generation to 142\n",
      "Ep done - 187620.\n",
      "Ep done - 187630.\n",
      "Ep done - 187640.\n",
      "Ep done - 187650.\n",
      "Ep done - 187660.\n",
      "Ep done - 187670.\n",
      "Ep done - 187680.\n",
      "Ep done - 187690.\n",
      "Ep done - 187700.\n",
      "Ep done - 187710.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 881      |\n",
      "|    time_elapsed    | 19039    |\n",
      "|    total_timesteps | 5412864  |\n",
      "---------------------------------\n",
      "Ep done - 187720.\n",
      "Ep done - 187730.\n",
      "Ep done - 187740.\n",
      "Ep done - 187750.\n",
      "Ep done - 187760.\n",
      "Ep done - 187770.\n",
      "Ep done - 187780.\n",
      "Ep done - 187790.\n",
      "Ep done - 187800.\n",
      "Ep done - 187810.\n",
      "Ep done - 187820.\n",
      "Ep done - 187830.\n",
      "Ep done - 187840.\n",
      "Ep done - 187850.\n",
      "Ep done - 187860.\n",
      "Ep done - 187870.\n",
      "Ep done - 187880.\n",
      "Ep done - 187890.\n",
      "Ep done - 187900.\n",
      "Ep done - 187910.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 284         |\n",
      "|    iterations           | 882         |\n",
      "|    time_elapsed         | 19065       |\n",
      "|    total_timesteps      | 5419008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001894319 |\n",
      "|    clip_fraction        | 0.000163    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000144   |\n",
      "|    explained_variance   | -0.364      |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.0553      |\n",
      "|    n_updates            | 28040       |\n",
      "|    policy_gradient_loss | 0.000209    |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Ep done - 187920.\n",
      "Ep done - 187930.\n",
      "Ep done - 187940.\n",
      "Ep done - 187950.\n",
      "Ep done - 54110.\n",
      "Ep done - 54120.\n",
      "Ep done - 54130.\n",
      "Ep done - 54140.\n",
      "Ep done - 54150.\n",
      "Ep done - 54160.\n",
      "Ep done - 54170.\n",
      "Ep done - 54180.\n",
      "Ep done - 54190.\n",
      "Ep done - 54200.\n",
      "Eval num_timesteps=5420000, episode_reward=0.18 +/- 0.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.18      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5420000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2859037 |\n",
      "|    clip_fraction        | 0.00835   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.29e-05 |\n",
      "|    explained_variance   | 0.274     |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.101     |\n",
      "|    n_updates            | 28050     |\n",
      "|    policy_gradient_loss | -0.000425 |\n",
      "|    value_loss           | 0.19      |\n",
      "---------------------------------------\n",
      "Ep done - 187960.\n",
      "Ep done - 187970.\n",
      "Ep done - 187980.\n",
      "Ep done - 187990.\n",
      "Ep done - 188000.\n",
      "Ep done - 188010.\n",
      "Ep done - 188020.\n",
      "Ep done - 188030.\n",
      "Ep done - 188040.\n",
      "Ep done - 188050.\n",
      "Ep done - 188060.\n",
      "Ep done - 188070.\n",
      "Ep done - 188080.\n",
      "Ep done - 188090.\n",
      "Ep done - 188100.\n",
      "Ep done - 188110.\n",
      "Ep done - 188120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 883      |\n",
      "|    time_elapsed    | 19094    |\n",
      "|    total_timesteps | 5425152  |\n",
      "---------------------------------\n",
      "Ep done - 188130.\n",
      "Ep done - 188140.\n",
      "Ep done - 188150.\n",
      "Ep done - 188160.\n",
      "Ep done - 188170.\n",
      "Ep done - 188180.\n",
      "Ep done - 188190.\n",
      "Ep done - 188200.\n",
      "Ep done - 188210.\n",
      "Ep done - 188220.\n",
      "Ep done - 188230.\n",
      "Ep done - 188240.\n",
      "Ep done - 188250.\n",
      "Ep done - 188260.\n",
      "Ep done - 188270.\n",
      "Ep done - 188280.\n",
      "Ep done - 54210.\n",
      "Ep done - 54220.\n",
      "Ep done - 54230.\n",
      "Ep done - 54240.\n",
      "Ep done - 54250.\n",
      "Ep done - 54260.\n",
      "Ep done - 54270.\n",
      "Ep done - 54280.\n",
      "Ep done - 54290.\n",
      "Ep done - 54300.\n",
      "Eval num_timesteps=5430000, episode_reward=0.30 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.3       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5430000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.8e-06  |\n",
      "|    explained_variance   | -0.517    |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.15      |\n",
      "|    n_updates            | 28060     |\n",
      "|    policy_gradient_loss | -5.87e-10 |\n",
      "|    value_loss           | 0.263     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.3\n",
      "SELFPLAY: new best model, bumping up generation to 143\n",
      "Ep done - 188290.\n",
      "Ep done - 188300.\n",
      "Ep done - 188310.\n",
      "Ep done - 188320.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 884      |\n",
      "|    time_elapsed    | 19120    |\n",
      "|    total_timesteps | 5431296  |\n",
      "---------------------------------\n",
      "Ep done - 188330.\n",
      "Ep done - 188340.\n",
      "Ep done - 188350.\n",
      "Ep done - 188360.\n",
      "Ep done - 188370.\n",
      "Ep done - 188380.\n",
      "Ep done - 188390.\n",
      "Ep done - 188400.\n",
      "Ep done - 188410.\n",
      "Ep done - 188420.\n",
      "Ep done - 188430.\n",
      "Ep done - 188440.\n",
      "Ep done - 188450.\n",
      "Ep done - 188460.\n",
      "Ep done - 188470.\n",
      "Ep done - 188480.\n",
      "Ep done - 188490.\n",
      "Ep done - 188500.\n",
      "Ep done - 188510.\n",
      "Ep done - 188520.\n",
      "Ep done - 188530.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.26      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 885       |\n",
      "|    time_elapsed         | 19140     |\n",
      "|    total_timesteps      | 5437440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.57e-06 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.175     |\n",
      "|    n_updates            | 28070     |\n",
      "|    policy_gradient_loss | 1.06e-09  |\n",
      "|    value_loss           | 0.272     |\n",
      "---------------------------------------\n",
      "Ep done - 188540.\n",
      "Ep done - 188550.\n",
      "Ep done - 188560.\n",
      "Ep done - 188570.\n",
      "Ep done - 188580.\n",
      "Ep done - 188590.\n",
      "Ep done - 188600.\n",
      "Ep done - 188610.\n",
      "Ep done - 54310.\n",
      "Ep done - 54320.\n",
      "Ep done - 54330.\n",
      "Ep done - 54340.\n",
      "Ep done - 54350.\n",
      "Ep done - 54360.\n",
      "Ep done - 54370.\n",
      "Ep done - 54380.\n",
      "Ep done - 54390.\n",
      "Ep done - 54400.\n",
      "Eval num_timesteps=5440000, episode_reward=0.36 +/- 0.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.36      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5440000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.41e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.137     |\n",
      "|    n_updates            | 28080     |\n",
      "|    policy_gradient_loss | -1.18e-10 |\n",
      "|    value_loss           | 0.265     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.36\n",
      "SELFPLAY: new best model, bumping up generation to 144\n",
      "Ep done - 188620.\n",
      "Ep done - 188630.\n",
      "Ep done - 188640.\n",
      "Ep done - 188650.\n",
      "Ep done - 188660.\n",
      "Ep done - 188670.\n",
      "Ep done - 188680.\n",
      "Ep done - 188690.\n",
      "Ep done - 188700.\n",
      "Ep done - 188710.\n",
      "Ep done - 188720.\n",
      "Ep done - 188730.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 886      |\n",
      "|    time_elapsed    | 19170    |\n",
      "|    total_timesteps | 5443584  |\n",
      "---------------------------------\n",
      "Ep done - 188740.\n",
      "Ep done - 188750.\n",
      "Ep done - 188760.\n",
      "Ep done - 188770.\n",
      "Ep done - 188780.\n",
      "Ep done - 188790.\n",
      "Ep done - 188800.\n",
      "Ep done - 188810.\n",
      "Ep done - 188820.\n",
      "Ep done - 188830.\n",
      "Ep done - 188840.\n",
      "Ep done - 188850.\n",
      "Ep done - 188860.\n",
      "Ep done - 188870.\n",
      "Ep done - 188880.\n",
      "Ep done - 188890.\n",
      "Ep done - 188900.\n",
      "Ep done - 188910.\n",
      "Ep done - 188920.\n",
      "Ep done - 188930.\n",
      "Ep done - 188940.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.36      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 887       |\n",
      "|    time_elapsed         | 19192     |\n",
      "|    total_timesteps      | 5449728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.28e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.114     |\n",
      "|    n_updates            | 28090     |\n",
      "|    policy_gradient_loss | -1.7e-10  |\n",
      "|    value_loss           | 0.287     |\n",
      "---------------------------------------\n",
      "Ep done - 188950.\n",
      "Ep done - 54410.\n",
      "Ep done - 54420.\n",
      "Ep done - 54430.\n",
      "Ep done - 54440.\n",
      "Ep done - 54450.\n",
      "Ep done - 54460.\n",
      "Ep done - 54470.\n",
      "Ep done - 54480.\n",
      "Ep done - 54490.\n",
      "Ep done - 54500.\n",
      "Eval num_timesteps=5450000, episode_reward=0.34 +/- 0.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.34      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5450000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.37e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.137     |\n",
      "|    n_updates            | 28100     |\n",
      "|    policy_gradient_loss | 3.84e-10  |\n",
      "|    value_loss           | 0.265     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.34\n",
      "SELFPLAY: new best model, bumping up generation to 145\n",
      "Ep done - 188960.\n",
      "Ep done - 188970.\n",
      "Ep done - 188980.\n",
      "Ep done - 188990.\n",
      "Ep done - 189000.\n",
      "Ep done - 189010.\n",
      "Ep done - 189020.\n",
      "Ep done - 189030.\n",
      "Ep done - 189040.\n",
      "Ep done - 189050.\n",
      "Ep done - 189060.\n",
      "Ep done - 189070.\n",
      "Ep done - 189080.\n",
      "Ep done - 189090.\n",
      "Ep done - 189100.\n",
      "Ep done - 189110.\n",
      "Ep done - 189120.\n",
      "Ep done - 189130.\n",
      "Ep done - 189140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 888      |\n",
      "|    time_elapsed    | 19225    |\n",
      "|    total_timesteps | 5455872  |\n",
      "---------------------------------\n",
      "Ep done - 189150.\n",
      "Ep done - 189160.\n",
      "Ep done - 189170.\n",
      "Ep done - 189180.\n",
      "Ep done - 189190.\n",
      "Ep done - 189200.\n",
      "Ep done - 189210.\n",
      "Ep done - 189220.\n",
      "Ep done - 189230.\n",
      "Ep done - 189240.\n",
      "Ep done - 189250.\n",
      "Ep done - 189260.\n",
      "Ep done - 189270.\n",
      "Ep done - 189280.\n",
      "Ep done - 54510.\n",
      "Ep done - 54520.\n",
      "Ep done - 54530.\n",
      "Ep done - 54540.\n",
      "Ep done - 54550.\n",
      "Ep done - 54560.\n",
      "Ep done - 54570.\n",
      "Ep done - 54580.\n",
      "Ep done - 54590.\n",
      "Ep done - 54600.\n",
      "Eval num_timesteps=5460000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.2       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5460000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.43e-06 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.132     |\n",
      "|    n_updates            | 28110     |\n",
      "|    policy_gradient_loss | -8.38e-10 |\n",
      "|    value_loss           | 0.27      |\n",
      "---------------------------------------\n",
      "Ep done - 189290.\n",
      "Ep done - 189300.\n",
      "Ep done - 189310.\n",
      "Ep done - 189320.\n",
      "Ep done - 189330.\n",
      "Ep done - 189340.\n",
      "Ep done - 189350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 889      |\n",
      "|    time_elapsed    | 19251    |\n",
      "|    total_timesteps | 5462016  |\n",
      "---------------------------------\n",
      "Ep done - 189360.\n",
      "Ep done - 189370.\n",
      "Ep done - 189380.\n",
      "Ep done - 189390.\n",
      "Ep done - 189400.\n",
      "Ep done - 189410.\n",
      "Ep done - 189420.\n",
      "Ep done - 189430.\n",
      "Ep done - 189440.\n",
      "Ep done - 189450.\n",
      "Ep done - 189460.\n",
      "Ep done - 189470.\n",
      "Ep done - 189480.\n",
      "Ep done - 189490.\n",
      "Ep done - 189500.\n",
      "Ep done - 189510.\n",
      "Ep done - 189520.\n",
      "Ep done - 189530.\n",
      "Ep done - 189540.\n",
      "Ep done - 189550.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.2       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 890       |\n",
      "|    time_elapsed         | 19275     |\n",
      "|    total_timesteps      | 5468160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.62e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.156     |\n",
      "|    n_updates            | 28120     |\n",
      "|    policy_gradient_loss | 6.31e-10  |\n",
      "|    value_loss           | 0.275     |\n",
      "---------------------------------------\n",
      "Ep done - 189560.\n",
      "Ep done - 189570.\n",
      "Ep done - 189580.\n",
      "Ep done - 189590.\n",
      "Ep done - 189600.\n",
      "Ep done - 189610.\n",
      "Ep done - 54610.\n",
      "Ep done - 54620.\n",
      "Ep done - 54630.\n",
      "Ep done - 54640.\n",
      "Ep done - 54650.\n",
      "Ep done - 54660.\n",
      "Ep done - 54670.\n",
      "Ep done - 54680.\n",
      "Ep done - 54690.\n",
      "Ep done - 54700.\n",
      "Eval num_timesteps=5470000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5470000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47227702 |\n",
      "|    clip_fraction        | 0.0163     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.56e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.181      |\n",
      "|    n_updates            | 28130      |\n",
      "|    policy_gradient_loss | 0.00272    |\n",
      "|    value_loss           | 0.278      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 146\n",
      "Ep done - 189620.\n",
      "Ep done - 189630.\n",
      "Ep done - 189640.\n",
      "Ep done - 189650.\n",
      "Ep done - 189660.\n",
      "Ep done - 189670.\n",
      "Ep done - 189680.\n",
      "Ep done - 189690.\n",
      "Ep done - 189700.\n",
      "Ep done - 189710.\n",
      "Ep done - 189720.\n",
      "Ep done - 189730.\n",
      "Ep done - 189740.\n",
      "Ep done - 189750.\n",
      "Ep done - 189760.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 891      |\n",
      "|    time_elapsed    | 19302    |\n",
      "|    total_timesteps | 5474304  |\n",
      "---------------------------------\n",
      "Ep done - 189770.\n",
      "Ep done - 189780.\n",
      "Ep done - 189790.\n",
      "Ep done - 189800.\n",
      "Ep done - 189810.\n",
      "Ep done - 189820.\n",
      "Ep done - 189830.\n",
      "Ep done - 189840.\n",
      "Ep done - 189850.\n",
      "Ep done - 189860.\n",
      "Ep done - 189870.\n",
      "Ep done - 189880.\n",
      "Ep done - 189890.\n",
      "Ep done - 189900.\n",
      "Ep done - 189910.\n",
      "Ep done - 189920.\n",
      "Ep done - 189930.\n",
      "Ep done - 189940.\n",
      "Ep done - 189950.\n",
      "Ep done - 54710.\n",
      "Ep done - 54720.\n",
      "Ep done - 54730.\n",
      "Ep done - 54740.\n",
      "Ep done - 54750.\n",
      "Ep done - 54760.\n",
      "Ep done - 54770.\n",
      "Ep done - 54780.\n",
      "Ep done - 54790.\n",
      "Ep done - 54800.\n",
      "Eval num_timesteps=5480000, episode_reward=0.54 +/- 0.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.54      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5480000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.22e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.105     |\n",
      "|    n_updates            | 28140     |\n",
      "|    policy_gradient_loss | -3.61e-10 |\n",
      "|    value_loss           | 0.216     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.54\n",
      "SELFPLAY: new best model, bumping up generation to 147\n",
      "Ep done - 189960.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 892      |\n",
      "|    time_elapsed    | 19326    |\n",
      "|    total_timesteps | 5480448  |\n",
      "---------------------------------\n",
      "Ep done - 189970.\n",
      "Ep done - 189980.\n",
      "Ep done - 189990.\n",
      "Ep done - 190000.\n",
      "Ep done - 190010.\n",
      "Ep done - 190020.\n",
      "Ep done - 190030.\n",
      "Ep done - 190040.\n",
      "Ep done - 190050.\n",
      "Ep done - 190060.\n",
      "Ep done - 190070.\n",
      "Ep done - 190080.\n",
      "Ep done - 190090.\n",
      "Ep done - 190100.\n",
      "Ep done - 190110.\n",
      "Ep done - 190120.\n",
      "Ep done - 190130.\n",
      "Ep done - 190140.\n",
      "Ep done - 190150.\n",
      "Ep done - 190160.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.54      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 893       |\n",
      "|    time_elapsed         | 19344     |\n",
      "|    total_timesteps      | 5486592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.71e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0936    |\n",
      "|    n_updates            | 28150     |\n",
      "|    policy_gradient_loss | -1.1e-10  |\n",
      "|    value_loss           | 0.209     |\n",
      "---------------------------------------\n",
      "Ep done - 190170.\n",
      "Ep done - 190180.\n",
      "Ep done - 190190.\n",
      "Ep done - 190200.\n",
      "Ep done - 190210.\n",
      "Ep done - 190220.\n",
      "Ep done - 190230.\n",
      "Ep done - 190240.\n",
      "Ep done - 190250.\n",
      "Ep done - 190260.\n",
      "Ep done - 190270.\n",
      "Ep done - 190280.\n",
      "Ep done - 54810.\n",
      "Ep done - 54820.\n",
      "Ep done - 54830.\n",
      "Ep done - 54840.\n",
      "Ep done - 54850.\n",
      "Ep done - 54860.\n",
      "Ep done - 54870.\n",
      "Ep done - 54880.\n",
      "Ep done - 54890.\n",
      "Ep done - 54900.\n",
      "Eval num_timesteps=5490000, episode_reward=0.50 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.5       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5490000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.05e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0973    |\n",
      "|    n_updates            | 28160     |\n",
      "|    policy_gradient_loss | -4.03e-11 |\n",
      "|    value_loss           | 0.199     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.5\n",
      "SELFPLAY: new best model, bumping up generation to 148\n",
      "Ep done - 190290.\n",
      "Ep done - 190300.\n",
      "Ep done - 190310.\n",
      "Ep done - 190320.\n",
      "Ep done - 190330.\n",
      "Ep done - 190340.\n",
      "Ep done - 190350.\n",
      "Ep done - 190360.\n",
      "Ep done - 190370.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 894      |\n",
      "|    time_elapsed    | 19365    |\n",
      "|    total_timesteps | 5492736  |\n",
      "---------------------------------\n",
      "Ep done - 190380.\n",
      "Ep done - 190390.\n",
      "Ep done - 190400.\n",
      "Ep done - 190410.\n",
      "Ep done - 190420.\n",
      "Ep done - 190430.\n",
      "Ep done - 190440.\n",
      "Ep done - 190450.\n",
      "Ep done - 190460.\n",
      "Ep done - 190470.\n",
      "Ep done - 190480.\n",
      "Ep done - 190490.\n",
      "Ep done - 190500.\n",
      "Ep done - 190510.\n",
      "Ep done - 190520.\n",
      "Ep done - 190530.\n",
      "Ep done - 190540.\n",
      "Ep done - 190550.\n",
      "Ep done - 190560.\n",
      "Ep done - 190570.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | 0.48     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 283      |\n",
      "|    iterations           | 895      |\n",
      "|    time_elapsed         | 19381    |\n",
      "|    total_timesteps      | 5498880  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.8e-06 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0864   |\n",
      "|    n_updates            | 28170    |\n",
      "|    policy_gradient_loss | 8.78e-10 |\n",
      "|    value_loss           | 0.204    |\n",
      "--------------------------------------\n",
      "Ep done - 190580.\n",
      "Ep done - 190590.\n",
      "Ep done - 190600.\n",
      "Ep done - 190610.\n",
      "Ep done - 54910.\n",
      "Ep done - 54920.\n",
      "Ep done - 54930.\n",
      "Ep done - 54940.\n",
      "Ep done - 54950.\n",
      "Ep done - 54960.\n",
      "Ep done - 54970.\n",
      "Ep done - 54980.\n",
      "Ep done - 54990.\n",
      "Ep done - 55000.\n",
      "Eval num_timesteps=5500000, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.89      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5500000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5226324 |\n",
      "|    clip_fraction        | 0.00931   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.32e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0938    |\n",
      "|    n_updates            | 28180     |\n",
      "|    policy_gradient_loss | -0.00356  |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.89\n",
      "SELFPLAY: new best model, bumping up generation to 149\n",
      "Ep done - 190620.\n",
      "Ep done - 190630.\n",
      "Ep done - 190640.\n",
      "Ep done - 190650.\n",
      "Ep done - 190660.\n",
      "Ep done - 190670.\n",
      "Ep done - 190680.\n",
      "Ep done - 190690.\n",
      "Ep done - 190700.\n",
      "Ep done - 190710.\n",
      "Ep done - 190720.\n",
      "Ep done - 190730.\n",
      "Ep done - 190740.\n",
      "Ep done - 190750.\n",
      "Ep done - 190760.\n",
      "Ep done - 190770.\n",
      "Ep done - 190780.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 0.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 896      |\n",
      "|    time_elapsed    | 19402    |\n",
      "|    total_timesteps | 5505024  |\n",
      "---------------------------------\n",
      "Ep done - 190790.\n",
      "Ep done - 190800.\n",
      "Ep done - 190810.\n",
      "Ep done - 190820.\n",
      "Ep done - 190830.\n",
      "Ep done - 190840.\n",
      "Ep done - 190850.\n",
      "Ep done - 190860.\n",
      "Ep done - 190870.\n",
      "Ep done - 190880.\n",
      "Ep done - 190890.\n",
      "Ep done - 190900.\n",
      "Ep done - 190910.\n",
      "Ep done - 190920.\n",
      "Ep done - 190930.\n",
      "Ep done - 190940.\n",
      "Ep done - 190950.\n",
      "Ep done - 55010.\n",
      "Ep done - 55020.\n",
      "Ep done - 55030.\n",
      "Ep done - 55040.\n",
      "Ep done - 55050.\n",
      "Ep done - 55060.\n",
      "Ep done - 55070.\n",
      "Ep done - 55080.\n",
      "Ep done - 55090.\n",
      "Ep done - 55100.\n",
      "Eval num_timesteps=5510000, episode_reward=0.10 +/- 0.99\n",
      "Episode length: 29.94 +/- 0.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 29.9         |\n",
      "|    mean_reward          | 0.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5510000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042893547 |\n",
      "|    clip_fraction        | 0.000423     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000232    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.138        |\n",
      "|    n_updates            | 28190        |\n",
      "|    policy_gradient_loss | 0.000196     |\n",
      "|    value_loss           | 0.277        |\n",
      "------------------------------------------\n",
      "Ep done - 190960.\n",
      "Ep done - 190970.\n",
      "Ep done - 190980.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 0.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 897      |\n",
      "|    time_elapsed    | 19424    |\n",
      "|    total_timesteps | 5511168  |\n",
      "---------------------------------\n",
      "Ep done - 190990.\n",
      "Ep done - 191000.\n",
      "Ep done - 191010.\n",
      "Ep done - 191020.\n",
      "Ep done - 191030.\n",
      "Ep done - 191040.\n",
      "Ep done - 191050.\n",
      "Ep done - 191060.\n",
      "Ep done - 191070.\n",
      "Ep done - 191080.\n",
      "Ep done - 191090.\n",
      "Ep done - 191100.\n",
      "Ep done - 191110.\n",
      "Ep done - 191120.\n",
      "Ep done - 191130.\n",
      "Ep done - 191140.\n",
      "Ep done - 191150.\n",
      "Ep done - 191160.\n",
      "Ep done - 191170.\n",
      "Ep done - 191180.\n",
      "Ep done - 191190.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 29.9         |\n",
      "|    ep_rew_mean          | 0.18         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 283          |\n",
      "|    iterations           | 898          |\n",
      "|    time_elapsed         | 19441        |\n",
      "|    total_timesteps      | 5517312      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.507252e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82e-05    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.118        |\n",
      "|    n_updates            | 28200        |\n",
      "|    policy_gradient_loss | 6.5e-08      |\n",
      "|    value_loss           | 0.297        |\n",
      "------------------------------------------\n",
      "Ep done - 191200.\n",
      "Ep done - 191210.\n",
      "Ep done - 191220.\n",
      "Ep done - 191230.\n",
      "Ep done - 191240.\n",
      "Ep done - 191250.\n",
      "Ep done - 191260.\n",
      "Ep done - 191270.\n",
      "Ep done - 191280.\n",
      "Ep done - 55110.\n",
      "Ep done - 55120.\n",
      "Ep done - 55130.\n",
      "Ep done - 55140.\n",
      "Ep done - 55150.\n",
      "Ep done - 55160.\n",
      "Ep done - 55170.\n",
      "Ep done - 55180.\n",
      "Ep done - 55190.\n",
      "Ep done - 55200.\n",
      "Eval num_timesteps=5520000, episode_reward=0.24 +/- 0.97\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.24       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5520000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15215449 |\n",
      "|    clip_fraction        | 0.00286    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.19e-05  |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.217      |\n",
      "|    n_updates            | 28210      |\n",
      "|    policy_gradient_loss | -0.000589  |\n",
      "|    value_loss           | 0.28       |\n",
      "----------------------------------------\n",
      "Ep done - 191290.\n",
      "Ep done - 191300.\n",
      "Ep done - 191310.\n",
      "Ep done - 191320.\n",
      "Ep done - 191330.\n",
      "Ep done - 191340.\n",
      "Ep done - 191350.\n",
      "Ep done - 191360.\n",
      "Ep done - 191370.\n",
      "Ep done - 191380.\n",
      "Ep done - 191390.\n",
      "Ep done - 191400.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 899      |\n",
      "|    time_elapsed    | 19465    |\n",
      "|    total_timesteps | 5523456  |\n",
      "---------------------------------\n",
      "Ep done - 191410.\n",
      "Ep done - 191420.\n",
      "Ep done - 191430.\n",
      "Ep done - 191440.\n",
      "Ep done - 191450.\n",
      "Ep done - 191460.\n",
      "Ep done - 191470.\n",
      "Ep done - 191480.\n",
      "Ep done - 191490.\n",
      "Ep done - 191500.\n",
      "Ep done - 191510.\n",
      "Ep done - 191520.\n",
      "Ep done - 191530.\n",
      "Ep done - 191540.\n",
      "Ep done - 191550.\n",
      "Ep done - 191560.\n",
      "Ep done - 191570.\n",
      "Ep done - 191580.\n",
      "Ep done - 191590.\n",
      "Ep done - 191600.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30           |\n",
      "|    ep_rew_mean          | 0.25         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 283          |\n",
      "|    iterations           | 900          |\n",
      "|    time_elapsed         | 19486        |\n",
      "|    total_timesteps      | 5529600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.566996e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24e-05    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.12         |\n",
      "|    n_updates            | 28220        |\n",
      "|    policy_gradient_loss | -5.3e-08     |\n",
      "|    value_loss           | 0.259        |\n",
      "------------------------------------------\n",
      "Ep done - 191610.\n",
      "Ep done - 55210.\n",
      "Ep done - 55220.\n",
      "Ep done - 55230.\n",
      "Ep done - 55240.\n",
      "Ep done - 55250.\n",
      "Ep done - 55260.\n",
      "Ep done - 55270.\n",
      "Ep done - 55280.\n",
      "Ep done - 55290.\n",
      "Ep done - 55300.\n",
      "Eval num_timesteps=5530000, episode_reward=0.04 +/- 0.99\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 30            |\n",
      "|    mean_reward          | 0.04          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 5530000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6969549e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.67e-05     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.007         |\n",
      "|    loss                 | 0.119         |\n",
      "|    n_updates            | 28230         |\n",
      "|    policy_gradient_loss | 2.87e-08      |\n",
      "|    value_loss           | 0.266         |\n",
      "-------------------------------------------\n",
      "Ep done - 191620.\n",
      "Ep done - 191630.\n",
      "Ep done - 191640.\n",
      "Ep done - 191650.\n",
      "Ep done - 191660.\n",
      "Ep done - 191670.\n",
      "Ep done - 191680.\n",
      "Ep done - 191690.\n",
      "Ep done - 191700.\n",
      "Ep done - 191710.\n",
      "Ep done - 191720.\n",
      "Ep done - 191730.\n",
      "Ep done - 191740.\n",
      "Ep done - 191750.\n",
      "Ep done - 191760.\n",
      "Ep done - 191770.\n",
      "Ep done - 191780.\n",
      "Ep done - 191790.\n",
      "Ep done - 191800.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 901      |\n",
      "|    time_elapsed    | 19512    |\n",
      "|    total_timesteps | 5535744  |\n",
      "---------------------------------\n",
      "Ep done - 191810.\n",
      "Ep done - 191820.\n",
      "Ep done - 191830.\n",
      "Ep done - 191840.\n",
      "Ep done - 191850.\n",
      "Ep done - 191860.\n",
      "Ep done - 191870.\n",
      "Ep done - 191880.\n",
      "Ep done - 191890.\n",
      "Ep done - 191900.\n",
      "Ep done - 191910.\n",
      "Ep done - 191920.\n",
      "Ep done - 191930.\n",
      "Ep done - 191940.\n",
      "Ep done - 191950.\n",
      "Ep done - 55310.\n",
      "Ep done - 55320.\n",
      "Ep done - 55330.\n",
      "Ep done - 55340.\n",
      "Ep done - 55350.\n",
      "Ep done - 55360.\n",
      "Ep done - 55370.\n",
      "Ep done - 55380.\n",
      "Ep done - 55390.\n",
      "Ep done - 55400.\n",
      "Eval num_timesteps=5540000, episode_reward=0.17 +/- 0.96\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 0.17         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5540000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.566996e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.87e-05    |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.007        |\n",
      "|    loss                 | 0.138        |\n",
      "|    n_updates            | 28240        |\n",
      "|    policy_gradient_loss | 1.42e-06     |\n",
      "|    value_loss           | 0.277        |\n",
      "------------------------------------------\n",
      "Ep done - 191960.\n",
      "Ep done - 191970.\n",
      "Ep done - 191980.\n",
      "Ep done - 191990.\n",
      "Ep done - 192000.\n",
      "Ep done - 192010.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 902      |\n",
      "|    time_elapsed    | 19535    |\n",
      "|    total_timesteps | 5541888  |\n",
      "---------------------------------\n",
      "Ep done - 192020.\n",
      "Ep done - 192030.\n",
      "Ep done - 192040.\n",
      "Ep done - 192050.\n",
      "Ep done - 192060.\n",
      "Ep done - 192070.\n",
      "Ep done - 192080.\n",
      "Ep done - 192090.\n",
      "Ep done - 192100.\n",
      "Ep done - 192110.\n",
      "Ep done - 192120.\n",
      "Ep done - 192130.\n",
      "Ep done - 192140.\n",
      "Ep done - 192150.\n",
      "Ep done - 192160.\n",
      "Ep done - 192170.\n",
      "Ep done - 192180.\n",
      "Ep done - 192190.\n",
      "Ep done - 192200.\n",
      "Ep done - 192210.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.15      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 903       |\n",
      "|    time_elapsed         | 19554     |\n",
      "|    total_timesteps      | 5548032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.34e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.2       |\n",
      "|    n_updates            | 28250     |\n",
      "|    policy_gradient_loss | -1.73e-09 |\n",
      "|    value_loss           | 0.255     |\n",
      "---------------------------------------\n",
      "Ep done - 192220.\n",
      "Ep done - 192230.\n",
      "Ep done - 192240.\n",
      "Ep done - 192250.\n",
      "Ep done - 192260.\n",
      "Ep done - 192270.\n",
      "Ep done - 192280.\n",
      "Ep done - 55410.\n",
      "Ep done - 55420.\n",
      "Ep done - 55430.\n",
      "Ep done - 55440.\n",
      "Ep done - 55450.\n",
      "Ep done - 55460.\n",
      "Ep done - 55470.\n",
      "Ep done - 55480.\n",
      "Ep done - 55490.\n",
      "Ep done - 55500.\n",
      "Eval num_timesteps=5550000, episode_reward=0.18 +/- 0.97\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.18      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5550000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.58e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.135     |\n",
      "|    n_updates            | 28260     |\n",
      "|    policy_gradient_loss | -1.53e-09 |\n",
      "|    value_loss           | 0.285     |\n",
      "---------------------------------------\n",
      "Ep done - 192290.\n",
      "Ep done - 192300.\n",
      "Ep done - 192310.\n",
      "Ep done - 192320.\n",
      "Ep done - 192330.\n",
      "Ep done - 192340.\n",
      "Ep done - 192350.\n",
      "Ep done - 192360.\n",
      "Ep done - 192370.\n",
      "Ep done - 192380.\n",
      "Ep done - 192390.\n",
      "Ep done - 192400.\n",
      "Ep done - 192410.\n",
      "Ep done - 192420.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.22     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 904      |\n",
      "|    time_elapsed    | 19579    |\n",
      "|    total_timesteps | 5554176  |\n",
      "---------------------------------\n",
      "Ep done - 192430.\n",
      "Ep done - 192440.\n",
      "Ep done - 192450.\n",
      "Ep done - 192460.\n",
      "Ep done - 192470.\n",
      "Ep done - 192480.\n",
      "Ep done - 192490.\n",
      "Ep done - 192500.\n",
      "Ep done - 192510.\n",
      "Ep done - 192520.\n",
      "Ep done - 192530.\n",
      "Ep done - 192540.\n",
      "Ep done - 192550.\n",
      "Ep done - 192560.\n",
      "Ep done - 192570.\n",
      "Ep done - 192580.\n",
      "Ep done - 192590.\n",
      "Ep done - 192600.\n",
      "Ep done - 192610.\n",
      "Ep done - 55510.\n",
      "Ep done - 55520.\n",
      "Ep done - 55530.\n",
      "Ep done - 55540.\n",
      "Ep done - 55550.\n",
      "Ep done - 55560.\n",
      "Ep done - 55570.\n",
      "Ep done - 55580.\n",
      "Ep done - 55590.\n",
      "Ep done - 55600.\n",
      "Eval num_timesteps=5560000, episode_reward=0.19 +/- 0.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.19      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5560000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.57e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.132     |\n",
      "|    n_updates            | 28270     |\n",
      "|    policy_gradient_loss | 4.34e-08  |\n",
      "|    value_loss           | 0.271     |\n",
      "---------------------------------------\n",
      "Ep done - 192620.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.21     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 905      |\n",
      "|    time_elapsed    | 19605    |\n",
      "|    total_timesteps | 5560320  |\n",
      "---------------------------------\n",
      "Ep done - 192630.\n",
      "Ep done - 192640.\n",
      "Ep done - 192650.\n",
      "Ep done - 192660.\n",
      "Ep done - 192670.\n",
      "Ep done - 192680.\n",
      "Ep done - 192690.\n",
      "Ep done - 192700.\n",
      "Ep done - 192710.\n",
      "Ep done - 192720.\n",
      "Ep done - 192730.\n",
      "Ep done - 192740.\n",
      "Ep done - 192750.\n",
      "Ep done - 192760.\n",
      "Ep done - 192770.\n",
      "Ep done - 192780.\n",
      "Ep done - 192790.\n",
      "Ep done - 192800.\n",
      "Ep done - 192810.\n",
      "Ep done - 192820.\n",
      "Ep done - 192830.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 906         |\n",
      "|    time_elapsed         | 19621       |\n",
      "|    total_timesteps      | 5566464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086775005 |\n",
      "|    clip_fraction        | 0.000602    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.96e-05   |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.135       |\n",
      "|    n_updates            | 28280       |\n",
      "|    policy_gradient_loss | -8.42e-05   |\n",
      "|    value_loss           | 0.274       |\n",
      "-----------------------------------------\n",
      "Ep done - 192840.\n",
      "Ep done - 192850.\n",
      "Ep done - 192860.\n",
      "Ep done - 192870.\n",
      "Ep done - 192880.\n",
      "Ep done - 192890.\n",
      "Ep done - 192900.\n",
      "Ep done - 192910.\n",
      "Ep done - 192920.\n",
      "Ep done - 192930.\n",
      "Ep done - 192940.\n",
      "Ep done - 192950.\n",
      "Ep done - 55610.\n",
      "Ep done - 55620.\n",
      "Ep done - 55630.\n",
      "Ep done - 55640.\n",
      "Ep done - 55650.\n",
      "Ep done - 55660.\n",
      "Ep done - 55670.\n",
      "Ep done - 55680.\n",
      "Ep done - 55690.\n",
      "Ep done - 55700.\n",
      "Eval num_timesteps=5570000, episode_reward=0.76 +/- 0.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.76      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5570000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.24e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0776    |\n",
      "|    n_updates            | 28290     |\n",
      "|    policy_gradient_loss | -3.11e-10 |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 150\n",
      "Ep done - 192960.\n",
      "Ep done - 192970.\n",
      "Ep done - 192980.\n",
      "Ep done - 192990.\n",
      "Ep done - 193000.\n",
      "Ep done - 193010.\n",
      "Ep done - 193020.\n",
      "Ep done - 193030.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 907      |\n",
      "|    time_elapsed    | 19643    |\n",
      "|    total_timesteps | 5572608  |\n",
      "---------------------------------\n",
      "Ep done - 193040.\n",
      "Ep done - 193050.\n",
      "Ep done - 193060.\n",
      "Ep done - 193070.\n",
      "Ep done - 193080.\n",
      "Ep done - 193090.\n",
      "Ep done - 193100.\n",
      "Ep done - 193110.\n",
      "Ep done - 193120.\n",
      "Ep done - 193130.\n",
      "Ep done - 193140.\n",
      "Ep done - 193150.\n",
      "Ep done - 193160.\n",
      "Ep done - 193170.\n",
      "Ep done - 193180.\n",
      "Ep done - 193190.\n",
      "Ep done - 193200.\n",
      "Ep done - 193210.\n",
      "Ep done - 193220.\n",
      "Ep done - 193230.\n",
      "Ep done - 193240.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 908         |\n",
      "|    time_elapsed         | 19668       |\n",
      "|    total_timesteps      | 5578752     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000882295 |\n",
      "|    clip_fraction        | 4.88e-05    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.86e-06   |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.007       |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 28300       |\n",
      "|    policy_gradient_loss | -1.3e-05    |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n",
      "Ep done - 193250.\n",
      "Ep done - 193260.\n",
      "Ep done - 193270.\n",
      "Ep done - 193280.\n",
      "Ep done - 55710.\n",
      "Ep done - 55720.\n",
      "Ep done - 55730.\n",
      "Ep done - 55740.\n",
      "Ep done - 55750.\n",
      "Ep done - 55760.\n",
      "Ep done - 55770.\n",
      "Ep done - 55780.\n",
      "Ep done - 55790.\n",
      "Ep done - 55800.\n",
      "Eval num_timesteps=5580000, episode_reward=-0.12 +/- 0.99\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | -0.12     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5580000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.71e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.163     |\n",
      "|    n_updates            | 28310     |\n",
      "|    policy_gradient_loss | -7.17e-10 |\n",
      "|    value_loss           | 0.3       |\n",
      "---------------------------------------\n",
      "Ep done - 193290.\n",
      "Ep done - 193300.\n",
      "Ep done - 193310.\n",
      "Ep done - 193320.\n",
      "Ep done - 193330.\n",
      "Ep done - 193340.\n",
      "Ep done - 193350.\n",
      "Ep done - 193360.\n",
      "Ep done - 193370.\n",
      "Ep done - 193380.\n",
      "Ep done - 193390.\n",
      "Ep done - 193400.\n",
      "Ep done - 193410.\n",
      "Ep done - 193420.\n",
      "Ep done - 193430.\n",
      "Ep done - 193440.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | -0.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 909      |\n",
      "|    time_elapsed    | 19705    |\n",
      "|    total_timesteps | 5584896  |\n",
      "---------------------------------\n",
      "Ep done - 193450.\n",
      "Ep done - 193460.\n",
      "Ep done - 193470.\n",
      "Ep done - 193480.\n",
      "Ep done - 193490.\n",
      "Ep done - 193500.\n",
      "Ep done - 193510.\n",
      "Ep done - 193520.\n",
      "Ep done - 193530.\n",
      "Ep done - 193540.\n",
      "Ep done - 193550.\n",
      "Ep done - 193560.\n",
      "Ep done - 193570.\n",
      "Ep done - 193580.\n",
      "Ep done - 193590.\n",
      "Ep done - 193600.\n",
      "Ep done - 193610.\n",
      "Ep done - 55810.\n",
      "Ep done - 55820.\n",
      "Ep done - 55830.\n",
      "Ep done - 55840.\n",
      "Ep done - 55850.\n",
      "Ep done - 55860.\n",
      "Ep done - 55870.\n",
      "Ep done - 55880.\n",
      "Ep done - 55890.\n",
      "Ep done - 55900.\n",
      "Eval num_timesteps=5590000, episode_reward=0.66 +/- 0.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5590000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15415488 |\n",
      "|    clip_fraction        | 0.00365    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.6e-05   |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.007      |\n",
      "|    loss                 | 0.14       |\n",
      "|    n_updates            | 28320      |\n",
      "|    policy_gradient_loss | -0.00121   |\n",
      "|    value_loss           | 0.296      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.66\n",
      "SELFPLAY: new best model, bumping up generation to 151\n",
      "Ep done - 193620.\n",
      "Ep done - 193630.\n",
      "Ep done - 193640.\n",
      "Ep done - 193650.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.74     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 910      |\n",
      "|    time_elapsed    | 19727    |\n",
      "|    total_timesteps | 5591040  |\n",
      "---------------------------------\n",
      "Ep done - 193660.\n",
      "Ep done - 193670.\n",
      "Ep done - 193680.\n",
      "Ep done - 193690.\n",
      "Ep done - 193700.\n",
      "Ep done - 193710.\n",
      "Ep done - 193720.\n",
      "Ep done - 193730.\n",
      "Ep done - 193740.\n",
      "Ep done - 193750.\n",
      "Ep done - 193760.\n",
      "Ep done - 193770.\n",
      "Ep done - 193780.\n",
      "Ep done - 193790.\n",
      "Ep done - 193800.\n",
      "Ep done - 193810.\n",
      "Ep done - 193820.\n",
      "Ep done - 193830.\n",
      "Ep done - 193840.\n",
      "Ep done - 193850.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.68      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 911       |\n",
      "|    time_elapsed         | 19746     |\n",
      "|    total_timesteps      | 5597184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.96e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0482    |\n",
      "|    n_updates            | 28330     |\n",
      "|    policy_gradient_loss | -8.26e-10 |\n",
      "|    value_loss           | 0.171     |\n",
      "---------------------------------------\n",
      "Ep done - 193860.\n",
      "Ep done - 193870.\n",
      "Ep done - 193880.\n",
      "Ep done - 193890.\n",
      "Ep done - 193900.\n",
      "Ep done - 193910.\n",
      "Ep done - 193920.\n",
      "Ep done - 193930.\n",
      "Ep done - 193940.\n",
      "Ep done - 193950.\n",
      "Ep done - 55910.\n",
      "Ep done - 55920.\n",
      "Ep done - 55930.\n",
      "Ep done - 55940.\n",
      "Ep done - 55950.\n",
      "Ep done - 55960.\n",
      "Ep done - 55970.\n",
      "Ep done - 55980.\n",
      "Ep done - 55990.\n",
      "Ep done - 56000.\n",
      "Eval num_timesteps=5600000, episode_reward=0.76 +/- 0.65\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.76      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5600000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.42e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0642    |\n",
      "|    n_updates            | 28340     |\n",
      "|    policy_gradient_loss | -8.31e-10 |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 152\n",
      "Ep done - 193960.\n",
      "Ep done - 193970.\n",
      "Ep done - 193980.\n",
      "Ep done - 193990.\n",
      "Ep done - 194000.\n",
      "Ep done - 194010.\n",
      "Ep done - 194020.\n",
      "Ep done - 194030.\n",
      "Ep done - 194040.\n",
      "Ep done - 194050.\n",
      "Ep done - 194060.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.8      |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 912      |\n",
      "|    time_elapsed    | 19770    |\n",
      "|    total_timesteps | 5603328  |\n",
      "---------------------------------\n",
      "Ep done - 194070.\n",
      "Ep done - 194080.\n",
      "Ep done - 194090.\n",
      "Ep done - 194100.\n",
      "Ep done - 194110.\n",
      "Ep done - 194120.\n",
      "Ep done - 194130.\n",
      "Ep done - 194140.\n",
      "Ep done - 194150.\n",
      "Ep done - 194160.\n",
      "Ep done - 194170.\n",
      "Ep done - 194180.\n",
      "Ep done - 194190.\n",
      "Ep done - 194200.\n",
      "Ep done - 194210.\n",
      "Ep done - 194220.\n",
      "Ep done - 194230.\n",
      "Ep done - 194240.\n",
      "Ep done - 194250.\n",
      "Ep done - 194260.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.56      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 913       |\n",
      "|    time_elapsed         | 19791     |\n",
      "|    total_timesteps      | 5609472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.76e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0784    |\n",
      "|    n_updates            | 28350     |\n",
      "|    policy_gradient_loss | -1.43e-10 |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "Ep done - 194270.\n",
      "Ep done - 194280.\n",
      "Ep done - 56010.\n",
      "Ep done - 56020.\n",
      "Ep done - 56030.\n",
      "Ep done - 56040.\n",
      "Ep done - 56050.\n",
      "Ep done - 56060.\n",
      "Ep done - 56070.\n",
      "Ep done - 56080.\n",
      "Ep done - 56090.\n",
      "Ep done - 56100.\n",
      "Eval num_timesteps=5610000, episode_reward=0.80 +/- 0.60\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.8       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5610000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.35e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.112     |\n",
      "|    n_updates            | 28360     |\n",
      "|    policy_gradient_loss | -6.98e-10 |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.8\n",
      "SELFPLAY: new best model, bumping up generation to 153\n",
      "Ep done - 194290.\n",
      "Ep done - 194300.\n",
      "Ep done - 194310.\n",
      "Ep done - 194320.\n",
      "Ep done - 194330.\n",
      "Ep done - 194340.\n",
      "Ep done - 194350.\n",
      "Ep done - 194360.\n",
      "Ep done - 194370.\n",
      "Ep done - 194380.\n",
      "Ep done - 194390.\n",
      "Ep done - 194400.\n",
      "Ep done - 194410.\n",
      "Ep done - 194420.\n",
      "Ep done - 194430.\n",
      "Ep done - 194440.\n",
      "Ep done - 194450.\n",
      "Ep done - 194460.\n",
      "Ep done - 194470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.74     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 914      |\n",
      "|    time_elapsed    | 19809    |\n",
      "|    total_timesteps | 5615616  |\n",
      "---------------------------------\n",
      "Ep done - 194480.\n",
      "Ep done - 194490.\n",
      "Ep done - 194500.\n",
      "Ep done - 194510.\n",
      "Ep done - 194520.\n",
      "Ep done - 194530.\n",
      "Ep done - 194540.\n",
      "Ep done - 194550.\n",
      "Ep done - 194560.\n",
      "Ep done - 194570.\n",
      "Ep done - 194580.\n",
      "Ep done - 194590.\n",
      "Ep done - 194600.\n",
      "Ep done - 194610.\n",
      "Ep done - 56110.\n",
      "Ep done - 56120.\n",
      "Ep done - 56130.\n",
      "Ep done - 56140.\n",
      "Ep done - 56150.\n",
      "Ep done - 56160.\n",
      "Ep done - 56170.\n",
      "Ep done - 56180.\n",
      "Ep done - 56190.\n",
      "Ep done - 56200.\n",
      "Eval num_timesteps=5620000, episode_reward=0.68 +/- 0.73\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.68      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5620000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.14e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0867    |\n",
      "|    n_updates            | 28370     |\n",
      "|    policy_gradient_loss | -2.66e-09 |\n",
      "|    value_loss           | 0.177     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 154\n",
      "Ep done - 194620.\n",
      "Ep done - 194630.\n",
      "Ep done - 194640.\n",
      "Ep done - 194650.\n",
      "Ep done - 194660.\n",
      "Ep done - 194670.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 915      |\n",
      "|    time_elapsed    | 19827    |\n",
      "|    total_timesteps | 5621760  |\n",
      "---------------------------------\n",
      "Ep done - 194680.\n",
      "Ep done - 194690.\n",
      "Ep done - 194700.\n",
      "Ep done - 194710.\n",
      "Ep done - 194720.\n",
      "Ep done - 194730.\n",
      "Ep done - 194740.\n",
      "Ep done - 194750.\n",
      "Ep done - 194760.\n",
      "Ep done - 194770.\n",
      "Ep done - 194780.\n",
      "Ep done - 194790.\n",
      "Ep done - 194800.\n",
      "Ep done - 194810.\n",
      "Ep done - 194820.\n",
      "Ep done - 194830.\n",
      "Ep done - 194840.\n",
      "Ep done - 194850.\n",
      "Ep done - 194860.\n",
      "Ep done - 194870.\n",
      "Ep done - 194880.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.56      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 916       |\n",
      "|    time_elapsed         | 19841     |\n",
      "|    total_timesteps      | 5627904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.69e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0874    |\n",
      "|    n_updates            | 28380     |\n",
      "|    policy_gradient_loss | 3.67e-10  |\n",
      "|    value_loss           | 0.138     |\n",
      "---------------------------------------\n",
      "Ep done - 194890.\n",
      "Ep done - 194900.\n",
      "Ep done - 194910.\n",
      "Ep done - 194920.\n",
      "Ep done - 194930.\n",
      "Ep done - 194940.\n",
      "Ep done - 194950.\n",
      "Ep done - 56210.\n",
      "Ep done - 56220.\n",
      "Ep done - 56230.\n",
      "Ep done - 56240.\n",
      "Ep done - 56250.\n",
      "Ep done - 56260.\n",
      "Ep done - 56270.\n",
      "Ep done - 56280.\n",
      "Ep done - 56290.\n",
      "Ep done - 56300.\n",
      "Eval num_timesteps=5630000, episode_reward=0.74 +/- 0.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.74      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5630000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.07e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.095     |\n",
      "|    n_updates            | 28390     |\n",
      "|    policy_gradient_loss | 2.51e-06  |\n",
      "|    value_loss           | 0.191     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.74\n",
      "SELFPLAY: new best model, bumping up generation to 155\n",
      "Ep done - 194960.\n",
      "Ep done - 194970.\n",
      "Ep done - 194980.\n",
      "Ep done - 194990.\n",
      "Ep done - 195000.\n",
      "Ep done - 195010.\n",
      "Ep done - 195020.\n",
      "Ep done - 195030.\n",
      "Ep done - 195040.\n",
      "Ep done - 195050.\n",
      "Ep done - 195060.\n",
      "Ep done - 195070.\n",
      "Ep done - 195080.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 917      |\n",
      "|    time_elapsed    | 19859    |\n",
      "|    total_timesteps | 5634048  |\n",
      "---------------------------------\n",
      "Ep done - 195090.\n",
      "Ep done - 195100.\n",
      "Ep done - 195110.\n",
      "Ep done - 195120.\n",
      "Ep done - 195130.\n",
      "Ep done - 195140.\n",
      "Ep done - 195150.\n",
      "Ep done - 195160.\n",
      "Ep done - 195170.\n",
      "Ep done - 195180.\n",
      "Ep done - 195190.\n",
      "Ep done - 195200.\n",
      "Ep done - 195210.\n",
      "Ep done - 195220.\n",
      "Ep done - 195230.\n",
      "Ep done - 195240.\n",
      "Ep done - 195250.\n",
      "Ep done - 195260.\n",
      "Ep done - 195270.\n",
      "Ep done - 195280.\n",
      "Ep done - 56310.\n",
      "Ep done - 56320.\n",
      "Ep done - 56330.\n",
      "Ep done - 56340.\n",
      "Ep done - 56350.\n",
      "Ep done - 56360.\n",
      "Ep done - 56370.\n",
      "Ep done - 56380.\n",
      "Ep done - 56390.\n",
      "Ep done - 56400.\n",
      "Eval num_timesteps=5640000, episode_reward=0.76 +/- 0.65\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.76      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5640000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.37e-05 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0652    |\n",
      "|    n_updates            | 28400     |\n",
      "|    policy_gradient_loss | -1.11e-10 |\n",
      "|    value_loss           | 0.137     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 156\n",
      "Ep done - 195290.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.54     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 918      |\n",
      "|    time_elapsed    | 19877    |\n",
      "|    total_timesteps | 5640192  |\n",
      "---------------------------------\n",
      "Ep done - 195300.\n",
      "Ep done - 195310.\n",
      "Ep done - 195320.\n",
      "Ep done - 195330.\n",
      "Ep done - 195340.\n",
      "Ep done - 195350.\n",
      "Ep done - 195360.\n",
      "Ep done - 195370.\n",
      "Ep done - 195380.\n",
      "Ep done - 195390.\n",
      "Ep done - 195400.\n",
      "Ep done - 195410.\n",
      "Ep done - 195420.\n",
      "Ep done - 195430.\n",
      "Ep done - 195440.\n",
      "Ep done - 195450.\n",
      "Ep done - 195460.\n",
      "Ep done - 195470.\n",
      "Ep done - 195480.\n",
      "Ep done - 195490.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.7       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 919       |\n",
      "|    time_elapsed         | 19890     |\n",
      "|    total_timesteps      | 5646336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.32e-05 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.11      |\n",
      "|    n_updates            | 28410     |\n",
      "|    policy_gradient_loss | -5.98e-10 |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "Ep done - 195500.\n",
      "Ep done - 195510.\n",
      "Ep done - 195520.\n",
      "Ep done - 195530.\n",
      "Ep done - 195540.\n",
      "Ep done - 195550.\n",
      "Ep done - 195560.\n",
      "Ep done - 195570.\n",
      "Ep done - 195580.\n",
      "Ep done - 195590.\n",
      "Ep done - 195600.\n",
      "Ep done - 195610.\n",
      "Ep done - 56410.\n",
      "Ep done - 56420.\n",
      "Ep done - 56430.\n",
      "Ep done - 56440.\n",
      "Ep done - 56450.\n",
      "Ep done - 56460.\n",
      "Ep done - 56470.\n",
      "Ep done - 56480.\n",
      "Ep done - 56490.\n",
      "Ep done - 56500.\n",
      "Eval num_timesteps=5650000, episode_reward=0.70 +/- 0.71\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.7       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5650000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.32e-05 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.114     |\n",
      "|    n_updates            | 28420     |\n",
      "|    policy_gradient_loss | 4.5e-10   |\n",
      "|    value_loss           | 0.167     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 157\n",
      "Ep done - 195620.\n",
      "Ep done - 195630.\n",
      "Ep done - 195640.\n",
      "Ep done - 195650.\n",
      "Ep done - 195660.\n",
      "Ep done - 195670.\n",
      "Ep done - 195680.\n",
      "Ep done - 195690.\n",
      "Ep done - 195700.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 920      |\n",
      "|    time_elapsed    | 19910    |\n",
      "|    total_timesteps | 5652480  |\n",
      "---------------------------------\n",
      "Ep done - 195710.\n",
      "Ep done - 195720.\n",
      "Ep done - 195730.\n",
      "Ep done - 195740.\n",
      "Ep done - 195750.\n",
      "Ep done - 195760.\n",
      "Ep done - 195770.\n",
      "Ep done - 195780.\n",
      "Ep done - 195790.\n",
      "Ep done - 195800.\n",
      "Ep done - 195810.\n",
      "Ep done - 195820.\n",
      "Ep done - 195830.\n",
      "Ep done - 195840.\n",
      "Ep done - 195850.\n",
      "Ep done - 195860.\n",
      "Ep done - 195870.\n",
      "Ep done - 195880.\n",
      "Ep done - 195890.\n",
      "Ep done - 195900.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.56      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 921       |\n",
      "|    time_elapsed         | 19928     |\n",
      "|    total_timesteps      | 5658624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.26e-06 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0765    |\n",
      "|    n_updates            | 28430     |\n",
      "|    policy_gradient_loss | -1.53e-09 |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Ep done - 195910.\n",
      "Ep done - 195920.\n",
      "Ep done - 195930.\n",
      "Ep done - 195940.\n",
      "Ep done - 195950.\n",
      "Ep done - 56510.\n",
      "Ep done - 56520.\n",
      "Ep done - 56530.\n",
      "Ep done - 56540.\n",
      "Ep done - 56550.\n",
      "Ep done - 56560.\n",
      "Ep done - 56570.\n",
      "Ep done - 56580.\n",
      "Ep done - 56590.\n",
      "Ep done - 56600.\n",
      "Eval num_timesteps=5660000, episode_reward=0.68 +/- 0.73\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.68      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5660000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.21e-06 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0648    |\n",
      "|    n_updates            | 28440     |\n",
      "|    policy_gradient_loss | 6.2e-10   |\n",
      "|    value_loss           | 0.174     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 158\n",
      "Ep done - 195960.\n",
      "Ep done - 195970.\n",
      "Ep done - 195980.\n",
      "Ep done - 195990.\n",
      "Ep done - 196000.\n",
      "Ep done - 196010.\n",
      "Ep done - 196020.\n",
      "Ep done - 196030.\n",
      "Ep done - 196040.\n",
      "Ep done - 196050.\n",
      "Ep done - 196060.\n",
      "Ep done - 196070.\n",
      "Ep done - 196080.\n",
      "Ep done - 196090.\n",
      "Ep done - 196100.\n",
      "Ep done - 196110.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 922      |\n",
      "|    time_elapsed    | 19951    |\n",
      "|    total_timesteps | 5664768  |\n",
      "---------------------------------\n",
      "Ep done - 196120.\n",
      "Ep done - 196130.\n",
      "Ep done - 196140.\n",
      "Ep done - 196150.\n",
      "Ep done - 196160.\n",
      "Ep done - 196170.\n",
      "Ep done - 196180.\n",
      "Ep done - 196190.\n",
      "Ep done - 196200.\n",
      "Ep done - 196210.\n",
      "Ep done - 196220.\n",
      "Ep done - 196230.\n",
      "Ep done - 196240.\n",
      "Ep done - 196250.\n",
      "Ep done - 196260.\n",
      "Ep done - 196270.\n",
      "Ep done - 196280.\n",
      "Ep done - 56610.\n",
      "Ep done - 56620.\n",
      "Ep done - 56630.\n",
      "Ep done - 56640.\n",
      "Ep done - 56650.\n",
      "Ep done - 56660.\n",
      "Ep done - 56670.\n",
      "Ep done - 56680.\n",
      "Ep done - 56690.\n",
      "Ep done - 56700.\n",
      "Eval num_timesteps=5670000, episode_reward=0.68 +/- 0.73\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.68      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5670000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.96e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0759    |\n",
      "|    n_updates            | 28450     |\n",
      "|    policy_gradient_loss | 7.82e-10  |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 159\n",
      "Ep done - 196290.\n",
      "Ep done - 196300.\n",
      "Ep done - 196310.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 923      |\n",
      "|    time_elapsed    | 19974    |\n",
      "|    total_timesteps | 5670912  |\n",
      "---------------------------------\n",
      "Ep done - 196320.\n",
      "Ep done - 196330.\n",
      "Ep done - 196340.\n",
      "Ep done - 196350.\n",
      "Ep done - 196360.\n",
      "Ep done - 196370.\n",
      "Ep done - 196380.\n",
      "Ep done - 196390.\n",
      "Ep done - 196400.\n",
      "Ep done - 196410.\n",
      "Ep done - 196420.\n",
      "Ep done - 196430.\n",
      "Ep done - 196440.\n",
      "Ep done - 196450.\n",
      "Ep done - 196460.\n",
      "Ep done - 196470.\n",
      "Ep done - 196480.\n",
      "Ep done - 196490.\n",
      "Ep done - 196500.\n",
      "Ep done - 196510.\n",
      "Ep done - 196520.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.56      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 924       |\n",
      "|    time_elapsed         | 19994     |\n",
      "|    total_timesteps      | 5677056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.09e-06 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0414    |\n",
      "|    n_updates            | 28460     |\n",
      "|    policy_gradient_loss | -1.51e-09 |\n",
      "|    value_loss           | 0.131     |\n",
      "---------------------------------------\n",
      "Ep done - 196530.\n",
      "Ep done - 196540.\n",
      "Ep done - 196550.\n",
      "Ep done - 196560.\n",
      "Ep done - 196570.\n",
      "Ep done - 196580.\n",
      "Ep done - 196590.\n",
      "Ep done - 196600.\n",
      "Ep done - 196610.\n",
      "Ep done - 56710.\n",
      "Ep done - 56720.\n",
      "Ep done - 56730.\n",
      "Ep done - 56740.\n",
      "Ep done - 56750.\n",
      "Ep done - 56760.\n",
      "Ep done - 56770.\n",
      "Ep done - 56780.\n",
      "Ep done - 56790.\n",
      "Ep done - 56800.\n",
      "Eval num_timesteps=5680000, episode_reward=0.62 +/- 0.78\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.62      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5680000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.72e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.071     |\n",
      "|    n_updates            | 28470     |\n",
      "|    policy_gradient_loss | -2.14e-10 |\n",
      "|    value_loss           | 0.184     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.62\n",
      "SELFPLAY: new best model, bumping up generation to 160\n",
      "Ep done - 196620.\n",
      "Ep done - 196630.\n",
      "Ep done - 196640.\n",
      "Ep done - 196650.\n",
      "Ep done - 196660.\n",
      "Ep done - 196670.\n",
      "Ep done - 196680.\n",
      "Ep done - 196690.\n",
      "Ep done - 196700.\n",
      "Ep done - 196710.\n",
      "Ep done - 196720.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 925      |\n",
      "|    time_elapsed    | 20019    |\n",
      "|    total_timesteps | 5683200  |\n",
      "---------------------------------\n",
      "Ep done - 196730.\n",
      "Ep done - 196740.\n",
      "Ep done - 196750.\n",
      "Ep done - 196760.\n",
      "Ep done - 196770.\n",
      "Ep done - 196780.\n",
      "Ep done - 196790.\n",
      "Ep done - 196800.\n",
      "Ep done - 196810.\n",
      "Ep done - 196820.\n",
      "Ep done - 196830.\n",
      "Ep done - 196840.\n",
      "Ep done - 196850.\n",
      "Ep done - 196860.\n",
      "Ep done - 196870.\n",
      "Ep done - 196880.\n",
      "Ep done - 196890.\n",
      "Ep done - 196900.\n",
      "Ep done - 196910.\n",
      "Ep done - 196920.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30       |\n",
      "|    ep_rew_mean          | 0.78     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 283      |\n",
      "|    iterations           | 926      |\n",
      "|    time_elapsed         | 20040    |\n",
      "|    total_timesteps      | 5689344  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -3.1e-06 |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.007    |\n",
      "|    loss                 | 0.0365   |\n",
      "|    n_updates            | 28480    |\n",
      "|    policy_gradient_loss | 1.36e-09 |\n",
      "|    value_loss           | 0.132    |\n",
      "--------------------------------------\n",
      "Ep done - 196930.\n",
      "Ep done - 196940.\n",
      "Ep done - 196950.\n",
      "Ep done - 56810.\n",
      "Ep done - 56820.\n",
      "Ep done - 56830.\n",
      "Ep done - 56840.\n",
      "Ep done - 56850.\n",
      "Ep done - 56860.\n",
      "Ep done - 56870.\n",
      "Ep done - 56880.\n",
      "Ep done - 56890.\n",
      "Ep done - 56900.\n",
      "Eval num_timesteps=5690000, episode_reward=0.72 +/- 0.69\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.72      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5690000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.78e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0663    |\n",
      "|    n_updates            | 28490     |\n",
      "|    policy_gradient_loss | -2.67e-10 |\n",
      "|    value_loss           | 0.113     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.72\n",
      "SELFPLAY: new best model, bumping up generation to 161\n",
      "Ep done - 196960.\n",
      "Ep done - 196970.\n",
      "Ep done - 196980.\n",
      "Ep done - 196990.\n",
      "Ep done - 197000.\n",
      "Ep done - 197010.\n",
      "Ep done - 197020.\n",
      "Ep done - 197030.\n",
      "Ep done - 197040.\n",
      "Ep done - 197050.\n",
      "Ep done - 197060.\n",
      "Ep done - 197070.\n",
      "Ep done - 197080.\n",
      "Ep done - 197090.\n",
      "Ep done - 197100.\n",
      "Ep done - 197110.\n",
      "Ep done - 197120.\n",
      "Ep done - 197130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 927      |\n",
      "|    time_elapsed    | 20064    |\n",
      "|    total_timesteps | 5695488  |\n",
      "---------------------------------\n",
      "Ep done - 197140.\n",
      "Ep done - 197150.\n",
      "Ep done - 197160.\n",
      "Ep done - 197170.\n",
      "Ep done - 197180.\n",
      "Ep done - 197190.\n",
      "Ep done - 197200.\n",
      "Ep done - 197210.\n",
      "Ep done - 197220.\n",
      "Ep done - 197230.\n",
      "Ep done - 197240.\n",
      "Ep done - 197250.\n",
      "Ep done - 197260.\n",
      "Ep done - 197270.\n",
      "Ep done - 197280.\n",
      "Ep done - 56910.\n",
      "Ep done - 56920.\n",
      "Ep done - 56930.\n",
      "Ep done - 56940.\n",
      "Ep done - 56950.\n",
      "Ep done - 56960.\n",
      "Ep done - 56970.\n",
      "Ep done - 56980.\n",
      "Ep done - 56990.\n",
      "Ep done - 57000.\n",
      "Eval num_timesteps=5700000, episode_reward=0.58 +/- 0.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.58      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5700000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.5e-06  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0479    |\n",
      "|    n_updates            | 28500     |\n",
      "|    policy_gradient_loss | -8.12e-10 |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.58\n",
      "SELFPLAY: new best model, bumping up generation to 162\n",
      "Ep done - 197290.\n",
      "Ep done - 197300.\n",
      "Ep done - 197310.\n",
      "Ep done - 197320.\n",
      "Ep done - 197330.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 928      |\n",
      "|    time_elapsed    | 20089    |\n",
      "|    total_timesteps | 5701632  |\n",
      "---------------------------------\n",
      "Ep done - 197340.\n",
      "Ep done - 197350.\n",
      "Ep done - 197360.\n",
      "Ep done - 197370.\n",
      "Ep done - 197380.\n",
      "Ep done - 197390.\n",
      "Ep done - 197400.\n",
      "Ep done - 197410.\n",
      "Ep done - 197420.\n",
      "Ep done - 197430.\n",
      "Ep done - 197440.\n",
      "Ep done - 197450.\n",
      "Ep done - 197460.\n",
      "Ep done - 197470.\n",
      "Ep done - 197480.\n",
      "Ep done - 197490.\n",
      "Ep done - 197500.\n",
      "Ep done - 197510.\n",
      "Ep done - 197520.\n",
      "Ep done - 197530.\n",
      "Ep done - 197540.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30        |\n",
      "|    ep_rew_mean          | 0.7       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 929       |\n",
      "|    time_elapsed         | 20111     |\n",
      "|    total_timesteps      | 5707776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.15e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0715    |\n",
      "|    n_updates            | 28510     |\n",
      "|    policy_gradient_loss | 2.36e-10  |\n",
      "|    value_loss           | 0.161     |\n",
      "---------------------------------------\n",
      "Ep done - 197550.\n",
      "Ep done - 197560.\n",
      "Ep done - 197570.\n",
      "Ep done - 197580.\n",
      "Ep done - 197590.\n",
      "Ep done - 197600.\n",
      "Ep done - 197610.\n",
      "Ep done - 57010.\n",
      "Ep done - 57020.\n",
      "Ep done - 57030.\n",
      "Ep done - 57040.\n",
      "Ep done - 57050.\n",
      "Ep done - 57060.\n",
      "Ep done - 57070.\n",
      "Ep done - 57080.\n",
      "Ep done - 57090.\n",
      "Ep done - 57100.\n",
      "Eval num_timesteps=5710000, episode_reward=0.74 +/- 0.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.74      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5710000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.07e-06 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.0935    |\n",
      "|    n_updates            | 28520     |\n",
      "|    policy_gradient_loss | -5.46e-10 |\n",
      "|    value_loss           | 0.174     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.74\n",
      "SELFPLAY: new best model, bumping up generation to 163\n",
      "Ep done - 197620.\n",
      "Ep done - 197630.\n",
      "Ep done - 197640.\n",
      "Ep done - 197650.\n",
      "Ep done - 197660.\n",
      "Ep done - 197670.\n",
      "Ep done - 197680.\n",
      "Ep done - 197690.\n",
      "Ep done - 197700.\n",
      "Ep done - 197710.\n",
      "Ep done - 197720.\n",
      "Ep done - 197730.\n",
      "Ep done - 197740.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 930      |\n",
      "|    time_elapsed    | 20135    |\n",
      "|    total_timesteps | 5713920  |\n",
      "---------------------------------\n",
      "Ep done - 197750.\n",
      "Ep done - 197760.\n",
      "Ep done - 197770.\n",
      "Ep done - 197780.\n",
      "Ep done - 197790.\n",
      "Ep done - 197800.\n",
      "Ep done - 197810.\n",
      "Ep done - 197820.\n",
      "Ep done - 197830.\n",
      "Ep done - 197840.\n",
      "Ep done - 197850.\n",
      "Ep done - 197860.\n",
      "Ep done - 197870.\n",
      "Ep done - 197880.\n",
      "Ep done - 197890.\n",
      "Ep done - 197900.\n",
      "Ep done - 197910.\n",
      "Ep done - 197920.\n",
      "Ep done - 197930.\n",
      "Ep done - 197940.\n",
      "Ep done - 197950.\n",
      "Ep done - 57110.\n",
      "Ep done - 57120.\n",
      "Ep done - 57130.\n",
      "Ep done - 57140.\n",
      "Ep done - 57150.\n",
      "Ep done - 57160.\n",
      "Ep done - 57170.\n",
      "Ep done - 57180.\n",
      "Ep done - 57190.\n",
      "Ep done - 57200.\n",
      "Eval num_timesteps=5720000, episode_reward=0.68 +/- 0.73\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 0.68      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5720000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.02e-06 |\n",
      "|    explained_variance   | 1.79e-07  |\n",
      "|    learning_rate        | 0.007     |\n",
      "|    loss                 | 0.127     |\n",
      "|    n_updates            | 28530     |\n",
      "|    policy_gradient_loss | -1.8e-09  |\n",
      "|    value_loss           | 0.185     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 164\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 931      |\n",
      "|    time_elapsed    | 20159    |\n",
      "|    total_timesteps | 5720064  |\n",
      "---------------------------------\n",
      "Ep done - 197960.\n",
      "Ep done - 197970.\n",
      "Ep done - 197980.\n",
      "Ep done - 197990.\n",
      "Ep done - 198000.\n",
      "Ep done - 198010.\n",
      "Ep done - 198020.\n",
      "Ep done - 198030.\n",
      "Ep done - 198040.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.28\n",
      "SELFPLAY: new best model, bumping up generation to 36\n",
      "Ep done - 15570.\n",
      "Ep done - 15580.\n",
      "Ep done - 15590.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.31     |\n",
      "| time/              |          |\n",
      "|    fps             | 153      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 3060     |\n",
      "|    total_timesteps | 471040   |\n",
      "---------------------------------\n",
      "Ep done - 15600.\n",
      "Ep done - 15610.\n",
      "Ep done - 15620.\n",
      "Ep done - 15630.\n",
      "Ep done - 15640.\n",
      "Ep done - 15650.\n",
      "Ep done - 15660.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 231         |\n",
      "|    time_elapsed         | 3069        |\n",
      "|    total_timesteps      | 473088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028639298 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Ep done - 15670.\n",
      "Ep done - 15680.\n",
      "Ep done - 15690.\n",
      "Ep done - 15700.\n",
      "Ep done - 15710.\n",
      "Ep done - 15720.\n",
      "Ep done - 15730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 232         |\n",
      "|    time_elapsed         | 3078        |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031163681 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.468      |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00954     |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 15740.\n",
      "Ep done - 15750.\n",
      "Ep done - 15760.\n",
      "Ep done - 15770.\n",
      "Ep done - 15780.\n",
      "Ep done - 15790.\n",
      "Ep done - 15800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 233         |\n",
      "|    time_elapsed         | 3088        |\n",
      "|    total_timesteps      | 477184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028831802 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00504     |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Ep done - 15810.\n",
      "Ep done - 15820.\n",
      "Ep done - 15830.\n",
      "Ep done - 15840.\n",
      "Ep done - 15850.\n",
      "Ep done - 15860.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.42       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 234        |\n",
      "|    time_elapsed         | 3097       |\n",
      "|    total_timesteps      | 479232     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03057297 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.48      |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0317     |\n",
      "|    n_updates            | 2330       |\n",
      "|    policy_gradient_loss | -0.0348    |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "Ep done - 15870.\n",
      "Ep done - 15880.\n",
      "Ep done - 15890.\n",
      "Ep done - 4710.\n",
      "Ep done - 4720.\n",
      "Ep done - 4730.\n",
      "Ep done - 4740.\n",
      "Ep done - 4750.\n",
      "Ep done - 4760.\n",
      "Ep done - 4770.\n",
      "Ep done - 4780.\n",
      "Ep done - 4790.\n",
      "Ep done - 4800.\n",
      "Eval num_timesteps=480000, episode_reward=0.36 +/- 0.92\n",
      "Episode length: 30.08 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.36       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 480000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02802231 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.479     |\n",
      "|    explained_variance   | 0.253      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00293    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.36\n",
      "SELFPLAY: new best model, bumping up generation to 37\n",
      "Ep done - 15900.\n",
      "Ep done - 15910.\n",
      "Ep done - 15920.\n",
      "Ep done - 15930.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 154      |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 3115     |\n",
      "|    total_timesteps | 481280   |\n",
      "---------------------------------\n",
      "Ep done - 15940.\n",
      "Ep done - 15950.\n",
      "Ep done - 15960.\n",
      "Ep done - 15970.\n",
      "Ep done - 15980.\n",
      "Ep done - 15990.\n",
      "Ep done - 16000.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.5        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 236        |\n",
      "|    time_elapsed         | 3124       |\n",
      "|    total_timesteps      | 483328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03941874 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.477     |\n",
      "|    explained_variance   | 0.254      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0161     |\n",
      "|    n_updates            | 2350       |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Ep done - 16010.\n",
      "Ep done - 16020.\n",
      "Ep done - 16030.\n",
      "Ep done - 16040.\n",
      "Ep done - 16050.\n",
      "Ep done - 16060.\n",
      "Ep done - 16070.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.42      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 154       |\n",
      "|    iterations           | 237       |\n",
      "|    time_elapsed         | 3134      |\n",
      "|    total_timesteps      | 485376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0317704 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.464    |\n",
      "|    explained_variance   | 0.114     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0166    |\n",
      "|    n_updates            | 2360      |\n",
      "|    policy_gradient_loss | -0.0347   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "Ep done - 16080.\n",
      "Ep done - 16090.\n",
      "Ep done - 16100.\n",
      "Ep done - 16110.\n",
      "Ep done - 16120.\n",
      "Ep done - 16130.\n",
      "Ep done - 16140.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 3143        |\n",
      "|    total_timesteps      | 487424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029776664 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 16150.\n",
      "Ep done - 16160.\n",
      "Ep done - 16170.\n",
      "Ep done - 16180.\n",
      "Ep done - 16190.\n",
      "Ep done - 16200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 239         |\n",
      "|    time_elapsed         | 3152        |\n",
      "|    total_timesteps      | 489472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029371463 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Ep done - 16210.\n",
      "Ep done - 16220.\n",
      "Ep done - 4810.\n",
      "Ep done - 4820.\n",
      "Ep done - 4830.\n",
      "Ep done - 4840.\n",
      "Ep done - 4850.\n",
      "Ep done - 4860.\n",
      "Ep done - 4870.\n",
      "Ep done - 4880.\n",
      "Ep done - 4890.\n",
      "Ep done - 4900.\n",
      "Eval num_timesteps=490000, episode_reward=0.44 +/- 0.88\n",
      "Episode length: 30.15 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 490000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03587226 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.455     |\n",
      "|    explained_variance   | 0.391      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0112     |\n",
      "|    n_updates            | 2390       |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    value_loss           | 0.198      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.44\n",
      "SELFPLAY: new best model, bumping up generation to 38\n",
      "Ep done - 16230.\n",
      "Ep done - 16240.\n",
      "Ep done - 16250.\n",
      "Ep done - 16260.\n",
      "Ep done - 16270.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 154      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 3171     |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Ep done - 16280.\n",
      "Ep done - 16290.\n",
      "Ep done - 16300.\n",
      "Ep done - 16310.\n",
      "Ep done - 16320.\n",
      "Ep done - 16330.\n",
      "Ep done - 16340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 241         |\n",
      "|    time_elapsed         | 3180        |\n",
      "|    total_timesteps      | 493568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029427323 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0123      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Ep done - 16350.\n",
      "Ep done - 16360.\n",
      "Ep done - 16370.\n",
      "Ep done - 16380.\n",
      "Ep done - 16390.\n",
      "Ep done - 16400.\n",
      "Ep done - 16410.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 3189        |\n",
      "|    total_timesteps      | 495616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033737183 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.43       |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0198      |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Ep done - 16420.\n",
      "Ep done - 16430.\n",
      "Ep done - 16440.\n",
      "Ep done - 16450.\n",
      "Ep done - 16460.\n",
      "Ep done - 16470.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 243         |\n",
      "|    time_elapsed         | 3199        |\n",
      "|    total_timesteps      | 497664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035282496 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.0689      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00894    |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Ep done - 16480.\n",
      "Ep done - 16490.\n",
      "Ep done - 16500.\n",
      "Ep done - 16510.\n",
      "Ep done - 16520.\n",
      "Ep done - 16530.\n",
      "Ep done - 16540.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 155        |\n",
      "|    iterations           | 244        |\n",
      "|    time_elapsed         | 3208       |\n",
      "|    total_timesteps      | 499712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03436216 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.419     |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0385     |\n",
      "|    n_updates            | 2430       |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Ep done - 16550.\n",
      "Ep done - 4910.\n",
      "Ep done - 4920.\n",
      "Ep done - 4930.\n",
      "Ep done - 4940.\n",
      "Ep done - 4950.\n",
      "Ep done - 4960.\n",
      "Ep done - 4970.\n",
      "Ep done - 4980.\n",
      "Ep done - 4990.\n",
      "Ep done - 5000.\n",
      "Eval num_timesteps=500000, episode_reward=0.50 +/- 0.84\n",
      "Episode length: 30.17 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033504046 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.5\n",
      "SELFPLAY: new best model, bumping up generation to 39\n",
      "Ep done - 16560.\n",
      "Ep done - 16570.\n",
      "Ep done - 16580.\n",
      "Ep done - 16590.\n",
      "Ep done - 16600.\n",
      "Ep done - 16610.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.58     |\n",
      "| time/              |          |\n",
      "|    fps             | 155      |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 3226     |\n",
      "|    total_timesteps | 501760   |\n",
      "---------------------------------\n",
      "Ep done - 16620.\n",
      "Ep done - 16630.\n",
      "Ep done - 16640.\n",
      "Ep done - 16650.\n",
      "Ep done - 16660.\n",
      "Ep done - 16670.\n",
      "Ep done - 16680.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 3235        |\n",
      "|    total_timesteps      | 503808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036805235 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.391      |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Ep done - 16690.\n",
      "Ep done - 16700.\n",
      "Ep done - 16710.\n",
      "Ep done - 16720.\n",
      "Ep done - 16730.\n",
      "Ep done - 16740.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 247         |\n",
      "|    time_elapsed         | 3244        |\n",
      "|    total_timesteps      | 505856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034850243 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0331      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "Ep done - 16750.\n",
      "Ep done - 16760.\n",
      "Ep done - 16770.\n",
      "Ep done - 16780.\n",
      "Ep done - 16790.\n",
      "Ep done - 16800.\n",
      "Ep done - 16810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 3254        |\n",
      "|    total_timesteps      | 507904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035851773 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00561    |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Ep done - 16820.\n",
      "Ep done - 16830.\n",
      "Ep done - 16840.\n",
      "Ep done - 16850.\n",
      "Ep done - 16860.\n",
      "Ep done - 16870.\n",
      "Ep done - 16880.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 249         |\n",
      "|    time_elapsed         | 3265        |\n",
      "|    total_timesteps      | 509952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028946381 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.353      |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00951    |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    value_loss           | 0.0969      |\n",
      "-----------------------------------------\n",
      "Ep done - 5010.\n",
      "Ep done - 5020.\n",
      "Ep done - 5030.\n",
      "Ep done - 5040.\n",
      "Ep done - 5050.\n",
      "Ep done - 5060.\n",
      "Ep done - 5070.\n",
      "Ep done - 5080.\n",
      "Ep done - 5090.\n",
      "Ep done - 5100.\n",
      "Eval num_timesteps=510000, episode_reward=0.68 +/- 0.69\n",
      "Episode length: 30.09 +/- 0.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 510000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03963881 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.393     |\n",
      "|    explained_variance   | 0.352      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00725    |\n",
      "|    n_updates            | 2490       |\n",
      "|    policy_gradient_loss | -0.0361    |\n",
      "|    value_loss           | 0.149      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 40\n",
      "Ep done - 16890.\n",
      "Ep done - 16900.\n",
      "Ep done - 16910.\n",
      "Ep done - 16920.\n",
      "Ep done - 16930.\n",
      "Ep done - 16940.\n",
      "Ep done - 16950.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 155      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 3283     |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Ep done - 16960.\n",
      "Ep done - 16970.\n",
      "Ep done - 16980.\n",
      "Ep done - 16990.\n",
      "Ep done - 17000.\n",
      "Ep done - 17010.\n",
      "Ep done - 17020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 251         |\n",
      "|    time_elapsed         | 3292        |\n",
      "|    total_timesteps      | 514048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035798624 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0275      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "Ep done - 17030.\n",
      "Ep done - 17040.\n",
      "Ep done - 17050.\n",
      "Ep done - 17060.\n",
      "Ep done - 17070.\n",
      "Ep done - 17080.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 252         |\n",
      "|    time_elapsed         | 3301        |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028365863 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Ep done - 17090.\n",
      "Ep done - 17100.\n",
      "Ep done - 17110.\n",
      "Ep done - 17120.\n",
      "Ep done - 17130.\n",
      "Ep done - 17140.\n",
      "Ep done - 17150.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 253        |\n",
      "|    time_elapsed         | 3311       |\n",
      "|    total_timesteps      | 518144     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03454088 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.379     |\n",
      "|    explained_variance   | 0.0935     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00941    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0289    |\n",
      "|    value_loss           | 0.102      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 17160.\n",
      "Ep done - 17170.\n",
      "Ep done - 17180.\n",
      "Ep done - 17190.\n",
      "Ep done - 17200.\n",
      "Ep done - 17210.\n",
      "Ep done - 5110.\n",
      "Ep done - 5120.\n",
      "Ep done - 5130.\n",
      "Ep done - 5140.\n",
      "Ep done - 5150.\n",
      "Ep done - 5160.\n",
      "Ep done - 5170.\n",
      "Ep done - 5180.\n",
      "Ep done - 5190.\n",
      "Ep done - 5200.\n",
      "Eval num_timesteps=520000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.28 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029928576 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00441     |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.0853      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.57\n",
      "SELFPLAY: new best model, bumping up generation to 41\n",
      "Ep done - 17220.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 3329     |\n",
      "|    total_timesteps | 520192   |\n",
      "---------------------------------\n",
      "Ep done - 17230.\n",
      "Ep done - 17240.\n",
      "Ep done - 17250.\n",
      "Ep done - 17260.\n",
      "Ep done - 17270.\n",
      "Ep done - 17280.\n",
      "Ep done - 17290.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 255         |\n",
      "|    time_elapsed         | 3338        |\n",
      "|    total_timesteps      | 522240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040780142 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "Ep done - 17300.\n",
      "Ep done - 17310.\n",
      "Ep done - 17320.\n",
      "Ep done - 17330.\n",
      "Ep done - 17340.\n",
      "Ep done - 17350.\n",
      "Ep done - 17360.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 3347        |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034862045 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.388      |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0165     |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    value_loss           | 0.0908      |\n",
      "-----------------------------------------\n",
      "Ep done - 17370.\n",
      "Ep done - 17380.\n",
      "Ep done - 17390.\n",
      "Ep done - 17400.\n",
      "Ep done - 17410.\n",
      "Ep done - 17420.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 257         |\n",
      "|    time_elapsed         | 3356        |\n",
      "|    total_timesteps      | 526336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031635195 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0015     |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "Ep done - 17430.\n",
      "Ep done - 17440.\n",
      "Ep done - 17450.\n",
      "Ep done - 17460.\n",
      "Ep done - 17470.\n",
      "Ep done - 17480.\n",
      "Ep done - 17490.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 258         |\n",
      "|    time_elapsed         | 3365        |\n",
      "|    total_timesteps      | 528384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035097376 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.364      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.0725      |\n",
      "-----------------------------------------\n",
      "Ep done - 17500.\n",
      "Ep done - 17510.\n",
      "Ep done - 17520.\n",
      "Ep done - 17530.\n",
      "Ep done - 17540.\n",
      "Ep done - 5210.\n",
      "Ep done - 5220.\n",
      "Ep done - 5230.\n",
      "Ep done - 5240.\n",
      "Ep done - 5250.\n",
      "Ep done - 5260.\n",
      "Ep done - 5270.\n",
      "Ep done - 5280.\n",
      "Ep done - 5290.\n",
      "Ep done - 5300.\n",
      "Eval num_timesteps=530000, episode_reward=0.76 +/- 0.62\n",
      "Episode length: 30.21 +/- 0.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 530000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030209864 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.0584      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 42\n",
      "Ep done - 17550.\n",
      "Ep done - 17560.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 259      |\n",
      "|    time_elapsed    | 3384     |\n",
      "|    total_timesteps | 530432   |\n",
      "---------------------------------\n",
      "Ep done - 17570.\n",
      "Ep done - 17580.\n",
      "Ep done - 17590.\n",
      "Ep done - 17600.\n",
      "Ep done - 17610.\n",
      "Ep done - 17620.\n",
      "Ep done - 17630.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 260         |\n",
      "|    time_elapsed         | 3393        |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029201604 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | -0.249      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00442    |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n",
      "Ep done - 17640.\n",
      "Ep done - 17650.\n",
      "Ep done - 17660.\n",
      "Ep done - 17670.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 17680.\n",
      "Ep done - 17690.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 261        |\n",
      "|    time_elapsed         | 3403       |\n",
      "|    total_timesteps      | 534528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03239207 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.389     |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    value_loss           | 0.0558     |\n",
      "----------------------------------------\n",
      "Ep done - 17700.\n",
      "Ep done - 17710.\n",
      "Ep done - 17720.\n",
      "Ep done - 17730.\n",
      "Ep done - 17740.\n",
      "Ep done - 17750.\n",
      "Ep done - 17760.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 262        |\n",
      "|    time_elapsed         | 3412       |\n",
      "|    total_timesteps      | 536576     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03447684 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.383     |\n",
      "|    explained_variance   | 0.29       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0236     |\n",
      "|    n_updates            | 2610       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    value_loss           | 0.0914     |\n",
      "----------------------------------------\n",
      "Ep done - 17770.\n",
      "Ep done - 17780.\n",
      "Ep done - 17790.\n",
      "Ep done - 17800.\n",
      "Ep done - 17810.\n",
      "Ep done - 17820.\n",
      "Ep done - 17830.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 263        |\n",
      "|    time_elapsed         | 3421       |\n",
      "|    total_timesteps      | 538624     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02810727 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.374     |\n",
      "|    explained_variance   | 0.353      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0092    |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    value_loss           | 0.0702     |\n",
      "----------------------------------------\n",
      "Ep done - 17840.\n",
      "Ep done - 17850.\n",
      "Ep done - 17860.\n",
      "Ep done - 17870.\n",
      "Ep done - 5310.\n",
      "Ep done - 5320.\n",
      "Ep done - 5330.\n",
      "Ep done - 5340.\n",
      "Ep done - 5350.\n",
      "Ep done - 5360.\n",
      "Ep done - 5370.\n",
      "Ep done - 5380.\n",
      "Ep done - 5390.\n",
      "Ep done - 5400.\n",
      "Eval num_timesteps=540000, episode_reward=0.70 +/- 0.71\n",
      "Episode length: 30.33 +/- 0.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031228447 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.362      |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0737      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 43\n",
      "Ep done - 17880.\n",
      "Ep done - 17890.\n",
      "Ep done - 17900.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 3439     |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Ep done - 17910.\n",
      "Ep done - 17920.\n",
      "Ep done - 17930.\n",
      "Ep done - 17940.\n",
      "Ep done - 17950.\n",
      "Ep done - 17960.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 265         |\n",
      "|    time_elapsed         | 3449        |\n",
      "|    total_timesteps      | 542720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039229874 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0319     |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.0582      |\n",
      "-----------------------------------------\n",
      "Ep done - 17970.\n",
      "Ep done - 17980.\n",
      "Ep done - 17990.\n",
      "Ep done - 18000.\n",
      "Ep done - 18010.\n",
      "Ep done - 18020.\n",
      "Ep done - 18030.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 266        |\n",
      "|    time_elapsed         | 3458       |\n",
      "|    total_timesteps      | 544768     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03240133 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.394     |\n",
      "|    explained_variance   | 0.437      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0213     |\n",
      "|    n_updates            | 2650       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Ep done - 18040.\n",
      "Ep done - 18050.\n",
      "Ep done - 18060.\n",
      "Ep done - 18070.\n",
      "Ep done - 18080.\n",
      "Ep done - 18090.\n",
      "Ep done - 18100.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 267        |\n",
      "|    time_elapsed         | 3467       |\n",
      "|    total_timesteps      | 546816     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04037878 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.387     |\n",
      "|    explained_variance   | 0.477      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0457    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0297    |\n",
      "|    value_loss           | 0.0968     |\n",
      "----------------------------------------\n",
      "Ep done - 18110.\n",
      "Ep done - 18120.\n",
      "Ep done - 18130.\n",
      "Ep done - 18140.\n",
      "Ep done - 18150.\n",
      "Ep done - 18160.\n",
      "Ep done - 18170.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 268         |\n",
      "|    time_elapsed         | 3476        |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037587367 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00313    |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.0723      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 18180.\n",
      "Ep done - 18190.\n",
      "Ep done - 18200.\n",
      "Ep done - 18210.\n",
      "Ep done - 5410.\n",
      "Ep done - 5420.\n",
      "Ep done - 5430.\n",
      "Ep done - 5440.\n",
      "Ep done - 5450.\n",
      "Ep done - 5460.\n",
      "Ep done - 5470.\n",
      "Ep done - 5480.\n",
      "Ep done - 5490.\n",
      "Ep done - 5500.\n",
      "Eval num_timesteps=550000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.22 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 550000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03591458 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.373     |\n",
      "|    explained_variance   | 0.341      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0262     |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.65\n",
      "SELFPLAY: new best model, bumping up generation to 44\n",
      "Ep done - 18220.\n",
      "Ep done - 18230.\n",
      "Ep done - 18240.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 269      |\n",
      "|    time_elapsed    | 3495     |\n",
      "|    total_timesteps | 550912   |\n",
      "---------------------------------\n",
      "Ep done - 18250.\n",
      "Ep done - 18260.\n",
      "Ep done - 18270.\n",
      "Ep done - 18280.\n",
      "Ep done - 18290.\n",
      "Ep done - 18300.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 270         |\n",
      "|    time_elapsed         | 3504        |\n",
      "|    total_timesteps      | 552960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039184228 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.388      |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    value_loss           | 0.0953      |\n",
      "-----------------------------------------\n",
      "Ep done - 18310.\n",
      "Ep done - 18320.\n",
      "Ep done - 18330.\n",
      "Ep done - 18340.\n",
      "Ep done - 18350.\n",
      "Ep done - 18360.\n",
      "Ep done - 18370.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 271         |\n",
      "|    time_elapsed         | 3513        |\n",
      "|    total_timesteps      | 555008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040366262 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.362      |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Ep done - 18380.\n",
      "Ep done - 18390.\n",
      "Ep done - 18400.\n",
      "Ep done - 18410.\n",
      "Ep done - 18420.\n",
      "Ep done - 18430.\n",
      "Ep done - 18440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 272         |\n",
      "|    time_elapsed         | 3522        |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037545413 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00138     |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "Ep done - 18450.\n",
      "Ep done - 18460.\n",
      "Ep done - 18470.\n",
      "Ep done - 18480.\n",
      "Ep done - 18490.\n",
      "Ep done - 18500.\n",
      "Ep done - 18510.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 273        |\n",
      "|    time_elapsed         | 3531       |\n",
      "|    total_timesteps      | 559104     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04759322 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.371     |\n",
      "|    explained_variance   | 0.328      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0345    |\n",
      "|    value_loss           | 0.145      |\n",
      "----------------------------------------\n",
      "Ep done - 18520.\n",
      "Ep done - 18530.\n",
      "Ep done - 18540.\n",
      "Ep done - 5510.\n",
      "Ep done - 5520.\n",
      "Ep done - 5530.\n",
      "Ep done - 5540.\n",
      "Ep done - 5550.\n",
      "Ep done - 5560.\n",
      "Ep done - 5570.\n",
      "Ep done - 5580.\n",
      "Ep done - 5590.\n",
      "Ep done - 5600.\n",
      "Eval num_timesteps=560000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.23 +/- 0.53\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 560000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03629961 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.368     |\n",
      "|    explained_variance   | 0.465      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 2730       |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    value_loss           | 0.118      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.65\n",
      "SELFPLAY: new best model, bumping up generation to 45\n",
      "Ep done - 18550.\n",
      "Ep done - 18560.\n",
      "Ep done - 18570.\n",
      "Ep done - 18580.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.65     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 3550     |\n",
      "|    total_timesteps | 561152   |\n",
      "---------------------------------\n",
      "Ep done - 18590.\n",
      "Ep done - 18600.\n",
      "Ep done - 18610.\n",
      "Ep done - 18620.\n",
      "Ep done - 18630.\n",
      "Ep done - 18640.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 275         |\n",
      "|    time_elapsed         | 3559        |\n",
      "|    total_timesteps      | 563200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043360487 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.359      |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.0961      |\n",
      "-----------------------------------------\n",
      "Ep done - 18650.\n",
      "Ep done - 18660.\n",
      "Ep done - 18670.\n",
      "Ep done - 18680.\n",
      "Ep done - 18690.\n",
      "Ep done - 18700.\n",
      "Ep done - 18710.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.71        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 276         |\n",
      "|    time_elapsed         | 3568        |\n",
      "|    total_timesteps      | 565248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039779715 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    value_loss           | 0.0944      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 18720.\n",
      "Ep done - 18730.\n",
      "Ep done - 18740.\n",
      "Ep done - 18750.\n",
      "Ep done - 18760.\n",
      "Ep done - 18770.\n",
      "Ep done - 18780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 277         |\n",
      "|    time_elapsed         | 3577        |\n",
      "|    total_timesteps      | 567296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032685764 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00193     |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.0791      |\n",
      "-----------------------------------------\n",
      "Ep done - 18790.\n",
      "Ep done - 18800.\n",
      "Ep done - 18810.\n",
      "Ep done - 18820.\n",
      "Ep done - 18830.\n",
      "Ep done - 18840.\n",
      "Ep done - 18850.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 278        |\n",
      "|    time_elapsed         | 3586       |\n",
      "|    total_timesteps      | 569344     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03804052 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.349     |\n",
      "|    explained_variance   | 0.491      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0124    |\n",
      "|    n_updates            | 2770       |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    value_loss           | 0.0836     |\n",
      "----------------------------------------\n",
      "Ep done - 18860.\n",
      "Ep done - 18870.\n",
      "Ep done - 5610.\n",
      "Ep done - 5620.\n",
      "Ep done - 5630.\n",
      "Ep done - 5640.\n",
      "Ep done - 5650.\n",
      "Ep done - 5660.\n",
      "Ep done - 5670.\n",
      "Ep done - 5680.\n",
      "Ep done - 5690.\n",
      "Ep done - 5700.\n",
      "Eval num_timesteps=570000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.16 +/- 0.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 570000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03493006 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.353     |\n",
      "|    explained_variance   | 0.474      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0216    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    value_loss           | 0.11       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.59\n",
      "SELFPLAY: new best model, bumping up generation to 46\n",
      "Ep done - 18880.\n",
      "Ep done - 18890.\n",
      "Ep done - 18900.\n",
      "Ep done - 18910.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 279      |\n",
      "|    time_elapsed    | 3604     |\n",
      "|    total_timesteps | 571392   |\n",
      "---------------------------------\n",
      "Ep done - 18920.\n",
      "Ep done - 18930.\n",
      "Ep done - 18940.\n",
      "Ep done - 18950.\n",
      "Ep done - 18960.\n",
      "Ep done - 18970.\n",
      "Ep done - 18980.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 280        |\n",
      "|    time_elapsed         | 3614       |\n",
      "|    total_timesteps      | 573440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03530073 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.36      |\n",
      "|    explained_variance   | 0.368      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00809    |\n",
      "|    n_updates            | 2790       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    value_loss           | 0.0902     |\n",
      "----------------------------------------\n",
      "Ep done - 18990.\n",
      "Ep done - 19000.\n",
      "Ep done - 19010.\n",
      "Ep done - 19020.\n",
      "Ep done - 19030.\n",
      "Ep done - 19040.\n",
      "Ep done - 19050.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 281        |\n",
      "|    time_elapsed         | 3623       |\n",
      "|    total_timesteps      | 575488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03469403 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.365     |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0329     |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    value_loss           | 0.131      |\n",
      "----------------------------------------\n",
      "Ep done - 19060.\n",
      "Ep done - 19070.\n",
      "Ep done - 19080.\n",
      "Ep done - 19090.\n",
      "Ep done - 19100.\n",
      "Ep done - 19110.\n",
      "Ep done - 19120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 282         |\n",
      "|    time_elapsed         | 3632        |\n",
      "|    total_timesteps      | 577536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027266767 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.339      |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Ep done - 19130.\n",
      "Ep done - 19140.\n",
      "Ep done - 19150.\n",
      "Ep done - 19160.\n",
      "Ep done - 19170.\n",
      "Ep done - 19180.\n",
      "Ep done - 19190.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.7       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 159       |\n",
      "|    iterations           | 283       |\n",
      "|    time_elapsed         | 3641      |\n",
      "|    total_timesteps      | 579584    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0477902 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.361    |\n",
      "|    explained_variance   | 0.392     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0236    |\n",
      "|    n_updates            | 2820      |\n",
      "|    policy_gradient_loss | -0.0283   |\n",
      "|    value_loss           | 0.0813    |\n",
      "---------------------------------------\n",
      "Ep done - 19200.\n",
      "Ep done - 5710.\n",
      "Ep done - 5720.\n",
      "Ep done - 5730.\n",
      "Ep done - 5740.\n",
      "Ep done - 5750.\n",
      "Ep done - 5760.\n",
      "Ep done - 5770.\n",
      "Ep done - 5780.\n",
      "Ep done - 5790.\n",
      "Ep done - 5800.\n",
      "Eval num_timesteps=580000, episode_reward=0.73 +/- 0.68\n",
      "Episode length: 30.31 +/- 0.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048305646 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0258     |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.0988      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.73\n",
      "SELFPLAY: new best model, bumping up generation to 47\n",
      "Ep done - 19210.\n",
      "Ep done - 19220.\n",
      "Ep done - 19230.\n",
      "Ep done - 19240.\n",
      "Ep done - 19250.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 3660     |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "Ep done - 19260.\n",
      "Ep done - 19270.\n",
      "Ep done - 19280.\n",
      "Ep done - 19290.\n",
      "Ep done - 19300.\n",
      "Ep done - 19310.\n",
      "Ep done - 19320.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 285         |\n",
      "|    time_elapsed         | 3669        |\n",
      "|    total_timesteps      | 583680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039100397 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.57        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    value_loss           | 0.0874      |\n",
      "-----------------------------------------\n",
      "Ep done - 19330.\n",
      "Ep done - 19340.\n",
      "Ep done - 19350.\n",
      "Ep done - 19360.\n",
      "Ep done - 19370.\n",
      "Ep done - 19380.\n",
      "Ep done - 19390.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 286        |\n",
      "|    time_elapsed         | 3678       |\n",
      "|    total_timesteps      | 585728     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03324843 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.337     |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0039     |\n",
      "|    n_updates            | 2850       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "Ep done - 19400.\n",
      "Ep done - 19410.\n",
      "Ep done - 19420.\n",
      "Ep done - 19430.\n",
      "Ep done - 19440.\n",
      "Ep done - 19450.\n",
      "Ep done - 19460.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 287         |\n",
      "|    time_elapsed         | 3688        |\n",
      "|    total_timesteps      | 587776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042998794 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.323      |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.0992      |\n",
      "-----------------------------------------\n",
      "Ep done - 19470.\n",
      "Ep done - 19480.\n",
      "Ep done - 19490.\n",
      "Ep done - 19500.\n",
      "Ep done - 19510.\n",
      "Ep done - 19520.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.74       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 288        |\n",
      "|    time_elapsed         | 3697       |\n",
      "|    total_timesteps      | 589824     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04501415 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.328     |\n",
      "|    explained_variance   | 0.551      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.017     |\n",
      "|    n_updates            | 2870       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    value_loss           | 0.0718     |\n",
      "----------------------------------------\n",
      "Ep done - 19530.\n",
      "Ep done - 5810.\n",
      "Ep done - 5820.\n",
      "Ep done - 5830.\n",
      "Ep done - 5840.\n",
      "Ep done - 5850.\n",
      "Ep done - 5860.\n",
      "Ep done - 5870.\n",
      "Ep done - 5880.\n",
      "Ep done - 5890.\n",
      "Ep done - 5900.\n",
      "Eval num_timesteps=590000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.27 +/- 0.49\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.3     |\n",
      "|    mean_reward          | 0.62     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 590000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.034277 |\n",
      "|    clip_fraction        | 0.16     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.343   |\n",
      "|    explained_variance   | 0.478    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.0164  |\n",
      "|    n_updates            | 2880     |\n",
      "|    policy_gradient_loss | -0.0247  |\n",
      "|    value_loss           | 0.0801   |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.62\n",
      "SELFPLAY: new best model, bumping up generation to 48\n",
      "Ep done - 19540.\n",
      "Ep done - 19550.\n",
      "Ep done - 19560.\n",
      "Ep done - 19570.\n",
      "Ep done - 19580.\n",
      "Ep done - 19590.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.74     |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 289      |\n",
      "|    time_elapsed    | 3715     |\n",
      "|    total_timesteps | 591872   |\n",
      "---------------------------------\n",
      "Ep done - 19600.\n",
      "Ep done - 19610.\n",
      "Ep done - 19620.\n",
      "Ep done - 19630.\n",
      "Ep done - 19640.\n",
      "Ep done - 19650.\n",
      "Ep done - 19660.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 290         |\n",
      "|    time_elapsed         | 3724        |\n",
      "|    total_timesteps      | 593920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030143682 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00225    |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Ep done - 19670.\n",
      "Ep done - 19680.\n",
      "Ep done - 19690.\n",
      "Ep done - 19700.\n",
      "Ep done - 19710.\n",
      "Ep done - 19720.\n",
      "Ep done - 19730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 3733        |\n",
      "|    total_timesteps      | 595968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037642214 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.355      |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0426      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "Ep done - 19740.\n",
      "Ep done - 19750.\n",
      "Ep done - 19760.\n",
      "Ep done - 19770.\n",
      "Ep done - 19780.\n",
      "Ep done - 19790.\n",
      "Ep done - 19800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 292         |\n",
      "|    time_elapsed         | 3743        |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033572752 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0202      |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 19810.\n",
      "Ep done - 19820.\n",
      "Ep done - 19830.\n",
      "Ep done - 19840.\n",
      "Ep done - 19850.\n",
      "Ep done - 19860.\n",
      "Ep done - 5910.\n",
      "Ep done - 5920.\n",
      "Ep done - 5930.\n",
      "Ep done - 5940.\n",
      "Ep done - 5950.\n",
      "Ep done - 5960.\n",
      "Ep done - 5970.\n",
      "Ep done - 5980.\n",
      "Ep done - 5990.\n",
      "Ep done - 6000.\n",
      "Eval num_timesteps=600000, episode_reward=0.49 +/- 0.85\n",
      "Episode length: 30.28 +/- 0.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044609495 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.325      |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0364      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.49\n",
      "SELFPLAY: new best model, bumping up generation to 49\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 3762     |\n",
      "|    total_timesteps | 600064   |\n",
      "---------------------------------\n",
      "Ep done - 19870.\n",
      "Ep done - 19880.\n",
      "Ep done - 19890.\n",
      "Ep done - 19900.\n",
      "Ep done - 19910.\n",
      "Ep done - 19920.\n",
      "Ep done - 19930.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.57       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 294        |\n",
      "|    time_elapsed         | 3771       |\n",
      "|    total_timesteps      | 602112     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04179779 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.338     |\n",
      "|    explained_variance   | 0.37       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00292   |\n",
      "|    n_updates            | 2930       |\n",
      "|    policy_gradient_loss | -0.0319    |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "Ep done - 19940.\n",
      "Ep done - 19950.\n",
      "Ep done - 19960.\n",
      "Ep done - 19970.\n",
      "Ep done - 19980.\n",
      "Ep done - 19990.\n",
      "Ep done - 20000.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.57       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 295        |\n",
      "|    time_elapsed         | 3780       |\n",
      "|    total_timesteps      | 604160     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03168325 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.326     |\n",
      "|    explained_variance   | 0.288      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Ep done - 20010.\n",
      "Ep done - 20020.\n",
      "Ep done - 20030.\n",
      "Ep done - 20040.\n",
      "Ep done - 20050.\n",
      "Ep done - 20060.\n",
      "Ep done - 20070.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 296        |\n",
      "|    time_elapsed         | 3789       |\n",
      "|    total_timesteps      | 606208     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04163842 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.319     |\n",
      "|    explained_variance   | 0.391      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 2950       |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    value_loss           | 0.133      |\n",
      "----------------------------------------\n",
      "Ep done - 20080.\n",
      "Ep done - 20090.\n",
      "Ep done - 20100.\n",
      "Ep done - 20110.\n",
      "Ep done - 20120.\n",
      "Ep done - 20130.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 297        |\n",
      "|    time_elapsed         | 3798       |\n",
      "|    total_timesteps      | 608256     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05259666 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.329     |\n",
      "|    explained_variance   | 0.316      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "Ep done - 20140.\n",
      "Ep done - 20150.\n",
      "Ep done - 20160.\n",
      "Ep done - 20170.\n",
      "Ep done - 20180.\n",
      "Ep done - 20190.\n",
      "Ep done - 6010.\n",
      "Ep done - 6020.\n",
      "Ep done - 6030.\n",
      "Ep done - 6040.\n",
      "Ep done - 6050.\n",
      "Ep done - 6060.\n",
      "Ep done - 6070.\n",
      "Ep done - 6080.\n",
      "Ep done - 6090.\n",
      "Ep done - 6100.\n",
      "Eval num_timesteps=610000, episode_reward=0.42 +/- 0.90\n",
      "Episode length: 30.16 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 610000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040196516 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.42\n",
      "SELFPLAY: new best model, bumping up generation to 50\n",
      "Ep done - 20200.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 3816     |\n",
      "|    total_timesteps | 610304   |\n",
      "---------------------------------\n",
      "Ep done - 20210.\n",
      "Ep done - 20220.\n",
      "Ep done - 20230.\n",
      "Ep done - 20240.\n",
      "Ep done - 20250.\n",
      "Ep done - 20260.\n",
      "Ep done - 20270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 299         |\n",
      "|    time_elapsed         | 3825        |\n",
      "|    total_timesteps      | 612352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043309458 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0275      |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Ep done - 20280.\n",
      "Ep done - 20290.\n",
      "Ep done - 20300.\n",
      "Ep done - 20310.\n",
      "Ep done - 20320.\n",
      "Ep done - 20330.\n",
      "Ep done - 20340.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 300        |\n",
      "|    time_elapsed         | 3835       |\n",
      "|    total_timesteps      | 614400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03668261 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.338     |\n",
      "|    explained_variance   | 0.255      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.041      |\n",
      "|    n_updates            | 2990       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 20350.\n",
      "Ep done - 20360.\n",
      "Ep done - 20370.\n",
      "Ep done - 20380.\n",
      "Ep done - 20390.\n",
      "Ep done - 20400.\n",
      "Ep done - 20410.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 301        |\n",
      "|    time_elapsed         | 3844       |\n",
      "|    total_timesteps      | 616448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03403146 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.294     |\n",
      "|    explained_variance   | 0.258      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00674    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0262    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "Ep done - 20420.\n",
      "Ep done - 20430.\n",
      "Ep done - 20440.\n",
      "Ep done - 20450.\n",
      "Ep done - 20460.\n",
      "Ep done - 20470.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 302         |\n",
      "|    time_elapsed         | 3853        |\n",
      "|    total_timesteps      | 618496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037598863 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0536      |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Ep done - 20480.\n",
      "Ep done - 20490.\n",
      "Ep done - 20500.\n",
      "Ep done - 20510.\n",
      "Ep done - 20520.\n",
      "Ep done - 6110.\n",
      "Ep done - 6120.\n",
      "Ep done - 6130.\n",
      "Ep done - 6140.\n",
      "Ep done - 6150.\n",
      "Ep done - 6160.\n",
      "Ep done - 6170.\n",
      "Ep done - 6180.\n",
      "Ep done - 6190.\n",
      "Ep done - 6200.\n",
      "Eval num_timesteps=620000, episode_reward=0.47 +/- 0.87\n",
      "Episode length: 30.25 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.47       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 620000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04175386 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.325     |\n",
      "|    explained_variance   | 0.185      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0454     |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.47\n",
      "SELFPLAY: new best model, bumping up generation to 51\n",
      "Ep done - 20530.\n",
      "Ep done - 20540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 3872     |\n",
      "|    total_timesteps | 620544   |\n",
      "---------------------------------\n",
      "Ep done - 20550.\n",
      "Ep done - 20560.\n",
      "Ep done - 20570.\n",
      "Ep done - 20580.\n",
      "Ep done - 20590.\n",
      "Ep done - 20600.\n",
      "Ep done - 20610.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 304         |\n",
      "|    time_elapsed         | 3881        |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033433735 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.316      |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0137      |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "Ep done - 20620.\n",
      "Ep done - 20630.\n",
      "Ep done - 20640.\n",
      "Ep done - 20650.\n",
      "Ep done - 20660.\n",
      "Ep done - 20670.\n",
      "Ep done - 20680.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 305         |\n",
      "|    time_elapsed         | 3890        |\n",
      "|    total_timesteps      | 624640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048142135 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 20690.\n",
      "Ep done - 20700.\n",
      "Ep done - 20710.\n",
      "Ep done - 20720.\n",
      "Ep done - 20730.\n",
      "Ep done - 20740.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 306         |\n",
      "|    time_elapsed         | 3899        |\n",
      "|    total_timesteps      | 626688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049989685 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00859     |\n",
      "|    n_updates            | 3050        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Ep done - 20750.\n",
      "Ep done - 20760.\n",
      "Ep done - 20770.\n",
      "Ep done - 20780.\n",
      "Ep done - 20790.\n",
      "Ep done - 20800.\n",
      "Ep done - 20810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.58        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 307         |\n",
      "|    time_elapsed         | 3908        |\n",
      "|    total_timesteps      | 628736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040880047 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.296      |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0081     |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 20820.\n",
      "Ep done - 20830.\n",
      "Ep done - 20840.\n",
      "Ep done - 20850.\n",
      "Ep done - 6210.\n",
      "Ep done - 6220.\n",
      "Ep done - 6230.\n",
      "Ep done - 6240.\n",
      "Ep done - 6250.\n",
      "Ep done - 6260.\n",
      "Ep done - 6270.\n",
      "Ep done - 6280.\n",
      "Ep done - 6290.\n",
      "Ep done - 6300.\n",
      "Eval num_timesteps=630000, episode_reward=0.50 +/- 0.87\n",
      "Episode length: 30.20 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 630000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046350118 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.5\n",
      "SELFPLAY: new best model, bumping up generation to 52\n",
      "Ep done - 20860.\n",
      "Ep done - 20870.\n",
      "Ep done - 20880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 3927     |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "Ep done - 20890.\n",
      "Ep done - 20900.\n",
      "Ep done - 20910.\n",
      "Ep done - 20920.\n",
      "Ep done - 20930.\n",
      "Ep done - 20940.\n",
      "Ep done - 20950.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 309         |\n",
      "|    time_elapsed         | 3936        |\n",
      "|    total_timesteps      | 632832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037381064 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.326      |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Ep done - 20960.\n",
      "Ep done - 20970.\n",
      "Ep done - 20980.\n",
      "Ep done - 20990.\n",
      "Ep done - 21000.\n",
      "Ep done - 21010.\n",
      "Ep done - 21020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 310         |\n",
      "|    time_elapsed         | 3945        |\n",
      "|    total_timesteps      | 634880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035384685 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00441     |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Ep done - 21030.\n",
      "Ep done - 21040.\n",
      "Ep done - 21050.\n",
      "Ep done - 21060.\n",
      "Ep done - 21070.\n",
      "Ep done - 21080.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.59      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 161       |\n",
      "|    iterations           | 311       |\n",
      "|    time_elapsed         | 3954      |\n",
      "|    total_timesteps      | 636928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0422196 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.3      |\n",
      "|    explained_variance   | 0.271     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0219    |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | -0.0246   |\n",
      "|    value_loss           | 0.108     |\n",
      "---------------------------------------\n",
      "Ep done - 21090.\n",
      "Ep done - 21100.\n",
      "Ep done - 21110.\n",
      "Ep done - 21120.\n",
      "Ep done - 21130.\n",
      "Ep done - 21140.\n",
      "Ep done - 21150.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 312         |\n",
      "|    time_elapsed         | 3963        |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039725684 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.304      |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "Ep done - 21160.\n",
      "Ep done - 21170.\n",
      "Ep done - 21180.\n",
      "Ep done - 21190.\n",
      "Ep done - 6310.\n",
      "Ep done - 6320.\n",
      "Ep done - 6330.\n",
      "Ep done - 6340.\n",
      "Ep done - 6350.\n",
      "Ep done - 6360.\n",
      "Ep done - 6370.\n",
      "Ep done - 6380.\n",
      "Ep done - 6390.\n",
      "Ep done - 6400.\n",
      "Eval num_timesteps=640000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.17 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049880527 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.3        |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00983     |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.52\n",
      "SELFPLAY: new best model, bumping up generation to 53\n",
      "Ep done - 21200.\n",
      "Ep done - 21210.\n",
      "Ep done - 21220.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 3982     |\n",
      "|    total_timesteps | 641024   |\n",
      "---------------------------------\n",
      "Ep done - 21230.\n",
      "Ep done - 21240.\n",
      "Ep done - 21250.\n",
      "Ep done - 21260.\n",
      "Ep done - 21270.\n",
      "Ep done - 21280.\n",
      "Ep done - 21290.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 314        |\n",
      "|    time_elapsed         | 3991       |\n",
      "|    total_timesteps      | 643072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03478316 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.307     |\n",
      "|    explained_variance   | 0.295      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 3130       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "Ep done - 21300.\n",
      "Ep done - 21310.\n",
      "Ep done - 21320.\n",
      "Ep done - 21330.\n",
      "Ep done - 21340.\n",
      "Ep done - 21350.\n",
      "Ep done - 21360.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 315        |\n",
      "|    time_elapsed         | 4000       |\n",
      "|    total_timesteps      | 645120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04085458 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.309     |\n",
      "|    explained_variance   | 0.303      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0165     |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "Ep done - 21370.\n",
      "Ep done - 21380.\n",
      "Ep done - 21390.\n",
      "Ep done - 21400.\n",
      "Ep done - 21410.\n",
      "Ep done - 21420.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 316         |\n",
      "|    time_elapsed         | 4009        |\n",
      "|    total_timesteps      | 647168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040228833 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 21430.\n",
      "Ep done - 21440.\n",
      "Ep done - 21450.\n",
      "Ep done - 21460.\n",
      "Ep done - 21470.\n",
      "Ep done - 21480.\n",
      "Ep done - 21490.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 317         |\n",
      "|    time_elapsed         | 4019        |\n",
      "|    total_timesteps      | 649216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044958454 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0311      |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Ep done - 21500.\n",
      "Ep done - 21510.\n",
      "Ep done - 21520.\n",
      "Ep done - 6410.\n",
      "Ep done - 6420.\n",
      "Ep done - 6430.\n",
      "Ep done - 6440.\n",
      "Ep done - 6450.\n",
      "Ep done - 6460.\n",
      "Ep done - 6470.\n",
      "Ep done - 6480.\n",
      "Ep done - 6490.\n",
      "Ep done - 6500.\n",
      "Eval num_timesteps=650000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.14 +/- 0.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 650000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035596795 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.294      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.57\n",
      "SELFPLAY: new best model, bumping up generation to 54\n",
      "Ep done - 21530.\n",
      "Ep done - 21540.\n",
      "Ep done - 21550.\n",
      "Ep done - 21560.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.59     |\n",
      "| time/              |          |\n",
      "|    fps             | 161      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 4037     |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "Ep done - 21570.\n",
      "Ep done - 21580.\n",
      "Ep done - 21590.\n",
      "Ep done - 21600.\n",
      "Ep done - 21610.\n",
      "Ep done - 21620.\n",
      "Ep done - 21630.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.58        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 319         |\n",
      "|    time_elapsed         | 4046        |\n",
      "|    total_timesteps      | 653312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041661162 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.296      |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 3180        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Ep done - 21640.\n",
      "Ep done - 21650.\n",
      "Ep done - 21660.\n",
      "Ep done - 21670.\n",
      "Ep done - 21680.\n",
      "Ep done - 21690.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 320         |\n",
      "|    time_elapsed         | 4055        |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038634695 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.311      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Ep done - 21700.\n",
      "Ep done - 21710.\n",
      "Ep done - 21720.\n",
      "Ep done - 21730.\n",
      "Ep done - 21740.\n",
      "Ep done - 21750.\n",
      "Ep done - 21760.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 321         |\n",
      "|    time_elapsed         | 4064        |\n",
      "|    total_timesteps      | 657408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033893593 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.302      |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0322      |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Ep done - 21770.\n",
      "Ep done - 21780.\n",
      "Ep done - 21790.\n",
      "Ep done - 21800.\n",
      "Ep done - 21810.\n",
      "Ep done - 21820.\n",
      "Ep done - 21830.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 322         |\n",
      "|    time_elapsed         | 4073        |\n",
      "|    total_timesteps      | 659456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043831665 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00303     |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.0997      |\n",
      "-----------------------------------------\n",
      "Ep done - 21840.\n",
      "Ep done - 21850.\n",
      "Ep done - 6510.\n",
      "Ep done - 6520.\n",
      "Ep done - 6530.\n",
      "Ep done - 6540.\n",
      "Ep done - 6550.\n",
      "Ep done - 6560.\n",
      "Ep done - 6570.\n",
      "Ep done - 6580.\n",
      "Ep done - 6590.\n",
      "Ep done - 6600.\n",
      "Eval num_timesteps=660000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.22 +/- 0.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 660000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03685048 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.312     |\n",
      "|    explained_variance   | 0.497      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0033    |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 55\n",
      "Ep done - 21860.\n",
      "Ep done - 21870.\n",
      "Ep done - 21880.\n",
      "Ep done - 21890.\n",
      "Ep done - 21900.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 161      |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 4092     |\n",
      "|    total_timesteps | 661504   |\n",
      "---------------------------------\n",
      "Ep done - 21910.\n",
      "Ep done - 21920.\n",
      "Ep done - 21930.\n",
      "Ep done - 21940.\n",
      "Ep done - 21950.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 21960.\n",
      "Ep done - 21970.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 324        |\n",
      "|    time_elapsed         | 4101       |\n",
      "|    total_timesteps      | 663552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03811264 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.3       |\n",
      "|    explained_variance   | 0.298      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0053    |\n",
      "|    n_updates            | 3230       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.131      |\n",
      "----------------------------------------\n",
      "Ep done - 21980.\n",
      "Ep done - 21990.\n",
      "Ep done - 22000.\n",
      "Ep done - 22010.\n",
      "Ep done - 22020.\n",
      "Ep done - 22030.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 325         |\n",
      "|    time_elapsed         | 4110        |\n",
      "|    total_timesteps      | 665600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036279533 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00954    |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Ep done - 22040.\n",
      "Ep done - 22050.\n",
      "Ep done - 22060.\n",
      "Ep done - 22070.\n",
      "Ep done - 22080.\n",
      "Ep done - 22090.\n",
      "Ep done - 22100.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 326        |\n",
      "|    time_elapsed         | 4119       |\n",
      "|    total_timesteps      | 667648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04980818 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.318     |\n",
      "|    explained_variance   | 0.404      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0277    |\n",
      "|    n_updates            | 3250       |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "Ep done - 22110.\n",
      "Ep done - 22120.\n",
      "Ep done - 22130.\n",
      "Ep done - 22140.\n",
      "Ep done - 22150.\n",
      "Ep done - 22160.\n",
      "Ep done - 22170.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 327         |\n",
      "|    time_elapsed         | 4128        |\n",
      "|    total_timesteps      | 669696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053845845 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.303      |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00316    |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Ep done - 22180.\n",
      "Ep done - 6610.\n",
      "Ep done - 6620.\n",
      "Ep done - 6630.\n",
      "Ep done - 6640.\n",
      "Ep done - 6650.\n",
      "Ep done - 6660.\n",
      "Ep done - 6670.\n",
      "Ep done - 6680.\n",
      "Ep done - 6690.\n",
      "Ep done - 6700.\n",
      "Eval num_timesteps=670000, episode_reward=0.74 +/- 0.66\n",
      "Episode length: 30.21 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.74       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 670000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03674811 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.284     |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0106     |\n",
      "|    n_updates            | 3270       |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.74\n",
      "SELFPLAY: new best model, bumping up generation to 56\n",
      "Ep done - 22190.\n",
      "Ep done - 22200.\n",
      "Ep done - 22210.\n",
      "Ep done - 22220.\n",
      "Ep done - 22230.\n",
      "Ep done - 22240.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 161      |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 4147     |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Ep done - 22250.\n",
      "Ep done - 22260.\n",
      "Ep done - 22270.\n",
      "Ep done - 22280.\n",
      "Ep done - 22290.\n",
      "Ep done - 22300.\n",
      "Ep done - 22310.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 329         |\n",
      "|    time_elapsed         | 4156        |\n",
      "|    total_timesteps      | 673792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036582306 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00739     |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Ep done - 22320.\n",
      "Ep done - 22330.\n",
      "Ep done - 22340.\n",
      "Ep done - 22350.\n",
      "Ep done - 22360.\n",
      "Ep done - 22370.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 330        |\n",
      "|    time_elapsed         | 4165       |\n",
      "|    total_timesteps      | 675840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05431831 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.265     |\n",
      "|    explained_variance   | 0.36       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0387     |\n",
      "|    n_updates            | 3290       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "Ep done - 22380.\n",
      "Ep done - 22390.\n",
      "Ep done - 22400.\n",
      "Ep done - 22410.\n",
      "Ep done - 22420.\n",
      "Ep done - 22430.\n",
      "Ep done - 22440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 331         |\n",
      "|    time_elapsed         | 4174        |\n",
      "|    total_timesteps      | 677888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048333377 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.0601      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 22450.\n",
      "Ep done - 22460.\n",
      "Ep done - 22470.\n",
      "Ep done - 22480.\n",
      "Ep done - 22490.\n",
      "Ep done - 22500.\n",
      "Ep done - 22510.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.71        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 4184        |\n",
      "|    total_timesteps      | 679936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041124206 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Ep done - 6710.\n",
      "Ep done - 6720.\n",
      "Ep done - 6730.\n",
      "Ep done - 6740.\n",
      "Ep done - 6750.\n",
      "Ep done - 6760.\n",
      "Ep done - 6770.\n",
      "Ep done - 6780.\n",
      "Ep done - 6790.\n",
      "Ep done - 6800.\n",
      "Eval num_timesteps=680000, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 30.21 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 680000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03638556 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | 0.212      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00717    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    value_loss           | 0.097      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.79\n",
      "SELFPLAY: new best model, bumping up generation to 57\n",
      "Ep done - 22520.\n",
      "Ep done - 22530.\n",
      "Ep done - 22540.\n",
      "Ep done - 22550.\n",
      "Ep done - 22560.\n",
      "Ep done - 22570.\n",
      "Ep done - 22580.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 162      |\n",
      "|    iterations      | 333      |\n",
      "|    time_elapsed    | 4202     |\n",
      "|    total_timesteps | 681984   |\n",
      "---------------------------------\n",
      "Ep done - 22590.\n",
      "Ep done - 22600.\n",
      "Ep done - 22610.\n",
      "Ep done - 22620.\n",
      "Ep done - 22630.\n",
      "Ep done - 22640.\n",
      "Ep done - 22650.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 334         |\n",
      "|    time_elapsed         | 4211        |\n",
      "|    total_timesteps      | 684032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039804175 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0356      |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Ep done - 22660.\n",
      "Ep done - 22670.\n",
      "Ep done - 22680.\n",
      "Ep done - 22690.\n",
      "Ep done - 22700.\n",
      "Ep done - 22710.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 335        |\n",
      "|    time_elapsed         | 4220       |\n",
      "|    total_timesteps      | 686080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04603356 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.26      |\n",
      "|    explained_variance   | 0.186      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0402     |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Ep done - 22720.\n",
      "Ep done - 22730.\n",
      "Ep done - 22740.\n",
      "Ep done - 22750.\n",
      "Ep done - 22760.\n",
      "Ep done - 22770.\n",
      "Ep done - 22780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 336         |\n",
      "|    time_elapsed         | 4229        |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038624663 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0124      |\n",
      "|    n_updates            | 3350        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 22790.\n",
      "Ep done - 22800.\n",
      "Ep done - 22810.\n",
      "Ep done - 22820.\n",
      "Ep done - 22830.\n",
      "Ep done - 22840.\n",
      "Ep done - 6810.\n",
      "Ep done - 6820.\n",
      "Ep done - 6830.\n",
      "Ep done - 6840.\n",
      "Ep done - 6850.\n",
      "Ep done - 6860.\n",
      "Ep done - 6870.\n",
      "Ep done - 6880.\n",
      "Ep done - 6890.\n",
      "Ep done - 6900.\n",
      "Eval num_timesteps=690000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.18 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 690000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03430425 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.254     |\n",
      "|    explained_variance   | 0.233      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0094     |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    value_loss           | 0.143      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.64\n",
      "SELFPLAY: new best model, bumping up generation to 58\n",
      "Ep done - 22850.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 162      |\n",
      "|    iterations      | 337      |\n",
      "|    time_elapsed    | 4248     |\n",
      "|    total_timesteps | 690176   |\n",
      "---------------------------------\n",
      "Ep done - 22860.\n",
      "Ep done - 22870.\n",
      "Ep done - 22880.\n",
      "Ep done - 22890.\n",
      "Ep done - 22900.\n",
      "Ep done - 22910.\n",
      "Ep done - 22920.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 338        |\n",
      "|    time_elapsed         | 4257       |\n",
      "|    total_timesteps      | 692224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04322489 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.292      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0113    |\n",
      "|    n_updates            | 3370       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "Ep done - 22930.\n",
      "Ep done - 22940.\n",
      "Ep done - 22950.\n",
      "Ep done - 22960.\n",
      "Ep done - 22970.\n",
      "Ep done - 22980.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 339         |\n",
      "|    time_elapsed         | 4266        |\n",
      "|    total_timesteps      | 694272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053199325 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.276      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0317      |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.0823      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 22990.\n",
      "Ep done - 23000.\n",
      "Ep done - 23010.\n",
      "Ep done - 23020.\n",
      "Ep done - 23030.\n",
      "Ep done - 23040.\n",
      "Ep done - 23050.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.65      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 162       |\n",
      "|    iterations           | 340       |\n",
      "|    time_elapsed         | 4276      |\n",
      "|    total_timesteps      | 696320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0529887 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.278    |\n",
      "|    explained_variance   | 0.263     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0262    |\n",
      "|    n_updates            | 3390      |\n",
      "|    policy_gradient_loss | -0.026    |\n",
      "|    value_loss           | 0.124     |\n",
      "---------------------------------------\n",
      "Ep done - 23060.\n",
      "Ep done - 23070.\n",
      "Ep done - 23080.\n",
      "Ep done - 23090.\n",
      "Ep done - 23100.\n",
      "Ep done - 23110.\n",
      "Ep done - 23120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 341         |\n",
      "|    time_elapsed         | 4285        |\n",
      "|    total_timesteps      | 698368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041927338 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.28       |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 3400        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Ep done - 23130.\n",
      "Ep done - 23140.\n",
      "Ep done - 23150.\n",
      "Ep done - 23160.\n",
      "Ep done - 23170.\n",
      "Ep done - 6910.\n",
      "Ep done - 6920.\n",
      "Ep done - 6930.\n",
      "Ep done - 6940.\n",
      "Ep done - 6950.\n",
      "Ep done - 6960.\n",
      "Ep done - 6970.\n",
      "Ep done - 6980.\n",
      "Ep done - 6990.\n",
      "Ep done - 7000.\n",
      "Eval num_timesteps=700000, episode_reward=0.66 +/- 0.72\n",
      "Episode length: 30.15 +/- 0.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 700000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033991836 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00254    |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.66\n",
      "SELFPLAY: new best model, bumping up generation to 59\n",
      "Ep done - 23180.\n",
      "Ep done - 23190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.81     |\n",
      "| time/              |          |\n",
      "|    fps             | 162      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 4303     |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "Ep done - 23200.\n",
      "Ep done - 23210.\n",
      "Ep done - 23220.\n",
      "Ep done - 23230.\n",
      "Ep done - 23240.\n",
      "Ep done - 23250.\n",
      "Ep done - 23260.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 343         |\n",
      "|    time_elapsed         | 4312        |\n",
      "|    total_timesteps      | 702464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043208882 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.258      |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00143     |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 0.0664      |\n",
      "-----------------------------------------\n",
      "Ep done - 23270.\n",
      "Ep done - 23280.\n",
      "Ep done - 23290.\n",
      "Ep done - 23300.\n",
      "Ep done - 23310.\n",
      "Ep done - 23320.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 344         |\n",
      "|    time_elapsed         | 4321        |\n",
      "|    total_timesteps      | 704512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030756341 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00598     |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 0.0966      |\n",
      "-----------------------------------------\n",
      "Ep done - 23330.\n",
      "Ep done - 23340.\n",
      "Ep done - 23350.\n",
      "Ep done - 23360.\n",
      "Ep done - 23370.\n",
      "Ep done - 23380.\n",
      "Ep done - 23390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 345         |\n",
      "|    time_elapsed         | 4331        |\n",
      "|    total_timesteps      | 706560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036867652 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0864      |\n",
      "|    n_updates            | 3440        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Ep done - 23400.\n",
      "Ep done - 23410.\n",
      "Ep done - 23420.\n",
      "Ep done - 23430.\n",
      "Ep done - 23440.\n",
      "Ep done - 23450.\n",
      "Ep done - 23460.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 346        |\n",
      "|    time_elapsed         | 4340       |\n",
      "|    total_timesteps      | 708608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03295829 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | 0.369      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0209     |\n",
      "|    n_updates            | 3450       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Ep done - 23470.\n",
      "Ep done - 23480.\n",
      "Ep done - 23490.\n",
      "Ep done - 23500.\n",
      "Ep done - 23510.\n",
      "Ep done - 7010.\n",
      "Ep done - 7020.\n",
      "Ep done - 7030.\n",
      "Ep done - 7040.\n",
      "Ep done - 7050.\n",
      "Ep done - 7060.\n",
      "Ep done - 7070.\n",
      "Ep done - 7080.\n",
      "Ep done - 7090.\n",
      "Ep done - 7100.\n",
      "Eval num_timesteps=710000, episode_reward=0.69 +/- 0.72\n",
      "Episode length: 30.19 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 710000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034872223 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0222     |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.69\n",
      "SELFPLAY: new best model, bumping up generation to 60\n",
      "Ep done - 23520.\n",
      "Ep done - 23530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.59     |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 347      |\n",
      "|    time_elapsed    | 4358     |\n",
      "|    total_timesteps | 710656   |\n",
      "---------------------------------\n",
      "Ep done - 23540.\n",
      "Ep done - 23550.\n",
      "Ep done - 23560.\n",
      "Ep done - 23570.\n",
      "Ep done - 23580.\n",
      "Ep done - 23590.\n",
      "Ep done - 23600.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 348        |\n",
      "|    time_elapsed         | 4367       |\n",
      "|    total_timesteps      | 712704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03607657 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.281     |\n",
      "|    explained_variance   | 0.413      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 3470       |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "Ep done - 23610.\n",
      "Ep done - 23620.\n",
      "Ep done - 23630.\n",
      "Ep done - 23640.\n",
      "Ep done - 23650.\n",
      "Ep done - 23660.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 349         |\n",
      "|    time_elapsed         | 4377        |\n",
      "|    total_timesteps      | 714752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042406023 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.001      |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.0887      |\n",
      "-----------------------------------------\n",
      "Ep done - 23670.\n",
      "Ep done - 23680.\n",
      "Ep done - 23690.\n",
      "Ep done - 23700.\n",
      "Ep done - 23710.\n",
      "Ep done - 23720.\n",
      "Ep done - 23730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 350         |\n",
      "|    time_elapsed         | 4386        |\n",
      "|    total_timesteps      | 716800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038179085 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0439      |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Ep done - 23740.\n",
      "Ep done - 23750.\n",
      "Ep done - 23760.\n",
      "Ep done - 23770.\n",
      "Ep done - 23780.\n",
      "Ep done - 23790.\n",
      "Ep done - 23800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 351         |\n",
      "|    time_elapsed         | 4395        |\n",
      "|    total_timesteps      | 718848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040271416 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00062     |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Ep done - 23810.\n",
      "Ep done - 23820.\n",
      "Ep done - 23830.\n",
      "Ep done - 23840.\n",
      "Ep done - 7110.\n",
      "Ep done - 7120.\n",
      "Ep done - 7130.\n",
      "Ep done - 7140.\n",
      "Ep done - 7150.\n",
      "Ep done - 7160.\n",
      "Ep done - 7170.\n",
      "Ep done - 7180.\n",
      "Ep done - 7190.\n",
      "Ep done - 7200.\n",
      "Eval num_timesteps=720000, episode_reward=0.75 +/- 0.65\n",
      "Episode length: 30.25 +/- 0.57\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.75       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 720000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03715581 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.264     |\n",
      "|    explained_variance   | 0.421      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00642    |\n",
      "|    n_updates            | 3510       |\n",
      "|    policy_gradient_loss | -0.0231    |\n",
      "|    value_loss           | 0.0977     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.75\n",
      "SELFPLAY: new best model, bumping up generation to 61\n",
      "Ep done - 23850.\n",
      "Ep done - 23860.\n",
      "Ep done - 23870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 4414     |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Ep done - 23880.\n",
      "Ep done - 23890.\n",
      "Ep done - 23900.\n",
      "Ep done - 23910.\n",
      "Ep done - 23920.\n",
      "Ep done - 23930.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 353         |\n",
      "|    time_elapsed         | 4423        |\n",
      "|    total_timesteps      | 722944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034397826 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00918     |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0856      |\n",
      "-----------------------------------------\n",
      "Ep done - 23940.\n",
      "Ep done - 23950.\n",
      "Ep done - 23960.\n",
      "Ep done - 23970.\n",
      "Ep done - 23980.\n",
      "Ep done - 23990.\n",
      "Ep done - 24000.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.45      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 163       |\n",
      "|    iterations           | 354       |\n",
      "|    time_elapsed         | 4432      |\n",
      "|    total_timesteps      | 724992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0417161 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.305    |\n",
      "|    explained_variance   | 0.45      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00117  |\n",
      "|    n_updates            | 3530      |\n",
      "|    policy_gradient_loss | -0.027    |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "Ep done - 24010.\n",
      "Ep done - 24020.\n",
      "Ep done - 24030.\n",
      "Ep done - 24040.\n",
      "Ep done - 24050.\n",
      "Ep done - 24060.\n",
      "Ep done - 24070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 355         |\n",
      "|    time_elapsed         | 4441        |\n",
      "|    total_timesteps      | 727040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043461114 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 24080.\n",
      "Ep done - 24090.\n",
      "Ep done - 24100.\n",
      "Ep done - 24110.\n",
      "Ep done - 24120.\n",
      "Ep done - 24130.\n",
      "Ep done - 24140.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 356         |\n",
      "|    time_elapsed         | 4450        |\n",
      "|    total_timesteps      | 729088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037916128 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Ep done - 24150.\n",
      "Ep done - 24160.\n",
      "Ep done - 24170.\n",
      "Ep done - 7210.\n",
      "Ep done - 7220.\n",
      "Ep done - 7230.\n",
      "Ep done - 7240.\n",
      "Ep done - 7250.\n",
      "Ep done - 7260.\n",
      "Ep done - 7270.\n",
      "Ep done - 7280.\n",
      "Ep done - 7290.\n",
      "Ep done - 7300.\n",
      "Eval num_timesteps=730000, episode_reward=0.73 +/- 0.68\n",
      "Episode length: 30.06 +/- 0.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 730000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041322704 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.279      |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.098       |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.73\n",
      "SELFPLAY: new best model, bumping up generation to 62\n",
      "Ep done - 24180.\n",
      "Ep done - 24190.\n",
      "Ep done - 24200.\n",
      "Ep done - 24210.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 357      |\n",
      "|    time_elapsed    | 4469     |\n",
      "|    total_timesteps | 731136   |\n",
      "---------------------------------\n",
      "Ep done - 24220.\n",
      "Ep done - 24230.\n",
      "Ep done - 24240.\n",
      "Ep done - 24250.\n",
      "Ep done - 24260.\n",
      "Ep done - 24270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 358         |\n",
      "|    time_elapsed         | 4478        |\n",
      "|    total_timesteps      | 733184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034216154 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00226    |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.0703      |\n",
      "-----------------------------------------\n",
      "Ep done - 24280.\n",
      "Ep done - 24290.\n",
      "Ep done - 24300.\n",
      "Ep done - 24310.\n",
      "Ep done - 24320.\n",
      "Ep done - 24330.\n",
      "Ep done - 24340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 359         |\n",
      "|    time_elapsed         | 4487        |\n",
      "|    total_timesteps      | 735232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035479628 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0652      |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.0981      |\n",
      "-----------------------------------------\n",
      "Ep done - 24350.\n",
      "Ep done - 24360.\n",
      "Ep done - 24370.\n",
      "Ep done - 24380.\n",
      "Ep done - 24390.\n",
      "Ep done - 24400.\n",
      "Ep done - 24410.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 360        |\n",
      "|    time_elapsed         | 4496       |\n",
      "|    total_timesteps      | 737280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04084664 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.264     |\n",
      "|    explained_variance   | 0.29       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00323    |\n",
      "|    n_updates            | 3590       |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "Ep done - 24420.\n",
      "Ep done - 24430.\n",
      "Ep done - 24440.\n",
      "Ep done - 24450.\n",
      "Ep done - 24460.\n",
      "Ep done - 24470.\n",
      "Ep done - 24480.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 361         |\n",
      "|    time_elapsed         | 4506        |\n",
      "|    total_timesteps      | 739328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047817506 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0356      |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Ep done - 24490.\n",
      "Ep done - 24500.\n",
      "Ep done - 7310.\n",
      "Ep done - 7320.\n",
      "Ep done - 7330.\n",
      "Ep done - 7340.\n",
      "Ep done - 7350.\n",
      "Ep done - 7360.\n",
      "Ep done - 7370.\n",
      "Ep done - 7380.\n",
      "Ep done - 7390.\n",
      "Ep done - 7400.\n",
      "Eval num_timesteps=740000, episode_reward=0.79 +/- 0.59\n",
      "Episode length: 30.22 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 740000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03978927 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.275     |\n",
      "|    explained_variance   | 0.455      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 3610       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    value_loss           | 0.0527     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.79\n",
      "SELFPLAY: new best model, bumping up generation to 63\n",
      "Ep done - 24510.\n",
      "Ep done - 24520.\n",
      "Ep done - 24530.\n",
      "Ep done - 24540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 4524     |\n",
      "|    total_timesteps | 741376   |\n",
      "---------------------------------\n",
      "Ep done - 24550.\n",
      "Ep done - 24560.\n",
      "Ep done - 24570.\n",
      "Ep done - 24580.\n",
      "Ep done - 24590.\n",
      "Ep done - 24600.\n",
      "Ep done - 24610.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 363         |\n",
      "|    time_elapsed         | 4533        |\n",
      "|    total_timesteps      | 743424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041021675 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0171      |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Ep done - 24620.\n",
      "Ep done - 24630.\n",
      "Ep done - 24640.\n",
      "Ep done - 24650.\n",
      "Ep done - 24660.\n",
      "Ep done - 24670.\n",
      "Ep done - 24680.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 364         |\n",
      "|    time_elapsed         | 4542        |\n",
      "|    total_timesteps      | 745472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043341406 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.296      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0196      |\n",
      "|    n_updates            | 3630        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 24690.\n",
      "Ep done - 24700.\n",
      "Ep done - 24710.\n",
      "Ep done - 24720.\n",
      "Ep done - 24730.\n",
      "Ep done - 24740.\n",
      "Ep done - 24750.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 365         |\n",
      "|    time_elapsed         | 4551        |\n",
      "|    total_timesteps      | 747520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040164605 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Ep done - 24760.\n",
      "Ep done - 24770.\n",
      "Ep done - 24780.\n",
      "Ep done - 24790.\n",
      "Ep done - 24800.\n",
      "Ep done - 24810.\n",
      "Ep done - 24820.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 366         |\n",
      "|    time_elapsed         | 4560        |\n",
      "|    total_timesteps      | 749568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030899674 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00655     |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 24830.\n",
      "Ep done - 7410.\n",
      "Ep done - 7420.\n",
      "Ep done - 7430.\n",
      "Ep done - 7440.\n",
      "Ep done - 7450.\n",
      "Ep done - 7460.\n",
      "Ep done - 7470.\n",
      "Ep done - 7480.\n",
      "Ep done - 7490.\n",
      "Ep done - 7500.\n",
      "Eval num_timesteps=750000, episode_reward=0.73 +/- 0.68\n",
      "Episode length: 30.24 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 750000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03604187 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.304     |\n",
      "|    explained_variance   | 0.334      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    value_loss           | 0.0908     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.73\n",
      "SELFPLAY: new best model, bumping up generation to 64\n",
      "Ep done - 24840.\n",
      "Ep done - 24850.\n",
      "Ep done - 24860.\n",
      "Ep done - 24870.\n",
      "Ep done - 24880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 367      |\n",
      "|    time_elapsed    | 4579     |\n",
      "|    total_timesteps | 751616   |\n",
      "---------------------------------\n",
      "Ep done - 24890.\n",
      "Ep done - 24900.\n",
      "Ep done - 24910.\n",
      "Ep done - 24920.\n",
      "Ep done - 24930.\n",
      "Ep done - 24940.\n",
      "Ep done - 24950.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 368         |\n",
      "|    time_elapsed         | 4588        |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035654403 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.019       |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "Ep done - 24960.\n",
      "Ep done - 24970.\n",
      "Ep done - 24980.\n",
      "Ep done - 24990.\n",
      "Ep done - 25000.\n",
      "Ep done - 25010.\n",
      "Ep done - 25020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 369         |\n",
      "|    time_elapsed         | 4597        |\n",
      "|    total_timesteps      | 755712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039278314 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0246      |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.0892      |\n",
      "-----------------------------------------\n",
      "Ep done - 25030.\n",
      "Ep done - 25040.\n",
      "Ep done - 25050.\n",
      "Ep done - 25060.\n",
      "Ep done - 25070.\n",
      "Ep done - 25080.\n",
      "Ep done - 25090.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 370         |\n",
      "|    time_elapsed         | 4607        |\n",
      "|    total_timesteps      | 757760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033049703 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.297      |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.022       |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 25100.\n",
      "Ep done - 25110.\n",
      "Ep done - 25120.\n",
      "Ep done - 25130.\n",
      "Ep done - 25140.\n",
      "Ep done - 25150.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 371         |\n",
      "|    time_elapsed         | 4616        |\n",
      "|    total_timesteps      | 759808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045327045 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.28       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Ep done - 25160.\n",
      "Ep done - 7510.\n",
      "Ep done - 7520.\n",
      "Ep done - 7530.\n",
      "Ep done - 7540.\n",
      "Ep done - 7550.\n",
      "Ep done - 7560.\n",
      "Ep done - 7570.\n",
      "Ep done - 7580.\n",
      "Ep done - 7590.\n",
      "Ep done - 7600.\n",
      "Eval num_timesteps=760000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.16 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 760000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04543481 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.301     |\n",
      "|    explained_variance   | 0.445      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0214     |\n",
      "|    n_updates            | 3710       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    value_loss           | 0.0829     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 65\n",
      "Ep done - 25170.\n",
      "Ep done - 25180.\n",
      "Ep done - 25190.\n",
      "Ep done - 25200.\n",
      "Ep done - 25210.\n",
      "Ep done - 25220.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 4634     |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "Ep done - 25230.\n",
      "Ep done - 25240.\n",
      "Ep done - 25250.\n",
      "Ep done - 25260.\n",
      "Ep done - 25270.\n",
      "Ep done - 25280.\n",
      "Ep done - 25290.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 373         |\n",
      "|    time_elapsed         | 4644        |\n",
      "|    total_timesteps      | 763904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037409727 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.285      |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0188      |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Ep done - 25300.\n",
      "Ep done - 25310.\n",
      "Ep done - 25320.\n",
      "Ep done - 25330.\n",
      "Ep done - 25340.\n",
      "Ep done - 25350.\n",
      "Ep done - 25360.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 374         |\n",
      "|    time_elapsed         | 4653        |\n",
      "|    total_timesteps      | 765952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041028574 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0121      |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Ep done - 25370.\n",
      "Ep done - 25380.\n",
      "Ep done - 25390.\n",
      "Ep done - 25400.\n",
      "Ep done - 25410.\n",
      "Ep done - 25420.\n",
      "Ep done - 25430.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 375         |\n",
      "|    time_elapsed         | 4662        |\n",
      "|    total_timesteps      | 768000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044086322 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.279      |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0346      |\n",
      "|    n_updates            | 3740        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Ep done - 25440.\n",
      "Ep done - 25450.\n",
      "Ep done - 25460.\n",
      "Ep done - 25470.\n",
      "Ep done - 25480.\n",
      "Ep done - 25490.\n",
      "Ep done - 7610.\n",
      "Ep done - 7620.\n",
      "Ep done - 7630.\n",
      "Ep done - 7640.\n",
      "Ep done - 7650.\n",
      "Ep done - 7660.\n",
      "Ep done - 7670.\n",
      "Ep done - 7680.\n",
      "Ep done - 7690.\n",
      "Ep done - 7700.\n",
      "Eval num_timesteps=770000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.15 +/- 0.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 770000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039108127 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0692      |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 66\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.49     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 4680     |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Ep done - 25500.\n",
      "Ep done - 25510.\n",
      "Ep done - 25520.\n",
      "Ep done - 25530.\n",
      "Ep done - 25540.\n",
      "Ep done - 25550.\n",
      "Ep done - 25560.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 377         |\n",
      "|    time_elapsed         | 4689        |\n",
      "|    total_timesteps      | 772096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049055643 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "Ep done - 25570.\n",
      "Ep done - 25580.\n",
      "Ep done - 25590.\n",
      "Ep done - 25600.\n",
      "Ep done - 25610.\n",
      "Ep done - 25620.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 25630.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 378        |\n",
      "|    time_elapsed         | 4699       |\n",
      "|    total_timesteps      | 774144     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05117338 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.284     |\n",
      "|    explained_variance   | 0.495      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00643   |\n",
      "|    n_updates            | 3770       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "Ep done - 25640.\n",
      "Ep done - 25650.\n",
      "Ep done - 25660.\n",
      "Ep done - 25670.\n",
      "Ep done - 25680.\n",
      "Ep done - 25690.\n",
      "Ep done - 25700.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 379         |\n",
      "|    time_elapsed         | 4708        |\n",
      "|    total_timesteps      | 776192      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044220738 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00728    |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "Ep done - 25710.\n",
      "Ep done - 25720.\n",
      "Ep done - 25730.\n",
      "Ep done - 25740.\n",
      "Ep done - 25750.\n",
      "Ep done - 25760.\n",
      "Ep done - 25770.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.42       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 380        |\n",
      "|    time_elapsed         | 4717       |\n",
      "|    total_timesteps      | 778240     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04610355 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.305     |\n",
      "|    explained_variance   | 0.512      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0428     |\n",
      "|    n_updates            | 3790       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "Ep done - 25780.\n",
      "Ep done - 25790.\n",
      "Ep done - 25800.\n",
      "Ep done - 25810.\n",
      "Ep done - 25820.\n",
      "Ep done - 7710.\n",
      "Ep done - 7720.\n",
      "Ep done - 7730.\n",
      "Ep done - 7740.\n",
      "Ep done - 7750.\n",
      "Ep done - 7760.\n",
      "Ep done - 7770.\n",
      "Ep done - 7780.\n",
      "Ep done - 7790.\n",
      "Ep done - 7800.\n",
      "Eval num_timesteps=780000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.23 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041373268 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.288      |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.52\n",
      "SELFPLAY: new best model, bumping up generation to 67\n",
      "Ep done - 25830.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 381      |\n",
      "|    time_elapsed    | 4735     |\n",
      "|    total_timesteps | 780288   |\n",
      "---------------------------------\n",
      "Ep done - 25840.\n",
      "Ep done - 25850.\n",
      "Ep done - 25860.\n",
      "Ep done - 25870.\n",
      "Ep done - 25880.\n",
      "Ep done - 25890.\n",
      "Ep done - 25900.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 4745        |\n",
      "|    total_timesteps      | 782336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037173532 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Ep done - 25910.\n",
      "Ep done - 25920.\n",
      "Ep done - 25930.\n",
      "Ep done - 25940.\n",
      "Ep done - 25950.\n",
      "Ep done - 25960.\n",
      "Ep done - 25970.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 383         |\n",
      "|    time_elapsed         | 4754        |\n",
      "|    total_timesteps      | 784384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054253396 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 3820        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "Ep done - 25980.\n",
      "Ep done - 25990.\n",
      "Ep done - 26000.\n",
      "Ep done - 26010.\n",
      "Ep done - 26020.\n",
      "Ep done - 26030.\n",
      "Ep done - 26040.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 384         |\n",
      "|    time_elapsed         | 4763        |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047210276 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.279      |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00412     |\n",
      "|    n_updates            | 3830        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 26050.\n",
      "Ep done - 26060.\n",
      "Ep done - 26070.\n",
      "Ep done - 26080.\n",
      "Ep done - 26090.\n",
      "Ep done - 26100.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 385         |\n",
      "|    time_elapsed         | 4772        |\n",
      "|    total_timesteps      | 788480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038226776 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00913     |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 26110.\n",
      "Ep done - 26120.\n",
      "Ep done - 26130.\n",
      "Ep done - 26140.\n",
      "Ep done - 26150.\n",
      "Ep done - 7810.\n",
      "Ep done - 7820.\n",
      "Ep done - 7830.\n",
      "Ep done - 7840.\n",
      "Ep done - 7850.\n",
      "Ep done - 7860.\n",
      "Ep done - 7870.\n",
      "Ep done - 7880.\n",
      "Ep done - 7890.\n",
      "Ep done - 7900.\n",
      "Eval num_timesteps=790000, episode_reward=0.56 +/- 0.82\n",
      "Episode length: 30.19 +/- 0.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 790000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04302481 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.249     |\n",
      "|    explained_variance   | 0.393      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0273     |\n",
      "|    n_updates            | 3850       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 68\n",
      "Ep done - 26160.\n",
      "Ep done - 26170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.73     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 4791     |\n",
      "|    total_timesteps | 790528   |\n",
      "---------------------------------\n",
      "Ep done - 26180.\n",
      "Ep done - 26190.\n",
      "Ep done - 26200.\n",
      "Ep done - 26210.\n",
      "Ep done - 26220.\n",
      "Ep done - 26230.\n",
      "Ep done - 26240.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.45       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 387        |\n",
      "|    time_elapsed         | 4800       |\n",
      "|    total_timesteps      | 792576     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05178717 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.241     |\n",
      "|    explained_variance   | 0.352      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00752   |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.0944     |\n",
      "----------------------------------------\n",
      "Ep done - 26250.\n",
      "Ep done - 26260.\n",
      "Ep done - 26270.\n",
      "Ep done - 26280.\n",
      "Ep done - 26290.\n",
      "Ep done - 26300.\n",
      "Ep done - 26310.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 388        |\n",
      "|    time_elapsed         | 4809       |\n",
      "|    total_timesteps      | 794624     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04040174 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.228     |\n",
      "|    explained_variance   | 0.433      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00758    |\n",
      "|    n_updates            | 3870       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Ep done - 26320.\n",
      "Ep done - 26330.\n",
      "Ep done - 26340.\n",
      "Ep done - 26350.\n",
      "Ep done - 26360.\n",
      "Ep done - 26370.\n",
      "Ep done - 26380.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 389         |\n",
      "|    time_elapsed         | 4819        |\n",
      "|    total_timesteps      | 796672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038781267 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0352      |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 0.0955      |\n",
      "-----------------------------------------\n",
      "Ep done - 26390.\n",
      "Ep done - 26400.\n",
      "Ep done - 26410.\n",
      "Ep done - 26420.\n",
      "Ep done - 26430.\n",
      "Ep done - 26440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 390         |\n",
      "|    time_elapsed         | 4828        |\n",
      "|    total_timesteps      | 798720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040669207 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0188      |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 26450.\n",
      "Ep done - 26460.\n",
      "Ep done - 26470.\n",
      "Ep done - 26480.\n",
      "Ep done - 26490.\n",
      "Ep done - 7910.\n",
      "Ep done - 7920.\n",
      "Ep done - 7930.\n",
      "Ep done - 7940.\n",
      "Ep done - 7950.\n",
      "Ep done - 7960.\n",
      "Ep done - 7970.\n",
      "Ep done - 7980.\n",
      "Ep done - 7990.\n",
      "Ep done - 8000.\n",
      "Eval num_timesteps=800000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.14 +/- 0.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044370867 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.62\n",
      "SELFPLAY: new best model, bumping up generation to 69\n",
      "Ep done - 26500.\n",
      "Ep done - 26510.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 391      |\n",
      "|    time_elapsed    | 4846     |\n",
      "|    total_timesteps | 800768   |\n",
      "---------------------------------\n",
      "Ep done - 26520.\n",
      "Ep done - 26530.\n",
      "Ep done - 26540.\n",
      "Ep done - 26550.\n",
      "Ep done - 26560.\n",
      "Ep done - 26570.\n",
      "Ep done - 26580.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 392         |\n",
      "|    time_elapsed         | 4855        |\n",
      "|    total_timesteps      | 802816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043309633 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00973     |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "Ep done - 26590.\n",
      "Ep done - 26600.\n",
      "Ep done - 26610.\n",
      "Ep done - 26620.\n",
      "Ep done - 26630.\n",
      "Ep done - 26640.\n",
      "Ep done - 26650.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 393         |\n",
      "|    time_elapsed         | 4865        |\n",
      "|    total_timesteps      | 804864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041192144 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.254      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 26660.\n",
      "Ep done - 26670.\n",
      "Ep done - 26680.\n",
      "Ep done - 26690.\n",
      "Ep done - 26700.\n",
      "Ep done - 26710.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 394         |\n",
      "|    time_elapsed         | 4874        |\n",
      "|    total_timesteps      | 806912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040266983 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0278      |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 26720.\n",
      "Ep done - 26730.\n",
      "Ep done - 26740.\n",
      "Ep done - 26750.\n",
      "Ep done - 26760.\n",
      "Ep done - 26770.\n",
      "Ep done - 26780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 395         |\n",
      "|    time_elapsed         | 4884        |\n",
      "|    total_timesteps      | 808960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041681416 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 3940        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Ep done - 26790.\n",
      "Ep done - 26800.\n",
      "Ep done - 26810.\n",
      "Ep done - 26820.\n",
      "Ep done - 8010.\n",
      "Ep done - 8020.\n",
      "Ep done - 8030.\n",
      "Ep done - 8040.\n",
      "Ep done - 8050.\n",
      "Ep done - 8060.\n",
      "Ep done - 8070.\n",
      "Ep done - 8080.\n",
      "Ep done - 8090.\n",
      "Ep done - 8100.\n",
      "Eval num_timesteps=810000, episode_reward=0.44 +/- 0.89\n",
      "Episode length: 30.08 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 810000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04001738 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.248     |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 3950       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.44\n",
      "SELFPLAY: new best model, bumping up generation to 70\n",
      "Ep done - 26830.\n",
      "Ep done - 26840.\n",
      "Ep done - 26850.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 4902     |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Ep done - 26860.\n",
      "Ep done - 26870.\n",
      "Ep done - 26880.\n",
      "Ep done - 26890.\n",
      "Ep done - 26900.\n",
      "Ep done - 26910.\n",
      "Ep done - 26920.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 397         |\n",
      "|    time_elapsed         | 4911        |\n",
      "|    total_timesteps      | 813056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032377914 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 3960        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Ep done - 26930.\n",
      "Ep done - 26940.\n",
      "Ep done - 26950.\n",
      "Ep done - 26960.\n",
      "Ep done - 26970.\n",
      "Ep done - 26980.\n",
      "Ep done - 26990.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 398         |\n",
      "|    time_elapsed         | 4920        |\n",
      "|    total_timesteps      | 815104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041726857 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.242      |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "Ep done - 27000.\n",
      "Ep done - 27010.\n",
      "Ep done - 27020.\n",
      "Ep done - 27030.\n",
      "Ep done - 27040.\n",
      "Ep done - 27050.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 399        |\n",
      "|    time_elapsed         | 4930       |\n",
      "|    total_timesteps      | 817152     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03988068 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.231     |\n",
      "|    explained_variance   | 0.192      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0137    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    value_loss           | 0.0866     |\n",
      "----------------------------------------\n",
      "Ep done - 27060.\n",
      "Ep done - 27070.\n",
      "Ep done - 27080.\n",
      "Ep done - 27090.\n",
      "Ep done - 27100.\n",
      "Ep done - 27110.\n",
      "Ep done - 27120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 400         |\n",
      "|    time_elapsed         | 4939        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040207613 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 27130.\n",
      "Ep done - 27140.\n",
      "Ep done - 27150.\n",
      "Ep done - 8110.\n",
      "Ep done - 8120.\n",
      "Ep done - 8130.\n",
      "Ep done - 8140.\n",
      "Ep done - 8150.\n",
      "Ep done - 8160.\n",
      "Ep done - 8170.\n",
      "Ep done - 8180.\n",
      "Ep done - 8190.\n",
      "Ep done - 8200.\n",
      "Eval num_timesteps=820000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.22 +/- 0.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040978573 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 4000        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.62\n",
      "SELFPLAY: new best model, bumping up generation to 71\n",
      "Ep done - 27160.\n",
      "Ep done - 27170.\n",
      "Ep done - 27180.\n",
      "Ep done - 27190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 401      |\n",
      "|    time_elapsed    | 4958     |\n",
      "|    total_timesteps | 821248   |\n",
      "---------------------------------\n",
      "Ep done - 27200.\n",
      "Ep done - 27210.\n",
      "Ep done - 27220.\n",
      "Ep done - 27230.\n",
      "Ep done - 27240.\n",
      "Ep done - 27250.\n",
      "Ep done - 27260.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.55       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 402        |\n",
      "|    time_elapsed         | 4967       |\n",
      "|    total_timesteps      | 823296     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03806655 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | 0.436      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0344     |\n",
      "|    n_updates            | 4010       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "Ep done - 27270.\n",
      "Ep done - 27280.\n",
      "Ep done - 27290.\n",
      "Ep done - 27300.\n",
      "Ep done - 27310.\n",
      "Ep done - 27320.\n",
      "Ep done - 27330.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 403        |\n",
      "|    time_elapsed         | 4976       |\n",
      "|    total_timesteps      | 825344     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04557497 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.247     |\n",
      "|    explained_variance   | 0.364      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0124     |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Ep done - 27340.\n",
      "Ep done - 27350.\n",
      "Ep done - 27360.\n",
      "Ep done - 27370.\n",
      "Ep done - 27380.\n",
      "Ep done - 27390.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 404        |\n",
      "|    time_elapsed         | 4986       |\n",
      "|    total_timesteps      | 827392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03469927 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.234     |\n",
      "|    explained_variance   | 0.289      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00498   |\n",
      "|    n_updates            | 4030       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "Ep done - 27400.\n",
      "Ep done - 27410.\n",
      "Ep done - 27420.\n",
      "Ep done - 27430.\n",
      "Ep done - 27440.\n",
      "Ep done - 27450.\n",
      "Ep done - 27460.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.63       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 405        |\n",
      "|    time_elapsed         | 4995       |\n",
      "|    total_timesteps      | 829440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04118479 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.245     |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0507     |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "Ep done - 27470.\n",
      "Ep done - 27480.\n",
      "Ep done - 8210.\n",
      "Ep done - 8220.\n",
      "Ep done - 8230.\n",
      "Ep done - 8240.\n",
      "Ep done - 8250.\n",
      "Ep done - 8260.\n",
      "Ep done - 8270.\n",
      "Ep done - 8280.\n",
      "Ep done - 8290.\n",
      "Ep done - 8300.\n",
      "Eval num_timesteps=830000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.20 +/- 0.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 830000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039532688 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00189    |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 72\n",
      "Ep done - 27490.\n",
      "Ep done - 27500.\n",
      "Ep done - 27510.\n",
      "Ep done - 27520.\n",
      "Ep done - 27530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 5014     |\n",
      "|    total_timesteps | 831488   |\n",
      "---------------------------------\n",
      "Ep done - 27540.\n",
      "Ep done - 27550.\n",
      "Ep done - 27560.\n",
      "Ep done - 27570.\n",
      "Ep done - 27580.\n",
      "Ep done - 27590.\n",
      "Ep done - 27600.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 407        |\n",
      "|    time_elapsed         | 5023       |\n",
      "|    total_timesteps      | 833536     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03985147 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.236     |\n",
      "|    explained_variance   | 0.258      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0547     |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0237    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Ep done - 27610.\n",
      "Ep done - 27620.\n",
      "Ep done - 27630.\n",
      "Ep done - 27640.\n",
      "Ep done - 27650.\n",
      "Ep done - 27660.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 408        |\n",
      "|    time_elapsed         | 5032       |\n",
      "|    total_timesteps      | 835584     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04159336 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.256     |\n",
      "|    explained_variance   | 0.2        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.014      |\n",
      "|    n_updates            | 4070       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 27670.\n",
      "Ep done - 27680.\n",
      "Ep done - 27690.\n",
      "Ep done - 27700.\n",
      "Ep done - 27710.\n",
      "Ep done - 27720.\n",
      "Ep done - 27730.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 409        |\n",
      "|    time_elapsed         | 5041       |\n",
      "|    total_timesteps      | 837632     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04047399 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | 0.316      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    value_loss           | 0.0849     |\n",
      "----------------------------------------\n",
      "Ep done - 27740.\n",
      "Ep done - 27750.\n",
      "Ep done - 27760.\n",
      "Ep done - 27770.\n",
      "Ep done - 27780.\n",
      "Ep done - 27790.\n",
      "Ep done - 27800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 410         |\n",
      "|    time_elapsed         | 5051        |\n",
      "|    total_timesteps      | 839680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043069396 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.257      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00683    |\n",
      "|    n_updates            | 4090        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.085       |\n",
      "-----------------------------------------\n",
      "Ep done - 27810.\n",
      "Ep done - 8310.\n",
      "Ep done - 8320.\n",
      "Ep done - 8330.\n",
      "Ep done - 8340.\n",
      "Ep done - 8350.\n",
      "Ep done - 8360.\n",
      "Ep done - 8370.\n",
      "Ep done - 8380.\n",
      "Ep done - 8390.\n",
      "Ep done - 8400.\n",
      "Eval num_timesteps=840000, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 30.21 +/- 0.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045856565 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.258      |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 4100        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.72\n",
      "SELFPLAY: new best model, bumping up generation to 73\n",
      "Ep done - 27820.\n",
      "Ep done - 27830.\n",
      "Ep done - 27840.\n",
      "Ep done - 27850.\n",
      "Ep done - 27860.\n",
      "Ep done - 27870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 411      |\n",
      "|    time_elapsed    | 5070     |\n",
      "|    total_timesteps | 841728   |\n",
      "---------------------------------\n",
      "Ep done - 27880.\n",
      "Ep done - 27890.\n",
      "Ep done - 27900.\n",
      "Ep done - 27910.\n",
      "Ep done - 27920.\n",
      "Ep done - 27930.\n",
      "Ep done - 27940.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 412         |\n",
      "|    time_elapsed         | 5079        |\n",
      "|    total_timesteps      | 843776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044109784 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.242      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0066      |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 27950.\n",
      "Ep done - 27960.\n",
      "Ep done - 27970.\n",
      "Ep done - 27980.\n",
      "Ep done - 27990.\n",
      "Ep done - 28000.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.55      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 166       |\n",
      "|    iterations           | 413       |\n",
      "|    time_elapsed         | 5088      |\n",
      "|    total_timesteps      | 845824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0439839 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.244    |\n",
      "|    explained_variance   | 0.421     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0195    |\n",
      "|    n_updates            | 4120      |\n",
      "|    policy_gradient_loss | -0.0229   |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "Ep done - 28010.\n",
      "Ep done - 28020.\n",
      "Ep done - 28030.\n",
      "Ep done - 28040.\n",
      "Ep done - 28050.\n",
      "Ep done - 28060.\n",
      "Ep done - 28070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 414         |\n",
      "|    time_elapsed         | 5098        |\n",
      "|    total_timesteps      | 847872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045566168 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0578      |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 28080.\n",
      "Ep done - 28090.\n",
      "Ep done - 28100.\n",
      "Ep done - 28110.\n",
      "Ep done - 28120.\n",
      "Ep done - 28130.\n",
      "Ep done - 28140.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.45       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 415        |\n",
      "|    time_elapsed         | 5107       |\n",
      "|    total_timesteps      | 849920     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03962121 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.229     |\n",
      "|    explained_variance   | 0.477      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00671   |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "Ep done - 8410.\n",
      "Ep done - 8420.\n",
      "Ep done - 8430.\n",
      "Ep done - 8440.\n",
      "Ep done - 8450.\n",
      "Ep done - 8460.\n",
      "Ep done - 8470.\n",
      "Ep done - 8480.\n",
      "Ep done - 8490.\n",
      "Ep done - 8500.\n",
      "Eval num_timesteps=850000, episode_reward=0.71 +/- 0.70\n",
      "Episode length: 30.20 +/- 0.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 850000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03723972 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.228     |\n",
      "|    explained_variance   | 0.445      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0434     |\n",
      "|    n_updates            | 4150       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.71\n",
      "SELFPLAY: new best model, bumping up generation to 74\n",
      "Ep done - 28150.\n",
      "Ep done - 28160.\n",
      "Ep done - 28170.\n",
      "Ep done - 28180.\n",
      "Ep done - 28190.\n",
      "Ep done - 28200.\n",
      "Ep done - 28210.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 5126     |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Ep done - 28220.\n",
      "Ep done - 28230.\n",
      "Ep done - 28240.\n",
      "Ep done - 28250.\n",
      "Ep done - 28260.\n",
      "Ep done - 28270.\n",
      "Ep done - 28280.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 417         |\n",
      "|    time_elapsed         | 5135        |\n",
      "|    total_timesteps      | 854016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049795844 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.031       |\n",
      "|    n_updates            | 4160        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 28290.\n",
      "Ep done - 28300.\n",
      "Ep done - 28310.\n",
      "Ep done - 28320.\n",
      "Ep done - 28330.\n",
      "Ep done - 28340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 418         |\n",
      "|    time_elapsed         | 5144        |\n",
      "|    total_timesteps      | 856064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054085612 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 28350.\n",
      "Ep done - 28360.\n",
      "Ep done - 28370.\n",
      "Ep done - 28380.\n",
      "Ep done - 28390.\n",
      "Ep done - 28400.\n",
      "Ep done - 28410.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.4        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 419        |\n",
      "|    time_elapsed         | 5153       |\n",
      "|    total_timesteps      | 858112     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03861413 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.209     |\n",
      "|    explained_variance   | 0.247      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0189     |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Ep done - 28420.\n",
      "Ep done - 28430.\n",
      "Ep done - 28440.\n",
      "Ep done - 28450.\n",
      "Ep done - 28460.\n",
      "Ep done - 28470.\n",
      "Ep done - 8510.\n",
      "Ep done - 8520.\n",
      "Ep done - 8530.\n",
      "Ep done - 8540.\n",
      "Ep done - 8550.\n",
      "Ep done - 8560.\n",
      "Ep done - 8570.\n",
      "Ep done - 8580.\n",
      "Ep done - 8590.\n",
      "Ep done - 8600.\n",
      "Eval num_timesteps=860000, episode_reward=0.61 +/- 0.79\n",
      "Episode length: 30.25 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047041457 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0581      |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.61\n",
      "SELFPLAY: new best model, bumping up generation to 75\n",
      "Ep done - 28480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 5172     |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "Ep done - 28490.\n",
      "Ep done - 28500.\n",
      "Ep done - 28510.\n",
      "Ep done - 28520.\n",
      "Ep done - 28530.\n",
      "Ep done - 28540.\n",
      "Ep done - 28550.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 421         |\n",
      "|    time_elapsed         | 5182        |\n",
      "|    total_timesteps      | 862208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038588893 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.205      |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0665      |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "Ep done - 28560.\n",
      "Ep done - 28570.\n",
      "Ep done - 28580.\n",
      "Ep done - 28590.\n",
      "Ep done - 28600.\n",
      "Ep done - 28610.\n",
      "Ep done - 28620.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 422         |\n",
      "|    time_elapsed         | 5191        |\n",
      "|    total_timesteps      | 864256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039061025 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0376      |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 28630.\n",
      "Ep done - 28640.\n",
      "Ep done - 28650.\n",
      "Ep done - 28660.\n",
      "Ep done - 28670.\n",
      "Ep done - 28680.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 423         |\n",
      "|    time_elapsed         | 5200        |\n",
      "|    total_timesteps      | 866304      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044100247 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0148      |\n",
      "|    n_updates            | 4220        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "Ep done - 28690.\n",
      "Ep done - 28700.\n",
      "Ep done - 28710.\n",
      "Ep done - 28720.\n",
      "Ep done - 28730.\n",
      "Ep done - 28740.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 28750.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 424        |\n",
      "|    time_elapsed         | 5209       |\n",
      "|    total_timesteps      | 868352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04622655 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.22      |\n",
      "|    explained_variance   | 0.209      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00651   |\n",
      "|    n_updates            | 4230       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Ep done - 28760.\n",
      "Ep done - 28770.\n",
      "Ep done - 28780.\n",
      "Ep done - 28790.\n",
      "Ep done - 28800.\n",
      "Ep done - 28810.\n",
      "Ep done - 8610.\n",
      "Ep done - 8620.\n",
      "Ep done - 8630.\n",
      "Ep done - 8640.\n",
      "Ep done - 8650.\n",
      "Ep done - 8660.\n",
      "Ep done - 8670.\n",
      "Ep done - 8680.\n",
      "Ep done - 8690.\n",
      "Ep done - 8700.\n",
      "Eval num_timesteps=870000, episode_reward=0.63 +/- 0.77\n",
      "Episode length: 30.24 +/- 0.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.63       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 870000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04074339 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.221     |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0565     |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.63\n",
      "SELFPLAY: new best model, bumping up generation to 76\n",
      "Ep done - 28820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 425      |\n",
      "|    time_elapsed    | 5228     |\n",
      "|    total_timesteps | 870400   |\n",
      "---------------------------------\n",
      "Ep done - 28830.\n",
      "Ep done - 28840.\n",
      "Ep done - 28850.\n",
      "Ep done - 28860.\n",
      "Ep done - 28870.\n",
      "Ep done - 28880.\n",
      "Ep done - 28890.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 426         |\n",
      "|    time_elapsed         | 5237        |\n",
      "|    total_timesteps      | 872448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039481863 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.209      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 4250        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.0939      |\n",
      "-----------------------------------------\n",
      "Ep done - 28900.\n",
      "Ep done - 28910.\n",
      "Ep done - 28920.\n",
      "Ep done - 28930.\n",
      "Ep done - 28940.\n",
      "Ep done - 28950.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 427        |\n",
      "|    time_elapsed         | 5247       |\n",
      "|    total_timesteps      | 874496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04343249 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.207     |\n",
      "|    explained_variance   | 0.443      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0617     |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "Ep done - 28960.\n",
      "Ep done - 28970.\n",
      "Ep done - 28980.\n",
      "Ep done - 28990.\n",
      "Ep done - 29000.\n",
      "Ep done - 29010.\n",
      "Ep done - 29020.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 428        |\n",
      "|    time_elapsed         | 5256       |\n",
      "|    total_timesteps      | 876544     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04471367 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.212     |\n",
      "|    explained_variance   | 0.509      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0029     |\n",
      "|    n_updates            | 4270       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    value_loss           | 0.0947     |\n",
      "----------------------------------------\n",
      "Ep done - 29030.\n",
      "Ep done - 29040.\n",
      "Ep done - 29050.\n",
      "Ep done - 29060.\n",
      "Ep done - 29070.\n",
      "Ep done - 29080.\n",
      "Ep done - 29090.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 429        |\n",
      "|    time_elapsed         | 5265       |\n",
      "|    total_timesteps      | 878592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04288338 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.206     |\n",
      "|    explained_variance   | 0.47       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0575     |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "Ep done - 29100.\n",
      "Ep done - 29110.\n",
      "Ep done - 29120.\n",
      "Ep done - 29130.\n",
      "Ep done - 29140.\n",
      "Ep done - 8710.\n",
      "Ep done - 8720.\n",
      "Ep done - 8730.\n",
      "Ep done - 8740.\n",
      "Ep done - 8750.\n",
      "Ep done - 8760.\n",
      "Ep done - 8770.\n",
      "Ep done - 8780.\n",
      "Ep done - 8790.\n",
      "Ep done - 8800.\n",
      "Eval num_timesteps=880000, episode_reward=0.48 +/- 0.88\n",
      "Episode length: 30.08 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.48       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 880000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05353629 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.184     |\n",
      "|    explained_variance   | 0.459      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0385     |\n",
      "|    n_updates            | 4290       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.48\n",
      "SELFPLAY: new best model, bumping up generation to 77\n",
      "Ep done - 29150.\n",
      "Ep done - 29160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 0.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 5284     |\n",
      "|    total_timesteps | 880640   |\n",
      "---------------------------------\n",
      "Ep done - 29170.\n",
      "Ep done - 29180.\n",
      "Ep done - 29190.\n",
      "Ep done - 29200.\n",
      "Ep done - 29210.\n",
      "Ep done - 29220.\n",
      "Ep done - 29230.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.1        |\n",
      "|    ep_rew_mean          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 431         |\n",
      "|    time_elapsed         | 5293        |\n",
      "|    total_timesteps      | 882688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055607487 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 4300        |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 29240.\n",
      "Ep done - 29250.\n",
      "Ep done - 29260.\n",
      "Ep done - 29270.\n",
      "Ep done - 29280.\n",
      "Ep done - 29290.\n",
      "Ep done - 29300.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.6        |\n",
      "|    ep_rew_mean          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 432         |\n",
      "|    time_elapsed         | 5302        |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037188195 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.228      |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 4310        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "Ep done - 29310.\n",
      "Ep done - 29320.\n",
      "Ep done - 29330.\n",
      "Ep done - 29340.\n",
      "Ep done - 29350.\n",
      "Ep done - 29360.\n",
      "Ep done - 29370.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 28.8       |\n",
      "|    ep_rew_mean          | 0.35       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 433        |\n",
      "|    time_elapsed         | 5312       |\n",
      "|    total_timesteps      | 886784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04685136 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.231     |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0387     |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "Ep done - 29380.\n",
      "Ep done - 29390.\n",
      "Ep done - 29400.\n",
      "Ep done - 29410.\n",
      "Ep done - 29420.\n",
      "Ep done - 29430.\n",
      "Ep done - 29440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28.7        |\n",
      "|    ep_rew_mean          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 434         |\n",
      "|    time_elapsed         | 5321        |\n",
      "|    total_timesteps      | 888832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037771262 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 4330        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Ep done - 29450.\n",
      "Ep done - 29460.\n",
      "Ep done - 29470.\n",
      "Ep done - 29480.\n",
      "Ep done - 8810.\n",
      "Ep done - 8820.\n",
      "Ep done - 8830.\n",
      "Ep done - 8840.\n",
      "Ep done - 8850.\n",
      "Ep done - 8860.\n",
      "Ep done - 8870.\n",
      "Ep done - 8880.\n",
      "Ep done - 8890.\n",
      "Ep done - 8900.\n",
      "Eval num_timesteps=890000, episode_reward=0.49 +/- 0.87\n",
      "Episode length: 29.42 +/- 3.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.4       |\n",
      "|    mean_reward          | 0.49       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 890000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05774898 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.202     |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.49\n",
      "SELFPLAY: new best model, bumping up generation to 78\n",
      "Ep done - 29490.\n",
      "Ep done - 29500.\n",
      "Ep done - 29510.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.6     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 435      |\n",
      "|    time_elapsed    | 5340     |\n",
      "|    total_timesteps | 890880   |\n",
      "---------------------------------\n",
      "Ep done - 29520.\n",
      "Ep done - 29530.\n",
      "Ep done - 29540.\n",
      "Ep done - 29550.\n",
      "Ep done - 29560.\n",
      "Ep done - 29570.\n",
      "Ep done - 29580.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.49       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 436        |\n",
      "|    time_elapsed         | 5349       |\n",
      "|    total_timesteps      | 892928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05291777 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.222     |\n",
      "|    explained_variance   | 0.343      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00793   |\n",
      "|    n_updates            | 4350       |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "Ep done - 29590.\n",
      "Ep done - 29600.\n",
      "Ep done - 29610.\n",
      "Ep done - 29620.\n",
      "Ep done - 29630.\n",
      "Ep done - 29640.\n",
      "Ep done - 29650.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 437         |\n",
      "|    time_elapsed         | 5358        |\n",
      "|    total_timesteps      | 894976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033090267 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0254      |\n",
      "|    n_updates            | 4360        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Ep done - 29660.\n",
      "Ep done - 29670.\n",
      "Ep done - 29680.\n",
      "Ep done - 29690.\n",
      "Ep done - 29700.\n",
      "Ep done - 29710.\n",
      "Ep done - 29720.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.5       |\n",
      "|    ep_rew_mean          | 0.4        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 438        |\n",
      "|    time_elapsed         | 5367       |\n",
      "|    total_timesteps      | 897024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03390434 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.203     |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.025      |\n",
      "|    n_updates            | 4370       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Ep done - 29730.\n",
      "Ep done - 29740.\n",
      "Ep done - 29750.\n",
      "Ep done - 29760.\n",
      "Ep done - 29770.\n",
      "Ep done - 29780.\n",
      "Ep done - 29790.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.4        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 439         |\n",
      "|    time_elapsed         | 5377        |\n",
      "|    total_timesteps      | 899072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045801118 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.209      |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0464      |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 29800.\n",
      "Ep done - 29810.\n",
      "Ep done - 29820.\n",
      "Ep done - 8910.\n",
      "Ep done - 8920.\n",
      "Ep done - 8930.\n",
      "Ep done - 8940.\n",
      "Ep done - 8950.\n",
      "Ep done - 8960.\n",
      "Ep done - 8970.\n",
      "Ep done - 8980.\n",
      "Ep done - 8990.\n",
      "Ep done - 9000.\n",
      "Eval num_timesteps=900000, episode_reward=0.57 +/- 0.82\n",
      "Episode length: 28.61 +/- 5.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 28.6        |\n",
      "|    mean_reward          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044796735 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.57\n",
      "SELFPLAY: new best model, bumping up generation to 79\n",
      "Ep done - 29830.\n",
      "Ep done - 29840.\n",
      "Ep done - 29850.\n",
      "Ep done - 29860.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.1     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 5395     |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Ep done - 29870.\n",
      "Ep done - 29880.\n",
      "Ep done - 29890.\n",
      "Ep done - 29900.\n",
      "Ep done - 29910.\n",
      "Ep done - 29920.\n",
      "Ep done - 29930.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 441        |\n",
      "|    time_elapsed         | 5404       |\n",
      "|    total_timesteps      | 903168     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05211205 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.212     |\n",
      "|    explained_variance   | 0.467      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00763   |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    value_loss           | 0.116      |\n",
      "----------------------------------------\n",
      "Ep done - 29940.\n",
      "Ep done - 29950.\n",
      "Ep done - 29960.\n",
      "Ep done - 29970.\n",
      "Ep done - 29980.\n",
      "Ep done - 29990.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 442         |\n",
      "|    time_elapsed         | 5413        |\n",
      "|    total_timesteps      | 905216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047195964 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.244      |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0145      |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "Ep done - 30000.\n",
      "Ep done - 30010.\n",
      "Ep done - 30020.\n",
      "Ep done - 30030.\n",
      "Ep done - 30040.\n",
      "Ep done - 30050.\n",
      "Ep done - 30060.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 443         |\n",
      "|    time_elapsed         | 5423        |\n",
      "|    total_timesteps      | 907264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058931828 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 4420        |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Ep done - 30070.\n",
      "Ep done - 30080.\n",
      "Ep done - 30090.\n",
      "Ep done - 30100.\n",
      "Ep done - 30110.\n",
      "Ep done - 30120.\n",
      "Ep done - 30130.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.29       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 444        |\n",
      "|    time_elapsed         | 5432       |\n",
      "|    total_timesteps      | 909312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04344323 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.246     |\n",
      "|    explained_variance   | 0.519      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00732   |\n",
      "|    n_updates            | 4430       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "Ep done - 30140.\n",
      "Ep done - 30150.\n",
      "Ep done - 9010.\n",
      "Ep done - 9020.\n",
      "Ep done - 9030.\n",
      "Ep done - 9040.\n",
      "Ep done - 9050.\n",
      "Ep done - 9060.\n",
      "Ep done - 9070.\n",
      "Ep done - 9080.\n",
      "Ep done - 9090.\n",
      "Ep done - 9100.\n",
      "Eval num_timesteps=910000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.18 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.53       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 910000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04347971 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.217     |\n",
      "|    explained_variance   | 0.294      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0685     |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.53\n",
      "SELFPLAY: new best model, bumping up generation to 80\n",
      "Ep done - 30160.\n",
      "Ep done - 30170.\n",
      "Ep done - 30180.\n",
      "Ep done - 30190.\n",
      "Ep done - 30200.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 445      |\n",
      "|    time_elapsed    | 5451     |\n",
      "|    total_timesteps | 911360   |\n",
      "---------------------------------\n",
      "Ep done - 30210.\n",
      "Ep done - 30220.\n",
      "Ep done - 30230.\n",
      "Ep done - 30240.\n",
      "Ep done - 30250.\n",
      "Ep done - 30260.\n",
      "Ep done - 30270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 446         |\n",
      "|    time_elapsed         | 5460        |\n",
      "|    total_timesteps      | 913408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037328564 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.238      |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00244    |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 30280.\n",
      "Ep done - 30290.\n",
      "Ep done - 30300.\n",
      "Ep done - 30310.\n",
      "Ep done - 30320.\n",
      "Ep done - 30330.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.13       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 447        |\n",
      "|    time_elapsed         | 5470       |\n",
      "|    total_timesteps      | 915456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04533232 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.246     |\n",
      "|    explained_variance   | 0.374      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0286     |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 30340.\n",
      "Ep done - 30350.\n",
      "Ep done - 30360.\n",
      "Ep done - 30370.\n",
      "Ep done - 30380.\n",
      "Ep done - 30390.\n",
      "Ep done - 30400.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 448        |\n",
      "|    time_elapsed         | 5479       |\n",
      "|    total_timesteps      | 917504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03981182 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.255     |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0227     |\n",
      "|    n_updates            | 4470       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    value_loss           | 0.216      |\n",
      "----------------------------------------\n",
      "Ep done - 30410.\n",
      "Ep done - 30420.\n",
      "Ep done - 30430.\n",
      "Ep done - 30440.\n",
      "Ep done - 30450.\n",
      "Ep done - 30460.\n",
      "Ep done - 30470.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 449         |\n",
      "|    time_elapsed         | 5488        |\n",
      "|    total_timesteps      | 919552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043232292 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0426      |\n",
      "|    n_updates            | 4480        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "Ep done - 30480.\n",
      "Ep done - 30490.\n",
      "Ep done - 9110.\n",
      "Ep done - 9120.\n",
      "Ep done - 9130.\n",
      "Ep done - 9140.\n",
      "Ep done - 9150.\n",
      "Ep done - 9160.\n",
      "Ep done - 9170.\n",
      "Ep done - 9180.\n",
      "Ep done - 9190.\n",
      "Ep done - 9200.\n",
      "Eval num_timesteps=920000, episode_reward=0.26 +/- 0.96\n",
      "Episode length: 30.18 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044422932 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.254      |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0342      |\n",
      "|    n_updates            | 4490        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.26\n",
      "SELFPLAY: new best model, bumping up generation to 81\n",
      "Ep done - 30500.\n",
      "Ep done - 30510.\n",
      "Ep done - 30520.\n",
      "Ep done - 30530.\n",
      "Ep done - 30540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 5507     |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "Ep done - 30550.\n",
      "Ep done - 30560.\n",
      "Ep done - 30570.\n",
      "Ep done - 30580.\n",
      "Ep done - 30590.\n",
      "Ep done - 30600.\n",
      "Ep done - 30610.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.36       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 451        |\n",
      "|    time_elapsed         | 5516       |\n",
      "|    total_timesteps      | 923648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05373078 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.229     |\n",
      "|    explained_variance   | 0.353      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0258     |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "Ep done - 30620.\n",
      "Ep done - 30630.\n",
      "Ep done - 30640.\n",
      "Ep done - 30650.\n",
      "Ep done - 30660.\n",
      "Ep done - 30670.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | 0.43       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 452        |\n",
      "|    time_elapsed         | 5525       |\n",
      "|    total_timesteps      | 925696     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05445145 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.233     |\n",
      "|    explained_variance   | 0.405      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0285     |\n",
      "|    n_updates            | 4510       |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n",
      "Ep done - 30680.\n",
      "Ep done - 30690.\n",
      "Ep done - 30700.\n",
      "Ep done - 30710.\n",
      "Ep done - 30720.\n",
      "Ep done - 30730.\n",
      "Ep done - 30740.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.8       |\n",
      "|    ep_rew_mean          | 0.53       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 453        |\n",
      "|    time_elapsed         | 5535       |\n",
      "|    total_timesteps      | 927744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04490687 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.241     |\n",
      "|    explained_variance   | 0.455      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "Ep done - 30750.\n",
      "Ep done - 30760.\n",
      "Ep done - 30770.\n",
      "Ep done - 30780.\n",
      "Ep done - 30790.\n",
      "Ep done - 30800.\n",
      "Ep done - 30810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 454         |\n",
      "|    time_elapsed         | 5544        |\n",
      "|    total_timesteps      | 929792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050227266 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0547      |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "Ep done - 30820.\n",
      "Ep done - 9210.\n",
      "Ep done - 9220.\n",
      "Ep done - 9230.\n",
      "Ep done - 9240.\n",
      "Ep done - 9250.\n",
      "Ep done - 9260.\n",
      "Ep done - 9270.\n",
      "Ep done - 9280.\n",
      "Ep done - 9290.\n",
      "Ep done - 9300.\n",
      "Eval num_timesteps=930000, episode_reward=0.39 +/- 0.90\n",
      "Episode length: 29.95 +/- 2.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.9        |\n",
      "|    mean_reward          | 0.39        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 930000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044989277 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0235      |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.205       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.39\n",
      "SELFPLAY: new best model, bumping up generation to 82\n",
      "Ep done - 30830.\n",
      "Ep done - 30840.\n",
      "Ep done - 30850.\n",
      "Ep done - 30860.\n",
      "Ep done - 30870.\n",
      "Ep done - 30880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 0.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 455      |\n",
      "|    time_elapsed    | 5562     |\n",
      "|    total_timesteps | 931840   |\n",
      "---------------------------------\n",
      "Ep done - 30890.\n",
      "Ep done - 30900.\n",
      "Ep done - 30910.\n",
      "Ep done - 30920.\n",
      "Ep done - 30930.\n",
      "Ep done - 30940.\n",
      "Ep done - 30950.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.6       |\n",
      "|    ep_rew_mean          | 0.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 456        |\n",
      "|    time_elapsed         | 5572       |\n",
      "|    total_timesteps      | 933888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04567308 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.249     |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 4550       |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "Ep done - 30960.\n",
      "Ep done - 30970.\n",
      "Ep done - 30980.\n",
      "Ep done - 30990.\n",
      "Ep done - 31000.\n",
      "Ep done - 31010.\n",
      "Ep done - 31020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.8        |\n",
      "|    ep_rew_mean          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 457         |\n",
      "|    time_elapsed         | 5581        |\n",
      "|    total_timesteps      | 935936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044257864 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0259      |\n",
      "|    n_updates            | 4560        |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "Ep done - 31030.\n",
      "Ep done - 31040.\n",
      "Ep done - 31050.\n",
      "Ep done - 31060.\n",
      "Ep done - 31070.\n",
      "Ep done - 31080.\n",
      "Ep done - 31090.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 458         |\n",
      "|    time_elapsed         | 5590        |\n",
      "|    total_timesteps      | 937984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038006626 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0123      |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Ep done - 31100.\n",
      "Ep done - 31110.\n",
      "Ep done - 31120.\n",
      "Ep done - 31130.\n",
      "Ep done - 31140.\n",
      "Ep done - 31150.\n",
      "Ep done - 9310.\n",
      "Ep done - 9320.\n",
      "Ep done - 9330.\n",
      "Ep done - 9340.\n",
      "Ep done - 9350.\n",
      "Ep done - 9360.\n",
      "Ep done - 9370.\n",
      "Ep done - 9380.\n",
      "Ep done - 9390.\n",
      "Ep done - 9400.\n",
      "Eval num_timesteps=940000, episode_reward=0.43 +/- 0.90\n",
      "Episode length: 30.16 +/- 0.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.43        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 940000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039132167 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0205      |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.43\n",
      "SELFPLAY: new best model, bumping up generation to 83\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 459      |\n",
      "|    time_elapsed    | 5609     |\n",
      "|    total_timesteps | 940032   |\n",
      "---------------------------------\n",
      "Ep done - 31160.\n",
      "Ep done - 31170.\n",
      "Ep done - 31180.\n",
      "Ep done - 31190.\n",
      "Ep done - 31200.\n",
      "Ep done - 31210.\n",
      "Ep done - 31220.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 460         |\n",
      "|    time_elapsed         | 5618        |\n",
      "|    total_timesteps      | 942080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042460565 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0341      |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Ep done - 31230.\n",
      "Ep done - 31240.\n",
      "Ep done - 31250.\n",
      "Ep done - 31260.\n",
      "Ep done - 31270.\n",
      "Ep done - 31280.\n",
      "Ep done - 31290.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 461         |\n",
      "|    time_elapsed         | 5627        |\n",
      "|    total_timesteps      | 944128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039197117 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 4600        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Ep done - 31300.\n",
      "Ep done - 31310.\n",
      "Ep done - 31320.\n",
      "Ep done - 31330.\n",
      "Ep done - 31340.\n",
      "Ep done - 31350.\n",
      "Ep done - 31360.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 462         |\n",
      "|    time_elapsed         | 5636        |\n",
      "|    total_timesteps      | 946176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052901983 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0343      |\n",
      "|    n_updates            | 4610        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "Ep done - 31370.\n",
      "Ep done - 31380.\n",
      "Ep done - 31390.\n",
      "Ep done - 31400.\n",
      "Ep done - 31410.\n",
      "Ep done - 31420.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 31430.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 463        |\n",
      "|    time_elapsed         | 5646       |\n",
      "|    total_timesteps      | 948224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03817923 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.256     |\n",
      "|    explained_variance   | 0.248      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0538     |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.207      |\n",
      "----------------------------------------\n",
      "Ep done - 31440.\n",
      "Ep done - 31450.\n",
      "Ep done - 31460.\n",
      "Ep done - 31470.\n",
      "Ep done - 31480.\n",
      "Ep done - 9410.\n",
      "Ep done - 9420.\n",
      "Ep done - 9430.\n",
      "Ep done - 9440.\n",
      "Ep done - 9450.\n",
      "Ep done - 9460.\n",
      "Ep done - 9470.\n",
      "Ep done - 9480.\n",
      "Ep done - 9490.\n",
      "Ep done - 9500.\n",
      "Eval num_timesteps=950000, episode_reward=0.44 +/- 0.89\n",
      "Episode length: 30.15 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 950000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04305911 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.25      |\n",
      "|    explained_variance   | 0.326      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0135    |\n",
      "|    n_updates            | 4630       |\n",
      "|    policy_gradient_loss | -0.0241    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.44\n",
      "SELFPLAY: new best model, bumping up generation to 84\n",
      "Ep done - 31490.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 5664     |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Ep done - 31500.\n",
      "Ep done - 31510.\n",
      "Ep done - 31520.\n",
      "Ep done - 31530.\n",
      "Ep done - 31540.\n",
      "Ep done - 31550.\n",
      "Ep done - 31560.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 465        |\n",
      "|    time_elapsed         | 5674       |\n",
      "|    total_timesteps      | 952320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03075062 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.247     |\n",
      "|    explained_variance   | 0.222      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0619     |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "Ep done - 31570.\n",
      "Ep done - 31580.\n",
      "Ep done - 31590.\n",
      "Ep done - 31600.\n",
      "Ep done - 31610.\n",
      "Ep done - 31620.\n",
      "Ep done - 31630.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 466         |\n",
      "|    time_elapsed         | 5683        |\n",
      "|    total_timesteps      | 954368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051397655 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.264      |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0173      |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "Ep done - 31640.\n",
      "Ep done - 31650.\n",
      "Ep done - 31660.\n",
      "Ep done - 31670.\n",
      "Ep done - 31680.\n",
      "Ep done - 31690.\n",
      "Ep done - 31700.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 467         |\n",
      "|    time_elapsed         | 5692        |\n",
      "|    total_timesteps      | 956416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051111907 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.252      |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 4660        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Ep done - 31710.\n",
      "Ep done - 31720.\n",
      "Ep done - 31730.\n",
      "Ep done - 31740.\n",
      "Ep done - 31750.\n",
      "Ep done - 31760.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.47       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 468        |\n",
      "|    time_elapsed         | 5701       |\n",
      "|    total_timesteps      | 958464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05088955 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.254     |\n",
      "|    explained_variance   | 0.393      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.000478   |\n",
      "|    n_updates            | 4670       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "Ep done - 31770.\n",
      "Ep done - 31780.\n",
      "Ep done - 31790.\n",
      "Ep done - 31800.\n",
      "Ep done - 31810.\n",
      "Ep done - 31820.\n",
      "Ep done - 9510.\n",
      "Ep done - 9520.\n",
      "Ep done - 9530.\n",
      "Ep done - 9540.\n",
      "Ep done - 9550.\n",
      "Ep done - 9560.\n",
      "Ep done - 9570.\n",
      "Ep done - 9580.\n",
      "Ep done - 9590.\n",
      "Ep done - 9600.\n",
      "Eval num_timesteps=960000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.23 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034435693 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0964      |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.53\n",
      "SELFPLAY: new best model, bumping up generation to 85\n",
      "Ep done - 31830.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 469      |\n",
      "|    time_elapsed    | 5720     |\n",
      "|    total_timesteps | 960512   |\n",
      "---------------------------------\n",
      "Ep done - 31840.\n",
      "Ep done - 31850.\n",
      "Ep done - 31860.\n",
      "Ep done - 31870.\n",
      "Ep done - 31880.\n",
      "Ep done - 31890.\n",
      "Ep done - 31900.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 470        |\n",
      "|    time_elapsed         | 5729       |\n",
      "|    total_timesteps      | 962560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03837693 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.251     |\n",
      "|    explained_variance   | 0.342      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0531     |\n",
      "|    n_updates            | 4690       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 31910.\n",
      "Ep done - 31920.\n",
      "Ep done - 31930.\n",
      "Ep done - 31940.\n",
      "Ep done - 31950.\n",
      "Ep done - 31960.\n",
      "Ep done - 31970.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.55       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 471        |\n",
      "|    time_elapsed         | 5739       |\n",
      "|    total_timesteps      | 964608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04042045 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.26      |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0404     |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    value_loss           | 0.136      |\n",
      "----------------------------------------\n",
      "Ep done - 31980.\n",
      "Ep done - 31990.\n",
      "Ep done - 32000.\n",
      "Ep done - 32010.\n",
      "Ep done - 32020.\n",
      "Ep done - 32030.\n",
      "Ep done - 32040.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 472        |\n",
      "|    time_elapsed         | 5748       |\n",
      "|    total_timesteps      | 966656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03465868 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.312      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.038      |\n",
      "|    n_updates            | 4710       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Ep done - 32050.\n",
      "Ep done - 32060.\n",
      "Ep done - 32070.\n",
      "Ep done - 32080.\n",
      "Ep done - 32090.\n",
      "Ep done - 32100.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.49       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 473        |\n",
      "|    time_elapsed         | 5757       |\n",
      "|    total_timesteps      | 968704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04198975 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.374      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.013      |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Ep done - 32110.\n",
      "Ep done - 32120.\n",
      "Ep done - 32130.\n",
      "Ep done - 32140.\n",
      "Ep done - 32150.\n",
      "Ep done - 9610.\n",
      "Ep done - 9620.\n",
      "Ep done - 9630.\n",
      "Ep done - 9640.\n",
      "Ep done - 9650.\n",
      "Ep done - 9660.\n",
      "Ep done - 9670.\n",
      "Ep done - 9680.\n",
      "Ep done - 9690.\n",
      "Ep done - 9700.\n",
      "Eval num_timesteps=970000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.24 +/- 0.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 970000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03893646 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.276     |\n",
      "|    explained_variance   | 0.378      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0102     |\n",
      "|    n_updates            | 4730       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 86\n",
      "Ep done - 32160.\n",
      "Ep done - 32170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 5776     |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "Ep done - 32180.\n",
      "Ep done - 32190.\n",
      "Ep done - 32200.\n",
      "Ep done - 32210.\n",
      "Ep done - 32220.\n",
      "Ep done - 32230.\n",
      "Ep done - 32240.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 475        |\n",
      "|    time_elapsed         | 5785       |\n",
      "|    total_timesteps      | 972800     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05214984 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.26      |\n",
      "|    explained_variance   | 0.322      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.134      |\n",
      "----------------------------------------\n",
      "Ep done - 32250.\n",
      "Ep done - 32260.\n",
      "Ep done - 32270.\n",
      "Ep done - 32280.\n",
      "Ep done - 32290.\n",
      "Ep done - 32300.\n",
      "Ep done - 32310.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.5        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 476        |\n",
      "|    time_elapsed         | 5794       |\n",
      "|    total_timesteps      | 974848     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03271644 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.245     |\n",
      "|    explained_variance   | 0.348      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0164     |\n",
      "|    n_updates            | 4750       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Ep done - 32320.\n",
      "Ep done - 32330.\n",
      "Ep done - 32340.\n",
      "Ep done - 32350.\n",
      "Ep done - 32360.\n",
      "Ep done - 32370.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 477         |\n",
      "|    time_elapsed         | 5804        |\n",
      "|    total_timesteps      | 976896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028175043 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00116    |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Ep done - 32380.\n",
      "Ep done - 32390.\n",
      "Ep done - 32400.\n",
      "Ep done - 32410.\n",
      "Ep done - 32420.\n",
      "Ep done - 32430.\n",
      "Ep done - 32440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 478         |\n",
      "|    time_elapsed         | 5813        |\n",
      "|    total_timesteps      | 978944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041862298 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0289      |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 32450.\n",
      "Ep done - 32460.\n",
      "Ep done - 32470.\n",
      "Ep done - 32480.\n",
      "Ep done - 9710.\n",
      "Ep done - 9720.\n",
      "Ep done - 9730.\n",
      "Ep done - 9740.\n",
      "Ep done - 9750.\n",
      "Ep done - 9760.\n",
      "Ep done - 9770.\n",
      "Ep done - 9780.\n",
      "Ep done - 9790.\n",
      "Ep done - 9800.\n",
      "Eval num_timesteps=980000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.17 +/- 0.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.58        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 980000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045215447 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.58\n",
      "SELFPLAY: new best model, bumping up generation to 87\n",
      "Ep done - 32490.\n",
      "Ep done - 32500.\n",
      "Ep done - 32510.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 479      |\n",
      "|    time_elapsed    | 5831     |\n",
      "|    total_timesteps | 980992   |\n",
      "---------------------------------\n",
      "Ep done - 32520.\n",
      "Ep done - 32530.\n",
      "Ep done - 32540.\n",
      "Ep done - 32550.\n",
      "Ep done - 32560.\n",
      "Ep done - 32570.\n",
      "Ep done - 32580.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.53       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 480        |\n",
      "|    time_elapsed         | 5841       |\n",
      "|    total_timesteps      | 983040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03914181 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.255     |\n",
      "|    explained_variance   | 0.346      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0229     |\n",
      "|    n_updates            | 4790       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "Ep done - 32590.\n",
      "Ep done - 32600.\n",
      "Ep done - 32610.\n",
      "Ep done - 32620.\n",
      "Ep done - 32630.\n",
      "Ep done - 32640.\n",
      "Ep done - 32650.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 481        |\n",
      "|    time_elapsed         | 5850       |\n",
      "|    total_timesteps      | 985088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03628048 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.25      |\n",
      "|    explained_variance   | 0.359      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0302     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Ep done - 32660.\n",
      "Ep done - 32670.\n",
      "Ep done - 32680.\n",
      "Ep done - 32690.\n",
      "Ep done - 32700.\n",
      "Ep done - 32710.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 482        |\n",
      "|    time_elapsed         | 5859       |\n",
      "|    total_timesteps      | 987136     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04125423 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.256     |\n",
      "|    explained_variance   | 0.265      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 4810       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n",
      "Ep done - 32720.\n",
      "Ep done - 32730.\n",
      "Ep done - 32740.\n",
      "Ep done - 32750.\n",
      "Ep done - 32760.\n",
      "Ep done - 32770.\n",
      "Ep done - 32780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 483         |\n",
      "|    time_elapsed         | 5868        |\n",
      "|    total_timesteps      | 989184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039338924 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0224      |\n",
      "|    n_updates            | 4820        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "Ep done - 32790.\n",
      "Ep done - 32800.\n",
      "Ep done - 32810.\n",
      "Ep done - 9810.\n",
      "Ep done - 9820.\n",
      "Ep done - 9830.\n",
      "Ep done - 9840.\n",
      "Ep done - 9850.\n",
      "Ep done - 9860.\n",
      "Ep done - 9870.\n",
      "Ep done - 9880.\n",
      "Ep done - 9890.\n",
      "Ep done - 9900.\n",
      "Eval num_timesteps=990000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 30.30 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.3       |\n",
      "|    mean_reward          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 990000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04882084 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.253     |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0247     |\n",
      "|    n_updates            | 4830       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.59\n",
      "SELFPLAY: new best model, bumping up generation to 88\n",
      "Ep done - 32820.\n",
      "Ep done - 32830.\n",
      "Ep done - 32840.\n",
      "Ep done - 32850.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 5887     |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "Ep done - 32860.\n",
      "Ep done - 32870.\n",
      "Ep done - 32880.\n",
      "Ep done - 32890.\n",
      "Ep done - 32900.\n",
      "Ep done - 32910.\n",
      "Ep done - 32920.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 485         |\n",
      "|    time_elapsed         | 5896        |\n",
      "|    total_timesteps      | 993280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037961617 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0207      |\n",
      "|    n_updates            | 4840        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Ep done - 32930.\n",
      "Ep done - 32940.\n",
      "Ep done - 32950.\n",
      "Ep done - 32960.\n",
      "Ep done - 32970.\n",
      "Ep done - 32980.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 486        |\n",
      "|    time_elapsed         | 5905       |\n",
      "|    total_timesteps      | 995328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04075996 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.267     |\n",
      "|    explained_variance   | 0.261      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0638     |\n",
      "|    n_updates            | 4850       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    value_loss           | 0.188      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 32990.\n",
      "Ep done - 33000.\n",
      "Ep done - 33010.\n",
      "Ep done - 33020.\n",
      "Ep done - 33030.\n",
      "Ep done - 33040.\n",
      "Ep done - 33050.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 487         |\n",
      "|    time_elapsed         | 5915        |\n",
      "|    total_timesteps      | 997376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039672844 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0303      |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Ep done - 33060.\n",
      "Ep done - 33070.\n",
      "Ep done - 33080.\n",
      "Ep done - 33090.\n",
      "Ep done - 33100.\n",
      "Ep done - 33110.\n",
      "Ep done - 33120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 488         |\n",
      "|    time_elapsed         | 5924        |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046096794 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0344      |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "Ep done - 33130.\n",
      "Ep done - 33140.\n",
      "Ep done - 9910.\n",
      "Ep done - 9920.\n",
      "Ep done - 9930.\n",
      "Ep done - 9940.\n",
      "Ep done - 9950.\n",
      "Ep done - 9960.\n",
      "Ep done - 9970.\n",
      "Ep done - 9980.\n",
      "Ep done - 9990.\n",
      "Ep done - 10000.\n",
      "Eval num_timesteps=1000000, episode_reward=0.48 +/- 0.87\n",
      "Episode length: 30.10 +/- 0.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.48        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034865946 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.48\n",
      "SELFPLAY: new best model, bumping up generation to 89\n",
      "Ep done - 33150.\n",
      "Ep done - 33160.\n",
      "Ep done - 33170.\n",
      "Ep done - 33180.\n",
      "Ep done - 33190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.49     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 489      |\n",
      "|    time_elapsed    | 5942     |\n",
      "|    total_timesteps | 1001472  |\n",
      "---------------------------------\n",
      "Ep done - 33200.\n",
      "Ep done - 33210.\n",
      "Ep done - 33220.\n",
      "Ep done - 33230.\n",
      "Ep done - 33240.\n",
      "Ep done - 33250.\n",
      "Ep done - 33260.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 490         |\n",
      "|    time_elapsed         | 5952        |\n",
      "|    total_timesteps      | 1003520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052442417 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 4890        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Ep done - 33270.\n",
      "Ep done - 33280.\n",
      "Ep done - 33290.\n",
      "Ep done - 33300.\n",
      "Ep done - 33310.\n",
      "Ep done - 33320.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 491         |\n",
      "|    time_elapsed         | 5961        |\n",
      "|    total_timesteps      | 1005568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037912257 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.253      |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 4900        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 33330.\n",
      "Ep done - 33340.\n",
      "Ep done - 33350.\n",
      "Ep done - 33360.\n",
      "Ep done - 33370.\n",
      "Ep done - 33380.\n",
      "Ep done - 33390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 492         |\n",
      "|    time_elapsed         | 5970        |\n",
      "|    total_timesteps      | 1007616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051644348 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00829     |\n",
      "|    n_updates            | 4910        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 33400.\n",
      "Ep done - 33410.\n",
      "Ep done - 33420.\n",
      "Ep done - 33430.\n",
      "Ep done - 33440.\n",
      "Ep done - 33450.\n",
      "Ep done - 33460.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.48       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 493        |\n",
      "|    time_elapsed         | 5979       |\n",
      "|    total_timesteps      | 1009664    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04323329 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.442      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0129     |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "Ep done - 33470.\n",
      "Ep done - 10010.\n",
      "Ep done - 10020.\n",
      "Ep done - 10030.\n",
      "Ep done - 10040.\n",
      "Ep done - 10050.\n",
      "Ep done - 10060.\n",
      "Ep done - 10070.\n",
      "Ep done - 10080.\n",
      "Ep done - 10090.\n",
      "Ep done - 10100.\n",
      "Eval num_timesteps=1010000, episode_reward=0.61 +/- 0.76\n",
      "Episode length: 30.17 +/- 0.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1010000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05148315 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.282     |\n",
      "|    explained_variance   | 0.353      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.015      |\n",
      "|    n_updates            | 4930       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.61\n",
      "SELFPLAY: new best model, bumping up generation to 90\n",
      "Ep done - 33480.\n",
      "Ep done - 33490.\n",
      "Ep done - 33500.\n",
      "Ep done - 33510.\n",
      "Ep done - 33520.\n",
      "Ep done - 33530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 494      |\n",
      "|    time_elapsed    | 5998     |\n",
      "|    total_timesteps | 1011712  |\n",
      "---------------------------------\n",
      "Ep done - 33540.\n",
      "Ep done - 33550.\n",
      "Ep done - 33560.\n",
      "Ep done - 33570.\n",
      "Ep done - 33580.\n",
      "Ep done - 33590.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 495         |\n",
      "|    time_elapsed         | 6007        |\n",
      "|    total_timesteps      | 1013760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042811718 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 4940        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "Ep done - 33600.\n",
      "Ep done - 33610.\n",
      "Ep done - 33620.\n",
      "Ep done - 33630.\n",
      "Ep done - 33640.\n",
      "Ep done - 33650.\n",
      "Ep done - 33660.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.34      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 168       |\n",
      "|    iterations           | 496       |\n",
      "|    time_elapsed         | 6017      |\n",
      "|    total_timesteps      | 1015808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0505374 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.263    |\n",
      "|    explained_variance   | 0.284     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0216    |\n",
      "|    n_updates            | 4950      |\n",
      "|    policy_gradient_loss | -0.0267   |\n",
      "|    value_loss           | 0.17      |\n",
      "---------------------------------------\n",
      "Ep done - 33670.\n",
      "Ep done - 33680.\n",
      "Ep done - 33690.\n",
      "Ep done - 33700.\n",
      "Ep done - 33710.\n",
      "Ep done - 33720.\n",
      "Ep done - 33730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 497         |\n",
      "|    time_elapsed         | 6026        |\n",
      "|    total_timesteps      | 1017856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040055912 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 4960        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n",
      "Ep done - 33740.\n",
      "Ep done - 33750.\n",
      "Ep done - 33760.\n",
      "Ep done - 33770.\n",
      "Ep done - 33780.\n",
      "Ep done - 33790.\n",
      "Ep done - 33800.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 498        |\n",
      "|    time_elapsed         | 6035       |\n",
      "|    total_timesteps      | 1019904    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04490103 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.264     |\n",
      "|    explained_variance   | 0.172      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0364     |\n",
      "|    n_updates            | 4970       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n",
      "Ep done - 10110.\n",
      "Ep done - 10120.\n",
      "Ep done - 10130.\n",
      "Ep done - 10140.\n",
      "Ep done - 10150.\n",
      "Ep done - 10160.\n",
      "Ep done - 10170.\n",
      "Ep done - 10180.\n",
      "Ep done - 10190.\n",
      "Ep done - 10200.\n",
      "Eval num_timesteps=1020000, episode_reward=0.63 +/- 0.77\n",
      "Episode length: 30.21 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045532696 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 4980        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.63\n",
      "SELFPLAY: new best model, bumping up generation to 91\n",
      "Ep done - 33810.\n",
      "Ep done - 33820.\n",
      "Ep done - 33830.\n",
      "Ep done - 33840.\n",
      "Ep done - 33850.\n",
      "Ep done - 33860.\n",
      "Ep done - 33870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 499      |\n",
      "|    time_elapsed    | 6054     |\n",
      "|    total_timesteps | 1021952  |\n",
      "---------------------------------\n",
      "Ep done - 33880.\n",
      "Ep done - 33890.\n",
      "Ep done - 33900.\n",
      "Ep done - 33910.\n",
      "Ep done - 33920.\n",
      "Ep done - 33930.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 500         |\n",
      "|    time_elapsed         | 6063        |\n",
      "|    total_timesteps      | 1024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050190985 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0355      |\n",
      "|    n_updates            | 4990        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 33940.\n",
      "Ep done - 33950.\n",
      "Ep done - 33960.\n",
      "Ep done - 33970.\n",
      "Ep done - 33980.\n",
      "Ep done - 33990.\n",
      "Ep done - 34000.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 501         |\n",
      "|    time_elapsed         | 6073        |\n",
      "|    total_timesteps      | 1026048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049194686 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0218      |\n",
      "|    n_updates            | 5000        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "Ep done - 34010.\n",
      "Ep done - 34020.\n",
      "Ep done - 34030.\n",
      "Ep done - 34040.\n",
      "Ep done - 34050.\n",
      "Ep done - 34060.\n",
      "Ep done - 34070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 502         |\n",
      "|    time_elapsed         | 6082        |\n",
      "|    total_timesteps      | 1028096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030771509 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.228      |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 5010        |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 34080.\n",
      "Ep done - 34090.\n",
      "Ep done - 34100.\n",
      "Ep done - 34110.\n",
      "Ep done - 34120.\n",
      "Ep done - 34130.\n",
      "Ep done - 10210.\n",
      "Ep done - 10220.\n",
      "Ep done - 10230.\n",
      "Ep done - 10240.\n",
      "Ep done - 10250.\n",
      "Ep done - 10260.\n",
      "Ep done - 10270.\n",
      "Ep done - 10280.\n",
      "Ep done - 10290.\n",
      "Ep done - 10300.\n",
      "Eval num_timesteps=1030000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.23 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1030000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03874775 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.252     |\n",
      "|    explained_variance   | 0.176      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00963    |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 92\n",
      "Ep done - 34140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 503      |\n",
      "|    time_elapsed    | 6100     |\n",
      "|    total_timesteps | 1030144  |\n",
      "---------------------------------\n",
      "Ep done - 34150.\n",
      "Ep done - 34160.\n",
      "Ep done - 34170.\n",
      "Ep done - 34180.\n",
      "Ep done - 34190.\n",
      "Ep done - 34200.\n",
      "Ep done - 34210.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 504        |\n",
      "|    time_elapsed         | 6110       |\n",
      "|    total_timesteps      | 1032192    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03897801 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.244     |\n",
      "|    explained_variance   | 0.436      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0481     |\n",
      "|    n_updates            | 5030       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "Ep done - 34220.\n",
      "Ep done - 34230.\n",
      "Ep done - 34240.\n",
      "Ep done - 34250.\n",
      "Ep done - 34260.\n",
      "Ep done - 34270.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 505        |\n",
      "|    time_elapsed         | 6119       |\n",
      "|    total_timesteps      | 1034240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04053717 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.257     |\n",
      "|    explained_variance   | 0.35       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0196     |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Ep done - 34280.\n",
      "Ep done - 34290.\n",
      "Ep done - 34300.\n",
      "Ep done - 34310.\n",
      "Ep done - 34320.\n",
      "Ep done - 34330.\n",
      "Ep done - 34340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 506         |\n",
      "|    time_elapsed         | 6128        |\n",
      "|    total_timesteps      | 1036288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039621897 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 5050        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "Ep done - 34350.\n",
      "Ep done - 34360.\n",
      "Ep done - 34370.\n",
      "Ep done - 34380.\n",
      "Ep done - 34390.\n",
      "Ep done - 34400.\n",
      "Ep done - 34410.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 507         |\n",
      "|    time_elapsed         | 6138        |\n",
      "|    total_timesteps      | 1038336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030603718 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.0832      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 5060        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Ep done - 34420.\n",
      "Ep done - 34430.\n",
      "Ep done - 34440.\n",
      "Ep done - 34450.\n",
      "Ep done - 34460.\n",
      "Ep done - 10310.\n",
      "Ep done - 10320.\n",
      "Ep done - 10330.\n",
      "Ep done - 10340.\n",
      "Ep done - 10350.\n",
      "Ep done - 10360.\n",
      "Ep done - 10370.\n",
      "Ep done - 10380.\n",
      "Ep done - 10390.\n",
      "Ep done - 10400.\n",
      "Eval num_timesteps=1040000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.23 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042947207 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.254      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 5070        |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.66\n",
      "SELFPLAY: new best model, bumping up generation to 93\n",
      "Ep done - 34470.\n",
      "Ep done - 34480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 508      |\n",
      "|    time_elapsed    | 6156     |\n",
      "|    total_timesteps | 1040384  |\n",
      "---------------------------------\n",
      "Ep done - 34490.\n",
      "Ep done - 34500.\n",
      "Ep done - 34510.\n",
      "Ep done - 34520.\n",
      "Ep done - 34530.\n",
      "Ep done - 34540.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 509         |\n",
      "|    time_elapsed         | 6166        |\n",
      "|    total_timesteps      | 1042432     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043246284 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.258      |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.019       |\n",
      "|    n_updates            | 5080        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 34550.\n",
      "Ep done - 34560.\n",
      "Ep done - 34570.\n",
      "Ep done - 34580.\n",
      "Ep done - 34590.\n",
      "Ep done - 34600.\n",
      "Ep done - 34610.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 510         |\n",
      "|    time_elapsed         | 6175        |\n",
      "|    total_timesteps      | 1044480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045740172 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.238      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00204    |\n",
      "|    n_updates            | 5090        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Ep done - 34620.\n",
      "Ep done - 34630.\n",
      "Ep done - 34640.\n",
      "Ep done - 34650.\n",
      "Ep done - 34660.\n",
      "Ep done - 34670.\n",
      "Ep done - 34680.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.55       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 511        |\n",
      "|    time_elapsed         | 6184       |\n",
      "|    total_timesteps      | 1046528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05266195 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.253     |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00648    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "Ep done - 34690.\n",
      "Ep done - 34700.\n",
      "Ep done - 34710.\n",
      "Ep done - 34720.\n",
      "Ep done - 34730.\n",
      "Ep done - 34740.\n",
      "Ep done - 34750.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 512        |\n",
      "|    time_elapsed         | 6194       |\n",
      "|    total_timesteps      | 1048576    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04196565 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.264     |\n",
      "|    explained_variance   | 0.427      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00427    |\n",
      "|    n_updates            | 5110       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Ep done - 34760.\n",
      "Ep done - 34770.\n",
      "Ep done - 34780.\n",
      "Ep done - 34790.\n",
      "Ep done - 34800.\n",
      "Ep done - 10410.\n",
      "Ep done - 10420.\n",
      "Ep done - 10430.\n",
      "Ep done - 10440.\n",
      "Ep done - 10450.\n",
      "Ep done - 10460.\n",
      "Ep done - 10470.\n",
      "Ep done - 10480.\n",
      "Ep done - 10490.\n",
      "Ep done - 10500.\n",
      "Eval num_timesteps=1050000, episode_reward=0.47 +/- 0.85\n",
      "Episode length: 30.13 +/- 0.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1050000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040842168 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 5120        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.47\n",
      "SELFPLAY: new best model, bumping up generation to 94\n",
      "Ep done - 34810.\n",
      "Ep done - 34820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 513      |\n",
      "|    time_elapsed    | 6212     |\n",
      "|    total_timesteps | 1050624  |\n",
      "---------------------------------\n",
      "Ep done - 34830.\n",
      "Ep done - 34840.\n",
      "Ep done - 34850.\n",
      "Ep done - 34860.\n",
      "Ep done - 34870.\n",
      "Ep done - 34880.\n",
      "Ep done - 34890.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 514        |\n",
      "|    time_elapsed         | 6221       |\n",
      "|    total_timesteps      | 1052672    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04509411 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.248     |\n",
      "|    explained_variance   | 0.289      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0525     |\n",
      "|    n_updates            | 5130       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "Ep done - 34900.\n",
      "Ep done - 34910.\n",
      "Ep done - 34920.\n",
      "Ep done - 34930.\n",
      "Ep done - 34940.\n",
      "Ep done - 34950.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 515        |\n",
      "|    time_elapsed         | 6231       |\n",
      "|    total_timesteps      | 1054720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04821443 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.249     |\n",
      "|    explained_variance   | 0.248      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0186     |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Ep done - 34960.\n",
      "Ep done - 34970.\n",
      "Ep done - 34980.\n",
      "Ep done - 34990.\n",
      "Ep done - 35000.\n",
      "Ep done - 35010.\n",
      "Ep done - 35020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 516         |\n",
      "|    time_elapsed         | 6240        |\n",
      "|    total_timesteps      | 1056768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044384364 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.231      |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 5150        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Ep done - 35030.\n",
      "Ep done - 35040.\n",
      "Ep done - 35050.\n",
      "Ep done - 35060.\n",
      "Ep done - 35070.\n",
      "Ep done - 35080.\n",
      "Ep done - 35090.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 517        |\n",
      "|    time_elapsed         | 6249       |\n",
      "|    total_timesteps      | 1058816    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03632034 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.234     |\n",
      "|    explained_variance   | 0.372      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0414     |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 35100.\n",
      "Ep done - 35110.\n",
      "Ep done - 35120.\n",
      "Ep done - 35130.\n",
      "Ep done - 10510.\n",
      "Ep done - 10520.\n",
      "Ep done - 10530.\n",
      "Ep done - 10540.\n",
      "Ep done - 10550.\n",
      "Ep done - 10560.\n",
      "Ep done - 10570.\n",
      "Ep done - 10580.\n",
      "Ep done - 10590.\n",
      "Ep done - 10600.\n",
      "Eval num_timesteps=1060000, episode_reward=0.50 +/- 0.85\n",
      "Episode length: 30.11 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060327254 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0271      |\n",
      "|    n_updates            | 5170        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.5\n",
      "SELFPLAY: new best model, bumping up generation to 95\n",
      "Ep done - 35140.\n",
      "Ep done - 35150.\n",
      "Ep done - 35160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.58     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 518      |\n",
      "|    time_elapsed    | 6268     |\n",
      "|    total_timesteps | 1060864  |\n",
      "---------------------------------\n",
      "Ep done - 35170.\n",
      "Ep done - 35180.\n",
      "Ep done - 35190.\n",
      "Ep done - 35200.\n",
      "Ep done - 35210.\n",
      "Ep done - 35220.\n",
      "Ep done - 35230.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 519         |\n",
      "|    time_elapsed         | 6278        |\n",
      "|    total_timesteps      | 1062912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034459308 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 5180        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Ep done - 35240.\n",
      "Ep done - 35250.\n",
      "Ep done - 35260.\n",
      "Ep done - 35270.\n",
      "Ep done - 35280.\n",
      "Ep done - 35290.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 520         |\n",
      "|    time_elapsed         | 6287        |\n",
      "|    total_timesteps      | 1064960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042276263 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.247      |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0297      |\n",
      "|    n_updates            | 5190        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Ep done - 35300.\n",
      "Ep done - 35310.\n",
      "Ep done - 35320.\n",
      "Ep done - 35330.\n",
      "Ep done - 35340.\n",
      "Ep done - 35350.\n",
      "Ep done - 35360.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.43       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 521        |\n",
      "|    time_elapsed         | 6296       |\n",
      "|    total_timesteps      | 1067008    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05296762 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.242     |\n",
      "|    explained_variance   | 0.244      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0443     |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Ep done - 35370.\n",
      "Ep done - 35380.\n",
      "Ep done - 35390.\n",
      "Ep done - 35400.\n",
      "Ep done - 35410.\n",
      "Ep done - 35420.\n",
      "Ep done - 35430.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 522         |\n",
      "|    time_elapsed         | 6305        |\n",
      "|    total_timesteps      | 1069056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042529184 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.247      |\n",
      "|    explained_variance   | 0.277       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0522      |\n",
      "|    n_updates            | 5210        |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "Ep done - 35440.\n",
      "Ep done - 35450.\n",
      "Ep done - 35460.\n",
      "Ep done - 10610.\n",
      "Ep done - 10620.\n",
      "Ep done - 10630.\n",
      "Ep done - 10640.\n",
      "Ep done - 10650.\n",
      "Ep done - 10660.\n",
      "Ep done - 10670.\n",
      "Ep done - 10680.\n",
      "Ep done - 10690.\n",
      "Ep done - 10700.\n",
      "Eval num_timesteps=1070000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.16 +/- 0.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.67       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1070000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03710662 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.239     |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0116     |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.67\n",
      "SELFPLAY: new best model, bumping up generation to 96\n",
      "Ep done - 35470.\n",
      "Ep done - 35480.\n",
      "Ep done - 35490.\n",
      "Ep done - 35500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 523      |\n",
      "|    time_elapsed    | 6324     |\n",
      "|    total_timesteps | 1071104  |\n",
      "---------------------------------\n",
      "Ep done - 35510.\n",
      "Ep done - 35520.\n",
      "Ep done - 35530.\n",
      "Ep done - 35540.\n",
      "Ep done - 35550.\n",
      "Ep done - 35560.\n",
      "Ep done - 35570.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 524         |\n",
      "|    time_elapsed         | 6333        |\n",
      "|    total_timesteps      | 1073152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039950848 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.224      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0184      |\n",
      "|    n_updates            | 5230        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "Ep done - 35580.\n",
      "Ep done - 35590.\n",
      "Ep done - 35600.\n",
      "Ep done - 35610.\n",
      "Ep done - 35620.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 35630.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 525         |\n",
      "|    time_elapsed         | 6343        |\n",
      "|    total_timesteps      | 1075200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050280638 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.224      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 5240        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Ep done - 35640.\n",
      "Ep done - 35650.\n",
      "Ep done - 35660.\n",
      "Ep done - 35670.\n",
      "Ep done - 35680.\n",
      "Ep done - 35690.\n",
      "Ep done - 35700.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 526         |\n",
      "|    time_elapsed         | 6352        |\n",
      "|    total_timesteps      | 1077248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039189495 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 5250        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.214       |\n",
      "-----------------------------------------\n",
      "Ep done - 35710.\n",
      "Ep done - 35720.\n",
      "Ep done - 35730.\n",
      "Ep done - 35740.\n",
      "Ep done - 35750.\n",
      "Ep done - 35760.\n",
      "Ep done - 35770.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 527         |\n",
      "|    time_elapsed         | 6361        |\n",
      "|    total_timesteps      | 1079296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041514885 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 5260        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Ep done - 35780.\n",
      "Ep done - 35790.\n",
      "Ep done - 10710.\n",
      "Ep done - 10720.\n",
      "Ep done - 10730.\n",
      "Ep done - 10740.\n",
      "Ep done - 10750.\n",
      "Ep done - 10760.\n",
      "Ep done - 10770.\n",
      "Ep done - 10780.\n",
      "Ep done - 10790.\n",
      "Ep done - 10800.\n",
      "Eval num_timesteps=1080000, episode_reward=0.78 +/- 0.58\n",
      "Episode length: 30.16 +/- 0.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1080000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03333773 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.216     |\n",
      "|    explained_variance   | 0.391      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0107     |\n",
      "|    n_updates            | 5270       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.78\n",
      "SELFPLAY: new best model, bumping up generation to 97\n",
      "Ep done - 35800.\n",
      "Ep done - 35810.\n",
      "Ep done - 35820.\n",
      "Ep done - 35830.\n",
      "Ep done - 35840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 528      |\n",
      "|    time_elapsed    | 6380     |\n",
      "|    total_timesteps | 1081344  |\n",
      "---------------------------------\n",
      "Ep done - 35850.\n",
      "Ep done - 35860.\n",
      "Ep done - 35870.\n",
      "Ep done - 35880.\n",
      "Ep done - 35890.\n",
      "Ep done - 35900.\n",
      "Ep done - 35910.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 529        |\n",
      "|    time_elapsed         | 6390       |\n",
      "|    total_timesteps      | 1083392    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04976645 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.214     |\n",
      "|    explained_variance   | 0.256      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0125     |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Ep done - 35920.\n",
      "Ep done - 35930.\n",
      "Ep done - 35940.\n",
      "Ep done - 35950.\n",
      "Ep done - 35960.\n",
      "Ep done - 35970.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 530         |\n",
      "|    time_elapsed         | 6399        |\n",
      "|    total_timesteps      | 1085440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036731668 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 5290        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 35980.\n",
      "Ep done - 35990.\n",
      "Ep done - 36000.\n",
      "Ep done - 36010.\n",
      "Ep done - 36020.\n",
      "Ep done - 36030.\n",
      "Ep done - 36040.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30.1     |\n",
      "|    ep_rew_mean          | 0.66     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 169      |\n",
      "|    iterations           | 531      |\n",
      "|    time_elapsed         | 6408     |\n",
      "|    total_timesteps      | 1087488  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.040906 |\n",
      "|    clip_fraction        | 0.134    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.19    |\n",
      "|    explained_variance   | 0.43     |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0.0388   |\n",
      "|    n_updates            | 5300     |\n",
      "|    policy_gradient_loss | -0.0203  |\n",
      "|    value_loss           | 0.147    |\n",
      "--------------------------------------\n",
      "Ep done - 36050.\n",
      "Ep done - 36060.\n",
      "Ep done - 36070.\n",
      "Ep done - 36080.\n",
      "Ep done - 36090.\n",
      "Ep done - 36100.\n",
      "Ep done - 36110.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 532         |\n",
      "|    time_elapsed         | 6418        |\n",
      "|    total_timesteps      | 1089536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035248257 |\n",
      "|    clip_fraction        | 0.0979      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.16       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0277      |\n",
      "|    n_updates            | 5310        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 0.09        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 36120.\n",
      "Ep done - 10810.\n",
      "Ep done - 10820.\n",
      "Ep done - 10830.\n",
      "Ep done - 10840.\n",
      "Ep done - 10850.\n",
      "Ep done - 10860.\n",
      "Ep done - 10870.\n",
      "Ep done - 10880.\n",
      "Ep done - 10890.\n",
      "Ep done - 10900.\n",
      "Eval num_timesteps=1090000, episode_reward=0.80 +/- 0.58\n",
      "Episode length: 30.15 +/- 0.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050621454 |\n",
      "|    clip_fraction        | 0.0956      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0304      |\n",
      "|    n_updates            | 5320        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.8\n",
      "SELFPLAY: new best model, bumping up generation to 98\n",
      "Ep done - 36130.\n",
      "Ep done - 36140.\n",
      "Ep done - 36150.\n",
      "Ep done - 36160.\n",
      "Ep done - 36170.\n",
      "Ep done - 36180.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 533      |\n",
      "|    time_elapsed    | 6437     |\n",
      "|    total_timesteps | 1091584  |\n",
      "---------------------------------\n",
      "Ep done - 36190.\n",
      "Ep done - 36200.\n",
      "Ep done - 36210.\n",
      "Ep done - 36220.\n",
      "Ep done - 36230.\n",
      "Ep done - 36240.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 534        |\n",
      "|    time_elapsed         | 6446       |\n",
      "|    total_timesteps      | 1093632    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04758173 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.169     |\n",
      "|    explained_variance   | 0.16       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0651     |\n",
      "|    n_updates            | 5330       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "Ep done - 36250.\n",
      "Ep done - 36260.\n",
      "Ep done - 36270.\n",
      "Ep done - 36280.\n",
      "Ep done - 36290.\n",
      "Ep done - 36300.\n",
      "Ep done - 36310.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 535         |\n",
      "|    time_elapsed         | 6455        |\n",
      "|    total_timesteps      | 1095680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047200173 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0133      |\n",
      "|    n_updates            | 5340        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Ep done - 36320.\n",
      "Ep done - 36330.\n",
      "Ep done - 36340.\n",
      "Ep done - 36350.\n",
      "Ep done - 36360.\n",
      "Ep done - 36370.\n",
      "Ep done - 36380.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 536         |\n",
      "|    time_elapsed         | 6465        |\n",
      "|    total_timesteps      | 1097728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040729634 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0261      |\n",
      "|    n_updates            | 5350        |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Ep done - 36390.\n",
      "Ep done - 36400.\n",
      "Ep done - 36410.\n",
      "Ep done - 36420.\n",
      "Ep done - 36430.\n",
      "Ep done - 36440.\n",
      "Ep done - 36450.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 537         |\n",
      "|    time_elapsed         | 6474        |\n",
      "|    total_timesteps      | 1099776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040101714 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.19       |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0299      |\n",
      "|    n_updates            | 5360        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "Ep done - 36460.\n",
      "Ep done - 10910.\n",
      "Ep done - 10920.\n",
      "Ep done - 10930.\n",
      "Ep done - 10940.\n",
      "Ep done - 10950.\n",
      "Ep done - 10960.\n",
      "Ep done - 10970.\n",
      "Ep done - 10980.\n",
      "Ep done - 10990.\n",
      "Ep done - 11000.\n",
      "Eval num_timesteps=1100000, episode_reward=0.67 +/- 0.74\n",
      "Episode length: 30.11 +/- 0.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043311797 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0786      |\n",
      "|    n_updates            | 5370        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.67\n",
      "SELFPLAY: new best model, bumping up generation to 99\n",
      "Ep done - 36470.\n",
      "Ep done - 36480.\n",
      "Ep done - 36490.\n",
      "Ep done - 36500.\n",
      "Ep done - 36510.\n",
      "Ep done - 36520.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 538      |\n",
      "|    time_elapsed    | 6493     |\n",
      "|    total_timesteps | 1101824  |\n",
      "---------------------------------\n",
      "Ep done - 36530.\n",
      "Ep done - 36540.\n",
      "Ep done - 36550.\n",
      "Ep done - 36560.\n",
      "Ep done - 36570.\n",
      "Ep done - 36580.\n",
      "Ep done - 36590.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 539        |\n",
      "|    time_elapsed         | 6502       |\n",
      "|    total_timesteps      | 1103872    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04396238 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.181     |\n",
      "|    explained_variance   | 0.0744     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0533     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Ep done - 36600.\n",
      "Ep done - 36610.\n",
      "Ep done - 36620.\n",
      "Ep done - 36630.\n",
      "Ep done - 36640.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 36650.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 540         |\n",
      "|    time_elapsed         | 6512        |\n",
      "|    total_timesteps      | 1105920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052182376 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0805      |\n",
      "|    n_updates            | 5390        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Ep done - 36660.\n",
      "Ep done - 36670.\n",
      "Ep done - 36680.\n",
      "Ep done - 36690.\n",
      "Ep done - 36700.\n",
      "Ep done - 36710.\n",
      "Ep done - 36720.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 541         |\n",
      "|    time_elapsed         | 6521        |\n",
      "|    total_timesteps      | 1107968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036159642 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0853      |\n",
      "|    n_updates            | 5400        |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "Ep done - 36730.\n",
      "Ep done - 36740.\n",
      "Ep done - 36750.\n",
      "Ep done - 36760.\n",
      "Ep done - 36770.\n",
      "Ep done - 36780.\n",
      "Ep done - 36790.\n",
      "Ep done - 11010.\n",
      "Ep done - 11020.\n",
      "Ep done - 11030.\n",
      "Ep done - 11040.\n",
      "Ep done - 11050.\n",
      "Ep done - 11060.\n",
      "Ep done - 11070.\n",
      "Ep done - 11080.\n",
      "Ep done - 11090.\n",
      "Ep done - 11100.\n",
      "Eval num_timesteps=1110000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.10 +/- 0.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1110000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042721942 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 5410        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 100\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 542      |\n",
      "|    time_elapsed    | 6539     |\n",
      "|    total_timesteps | 1110016  |\n",
      "---------------------------------\n",
      "Ep done - 36800.\n",
      "Ep done - 36810.\n",
      "Ep done - 36820.\n",
      "Ep done - 36830.\n",
      "Ep done - 36840.\n",
      "Ep done - 36850.\n",
      "Ep done - 36860.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 543         |\n",
      "|    time_elapsed         | 6549        |\n",
      "|    total_timesteps      | 1112064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040159956 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00411     |\n",
      "|    n_updates            | 5420        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 36870.\n",
      "Ep done - 36880.\n",
      "Ep done - 36890.\n",
      "Ep done - 36900.\n",
      "Ep done - 36910.\n",
      "Ep done - 36920.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 544         |\n",
      "|    time_elapsed         | 6558        |\n",
      "|    total_timesteps      | 1114112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039068706 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 5430        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Ep done - 36930.\n",
      "Ep done - 36940.\n",
      "Ep done - 36950.\n",
      "Ep done - 36960.\n",
      "Ep done - 36970.\n",
      "Ep done - 36980.\n",
      "Ep done - 36990.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.65      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 169       |\n",
      "|    iterations           | 545       |\n",
      "|    time_elapsed         | 6567      |\n",
      "|    total_timesteps      | 1116160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0333879 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.181    |\n",
      "|    explained_variance   | 0.449     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0491    |\n",
      "|    n_updates            | 5440      |\n",
      "|    policy_gradient_loss | -0.02     |\n",
      "|    value_loss           | 0.0786    |\n",
      "---------------------------------------\n",
      "Ep done - 37000.\n",
      "Ep done - 37010.\n",
      "Ep done - 37020.\n",
      "Ep done - 37030.\n",
      "Ep done - 37040.\n",
      "Ep done - 37050.\n",
      "Ep done - 37060.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 546         |\n",
      "|    time_elapsed         | 6577        |\n",
      "|    total_timesteps      | 1118208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049022995 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 5450        |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Ep done - 37070.\n",
      "Ep done - 37080.\n",
      "Ep done - 37090.\n",
      "Ep done - 37100.\n",
      "Ep done - 37110.\n",
      "Ep done - 37120.\n",
      "Ep done - 11110.\n",
      "Ep done - 11120.\n",
      "Ep done - 11130.\n",
      "Ep done - 11140.\n",
      "Ep done - 11150.\n",
      "Ep done - 11160.\n",
      "Ep done - 11170.\n",
      "Ep done - 11180.\n",
      "Ep done - 11190.\n",
      "Ep done - 11200.\n",
      "Eval num_timesteps=1120000, episode_reward=0.78 +/- 0.63\n",
      "Episode length: 30.20 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.78        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047805544 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.142      |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00341     |\n",
      "|    n_updates            | 5460        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.0569      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.78\n",
      "SELFPLAY: new best model, bumping up generation to 101\n",
      "Ep done - 37130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 547      |\n",
      "|    time_elapsed    | 6595     |\n",
      "|    total_timesteps | 1120256  |\n",
      "---------------------------------\n",
      "Ep done - 37140.\n",
      "Ep done - 37150.\n",
      "Ep done - 37160.\n",
      "Ep done - 37170.\n",
      "Ep done - 37180.\n",
      "Ep done - 37190.\n",
      "Ep done - 37200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 548         |\n",
      "|    time_elapsed         | 6605        |\n",
      "|    total_timesteps      | 1122304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044635125 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.144      |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0575      |\n",
      "|    n_updates            | 5470        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "Ep done - 37210.\n",
      "Ep done - 37220.\n",
      "Ep done - 37230.\n",
      "Ep done - 37240.\n",
      "Ep done - 37250.\n",
      "Ep done - 37260.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 549        |\n",
      "|    time_elapsed         | 6614       |\n",
      "|    total_timesteps      | 1124352    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03388484 |\n",
      "|    clip_fraction        | 0.0861     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.123     |\n",
      "|    explained_variance   | 0.0948     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0204     |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    value_loss           | 0.0933     |\n",
      "----------------------------------------\n",
      "Ep done - 37270.\n",
      "Ep done - 37280.\n",
      "Ep done - 37290.\n",
      "Ep done - 37300.\n",
      "Ep done - 37310.\n",
      "Ep done - 37320.\n",
      "Ep done - 37330.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 550         |\n",
      "|    time_elapsed         | 6624        |\n",
      "|    total_timesteps      | 1126400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041098367 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.142      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 5490        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Ep done - 37340.\n",
      "Ep done - 37350.\n",
      "Ep done - 37360.\n",
      "Ep done - 37370.\n",
      "Ep done - 37380.\n",
      "Ep done - 37390.\n",
      "Ep done - 37400.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 551        |\n",
      "|    time_elapsed         | 6633       |\n",
      "|    total_timesteps      | 1128448    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03404434 |\n",
      "|    clip_fraction        | 0.0829     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.133     |\n",
      "|    explained_variance   | 0.122      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0483     |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Ep done - 37410.\n",
      "Ep done - 37420.\n",
      "Ep done - 37430.\n",
      "Ep done - 37440.\n",
      "Ep done - 37450.\n",
      "Ep done - 11210.\n",
      "Ep done - 11220.\n",
      "Ep done - 11230.\n",
      "Ep done - 11240.\n",
      "Ep done - 11250.\n",
      "Ep done - 11260.\n",
      "Ep done - 11270.\n",
      "Ep done - 11280.\n",
      "Ep done - 11290.\n",
      "Ep done - 11300.\n",
      "Eval num_timesteps=1130000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.14 +/- 0.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1130000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047207117 |\n",
      "|    clip_fraction        | 0.0965      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.134      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0335      |\n",
      "|    n_updates            | 5510        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 102\n",
      "Ep done - 37460.\n",
      "Ep done - 37470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 552      |\n",
      "|    time_elapsed    | 6652     |\n",
      "|    total_timesteps | 1130496  |\n",
      "---------------------------------\n",
      "Ep done - 37480.\n",
      "Ep done - 37490.\n",
      "Ep done - 37500.\n",
      "Ep done - 37510.\n",
      "Ep done - 37520.\n",
      "Ep done - 37530.\n",
      "Ep done - 37540.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.63       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 553        |\n",
      "|    time_elapsed         | 6661       |\n",
      "|    total_timesteps      | 1132544    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03947856 |\n",
      "|    clip_fraction        | 0.0856     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.127     |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0202     |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    value_loss           | 0.161      |\n",
      "----------------------------------------\n",
      "Ep done - 37550.\n",
      "Ep done - 37560.\n",
      "Ep done - 37570.\n",
      "Ep done - 37580.\n",
      "Ep done - 37590.\n",
      "Ep done - 37600.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.68      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 170       |\n",
      "|    iterations           | 554       |\n",
      "|    time_elapsed         | 6670      |\n",
      "|    total_timesteps      | 1134592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0364751 |\n",
      "|    clip_fraction        | 0.0935    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.132    |\n",
      "|    explained_variance   | 0.26      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00571   |\n",
      "|    n_updates            | 5530      |\n",
      "|    policy_gradient_loss | -0.0185   |\n",
      "|    value_loss           | 0.122     |\n",
      "---------------------------------------\n",
      "Ep done - 37610.\n",
      "Ep done - 37620.\n",
      "Ep done - 37630.\n",
      "Ep done - 37640.\n",
      "Ep done - 37650.\n",
      "Ep done - 37660.\n",
      "Ep done - 37670.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 555         |\n",
      "|    time_elapsed         | 6680        |\n",
      "|    total_timesteps      | 1136640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043442197 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.136      |\n",
      "|    explained_variance   | -0.0275     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 5540        |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 37680.\n",
      "Ep done - 37690.\n",
      "Ep done - 37700.\n",
      "Ep done - 37710.\n",
      "Ep done - 37720.\n",
      "Ep done - 37730.\n",
      "Ep done - 37740.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 556        |\n",
      "|    time_elapsed         | 6689       |\n",
      "|    total_timesteps      | 1138688    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02916764 |\n",
      "|    clip_fraction        | 0.0876     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.128     |\n",
      "|    explained_variance   | 0.247      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0247     |\n",
      "|    n_updates            | 5550       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    value_loss           | 0.096      |\n",
      "----------------------------------------\n",
      "Ep done - 37750.\n",
      "Ep done - 37760.\n",
      "Ep done - 37770.\n",
      "Ep done - 37780.\n",
      "Ep done - 11310.\n",
      "Ep done - 11320.\n",
      "Ep done - 11330.\n",
      "Ep done - 11340.\n",
      "Ep done - 11350.\n",
      "Ep done - 11360.\n",
      "Ep done - 11370.\n",
      "Ep done - 11380.\n",
      "Ep done - 11390.\n",
      "Ep done - 11400.\n",
      "Eval num_timesteps=1140000, episode_reward=0.54 +/- 0.83\n",
      "Episode length: 30.18 +/- 0.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035643794 |\n",
      "|    clip_fraction        | 0.0912      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0867      |\n",
      "|    n_updates            | 5560        |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.54\n",
      "SELFPLAY: new best model, bumping up generation to 103\n",
      "Ep done - 37790.\n",
      "Ep done - 37800.\n",
      "Ep done - 37810.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 557      |\n",
      "|    time_elapsed    | 6708     |\n",
      "|    total_timesteps | 1140736  |\n",
      "---------------------------------\n",
      "Ep done - 37820.\n",
      "Ep done - 37830.\n",
      "Ep done - 37840.\n",
      "Ep done - 37850.\n",
      "Ep done - 37860.\n",
      "Ep done - 37870.\n",
      "Ep done - 37880.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 558        |\n",
      "|    time_elapsed         | 6717       |\n",
      "|    total_timesteps      | 1142784    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03481961 |\n",
      "|    clip_fraction        | 0.0858     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.118     |\n",
      "|    explained_variance   | 0.358      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.059      |\n",
      "|    n_updates            | 5570       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    value_loss           | 0.144      |\n",
      "----------------------------------------\n",
      "Ep done - 37890.\n",
      "Ep done - 37900.\n",
      "Ep done - 37910.\n",
      "Ep done - 37920.\n",
      "Ep done - 37930.\n",
      "Ep done - 37940.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 559        |\n",
      "|    time_elapsed         | 6726       |\n",
      "|    total_timesteps      | 1144832    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05693128 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.148     |\n",
      "|    explained_variance   | 0.282      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0294     |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Ep done - 37950.\n",
      "Ep done - 37960.\n",
      "Ep done - 37970.\n",
      "Ep done - 37980.\n",
      "Ep done - 37990.\n",
      "Ep done - 38000.\n",
      "Ep done - 38010.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.61      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 170       |\n",
      "|    iterations           | 560       |\n",
      "|    time_elapsed         | 6736      |\n",
      "|    total_timesteps      | 1146880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0394685 |\n",
      "|    clip_fraction        | 0.0976    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.139    |\n",
      "|    explained_variance   | 0.199     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0493    |\n",
      "|    n_updates            | 5590      |\n",
      "|    policy_gradient_loss | -0.0164   |\n",
      "|    value_loss           | 0.137     |\n",
      "---------------------------------------\n",
      "Ep done - 38020.\n",
      "Ep done - 38030.\n",
      "Ep done - 38040.\n",
      "Ep done - 38050.\n",
      "Ep done - 38060.\n",
      "Ep done - 38070.\n",
      "Ep done - 38080.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 561         |\n",
      "|    time_elapsed         | 6745        |\n",
      "|    total_timesteps      | 1148928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058157794 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 5600        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Ep done - 38090.\n",
      "Ep done - 38100.\n",
      "Ep done - 38110.\n",
      "Ep done - 11410.\n",
      "Ep done - 11420.\n",
      "Ep done - 11430.\n",
      "Ep done - 11440.\n",
      "Ep done - 11450.\n",
      "Ep done - 11460.\n",
      "Ep done - 11470.\n",
      "Ep done - 11480.\n",
      "Ep done - 11490.\n",
      "Ep done - 11500.\n",
      "Eval num_timesteps=1150000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.19 +/- 0.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.67       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1150000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05622049 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.134     |\n",
      "|    explained_variance   | 0.417      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0135     |\n",
      "|    n_updates            | 5610       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    value_loss           | 0.0984     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.67\n",
      "SELFPLAY: new best model, bumping up generation to 104\n",
      "Ep done - 38120.\n",
      "Ep done - 38130.\n",
      "Ep done - 38140.\n",
      "Ep done - 38150.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 562      |\n",
      "|    time_elapsed    | 6764     |\n",
      "|    total_timesteps | 1150976  |\n",
      "---------------------------------\n",
      "Ep done - 38160.\n",
      "Ep done - 38170.\n",
      "Ep done - 38180.\n",
      "Ep done - 38190.\n",
      "Ep done - 38200.\n",
      "Ep done - 38210.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.58        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 563         |\n",
      "|    time_elapsed         | 6773        |\n",
      "|    total_timesteps      | 1153024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038103797 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0762      |\n",
      "|    n_updates            | 5620        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 38220.\n",
      "Ep done - 38230.\n",
      "Ep done - 38240.\n",
      "Ep done - 38250.\n",
      "Ep done - 38260.\n",
      "Ep done - 38270.\n",
      "Ep done - 38280.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 564        |\n",
      "|    time_elapsed         | 6782       |\n",
      "|    total_timesteps      | 1155072    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03573143 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.151     |\n",
      "|    explained_variance   | -0.0294    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0723     |\n",
      "|    n_updates            | 5630       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "Ep done - 38290.\n",
      "Ep done - 38300.\n",
      "Ep done - 38310.\n",
      "Ep done - 38320.\n",
      "Ep done - 38330.\n",
      "Ep done - 38340.\n",
      "Ep done - 38350.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 565         |\n",
      "|    time_elapsed         | 6792        |\n",
      "|    total_timesteps      | 1157120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050151974 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.151      |\n",
      "|    explained_variance   | 0.0146      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0313      |\n",
      "|    n_updates            | 5640        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Ep done - 38360.\n",
      "Ep done - 38370.\n",
      "Ep done - 38380.\n",
      "Ep done - 38390.\n",
      "Ep done - 38400.\n",
      "Ep done - 38410.\n",
      "Ep done - 38420.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 566         |\n",
      "|    time_elapsed         | 6801        |\n",
      "|    total_timesteps      | 1159168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049421526 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.16       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00811    |\n",
      "|    n_updates            | 5650        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Ep done - 38430.\n",
      "Ep done - 38440.\n",
      "Ep done - 38450.\n",
      "Ep done - 11510.\n",
      "Ep done - 11520.\n",
      "Ep done - 11530.\n",
      "Ep done - 11540.\n",
      "Ep done - 11550.\n",
      "Ep done - 11560.\n",
      "Ep done - 11570.\n",
      "Ep done - 11580.\n",
      "Ep done - 11590.\n",
      "Ep done - 11600.\n",
      "Eval num_timesteps=1160000, episode_reward=0.71 +/- 0.67\n",
      "Episode length: 30.21 +/- 0.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.71        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033828907 |\n",
      "|    clip_fraction        | 0.0919      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0457      |\n",
      "|    n_updates            | 5660        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.71\n",
      "SELFPLAY: new best model, bumping up generation to 105\n",
      "Ep done - 38460.\n",
      "Ep done - 38470.\n",
      "Ep done - 38480.\n",
      "Ep done - 38490.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.47     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 567      |\n",
      "|    time_elapsed    | 6819     |\n",
      "|    total_timesteps | 1161216  |\n",
      "---------------------------------\n",
      "Ep done - 38500.\n",
      "Ep done - 38510.\n",
      "Ep done - 38520.\n",
      "Ep done - 38530.\n",
      "Ep done - 38540.\n",
      "Ep done - 38550.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 568         |\n",
      "|    time_elapsed         | 6829        |\n",
      "|    total_timesteps      | 1163264     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046934225 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.188      |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0203      |\n",
      "|    n_updates            | 5670        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Ep done - 38560.\n",
      "Ep done - 38570.\n",
      "Ep done - 38580.\n",
      "Ep done - 38590.\n",
      "Ep done - 38600.\n",
      "Ep done - 38610.\n",
      "Ep done - 38620.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 569         |\n",
      "|    time_elapsed         | 6838        |\n",
      "|    total_timesteps      | 1165312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053857513 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0433      |\n",
      "|    n_updates            | 5680        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "Ep done - 38630.\n",
      "Ep done - 38640.\n",
      "Ep done - 38650.\n",
      "Ep done - 38660.\n",
      "Ep done - 38670.\n",
      "Ep done - 38680.\n",
      "Ep done - 38690.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 570         |\n",
      "|    time_elapsed         | 6847        |\n",
      "|    total_timesteps      | 1167360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050546944 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0362      |\n",
      "|    n_updates            | 5690        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Ep done - 38700.\n",
      "Ep done - 38710.\n",
      "Ep done - 38720.\n",
      "Ep done - 38730.\n",
      "Ep done - 38740.\n",
      "Ep done - 38750.\n",
      "Ep done - 38760.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 571         |\n",
      "|    time_elapsed         | 6857        |\n",
      "|    total_timesteps      | 1169408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048384253 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.219      |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 5700        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 38770.\n",
      "Ep done - 38780.\n",
      "Ep done - 11610.\n",
      "Ep done - 11620.\n",
      "Ep done - 11630.\n",
      "Ep done - 11640.\n",
      "Ep done - 11650.\n",
      "Ep done - 11660.\n",
      "Ep done - 11670.\n",
      "Ep done - 11680.\n",
      "Ep done - 11690.\n",
      "Ep done - 11700.\n",
      "Eval num_timesteps=1170000, episode_reward=0.44 +/- 0.89\n",
      "Episode length: 30.13 +/- 0.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1170000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047705427 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0237      |\n",
      "|    n_updates            | 5710        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.44\n",
      "SELFPLAY: new best model, bumping up generation to 106\n",
      "Ep done - 38790.\n",
      "Ep done - 38800.\n",
      "Ep done - 38810.\n",
      "Ep done - 38820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 572      |\n",
      "|    time_elapsed    | 6876     |\n",
      "|    total_timesteps | 1171456  |\n",
      "---------------------------------\n",
      "Ep done - 38830.\n",
      "Ep done - 38840.\n",
      "Ep done - 38850.\n",
      "Ep done - 38860.\n",
      "Ep done - 38870.\n",
      "Ep done - 38880.\n",
      "Ep done - 38890.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 573        |\n",
      "|    time_elapsed         | 6886       |\n",
      "|    total_timesteps      | 1173504    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05263876 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.208     |\n",
      "|    explained_variance   | 0.264      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0196     |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "Ep done - 38900.\n",
      "Ep done - 38910.\n",
      "Ep done - 38920.\n",
      "Ep done - 38930.\n",
      "Ep done - 38940.\n",
      "Ep done - 38950.\n",
      "Ep done - 38960.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 574         |\n",
      "|    time_elapsed         | 6895        |\n",
      "|    total_timesteps      | 1175552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037859928 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0654      |\n",
      "|    n_updates            | 5730        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n",
      "Ep done - 38970.\n",
      "Ep done - 38980.\n",
      "Ep done - 38990.\n",
      "Ep done - 39000.\n",
      "Ep done - 39010.\n",
      "Ep done - 39020.\n",
      "Ep done - 39030.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 575         |\n",
      "|    time_elapsed         | 6904        |\n",
      "|    total_timesteps      | 1177600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042589046 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 5740        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Ep done - 39040.\n",
      "Ep done - 39050.\n",
      "Ep done - 39060.\n",
      "Ep done - 39070.\n",
      "Ep done - 39080.\n",
      "Ep done - 39090.\n",
      "Ep done - 39100.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 576         |\n",
      "|    time_elapsed         | 6914        |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045043357 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 5750        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 39110.\n",
      "Ep done - 11710.\n",
      "Ep done - 11720.\n",
      "Ep done - 11730.\n",
      "Ep done - 11740.\n",
      "Ep done - 11750.\n",
      "Ep done - 11760.\n",
      "Ep done - 11770.\n",
      "Ep done - 11780.\n",
      "Ep done - 11790.\n",
      "Ep done - 11800.\n",
      "Eval num_timesteps=1180000, episode_reward=0.51 +/- 0.83\n",
      "Episode length: 30.08 +/- 0.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1180000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05135953 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.18      |\n",
      "|    explained_variance   | 0.256      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0479     |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.51\n",
      "SELFPLAY: new best model, bumping up generation to 107\n",
      "Ep done - 39120.\n",
      "Ep done - 39130.\n",
      "Ep done - 39140.\n",
      "Ep done - 39150.\n",
      "Ep done - 39160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 577      |\n",
      "|    time_elapsed    | 6932     |\n",
      "|    total_timesteps | 1181696  |\n",
      "---------------------------------\n",
      "Ep done - 39170.\n",
      "Ep done - 39180.\n",
      "Ep done - 39190.\n",
      "Ep done - 39200.\n",
      "Ep done - 39210.\n",
      "Ep done - 39220.\n",
      "Ep done - 39230.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 578        |\n",
      "|    time_elapsed         | 6941       |\n",
      "|    total_timesteps      | 1183744    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04353184 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.162     |\n",
      "|    explained_variance   | 0.249      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0643     |\n",
      "|    n_updates            | 5770       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "Ep done - 39240.\n",
      "Ep done - 39250.\n",
      "Ep done - 39260.\n",
      "Ep done - 39270.\n",
      "Ep done - 39280.\n",
      "Ep done - 39290.\n",
      "Ep done - 39300.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 579        |\n",
      "|    time_elapsed         | 6951       |\n",
      "|    total_timesteps      | 1185792    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03711496 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.176     |\n",
      "|    explained_variance   | 0.318      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0127     |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.109      |\n",
      "----------------------------------------\n",
      "Ep done - 39310.\n",
      "Ep done - 39320.\n",
      "Ep done - 39330.\n",
      "Ep done - 39340.\n",
      "Ep done - 39350.\n",
      "Ep done - 39360.\n",
      "Ep done - 39370.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 580         |\n",
      "|    time_elapsed         | 6960        |\n",
      "|    total_timesteps      | 1187840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041211233 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00545     |\n",
      "|    n_updates            | 5790        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Ep done - 39380.\n",
      "Ep done - 39390.\n",
      "Ep done - 39400.\n",
      "Ep done - 39410.\n",
      "Ep done - 39420.\n",
      "Ep done - 39430.\n",
      "Ep done - 39440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 581         |\n",
      "|    time_elapsed         | 6969        |\n",
      "|    total_timesteps      | 1189888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034201536 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.0928      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 5800        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Ep done - 11810.\n",
      "Ep done - 11820.\n",
      "Ep done - 11830.\n",
      "Ep done - 11840.\n",
      "Ep done - 11850.\n",
      "Ep done - 11860.\n",
      "Ep done - 11870.\n",
      "Ep done - 11880.\n",
      "Ep done - 11890.\n",
      "Ep done - 11900.\n",
      "Eval num_timesteps=1190000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.25 +/- 0.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1190000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03688402 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.178     |\n",
      "|    explained_variance   | 0.17       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0418     |\n",
      "|    n_updates            | 5810       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.6\n",
      "SELFPLAY: new best model, bumping up generation to 108\n",
      "Ep done - 39450.\n",
      "Ep done - 39460.\n",
      "Ep done - 39470.\n",
      "Ep done - 39480.\n",
      "Ep done - 39490.\n",
      "Ep done - 39500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 582      |\n",
      "|    time_elapsed    | 6988     |\n",
      "|    total_timesteps | 1191936  |\n",
      "---------------------------------\n",
      "Ep done - 39510.\n",
      "Ep done - 39520.\n",
      "Ep done - 39530.\n",
      "Ep done - 39540.\n",
      "Ep done - 39550.\n",
      "Ep done - 39560.\n",
      "Ep done - 39570.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 583         |\n",
      "|    time_elapsed         | 6998        |\n",
      "|    total_timesteps      | 1193984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029212948 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00189    |\n",
      "|    n_updates            | 5820        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.0729      |\n",
      "-----------------------------------------\n",
      "Ep done - 39580.\n",
      "Ep done - 39590.\n",
      "Ep done - 39600.\n",
      "Ep done - 39610.\n",
      "Ep done - 39620.\n",
      "Ep done - 39630.\n",
      "Ep done - 39640.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 584         |\n",
      "|    time_elapsed         | 7007        |\n",
      "|    total_timesteps      | 1196032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043815713 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.154      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0209      |\n",
      "|    n_updates            | 5830        |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Ep done - 39650.\n",
      "Ep done - 39660.\n",
      "Ep done - 39670.\n",
      "Ep done - 39680.\n",
      "Ep done - 39690.\n",
      "Ep done - 39700.\n",
      "Ep done - 39710.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 585        |\n",
      "|    time_elapsed         | 7016       |\n",
      "|    total_timesteps      | 1198080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03365908 |\n",
      "|    clip_fraction        | 0.0928     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.135     |\n",
      "|    explained_variance   | 0.252      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00878    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    value_loss           | 0.0737     |\n",
      "----------------------------------------\n",
      "Ep done - 39720.\n",
      "Ep done - 39730.\n",
      "Ep done - 39740.\n",
      "Ep done - 39750.\n",
      "Ep done - 39760.\n",
      "Ep done - 39770.\n",
      "Ep done - 11910.\n",
      "Ep done - 11920.\n",
      "Ep done - 11930.\n",
      "Ep done - 11940.\n",
      "Ep done - 11950.\n",
      "Ep done - 11960.\n",
      "Ep done - 11970.\n",
      "Ep done - 11980.\n",
      "Ep done - 11990.\n",
      "Ep done - 12000.\n",
      "Eval num_timesteps=1200000, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 30.14 +/- 0.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.79        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030612636 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.155      |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00335    |\n",
      "|    n_updates            | 5850        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.0743      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.79\n",
      "SELFPLAY: new best model, bumping up generation to 109\n",
      "Ep done - 39780.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.78     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 586      |\n",
      "|    time_elapsed    | 7035     |\n",
      "|    total_timesteps | 1200128  |\n",
      "---------------------------------\n",
      "Ep done - 39790.\n",
      "Ep done - 39800.\n",
      "Ep done - 39810.\n",
      "Ep done - 39820.\n",
      "Ep done - 39830.\n",
      "Ep done - 39840.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.87        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 587         |\n",
      "|    time_elapsed         | 7044        |\n",
      "|    total_timesteps      | 1202176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031899735 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.0195      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 5860        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Ep done - 39850.\n",
      "Ep done - 39860.\n",
      "Ep done - 39870.\n",
      "Ep done - 39880.\n",
      "Ep done - 39890.\n",
      "Ep done - 39900.\n",
      "Ep done - 39910.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 588         |\n",
      "|    time_elapsed         | 7053        |\n",
      "|    total_timesteps      | 1204224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038533993 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.173      |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 5870        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.0527      |\n",
      "-----------------------------------------\n",
      "Ep done - 39920.\n",
      "Ep done - 39930.\n",
      "Ep done - 39940.\n",
      "Ep done - 39950.\n",
      "Ep done - 39960.\n",
      "Ep done - 39970.\n",
      "Ep done - 39980.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 589         |\n",
      "|    time_elapsed         | 7063        |\n",
      "|    total_timesteps      | 1206272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042959347 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0518      |\n",
      "|    n_updates            | 5880        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Ep done - 39990.\n",
      "Ep done - 40000.\n",
      "Ep done - 40010.\n",
      "Ep done - 40020.\n",
      "Ep done - 40030.\n",
      "Ep done - 40040.\n",
      "Ep done - 40050.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 590         |\n",
      "|    time_elapsed         | 7072        |\n",
      "|    total_timesteps      | 1208320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048923016 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.147      |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00525    |\n",
      "|    n_updates            | 5890        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 0.072       |\n",
      "-----------------------------------------\n",
      "Ep done - 40060.\n",
      "Ep done - 40070.\n",
      "Ep done - 40080.\n",
      "Ep done - 40090.\n",
      "Ep done - 40100.\n",
      "Ep done - 12010.\n",
      "Ep done - 12020.\n",
      "Ep done - 12030.\n",
      "Ep done - 12040.\n",
      "Ep done - 12050.\n",
      "Ep done - 12060.\n",
      "Ep done - 12070.\n",
      "Ep done - 12080.\n",
      "Ep done - 12090.\n",
      "Ep done - 12100.\n",
      "Eval num_timesteps=1210000, episode_reward=0.76 +/- 0.65\n",
      "Episode length: 30.12 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1210000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034235865 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 5900        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 0.0688      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 110\n",
      "Ep done - 40110.\n",
      "Ep done - 40120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.8      |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 591      |\n",
      "|    time_elapsed    | 7090     |\n",
      "|    total_timesteps | 1210368  |\n",
      "---------------------------------\n",
      "Ep done - 40130.\n",
      "Ep done - 40140.\n",
      "Ep done - 40150.\n",
      "Ep done - 40160.\n",
      "Ep done - 40170.\n",
      "Ep done - 40180.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 592        |\n",
      "|    time_elapsed         | 7100       |\n",
      "|    total_timesteps      | 1212416    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03693305 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.141     |\n",
      "|    explained_variance   | 0.295      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0189    |\n",
      "|    n_updates            | 5910       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    value_loss           | 0.0596     |\n",
      "----------------------------------------\n",
      "Ep done - 40190.\n",
      "Ep done - 40200.\n",
      "Ep done - 40210.\n",
      "Ep done - 40220.\n",
      "Ep done - 40230.\n",
      "Ep done - 40240.\n",
      "Ep done - 40250.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 593        |\n",
      "|    time_elapsed         | 7109       |\n",
      "|    total_timesteps      | 1214464    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05796275 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.187     |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0325     |\n",
      "|    n_updates            | 5920       |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "Ep done - 40260.\n",
      "Ep done - 40270.\n",
      "Ep done - 40280.\n",
      "Ep done - 40290.\n",
      "Ep done - 40300.\n",
      "Ep done - 40310.\n",
      "Ep done - 40320.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 594         |\n",
      "|    time_elapsed         | 7119        |\n",
      "|    total_timesteps      | 1216512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042035002 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 5930        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 40330.\n",
      "Ep done - 40340.\n",
      "Ep done - 40350.\n",
      "Ep done - 40360.\n",
      "Ep done - 40370.\n",
      "Ep done - 40380.\n",
      "Ep done - 40390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 595         |\n",
      "|    time_elapsed         | 7128        |\n",
      "|    total_timesteps      | 1218560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075213075 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0644      |\n",
      "|    n_updates            | 5940        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 40400.\n",
      "Ep done - 40410.\n",
      "Ep done - 40420.\n",
      "Ep done - 40430.\n",
      "Ep done - 40440.\n",
      "Ep done - 12110.\n",
      "Ep done - 12120.\n",
      "Ep done - 12130.\n",
      "Ep done - 12140.\n",
      "Ep done - 12150.\n",
      "Ep done - 12160.\n",
      "Ep done - 12170.\n",
      "Ep done - 12180.\n",
      "Ep done - 12190.\n",
      "Ep done - 12200.\n",
      "Eval num_timesteps=1220000, episode_reward=0.61 +/- 0.76\n",
      "Episode length: 30.08 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1220000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041512527 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 5950        |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.61\n",
      "SELFPLAY: new best model, bumping up generation to 111\n",
      "Ep done - 40450.\n",
      "Ep done - 40460.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 596      |\n",
      "|    time_elapsed    | 7146     |\n",
      "|    total_timesteps | 1220608  |\n",
      "---------------------------------\n",
      "Ep done - 40470.\n",
      "Ep done - 40480.\n",
      "Ep done - 40490.\n",
      "Ep done - 40500.\n",
      "Ep done - 40510.\n",
      "Ep done - 40520.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 597         |\n",
      "|    time_elapsed         | 7156        |\n",
      "|    total_timesteps      | 1222656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038927305 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.17       |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 5960        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Ep done - 40530.\n",
      "Ep done - 40540.\n",
      "Ep done - 40550.\n",
      "Ep done - 40560.\n",
      "Ep done - 40570.\n",
      "Ep done - 40580.\n",
      "Ep done - 40590.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 598         |\n",
      "|    time_elapsed         | 7165        |\n",
      "|    total_timesteps      | 1224704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037362866 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 5970        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "Ep done - 40600.\n",
      "Ep done - 40610.\n",
      "Ep done - 40620.\n",
      "Ep done - 40630.\n",
      "Ep done - 40640.\n",
      "Ep done - 40650.\n",
      "Ep done - 40660.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 599        |\n",
      "|    time_elapsed         | 7174       |\n",
      "|    total_timesteps      | 1226752    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04768973 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.168     |\n",
      "|    explained_variance   | 0.386      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0339     |\n",
      "|    n_updates            | 5980       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Ep done - 40670.\n",
      "Ep done - 40680.\n",
      "Ep done - 40690.\n",
      "Ep done - 40700.\n",
      "Ep done - 40710.\n",
      "Ep done - 40720.\n",
      "Ep done - 40730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 600         |\n",
      "|    time_elapsed         | 7184        |\n",
      "|    total_timesteps      | 1228800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038282152 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.165      |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00663     |\n",
      "|    n_updates            | 5990        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "Ep done - 40740.\n",
      "Ep done - 40750.\n",
      "Ep done - 40760.\n",
      "Ep done - 40770.\n",
      "Ep done - 12210.\n",
      "Ep done - 12220.\n",
      "Ep done - 12230.\n",
      "Ep done - 12240.\n",
      "Ep done - 12250.\n",
      "Ep done - 12260.\n",
      "Ep done - 12270.\n",
      "Ep done - 12280.\n",
      "Ep done - 12290.\n",
      "Ep done - 12300.\n",
      "Eval num_timesteps=1230000, episode_reward=0.76 +/- 0.65\n",
      "Episode length: 30.18 +/- 0.83\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1230000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03869886 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.162     |\n",
      "|    explained_variance   | 0.373      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0106     |\n",
      "|    n_updates            | 6000       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    value_loss           | 0.109      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 112\n",
      "Ep done - 40780.\n",
      "Ep done - 40790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 601      |\n",
      "|    time_elapsed    | 7202     |\n",
      "|    total_timesteps | 1230848  |\n",
      "---------------------------------\n",
      "Ep done - 40800.\n",
      "Ep done - 40810.\n",
      "Ep done - 40820.\n",
      "Ep done - 40830.\n",
      "Ep done - 40840.\n",
      "Ep done - 40850.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 40860.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 602         |\n",
      "|    time_elapsed         | 7212        |\n",
      "|    total_timesteps      | 1232896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041216135 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 6010        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Ep done - 40870.\n",
      "Ep done - 40880.\n",
      "Ep done - 40890.\n",
      "Ep done - 40900.\n",
      "Ep done - 40910.\n",
      "Ep done - 40920.\n",
      "Ep done - 40930.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 603        |\n",
      "|    time_elapsed         | 7221       |\n",
      "|    total_timesteps      | 1234944    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03418038 |\n",
      "|    clip_fraction        | 0.0946     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.149     |\n",
      "|    explained_variance   | 0.331      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00591    |\n",
      "|    n_updates            | 6020       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "Ep done - 40940.\n",
      "Ep done - 40950.\n",
      "Ep done - 40960.\n",
      "Ep done - 40970.\n",
      "Ep done - 40980.\n",
      "Ep done - 40990.\n",
      "Ep done - 41000.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 604        |\n",
      "|    time_elapsed         | 7230       |\n",
      "|    total_timesteps      | 1236992    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04248387 |\n",
      "|    clip_fraction        | 0.0978     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.149     |\n",
      "|    explained_variance   | 0.466      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 6030       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    value_loss           | 0.0947     |\n",
      "----------------------------------------\n",
      "Ep done - 41010.\n",
      "Ep done - 41020.\n",
      "Ep done - 41030.\n",
      "Ep done - 41040.\n",
      "Ep done - 41050.\n",
      "Ep done - 41060.\n",
      "Ep done - 41070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 605         |\n",
      "|    time_elapsed         | 7240        |\n",
      "|    total_timesteps      | 1239040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050052077 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00289     |\n",
      "|    n_updates            | 6040        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Ep done - 41080.\n",
      "Ep done - 41090.\n",
      "Ep done - 41100.\n",
      "Ep done - 12310.\n",
      "Ep done - 12320.\n",
      "Ep done - 12330.\n",
      "Ep done - 12340.\n",
      "Ep done - 12350.\n",
      "Ep done - 12360.\n",
      "Ep done - 12370.\n",
      "Ep done - 12380.\n",
      "Ep done - 12390.\n",
      "Ep done - 12400.\n",
      "Eval num_timesteps=1240000, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 30.15 +/- 0.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.79        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049197793 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.157      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 6050        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.0886      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.79\n",
      "SELFPLAY: new best model, bumping up generation to 113\n",
      "Ep done - 41110.\n",
      "Ep done - 41120.\n",
      "Ep done - 41130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 606      |\n",
      "|    time_elapsed    | 7258     |\n",
      "|    total_timesteps | 1241088  |\n",
      "---------------------------------\n",
      "Ep done - 41140.\n",
      "Ep done - 41150.\n",
      "Ep done - 41160.\n",
      "Ep done - 41170.\n",
      "Ep done - 41180.\n",
      "Ep done - 41190.\n",
      "Ep done - 41200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 607         |\n",
      "|    time_elapsed         | 7268        |\n",
      "|    total_timesteps      | 1243136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056756575 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.177      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.016       |\n",
      "|    n_updates            | 6060        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 41210.\n",
      "Ep done - 41220.\n",
      "Ep done - 41230.\n",
      "Ep done - 41240.\n",
      "Ep done - 41250.\n",
      "Ep done - 41260.\n",
      "Ep done - 41270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 608         |\n",
      "|    time_elapsed         | 7277        |\n",
      "|    total_timesteps      | 1245184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047603093 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 6070        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 41280.\n",
      "Ep done - 41290.\n",
      "Ep done - 41300.\n",
      "Ep done - 41310.\n",
      "Ep done - 41320.\n",
      "Ep done - 41330.\n",
      "Ep done - 41340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 609         |\n",
      "|    time_elapsed         | 7286        |\n",
      "|    total_timesteps      | 1247232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041817576 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0125      |\n",
      "|    n_updates            | 6080        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 41350.\n",
      "Ep done - 41360.\n",
      "Ep done - 41370.\n",
      "Ep done - 41380.\n",
      "Ep done - 41390.\n",
      "Ep done - 41400.\n",
      "Ep done - 41410.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 610         |\n",
      "|    time_elapsed         | 7296        |\n",
      "|    total_timesteps      | 1249280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040091306 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.154      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0225      |\n",
      "|    n_updates            | 6090        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Ep done - 41420.\n",
      "Ep done - 41430.\n",
      "Ep done - 12410.\n",
      "Ep done - 12420.\n",
      "Ep done - 12430.\n",
      "Ep done - 12440.\n",
      "Ep done - 12450.\n",
      "Ep done - 12460.\n",
      "Ep done - 12470.\n",
      "Ep done - 12480.\n",
      "Ep done - 12490.\n",
      "Ep done - 12500.\n",
      "Eval num_timesteps=1250000, episode_reward=0.77 +/- 0.63\n",
      "Episode length: 30.16 +/- 0.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1250000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05249697 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.169     |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0148     |\n",
      "|    n_updates            | 6100       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    value_loss           | 0.0869     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.77\n",
      "SELFPLAY: new best model, bumping up generation to 114\n",
      "Ep done - 41440.\n",
      "Ep done - 41450.\n",
      "Ep done - 41460.\n",
      "Ep done - 41470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 171      |\n",
      "|    iterations      | 611      |\n",
      "|    time_elapsed    | 7314     |\n",
      "|    total_timesteps | 1251328  |\n",
      "---------------------------------\n",
      "Ep done - 41480.\n",
      "Ep done - 41490.\n",
      "Ep done - 41500.\n",
      "Ep done - 41510.\n",
      "Ep done - 41520.\n",
      "Ep done - 41530.\n",
      "Ep done - 41540.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 612         |\n",
      "|    time_elapsed         | 7324        |\n",
      "|    total_timesteps      | 1253376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052148137 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.025       |\n",
      "|    n_updates            | 6110        |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Ep done - 41550.\n",
      "Ep done - 41560.\n",
      "Ep done - 41570.\n",
      "Ep done - 41580.\n",
      "Ep done - 41590.\n",
      "Ep done - 41600.\n",
      "Ep done - 41610.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 613         |\n",
      "|    time_elapsed         | 7333        |\n",
      "|    total_timesteps      | 1255424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046819847 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 6120        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Ep done - 41620.\n",
      "Ep done - 41630.\n",
      "Ep done - 41640.\n",
      "Ep done - 41650.\n",
      "Ep done - 41660.\n",
      "Ep done - 41670.\n",
      "Ep done - 41680.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.55       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 614        |\n",
      "|    time_elapsed         | 7343       |\n",
      "|    total_timesteps      | 1257472    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06483002 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.201     |\n",
      "|    explained_variance   | 0.371      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0161     |\n",
      "|    n_updates            | 6130       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Ep done - 41690.\n",
      "Ep done - 41700.\n",
      "Ep done - 41710.\n",
      "Ep done - 41720.\n",
      "Ep done - 41730.\n",
      "Ep done - 41740.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.67       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 615        |\n",
      "|    time_elapsed         | 7352       |\n",
      "|    total_timesteps      | 1259520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05261697 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.201     |\n",
      "|    explained_variance   | 0.224      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0522     |\n",
      "|    n_updates            | 6140       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n",
      "Ep done - 41750.\n",
      "Ep done - 41760.\n",
      "Ep done - 12510.\n",
      "Ep done - 12520.\n",
      "Ep done - 12530.\n",
      "Ep done - 12540.\n",
      "Ep done - 12550.\n",
      "Ep done - 12560.\n",
      "Ep done - 12570.\n",
      "Ep done - 12580.\n",
      "Ep done - 12590.\n",
      "Ep done - 12600.\n",
      "Eval num_timesteps=1260000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.15 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049838714 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0194      |\n",
      "|    n_updates            | 6150        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.0931      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.66\n",
      "SELFPLAY: new best model, bumping up generation to 115\n",
      "Ep done - 41770.\n",
      "Ep done - 41780.\n",
      "Ep done - 41790.\n",
      "Ep done - 41800.\n",
      "Ep done - 41810.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 171      |\n",
      "|    iterations      | 616      |\n",
      "|    time_elapsed    | 7371     |\n",
      "|    total_timesteps | 1261568  |\n",
      "---------------------------------\n",
      "Ep done - 41820.\n",
      "Ep done - 41830.\n",
      "Ep done - 41840.\n",
      "Ep done - 41850.\n",
      "Ep done - 41860.\n",
      "Ep done - 41870.\n",
      "Ep done - 41880.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 617        |\n",
      "|    time_elapsed         | 7380       |\n",
      "|    total_timesteps      | 1263616    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04153005 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.225     |\n",
      "|    explained_variance   | 0.233      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0283     |\n",
      "|    n_updates            | 6160       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "Ep done - 41890.\n",
      "Ep done - 41900.\n",
      "Ep done - 41910.\n",
      "Ep done - 41920.\n",
      "Ep done - 41930.\n",
      "Ep done - 41940.\n",
      "Ep done - 41950.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.63       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 618        |\n",
      "|    time_elapsed         | 7390       |\n",
      "|    total_timesteps      | 1265664    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04852904 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.211     |\n",
      "|    explained_variance   | 0.329      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0172     |\n",
      "|    n_updates            | 6170       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "eval_callback = SelfPlayCallback(    \n",
    "    env_eval,\n",
    "    best_model_save_path=LOGDIR,\n",
    "    log_path=LOGDIR,\n",
    "    eval_freq=EVAL_FREQ,\n",
    "    n_eval_episodes=EVAL_EPISODES,\n",
    "    deterministic=False \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6f4d6-e4af-40f6-8dc5-bb4a93a76e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f1fb305-2d50-439b-8ea9-721ed92a9809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b754f-325b-4cd6-889a-684b154e3b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2cc1ebe0-55da-483a-98b8-480abe6723cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_model_copy.predict(env.reset()[0], action_masks=env.action_masks())\n",
    "# model.predict(env.reset()[0], action_masks=env.action_masks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "af50891d-79a1-4474-96de-fe47e069f773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('board',\n",
       "              array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 1, 2, 0, 0, 0],\n",
       "                     [0, 0, 0, 2, 1, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0]])),\n",
       "             ('player', 1)])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48957583-55ae-4d37-a91a-4d3a0d09a152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076a2e9-f6ec-4ace-a5c7-6d6863935c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.unwrapped.get_obs()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "19b6d5cf-09f8-463b-a95d-3a9add321149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obs = spaces.flatten(env.unwrapped.observation_space, obs)\n",
    "new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8ed10-d6d0-4363-9516-89dda14fd7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f7da2c24-2262-4c6c-8464-ae1cb2390730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(22), None)\n",
      "(array(29), None)\n",
      "(array(41), None)\n",
      "(array(28), None)\n",
      "(array(5), None)\n",
      "(array(46), None)\n",
      "(array(24), None)\n",
      "(array(13), None)\n",
      "(array(40), None)\n",
      "(array(59), None)\n",
      "(array(13), None)\n",
      "(array(45), None)\n",
      "(array(18), None)\n",
      "(array(62), None)\n",
      "(array(51), None)\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(model.predict(new_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ede4294c-f6cf-4fff-b4ae-83f2a1c14ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 2, 2, 2, 1, 0, 1, 0, 2, 2, 2, 2, 2,\n",
       "       1, 2, 1, 0, 2, 2, 1, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 0, 0, 0, 1, 0,\n",
       "       2, 2, 1, 1, 0, 2, 1, 0, 2, 2, 2, 1, 2, 0, 0, 0, 2, 0, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc4e49-8b25-4a20-9d99-6e9b04d61b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0969e-6711-4e9a-9ceb-3e6ea3b9d304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5ec1f886-e37f-496d-b6e5-dd33ccd191fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval = OthelloEnv()\n",
    "env_eval = Monitor(env=env_eval)\n",
    "env_eval = FlattenObservation(env_eval)\n",
    "\n",
    "env_eval = DummyVecEnv(env_fns=[lambda: env_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2b4b5155-d19f-4bc2-91da-bbc09b488fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MaskablePPO.load('ppo_masked_selfplay/history_00000385.zip')\n",
    "model_random = MaskablePPO.load('ppo_masked_selfplay/history_00000170.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9b6d8668-3386-434d-97b8-a8ed3ba6ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval.envs[0].unwrapped.change_to_latest_agent(model_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d252cb9d-8f37-4209-9da1-80252d48d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 310.\n",
      "Ep done - 320.\n",
      "Ep done - 330.\n",
      "Ep done - 340.\n",
      "Ep done - 350.\n",
      "Ep done - 360.\n",
      "Ep done - 370.\n",
      "Ep done - 380.\n",
      "Ep done - 390.\n",
      "Ep done - 400.\n"
     ]
    }
   ],
   "source": [
    "episode_rewards, episode_lengths = evaluate_policy(\n",
    "                model1,\n",
    "                env_eval,\n",
    "                n_eval_episodes=100,                \n",
    "                deterministic=True,\n",
    "                return_episode_rewards=True,\n",
    "                warn=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3527ceef-0eb6-4e5c-bf97-1e83b354a502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed67b8-6289-4c32-970e-362e1f44165d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7ebfd-6326-45c0-be8e-0bc89976894e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
